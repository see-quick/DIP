[INFO] Scanning for projects...
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Build Order:
[INFO] 
[INFO] Strimzi - Apache Kafka on Kubernetes and OpenShift                 [pom]
[INFO] test                                                               [jar]
[INFO] crd-annotations                                                    [jar]
[INFO] crd-generator                                                      [jar]
[INFO] api                                                                [jar]
[INFO] mockkube                                                           [jar]
[INFO] config-model                                                       [jar]
[INFO] certificate-manager                                                [jar]
[INFO] operator-common                                                    [jar]
[INFO] systemtest                                                         [jar]
[INFO] 
[INFO] -------------------------< io.strimzi:strimzi >-------------------------
[INFO] Building Strimzi - Apache Kafka on Kubernetes and OpenShift 0.29.0-SNAPSHOT [1/10]
[INFO] --------------------------------[ pom ]---------------------------------
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ strimzi ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ strimzi ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ strimzi ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/target/jacoco.exec
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:report (report) @ strimzi ---
[INFO] Skipping JaCoCo execution due to missing execution data file.
[INFO] 
[INFO] >>> maven-source-plugin:3.0.1:jar (attach-sources) > generate-sources @ strimzi >>>
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ strimzi ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ strimzi ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ strimzi ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/target/jacoco.exec
[INFO] 
[INFO] <<< maven-source-plugin:3.0.1:jar (attach-sources) < generate-sources @ strimzi <<<
[INFO] 
[INFO] 
[INFO] --- maven-source-plugin:3.0.1:jar (attach-sources) @ strimzi ---
[INFO] 
[INFO] --- maven-javadoc-plugin:3.1.0:jar (attach-javadocs) @ strimzi ---
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:integration-test (default) @ strimzi ---
[INFO] No tests to run.
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:verify (default) @ strimzi ---
[INFO] 
[INFO] --- maven-dependency-plugin:3.1.1:analyze-only (analyze) @ strimzi ---
[INFO] Skipping pom project
[INFO] 
[INFO] --------------------------< io.strimzi:test >---------------------------
[INFO] Building test 0.29.0-SNAPSHOT                                     [2/10]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ test ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ test ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ test ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/test/target/jacoco.exec
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ test ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/cloud-user/strimzi-kafka-operator/test/src/main/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:compile (default-compile) @ test ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ test ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/cloud-user/strimzi-kafka-operator/test/src/test/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:testCompile (default-testCompile) @ test ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:3.0.0-M5:test (default-test) @ test ---
[INFO] No tests to run.
[INFO] 
[INFO] --- maven-jar-plugin:3.1.0:jar (default-jar) @ test ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:report (report) @ test ---
[INFO] Skipping JaCoCo execution due to missing execution data file.
[INFO] 
[INFO] >>> maven-source-plugin:3.0.1:jar (attach-sources) > generate-sources @ test >>>
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ test ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ test ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ test ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/test/target/jacoco.exec
[INFO] 
[INFO] <<< maven-source-plugin:3.0.1:jar (attach-sources) < generate-sources @ test <<<
[INFO] 
[INFO] 
[INFO] --- maven-source-plugin:3.0.1:jar (attach-sources) @ test ---
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/test/target/test-0.29.0-SNAPSHOT-sources.jar
[INFO] 
[INFO] --- maven-javadoc-plugin:3.1.0:jar (attach-javadocs) @ test ---
[INFO] 
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/EmbeddedZooKeeper.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/WaitException.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/executor/ExecResult.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/executor/Exec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/k8s/cluster/OpenShift.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/k8s/cluster/KubeCluster.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/k8s/cluster/Kubernetes.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/k8s/cluster/Minikube.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/k8s/cmdClient/Kubectl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/k8s/cmdClient/Oc.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/k8s/cmdClient/BaseCmdKubeClient.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/k8s/cmdClient/KubeCmdClient.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/k8s/exceptions/KubeClusterException.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/k8s/exceptions/NoClusterException.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/k8s/HelmClient.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/k8s/KubeClient.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/k8s/KubeClusterResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/annotations/IsolatedTest.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/annotations/ParallelSuite.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/annotations/ParallelTest.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/annotations/IsolatedSuite.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/interfaces/ExtensionContextParameterResolver.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/interfaces/TestSeparator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/TestUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/logs/CollectorElement.java...
Constructing Javadoc information...
Standard Doclet version 11.0.11
Building tree for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/annotations/IsolatedSuite.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/annotations/IsolatedTest.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/annotations/ParallelSuite.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/annotations/ParallelTest.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cluster/KubeCluster.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cluster/Kubernetes.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cluster/Minikube.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cluster/OpenShift.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cmdClient/BaseCmdKubeClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cmdClient/KubeCmdClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cmdClient/Kubectl.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cmdClient/Oc.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/exceptions/KubeClusterException.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/exceptions/KubeClusterException.AlreadyExists.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/exceptions/KubeClusterException.InvalidResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/exceptions/KubeClusterException.NotFound.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/exceptions/NoClusterException.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/executor/Exec.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/executor/ExecResult.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/interfaces/ExtensionContextParameterResolver.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/interfaces/TestSeparator.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/HelmClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/KubeClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/KubeClusterResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/logs/CollectorElement.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/EmbeddedZooKeeper.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/TestUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/WaitException.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/annotations/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/annotations/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/executor/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/executor/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/interfaces/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/interfaces/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cluster/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cluster/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cmdClient/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cmdClient/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/exceptions/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/exceptions/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/logs/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/logs/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/constant-values.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/serialized-form.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/class-use/EmbeddedZooKeeper.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/class-use/WaitException.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/executor/class-use/ExecResult.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/executor/class-use/Exec.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cluster/class-use/OpenShift.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cluster/class-use/KubeCluster.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cluster/class-use/Kubernetes.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cluster/class-use/Minikube.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cmdClient/class-use/Kubectl.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cmdClient/class-use/Oc.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cmdClient/class-use/BaseCmdKubeClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cmdClient/class-use/KubeCmdClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/exceptions/class-use/KubeClusterException.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/exceptions/class-use/KubeClusterException.InvalidResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/exceptions/class-use/KubeClusterException.AlreadyExists.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/exceptions/class-use/KubeClusterException.NotFound.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/exceptions/class-use/NoClusterException.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/class-use/HelmClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/class-use/KubeClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/class-use/KubeClusterResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/annotations/class-use/IsolatedTest.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/annotations/class-use/ParallelSuite.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/annotations/class-use/ParallelTest.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/annotations/class-use/IsolatedSuite.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/interfaces/class-use/ExtensionContextParameterResolver.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/interfaces/class-use/TestSeparator.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/class-use/TestUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/logs/class-use/CollectorElement.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/annotations/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/executor/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/interfaces/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cluster/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cmdClient/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/exceptions/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/logs/package-use.html...
Building index for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/overview-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/index-all.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/allclasses-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/allpackages-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/deprecated-list.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/index.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/overview-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/help-doc.html...
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/test/target/test-0.29.0-SNAPSHOT-javadoc.jar
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:integration-test (default) @ test ---
[INFO] No tests to run.
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:verify (default) @ test ---
[INFO] 
[INFO] --- maven-dependency-plugin:3.1.1:analyze-only (analyze) @ test ---
[INFO] No dependency problems found
[INFO] 
[INFO] ---------------------< io.strimzi:crd-annotations >---------------------
[INFO] Building crd-annotations 0.29.0-SNAPSHOT                          [3/10]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ crd-annotations ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ crd-annotations ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ crd-annotations ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/crd-annotations/target/jacoco.exec
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ crd-annotations ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/cloud-user/strimzi-kafka-operator/crd-annotations/src/main/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:compile (default-compile) @ crd-annotations ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ crd-annotations ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/cloud-user/strimzi-kafka-operator/crd-annotations/src/test/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:testCompile (default-testCompile) @ crd-annotations ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:3.0.0-M5:test (default-test) @ crd-annotations ---
[INFO] 
[INFO] -------------------------------------------------------
[INFO]  T E S T S
[INFO] -------------------------------------------------------
[INFO] 
[INFO] Results:
[INFO] 
[INFO] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[INFO] 
[INFO] 
[INFO] --- maven-jar-plugin:3.1.0:jar (default-jar) @ crd-annotations ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:report (report) @ crd-annotations ---
[INFO] Skipping JaCoCo execution due to missing execution data file.
[INFO] 
[INFO] >>> maven-source-plugin:3.0.1:jar (attach-sources) > generate-sources @ crd-annotations >>>
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ crd-annotations ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ crd-annotations ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ crd-annotations ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/crd-annotations/target/jacoco.exec
[INFO] 
[INFO] <<< maven-source-plugin:3.0.1:jar (attach-sources) < generate-sources @ crd-annotations <<<
[INFO] 
[INFO] 
[INFO] --- maven-source-plugin:3.0.1:jar (attach-sources) @ crd-annotations ---
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/crd-annotations-0.29.0-SNAPSHOT-sources.jar
[INFO] 
[INFO] --- maven-javadoc-plugin:3.1.0:jar (attach-javadocs) @ crd-annotations ---
[INFO] 
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-annotations/src/main/java/io/strimzi/api/annotations/DeprecatedProperty.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-annotations/src/main/java/io/strimzi/api/annotations/ApiVersion.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-annotations/src/main/java/io/strimzi/api/annotations/DeprecatedType.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-annotations/src/main/java/io/strimzi/api/annotations/KubeVersion.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-annotations/src/main/java/io/strimzi/api/annotations/VersionRange.java...
Constructing Javadoc information...
Standard Doclet version 11.0.11
Building tree for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/io/strimzi/api/annotations/ApiVersion.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/io/strimzi/api/annotations/DeprecatedProperty.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/io/strimzi/api/annotations/DeprecatedType.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/io/strimzi/api/annotations/KubeVersion.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/io/strimzi/api/annotations/VersionRange.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/io/strimzi/api/annotations/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/io/strimzi/api/annotations/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/constant-values.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/io/strimzi/api/annotations/class-use/DeprecatedProperty.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/io/strimzi/api/annotations/class-use/ApiVersion.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/io/strimzi/api/annotations/class-use/DeprecatedType.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/io/strimzi/api/annotations/class-use/KubeVersion.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/io/strimzi/api/annotations/class-use/VersionRange.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/io/strimzi/api/annotations/package-use.html...
Building index for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/overview-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/index-all.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/allclasses-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/allpackages-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/deprecated-list.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/index.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/help-doc.html...
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/crd-annotations-0.29.0-SNAPSHOT-javadoc.jar
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:integration-test (default) @ crd-annotations ---
[WARNING] useSystemClassLoader setting has no effect when not forking
[WARNING] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:verify (default) @ crd-annotations ---
[INFO] 
[INFO] --- maven-dependency-plugin:3.1.1:analyze-only (analyze) @ crd-annotations ---
[INFO] No dependency problems found
[INFO] 
[INFO] ----------------------< io.strimzi:crd-generator >----------------------
[INFO] Building crd-generator 0.29.0-SNAPSHOT                            [4/10]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ crd-generator ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ crd-generator ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ crd-generator ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/crd-generator/target/jacoco.exec
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ crd-generator ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:compile (default-compile) @ crd-generator ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ crd-generator ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 7 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:testCompile (default-testCompile) @ crd-generator ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:3.0.0-M5:test (default-test) @ crd-generator ---
[INFO] 
[INFO] -------------------------------------------------------
[INFO]  T E S T S
[INFO] -------------------------------------------------------
[INFO] 
[INFO] Results:
[INFO] 
[INFO] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[INFO] 
[INFO] 
[INFO] --- maven-jar-plugin:3.1.0:jar (default-jar) @ crd-generator ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:report (report) @ crd-generator ---
[INFO] Skipping JaCoCo execution due to missing execution data file.
[INFO] 
[INFO] >>> maven-source-plugin:3.0.1:jar (attach-sources) > generate-sources @ crd-generator >>>
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ crd-generator ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ crd-generator ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ crd-generator ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/crd-generator/target/jacoco.exec
[INFO] 
[INFO] <<< maven-source-plugin:3.0.1:jar (attach-sources) < generate-sources @ crd-generator <<<
[INFO] 
[INFO] 
[INFO] --- maven-source-plugin:3.0.1:jar (attach-sources) @ crd-generator ---
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/crd-generator/target/crd-generator-0.29.0-SNAPSHOT-sources.jar
[INFO] 
[INFO] --- maven-javadoc-plugin:3.1.0:jar (attach-javadocs) @ crd-generator ---
[INFO] 
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/KubeLinker.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/Linker.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/OpenShiftOriginLinker.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/PropertyType.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/Schema.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/annotations/Alternation.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/annotations/DescriptionFile.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/annotations/Example.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/annotations/KubeLink.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/annotations/OneOf.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/annotations/Type.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/annotations/Alternative.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/annotations/Description.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/annotations/Maximum.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/annotations/Minimum.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/annotations/MinimumItems.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/annotations/PresentInVersions.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/annotations/Crd.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/annotations/Pattern.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/Property.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/CrdGenerator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/DocGenerator.java...
Constructing Javadoc information...
Standard Doclet version 11.0.11
Building tree for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Alternation.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Alternative.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Crd.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Crd.Spec.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Crd.Spec.AdditionalPrinterColumn.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Crd.Spec.Names.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Crd.Spec.Subresources.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Crd.Spec.Subresources.Scale.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Crd.Spec.Subresources.Status.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Crd.Spec.Version.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Description.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Description.List.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/DescriptionFile.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Example.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/KubeLink.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Maximum.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Maximum.List.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Minimum.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Minimum.List.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/MinimumItems.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/MinimumItems.List.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/OneOf.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/OneOf.Alternative.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/OneOf.Alternative.Property.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Pattern.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Pattern.List.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/PresentInVersions.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Type.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/CrdGenerator.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/CrdGenerator.ConversionStrategy.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/CrdGenerator.DefaultReporter.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/CrdGenerator.NoneConversionStrategy.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/CrdGenerator.Reporter.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/CrdGenerator.WebhookConversionStrategy.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/DocGenerator.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/KubeLinker.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/Linker.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/OpenShiftOriginLinker.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/PropertyType.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/constant-values.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/class-use/KubeLinker.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/class-use/Linker.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/class-use/OpenShiftOriginLinker.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/class-use/PropertyType.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Alternation.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/DescriptionFile.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Example.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/KubeLink.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/OneOf.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/OneOf.Alternative.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/OneOf.Alternative.Property.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Type.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Alternative.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Description.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Description.List.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Maximum.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Maximum.List.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Minimum.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Minimum.List.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/MinimumItems.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/MinimumItems.List.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/PresentInVersions.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Crd.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Crd.Spec.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Crd.Spec.AdditionalPrinterColumn.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Crd.Spec.Subresources.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Crd.Spec.Subresources.Scale.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Crd.Spec.Subresources.Status.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Crd.Spec.Version.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Crd.Spec.Names.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Pattern.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Pattern.List.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/class-use/CrdGenerator.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/class-use/CrdGenerator.WebhookConversionStrategy.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/class-use/CrdGenerator.NoneConversionStrategy.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/class-use/CrdGenerator.ConversionStrategy.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/class-use/CrdGenerator.DefaultReporter.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/class-use/CrdGenerator.Reporter.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/class-use/DocGenerator.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/package-use.html...
Building index for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/overview-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/index-all.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/allclasses-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/allpackages-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/deprecated-list.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/index.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/overview-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/help-doc.html...
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/crd-generator/target/crd-generator-0.29.0-SNAPSHOT-javadoc.jar
[INFO] 
[INFO] --- maven-shade-plugin:3.1.0:shade (default) @ crd-generator ---
[INFO] Including io.strimzi:crd-annotations:jar:0.29.0-SNAPSHOT in the shaded jar.
[INFO] Including com.fasterxml.jackson.core:jackson-core:jar:2.12.6 in the shaded jar.
[INFO] Including com.fasterxml.jackson.core:jackson-databind:jar:2.12.6 in the shaded jar.
[INFO] Including com.fasterxml.jackson.dataformat:jackson-dataformat-yaml:jar:2.12.6 in the shaded jar.
[INFO] Including org.yaml:snakeyaml:jar:1.27 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-client:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-rbac:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-admissionregistration:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-apps:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-autoscaling:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-apiextensions:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-batch:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-certificates:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-coordination:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-discovery:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-events:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-extensions:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-flowcontrol:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-networking:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-metrics:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-policy:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-scheduling:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-storageclass:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-node:jar:5.12.0 in the shaded jar.
[INFO] Including com.squareup.okhttp3:okhttp:jar:3.12.12 in the shaded jar.
[INFO] Including com.squareup.okio:okio:jar:1.15.0 in the shaded jar.
[INFO] Including com.squareup.okhttp3:logging-interceptor:jar:3.12.12 in the shaded jar.
[INFO] Including org.slf4j:slf4j-api:jar:1.7.36 in the shaded jar.
[INFO] Including com.fasterxml.jackson.datatype:jackson-datatype-jsr310:jar:2.13.1 in the shaded jar.
[INFO] Including io.fabric8:zjsonpatch:jar:0.3.0 in the shaded jar.
[INFO] Including com.github.mifmif:generex:jar:1.0.2 in the shaded jar.
[INFO] Including dk.brics.automaton:automaton:jar:1.11-8 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-core:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-common:jar:5.12.0 in the shaded jar.
[INFO] Including com.fasterxml.jackson.core:jackson-annotations:jar:2.12.6 in the shaded jar.
[WARNING] Discovered module-info.class. Shading will break its strong encapsulation.
[WARNING] Discovered module-info.class. Shading will break its strong encapsulation.
[WARNING] Discovered module-info.class. Shading will break its strong encapsulation.
[WARNING] Discovered module-info.class. Shading will break its strong encapsulation.
[WARNING] kubernetes-model-coordination-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 18 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseListFluentImpl$ItemsNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseFluentImpl$SpecNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseSpec
[WARNING]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseFluent$SpecNested
[WARNING]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseListFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseSpecBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseSpecFluent
[WARNING]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseFluentImpl$MetadataNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseFluent
[WARNING]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseListFluent
[WARNING]   - 8 more...
[WARNING] logging-interceptor-3.12.12.jar, crd-generator-0.29.0-SNAPSHOT.jar define 8 overlapping classes: 
[WARNING]   - okhttp3.logging.HttpLoggingInterceptor$Logger$1
[WARNING]   - okhttp3.logging.LoggingEventListener$Factory
[WARNING]   - okhttp3.logging.HttpLoggingInterceptor$Level
[WARNING]   - okhttp3.logging.HttpLoggingInterceptor
[WARNING]   - okhttp3.logging.package-info
[WARNING]   - okhttp3.logging.LoggingEventListener
[WARNING]   - okhttp3.logging.LoggingEventListener$1
[WARNING]   - okhttp3.logging.HttpLoggingInterceptor$Logger
[WARNING] kubernetes-client-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 536 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.client.internal.CertUtils
[WARNING]   - io.fabric8.kubernetes.client.CustomResource
[WARNING]   - io.fabric8.kubernetes.client.osgi.ManagedKubernetesClient
[WARNING]   - io.fabric8.kubernetes.client.V1beta1ApiextensionAPIGroupDSL
[WARNING]   - io.fabric8.kubernetes.client.internal.PatchUtils$SingletonHolder
[WARNING]   - io.fabric8.kubernetes.client.VersionInfo$1
[WARNING]   - io.fabric8.kubernetes.client.utils.ReplaceValueStream
[WARNING]   - io.fabric8.kubernetes.client.dsl.CreateFromServerGettable
[WARNING]   - io.fabric8.kubernetes.client.dsl.ApiextensionsAPIGroupDSL
[WARNING]   - io.fabric8.kubernetes.client.dsl.Containerable
[WARNING]   - 526 more...
[WARNING] kubernetes-model-events-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 44 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventFluent$MetadataNested
[WARNING]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventFluent$SeriesNested
[WARNING]   - io.fabric8.kubernetes.api.model.events.v1.EventList
[WARNING]   - io.fabric8.kubernetes.api.model.events.v1.EventListBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventFluent$RegardingNested
[WARNING]   - io.fabric8.kubernetes.api.model.events.v1.EventSeriesFluent
[WARNING]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventFluent
[WARNING]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventListFluent$ItemsNested
[WARNING]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventListFluentImpl$ItemsNestedImpl
[WARNING]   - 34 more...
[WARNING] crd-generator-0.29.0-SNAPSHOT.jar, automaton-1.11-8.jar define 25 overlapping classes: 
[WARNING]   - dk.brics.automaton.AutomatonMatcher
[WARNING]   - dk.brics.automaton.ShuffleOperations$ShuffleConfiguration
[WARNING]   - dk.brics.automaton.RegExp$Kind
[WARNING]   - dk.brics.automaton.RunAutomaton
[WARNING]   - dk.brics.automaton.Automaton
[WARNING]   - dk.brics.automaton.RegExp
[WARNING]   - dk.brics.automaton.AutomatonProvider
[WARNING]   - dk.brics.automaton.RegExp$1
[WARNING]   - dk.brics.automaton.MinimizationOperations$StateListNode
[WARNING]   - dk.brics.automaton.State
[WARNING]   - 15 more...
[WARNING] kubernetes-model-metrics-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 30 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.PodMetricsFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.NodeMetricsListFluentImpl$ItemsNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.ContainerMetricsFluent
[WARNING]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.NodeMetrics
[WARNING]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.NodeMetricsListFluent
[WARNING]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.NodeMetricsListFluent$ItemsNested
[WARNING]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.NodeMetricsFluent$MetadataNested
[WARNING]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.PodMetricsFluentImpl$ContainersNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.PodMetricsListFluentImpl$ItemsNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.PodMetricsListBuilder
[WARNING]   - 20 more...
[WARNING] kubernetes-model-networking-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 234 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.networking.v1beta1.IngressClassListFluent$ItemsNested
[WARNING]   - io.fabric8.kubernetes.api.model.networking.v1beta1.IngressClassListFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.networking.v1.IngressServiceBackend
[WARNING]   - io.fabric8.kubernetes.api.model.networking.v1.NetworkPolicyPort
[WARNING]   - io.fabric8.kubernetes.api.model.networking.v1beta1.IngressFluentImpl$StatusNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.networking.v1beta1.IngressClassSpec
[WARNING]   - io.fabric8.kubernetes.api.model.networking.v1.IngressStatus
[WARNING]   - io.fabric8.kubernetes.api.model.networking.v1.IngressClassFluent$SpecNested
[WARNING]   - io.fabric8.kubernetes.api.model.networking.v1.NetworkPolicyBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.networking.v1.IngressRuleFluentImpl$HttpNestedImpl
[WARNING]   - 224 more...
[WARNING] kubernetes-model-apiextensions-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 350 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.CustomResourceDefinitionVersionBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.JSONSchemaPropsOrBoolBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.CustomResourceDefinitionSpecFluent$ValidationNested
[WARNING]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.CustomResourceDefinitionVersionFluentImpl$SchemaNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.JSONSchemaPropsOrStringArraySerDe$Deserializer$1
[WARNING]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.CustomResourceValidationFluentImpl$OpenAPIV3SchemaNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.JSONSchemaPropsFluentImpl$NotNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.WebhookClientConfigFluentImpl$ServiceNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.JSONSchemaPropsOrArrayFluent$SchemaNested
[WARNING]   - io.fabric8.kubernetes.api.model.apiextensions.v1.JSONSchemaPropsOrBoolSerDe
[WARNING]   - 340 more...
[WARNING] generex-1.0.2.jar, crd-generator-0.29.0-SNAPSHOT.jar define 7 overlapping classes: 
[WARNING]   - com.mifmif.common.regex.GenerexIterator
[WARNING]   - com.mifmif.common.regex.Generex
[WARNING]   - com.mifmif.common.regex.GenerexIterator$Step
[WARNING]   - com.mifmif.common.regex.Node
[WARNING]   - com.mifmif.common.regex.Main
[WARNING]   - com.mifmif.common.regex.util.Iterable
[WARNING]   - com.mifmif.common.regex.util.Iterator
[WARNING] kubernetes-model-autoscaling-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 350 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta1.MetricSpecFluentImpl$ObjectNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta2.HorizontalPodAutoscalerSpec
[WARNING]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta1.CrossVersionObjectReferenceBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta1.HorizontalPodAutoscalerStatusFluent
[WARNING]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta2.HorizontalPodAutoscalerFluent$SpecNested
[WARNING]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta2.ContainerResourceMetricStatusFluent
[WARNING]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta1.HorizontalPodAutoscalerStatus
[WARNING]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta1.MetricStatusFluent$ObjectNested
[WARNING]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta2.HorizontalPodAutoscalerSpecFluent$ScaleTargetRefNested
[WARNING]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta1.HorizontalPodAutoscalerFluent$SpecNested
[WARNING]   - 340 more...
[WARNING] crd-generator-0.29.0-SNAPSHOT.jar, zjsonpatch-0.3.0.jar define 24 overlapping classes: 
[WARNING]   - io.fabric8.zjsonpatch.internal.collections4.sequence.InsertCommand
[WARNING]   - io.fabric8.zjsonpatch.Operation
[WARNING]   - io.fabric8.zjsonpatch.internal.collections4.sequence.CommandVisitor
[WARNING]   - io.fabric8.zjsonpatch.internal.guava.Strings
[WARNING]   - io.fabric8.zjsonpatch.internal.collections4.sequence.EditCommand
[WARNING]   - io.fabric8.zjsonpatch.JsonDiff$EncodePathFunction
[WARNING]   - io.fabric8.zjsonpatch.internal.collections4.sequence.SequencesComparator
[WARNING]   - io.fabric8.zjsonpatch.Diff
[WARNING]   - io.fabric8.zjsonpatch.internal.collections4.ListUtils
[WARNING]   - io.fabric8.zjsonpatch.JsonPatch
[WARNING]   - 14 more...
[WARNING] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-admissionregistration-5.12.0.jar define 362 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.admissionregistration.v1beta1.MutatingWebhookFluent$ObjectSelectorNested
[WARNING]   - io.fabric8.kubernetes.api.model.authorization.v1.SubjectAccessReviewSpecFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.authorization.v1beta1.SubjectRulesReviewStatusBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.admissionregistration.v1.ValidatingWebhookConfigurationBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.authentication.TokenReviewFluentImpl$MetadataNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.authorization.v1beta1.SelfSubjectRulesReviewSpec
[WARNING]   - io.fabric8.kubernetes.api.model.admissionregistration.v1beta1.MutatingWebhookFluentImpl$NamespaceSelectorNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.admissionregistration.v1beta1.MutatingWebhookConfigurationFluentImpl$WebhooksNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.authorization.v1beta1.SelfSubjectAccessReviewFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.admissionregistration.v1.MutatingWebhookFluent$ClientConfigNested
[WARNING]   - 352 more...
[WARNING] kubernetes-model-rbac-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 80 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.rbac.SubjectBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.rbac.ClusterRoleBindingFluent
[WARNING]   - io.fabric8.kubernetes.api.model.rbac.RoleListFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.rbac.ClusterRoleBindingFluent$MetadataNested
[WARNING]   - io.fabric8.kubernetes.api.model.rbac.RoleBindingBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.rbac.ClusterRoleBindingBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.rbac.AggregationRuleFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.rbac.SubjectFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.rbac.ClusterRoleListFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.rbac.PolicyRuleFluent
[WARNING]   - 70 more...
[WARNING] kubernetes-model-certificates-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 60 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.certificates.v1.CertificateSigningRequestStatusFluent$ConditionsNested
[WARNING]   - io.fabric8.kubernetes.api.model.certificates.v1.CertificateSigningRequestFluent$SpecNested
[WARNING]   - io.fabric8.kubernetes.api.model.certificates.v1.CertificateSigningRequestFluentImpl$StatusNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.certificates.v1beta1.CertificateSigningRequestListFluent$ItemsNested
[WARNING]   - io.fabric8.kubernetes.api.model.certificates.v1beta1.CertificateSigningRequestStatusFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.certificates.v1beta1.CertificateSigningRequestFluent$MetadataNested
[WARNING]   - io.fabric8.kubernetes.api.model.certificates.v1beta1.CertificateSigningRequestConditionFluent
[WARNING]   - io.fabric8.kubernetes.api.model.certificates.v1.CertificateSigningRequestStatusBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.certificates.v1beta1.CertificateSigningRequestStatusFluent$ConditionsNested
[WARNING]   - io.fabric8.kubernetes.api.model.certificates.v1.CertificateSigningRequestFluentImpl
[WARNING]   - 50 more...
[WARNING] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-policy-5.12.0.jar define 162 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.policy.v1.PodDisruptionBudgetList
[WARNING]   - io.fabric8.kubernetes.api.model.policy.v1beta1.HostPortRangeBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.policy.v1beta1.EvictionFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.policy.v1beta1.PodSecurityPolicyFluent
[WARNING]   - io.fabric8.kubernetes.api.model.policy.v1beta1.PodSecurityPolicySpecFluentImpl$AllowedCSIDriversNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.policy.v1beta1.PodSecurityPolicyListFluentImpl$ItemsNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.policy.v1beta1.AllowedFlexVolumeBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.policy.v1beta1.IDRangeFluent
[WARNING]   - io.fabric8.kubernetes.api.model.policy.v1beta1.SELinuxStrategyOptionsFluent
[WARNING]   - io.fabric8.kubernetes.api.model.policy.v1beta1.PodSecurityPolicySpecFluentImpl$FsGroupNestedImpl
[WARNING]   - 152 more...
[WARNING] jackson-datatype-jsr310-2.13.1.jar, crd-generator-0.29.0-SNAPSHOT.jar define 59 overlapping classes: 
[WARNING]   - com.fasterxml.jackson.datatype.jsr310.deser.LocalDateDeserializer
[WARNING]   - com.fasterxml.jackson.datatype.jsr310.deser.key.Jsr310KeyDeserializer
[WARNING]   - com.fasterxml.jackson.datatype.jsr310.PackageVersion
[WARNING]   - com.fasterxml.jackson.datatype.jsr310.deser.YearDeserializer
[WARNING]   - com.fasterxml.jackson.datatype.jsr310.ser.key.Jsr310NullKeySerializer
[WARNING]   - com.fasterxml.jackson.datatype.jsr310.deser.key.LocalDateTimeKeyDeserializer
[WARNING]   - com.fasterxml.jackson.datatype.jsr310.util.DurationUnitConverter
[WARNING]   - com.fasterxml.jackson.datatype.jsr310.ser.InstantSerializerBase
[WARNING]   - com.fasterxml.jackson.datatype.jsr310.ser.LocalDateTimeSerializer
[WARNING]   - com.fasterxml.jackson.datatype.jsr310.ser.OffsetDateTimeSerializer
[WARNING]   - 49 more...
[WARNING] kubernetes-model-flowcontrol-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 132 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.PriorityLevelConfigurationFluentImpl$SpecNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.FlowSchemaConditionFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.FlowDistinguisherMethodBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.PriorityLevelConfigurationReferenceBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.QueuingConfiguration
[WARNING]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.QueuingConfigurationFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.PriorityLevelConfiguration
[WARNING]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.PriorityLevelConfigurationReference
[WARNING]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.PolicyRulesWithSubjects
[WARNING]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.PriorityLevelConfigurationListFluent$ItemsNested
[WARNING]   - 122 more...
[WARNING] crd-generator-0.29.0-SNAPSHOT.jar, crd-annotations-0.29.0-SNAPSHOT.jar define 8 overlapping classes: 
[WARNING]   - io.strimzi.api.annotations.VersionRange
[WARNING]   - io.strimzi.api.annotations.ApiVersion
[WARNING]   - io.strimzi.api.annotations.ApiVersion$Stability
[WARNING]   - io.strimzi.api.annotations.ApiVersion$1
[WARNING]   - io.strimzi.api.annotations.DeprecatedType
[WARNING]   - io.strimzi.api.annotations.DeprecatedProperty
[WARNING]   - io.strimzi.api.annotations.VersionRange$VersionParser
[WARNING]   - io.strimzi.api.annotations.KubeVersion
[WARNING] jackson-core-2.12.6.jar, crd-generator-0.29.0-SNAPSHOT.jar define 124 overlapping classes: 
[WARNING]   - com.fasterxml.jackson.core.JsonGenerator$Feature
[WARNING]   - com.fasterxml.jackson.core.json.JsonReadFeature
[WARNING]   - com.fasterxml.jackson.core.util.ThreadLocalBufferManager$ThreadLocalBufferManagerHolder
[WARNING]   - com.fasterxml.jackson.core.util.Separators
[WARNING]   - com.fasterxml.jackson.core.io.SegmentedStringWriter
[WARNING]   - com.fasterxml.jackson.core.TreeNode
[WARNING]   - com.fasterxml.jackson.core.sym.Name
[WARNING]   - com.fasterxml.jackson.core.util.RequestPayload
[WARNING]   - com.fasterxml.jackson.core.util.JsonGeneratorDelegate
[WARNING]   - com.fasterxml.jackson.core.async.NonBlockingInputFeeder
[WARNING]   - 114 more...
[WARNING] crd-generator-0.29.0-SNAPSHOT.jar, okio-1.15.0.jar define 44 overlapping classes: 
[WARNING]   - okio.ByteString
[WARNING]   - okio.Source
[WARNING]   - okio.ForwardingSink
[WARNING]   - okio.BufferedSource
[WARNING]   - okio.Util
[WARNING]   - okio.AsyncTimeout$1
[WARNING]   - okio.HashingSource
[WARNING]   - okio.GzipSink
[WARNING]   - okio.Okio$1
[WARNING]   - okio.Pipe$PipeSink
[WARNING]   - 34 more...
[WARNING] crd-generator-0.29.0-SNAPSHOT.jar, jackson-databind-2.12.6.jar define 700 overlapping classes: 
[WARNING]   - com.fasterxml.jackson.databind.introspect.AnnotationCollector$NoAnnotations
[WARNING]   - com.fasterxml.jackson.databind.jsontype.BasicPolymorphicTypeValidator$Builder
[WARNING]   - com.fasterxml.jackson.databind.BeanDescription
[WARNING]   - com.fasterxml.jackson.databind.deser.impl.BeanAsArrayBuilderDeserializer
[WARNING]   - com.fasterxml.jackson.databind.introspect.AnnotatedMethodMap
[WARNING]   - com.fasterxml.jackson.databind.SerializerProvider
[WARNING]   - com.fasterxml.jackson.databind.introspect.AnnotationCollector$OneAnnotation
[WARNING]   - com.fasterxml.jackson.databind.ser.std.StaticListSerializerBase
[WARNING]   - com.fasterxml.jackson.databind.ser.std.NumberSerializers$ShortSerializer
[WARNING]   - com.fasterxml.jackson.databind.ser.BeanSerializerFactory
[WARNING]   - 690 more...
[WARNING] kubernetes-model-discovery-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 88 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.EndpointSliceFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.EndpointFluent
[WARNING]   - io.fabric8.kubernetes.api.model.discovery.v1.EndpointSliceBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.EndpointPort
[WARNING]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.ForZoneBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.discovery.v1.EndpointFluent$TargetRefNested
[WARNING]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.EndpointFluentImpl$ConditionsNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.discovery.v1.EndpointSliceListFluentImpl$ItemsNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.EndpointConditionsFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.EndpointSliceListFluentImpl$ItemsNestedImpl
[WARNING]   - 78 more...
[WARNING] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-extensions-5.12.0.jar define 264 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.extensions.DaemonSetConditionBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.extensions.NetworkPolicyListFluent$ItemsNested
[WARNING]   - io.fabric8.kubernetes.api.model.extensions.DeploymentStrategyFluent
[WARNING]   - io.fabric8.kubernetes.api.model.extensions.IngressListFluentImpl$ItemsNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.extensions.NetworkPolicySpecFluent$IngressNested
[WARNING]   - io.fabric8.kubernetes.api.model.extensions.IngressStatus
[WARNING]   - io.fabric8.kubernetes.api.model.extensions.DaemonSetFluentImpl$SpecNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.extensions.IngressSpecFluent$RulesNested
[WARNING]   - io.fabric8.kubernetes.api.model.extensions.DaemonSetSpecFluent$UpdateStrategyNested
[WARNING]   - io.fabric8.kubernetes.api.model.extensions.NetworkPolicyPeerBuilder
[WARNING]   - 254 more...
[WARNING] snakeyaml-1.27.jar, crd-generator-0.29.0-SNAPSHOT.jar define 216 overlapping classes: 
[WARNING]   - org.yaml.snakeyaml.emitter.Emitter$ExpectBlockMappingValue
[WARNING]   - org.yaml.snakeyaml.parser.ParserImpl$ParseBlockNode
[WARNING]   - org.yaml.snakeyaml.emitter.Emitter$ExpectBlockMappingSimpleValue
[WARNING]   - org.yaml.snakeyaml.emitter.Emitter$ExpectDocumentEnd
[WARNING]   - org.yaml.snakeyaml.Yaml$3
[WARNING]   - org.yaml.snakeyaml.emitter.Emitter$ExpectBlockSequenceItem
[WARNING]   - org.yaml.snakeyaml.parser.ParserImpl$ParseBlockSequenceEntry
[WARNING]   - org.yaml.snakeyaml.util.ArrayUtils
[WARNING]   - org.yaml.snakeyaml.tokens.Token$ID
[WARNING]   - org.yaml.snakeyaml.reader.StreamReader
[WARNING]   - 206 more...
[WARNING] kubernetes-model-node-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 78 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.node.v1beta1.OverheadBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.node.v1.RuntimeClassListFluent$ItemsNested
[WARNING]   - io.fabric8.kubernetes.api.model.node.v1alpha1.Scheduling
[WARNING]   - io.fabric8.kubernetes.api.model.node.v1beta1.RuntimeClassFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.node.v1beta1.RuntimeClassListBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.node.v1.RuntimeClassFluent
[WARNING]   - io.fabric8.kubernetes.api.model.node.v1alpha1.SchedulingFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.node.v1alpha1.RuntimeClassSpecFluent$OverheadNested
[WARNING]   - io.fabric8.kubernetes.api.model.node.v1.RuntimeClassBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.node.v1.RuntimeClassFluentImpl
[WARNING]   - 68 more...
[WARNING] kubernetes-model-core-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 2394 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.BaseKubernetesListFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.StatusBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.KubeSchemaFluentImpl$APIResourceNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.NodeListBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.ResourceQuotaListFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.WatchEventFluentImpl$APIServiceStatusObjectNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.WatchEventFluent$VsphereVirtualDiskVolumeSourceObjectNested
[WARNING]   - io.fabric8.kubernetes.api.model.ProbeFluentImpl$HttpGetNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.PatchOptionsFluent
[WARNING]   - io.fabric8.kubernetes.api.model.ServerAddressByClientCIDRFluentImpl
[WARNING]   - 2384 more...
[WARNING] crd-generator-0.29.0-SNAPSHOT.jar, jackson-annotations-2.12.6.jar define 71 overlapping classes: 
[WARNING]   - com.fasterxml.jackson.annotation.JsonAutoDetect
[WARNING]   - com.fasterxml.jackson.annotation.JsonInclude
[WARNING]   - com.fasterxml.jackson.annotation.ObjectIdGenerators
[WARNING]   - com.fasterxml.jackson.annotation.JsonFormat$Features
[WARNING]   - com.fasterxml.jackson.annotation.JsonFormat$Feature
[WARNING]   - com.fasterxml.jackson.annotation.JsonIgnore
[WARNING]   - com.fasterxml.jackson.annotation.JsonSetter
[WARNING]   - com.fasterxml.jackson.annotation.JsonTypeInfo$None
[WARNING]   - com.fasterxml.jackson.annotation.JsonFormat$Shape
[WARNING]   - com.fasterxml.jackson.annotation.JsonSubTypes
[WARNING]   - 61 more...
[WARNING] crd-generator-0.29.0-SNAPSHOT.jar, slf4j-api-1.7.36.jar define 34 overlapping classes: 
[WARNING]   - org.slf4j.helpers.SubstituteLogger
[WARNING]   - org.slf4j.helpers.NamedLoggerBase
[WARNING]   - org.slf4j.helpers.NOPMDCAdapter
[WARNING]   - org.slf4j.MarkerFactory
[WARNING]   - org.slf4j.helpers.BasicMarker
[WARNING]   - org.slf4j.spi.LoggerFactoryBinder
[WARNING]   - org.slf4j.MDC$MDCCloseable
[WARNING]   - org.slf4j.spi.LocationAwareLogger
[WARNING]   - org.slf4j.helpers.MessageFormatter
[WARNING]   - org.slf4j.helpers.Util$ClassContextSecurityManager
[WARNING]   - 24 more...
[WARNING] jackson-dataformat-yaml-2.12.6.jar, crd-generator-0.29.0-SNAPSHOT.jar define 17 overlapping classes: 
[WARNING]   - com.fasterxml.jackson.dataformat.yaml.YAMLMapper$Builder
[WARNING]   - com.fasterxml.jackson.dataformat.yaml.snakeyaml.error.Mark
[WARNING]   - com.fasterxml.jackson.dataformat.yaml.YAMLGenerator
[WARNING]   - com.fasterxml.jackson.dataformat.yaml.UTF8Reader
[WARNING]   - com.fasterxml.jackson.dataformat.yaml.YAMLParser
[WARNING]   - com.fasterxml.jackson.dataformat.yaml.util.StringQuotingChecker
[WARNING]   - com.fasterxml.jackson.dataformat.yaml.YAMLGenerator$Feature
[WARNING]   - com.fasterxml.jackson.dataformat.yaml.JacksonYAMLParseException
[WARNING]   - com.fasterxml.jackson.dataformat.yaml.YAMLParser$Feature
[WARNING]   - com.fasterxml.jackson.dataformat.yaml.util.StringQuotingChecker$Default
[WARNING]   - 7 more...
[WARNING] kubernetes-model-apps-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 212 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.apps.ReplicaSetSpec
[WARNING]   - io.fabric8.kubernetes.api.model.apps.StatefulSetConditionFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.apps.DeploymentStrategyFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.apps.ControllerRevisionFluent$DeploymentDataNested
[WARNING]   - io.fabric8.kubernetes.api.model.apps.ReplicaSetSpecFluent
[WARNING]   - io.fabric8.kubernetes.api.model.apps.DeploymentFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.apps.ReplicaSetStatusFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.apps.ControllerRevisionFluentImpl$PersistentVolumeClaimDataNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.apps.ReplicaSetCondition
[WARNING]   - io.fabric8.kubernetes.api.model.apps.StatefulSetSpecFluent$UpdateStrategyNested
[WARNING]   - 202 more...
[WARNING] kubernetes-model-scheduling-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 24 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClassFluent
[WARNING]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClassFluentImpl$MetadataNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClassListFluent$ItemsNested
[WARNING]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClassList
[WARNING]   - io.fabric8.kubernetes.api.model.scheduling.v1beta1.PriorityClassListFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClassListFluent
[WARNING]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClass
[WARNING]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClassListFluentImpl$ItemsNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.scheduling.v1beta1.PriorityClassBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.scheduling.v1beta1.PriorityClassFluent$MetadataNested
[WARNING]   - 14 more...
[WARNING] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-common-5.12.0.jar define 16 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.model.annotation.Plural
[WARNING]   - io.fabric8.kubernetes.model.annotation.Group
[WARNING]   - io.fabric8.kubernetes.model.jackson.JsonUnwrappedDeserializer$CancelUnwrapped
[WARNING]   - io.fabric8.kubernetes.model.jackson.JsonUnwrappedDeserializer
[WARNING]   - io.fabric8.kubernetes.model.annotation.PrinterColumn
[WARNING]   - io.fabric8.kubernetes.model.jackson.UnwrappedTypeResolverBuilder
[WARNING]   - io.fabric8.kubernetes.model.annotation.Singular
[WARNING]   - io.fabric8.kubernetes.model.annotation.StatusReplicas
[WARNING]   - io.fabric8.kubernetes.model.annotation.SpecReplicas
[WARNING]   - io.fabric8.kubernetes.model.annotation.Version
[WARNING]   - 6 more...
[WARNING] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-batch-5.12.0.jar define 112 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.batch.v1.JobFluentImpl$StatusNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.batch.v1.CronJobStatusFluentImpl$ActiveNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.batch.v1.JobSpecFluent$TemplateNested
[WARNING]   - io.fabric8.kubernetes.api.model.batch.v1beta1.CronJobListFluentImpl$ItemsNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.batch.v1beta1.CronJobSpecFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.batch.v1.JobSpecFluentImpl$TemplateNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.batch.v1.Job
[WARNING]   - io.fabric8.kubernetes.api.model.batch.v1beta1.CronJobFluent
[WARNING]   - io.fabric8.kubernetes.api.model.batch.v1beta1.CronJobListFluent
[WARNING]   - io.fabric8.kubernetes.api.model.batch.v1.CronJobListFluent
[WARNING]   - 102 more...
[WARNING] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-storageclass-5.12.0.jar define 172 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.storage.v1beta1.CSIStorageCapacityListFluentImpl$ItemsNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.storage.CSINodeFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.storage.CSINodeDriverFluentImpl$AllocatableNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.storage.StorageClass
[WARNING]   - io.fabric8.kubernetes.api.model.storage.VolumeAttachmentSourceBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.storage.VolumeAttachmentSourceFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.storage.TokenRequestFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.storage.v1beta1.CSINodeDriverFluent$AllocatableNested
[WARNING]   - io.fabric8.kubernetes.api.model.storage.v1beta1.CSIDriverSpecBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.storage.VolumeAttachmentSpecFluent
[WARNING]   - 162 more...
[WARNING] okhttp-3.12.12.jar, crd-generator-0.29.0-SNAPSHOT.jar define 208 overlapping classes: 
[WARNING]   - okhttp3.WebSocket
[WARNING]   - okhttp3.Cookie$Builder
[WARNING]   - okhttp3.internal.http.HttpHeaders
[WARNING]   - okhttp3.internal.http2.Http2Connection$ReaderRunnable
[WARNING]   - okhttp3.internal.http2.Http2Reader$ContinuationSource
[WARNING]   - okhttp3.internal.tls.OkHostnameVerifier
[WARNING]   - okhttp3.Cache$Entry
[WARNING]   - okhttp3.internal.http2.Http2Connection$3
[WARNING]   - okhttp3.internal.ws.RealWebSocket$Streams
[WARNING]   - okhttp3.CacheControl$Builder
[WARNING]   - 198 more...
[WARNING] maven-shade-plugin has detected that some class files are
[WARNING] present in two or more JARs. When this happens, only one
[WARNING] single version of the class is copied to the uber jar.
[WARNING] Usually this is not harmful and you can skip these warnings,
[WARNING] otherwise try to manually exclude artifacts based on
[WARNING] mvn dependency:tree -Ddetail=true and the above output.
[WARNING] See http://maven.apache.org/plugins/maven-shade-plugin/
[INFO] Replacing original artifact with shaded artifact.
[INFO] Replacing /home/cloud-user/strimzi-kafka-operator/crd-generator/target/crd-generator-0.29.0-SNAPSHOT.jar with /home/cloud-user/strimzi-kafka-operator/crd-generator/target/crd-generator-0.29.0-SNAPSHOT-shaded.jar
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:integration-test (default) @ crd-generator ---
[WARNING] useSystemClassLoader setting has no effect when not forking
[WARNING] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:verify (default) @ crd-generator ---
[INFO] 
[INFO] --- maven-dependency-plugin:3.1.1:analyze-only (analyze) @ crd-generator ---
[INFO] No dependency problems found
[INFO] 
[INFO] ---------------------------< io.strimzi:api >---------------------------
[INFO] Building api 0.29.0-SNAPSHOT                                      [5/10]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ api ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ api ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ api ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/api/target/jacoco.exec
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ api ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/cloud-user/strimzi-kafka-operator/api/src/main/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:compile (default-compile) @ api ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- exec-maven-plugin:1.6.0:exec (generate-crd-co-install-v1) @ api ---
[INFO] 
[INFO] --- exec-maven-plugin:1.6.0:exec (generate-crd-co-install-v1-eo) @ api ---
[INFO] 
[INFO] --- exec-maven-plugin:1.6.0:exec (generate-doc) @ api ---
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ api ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 99 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:testCompile (default-testCompile) @ api ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:testCompile (default-test-compile) @ api ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:3.0.0-M5:test (default-test) @ api ---
[INFO] 
[INFO] -------------------------------------------------------
[INFO]  T E S T S
[INFO] -------------------------------------------------------
[INFO] 
[INFO] Results:
[INFO] 
[INFO] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[INFO] 
[INFO] 
[INFO] --- maven-jar-plugin:3.1.0:jar (default-jar) @ api ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:report (report) @ api ---
[INFO] Skipping JaCoCo execution due to missing execution data file.
[INFO] 
[INFO] >>> maven-source-plugin:3.0.1:jar (attach-sources) > generate-sources @ api >>>
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ api ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ api ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ api ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/api/target/jacoco.exec
[INFO] 
[INFO] <<< maven-source-plugin:3.0.1:jar (attach-sources) < generate-sources @ api <<<
[INFO] 
[INFO] 
[INFO] --- maven-source-plugin:3.0.1:jar (attach-sources) @ api ---
[INFO] 
[INFO] --- maven-javadoc-plugin:3.1.0:jar (attach-javadocs) @ api ---
[INFO] 
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/KafkaBridgeList.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/KafkaConnectList.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/KafkaConnectorList.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/KafkaList.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/KafkaMirrorMaker2List.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/KafkaMirrorMakerList.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/KafkaRebalanceList.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/KafkaTopicList.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/KafkaUserList.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/AclRuleClusterResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/CruiseControlResources.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/AclOperation.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/AclResourcePatternType.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/AclRuleResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/AclRuleGroupResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/AclRuleTopicResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/AclRuleTransactionalIdResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/ExternalLogging.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/AclRuleType.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/CertAndKeySecretSource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/CertSecretSource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/CruiseControlSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/CertificateExpirationPolicy.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaConnect.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/ContainerEnvVar.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/EntityOperatorSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/EntityTopicOperatorSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/EntityUserOperatorSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/HasConfigurableMetrics.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/InlineLogging.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/JvmOptions.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/GenericSecretSource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/ClientTls.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/JmxTransResources.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/JmxTransSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaAuthorization.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaAuthorizationKeycloak.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaAuthorizationOpa.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaAuthorizationSimple.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaBridge.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/Probe.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaBridgeClientSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaBridgeConsumerSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaBridgeHttpConfig.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaBridgeHttpCors.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaBridgeProducerSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaBridgeResources.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaClusterSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaConnector.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/Constants.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaConnectResources.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/Spec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/AbstractKafkaConnectSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/AclRule.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/CertificateAuthority.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaConnectSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaExporterSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaJmxAuthentication.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaConnectorSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaExporterResources.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaJmxAuthenticationPassword.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaMirrorMaker.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaJmxOptions.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaMirrorMaker2.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaMirrorMaker2Spec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaMirrorMakerSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaMirrorMaker2ConnectorSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaMirrorMaker2MirrorSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaMirrorMaker2Resources.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaRebalance.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/HasSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaMirrorMakerConsumerSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaMirrorMakerProducerSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaMirrorMakerResources.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaResources.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaBridgeSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaRebalanceSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaTopic.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaUser.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaTopicSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaUserAuthentication.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaUserAuthorization.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaUserAuthorizationSimple.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaUserQuotas.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaUserScramSha512ClientAuthentication.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaUserSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaUserTlsClientAuthentication.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/MetricsConfig.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/Password.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/Logging.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/PasswordSecretSource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/Rack.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/Sidecar.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/SystemProperty.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/TlsClientAuthentication.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/TlsSidecar.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/TlsSidecarLogLevel.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/Kafka.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/UnknownPropertyPreserving.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/ZookeeperClusterSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/authentication/KafkaClientAuthentication.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationOAuth.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationPlain.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScram.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha256.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha512.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationTls.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/balancing/KafkaRebalanceAnnotation.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/balancing/KafkaRebalanceState.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/balancing/BrokerCapacity.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/ConnectorPlugin.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnv.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/ExternalConfigurationVolumeSource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnvVarSource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/build/Build.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/build/Plugin.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/build/Artifact.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/build/DockerOutput.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/build/DownloadableArtifact.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/build/ImageStreamOutput.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/build/JarArtifact.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/build/MavenArtifact.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/build/OtherArtifact.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/build/Output.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/build/TgzArtifact.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/build/ZipArtifact.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/ExternalConfiguration.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/listener/NodeAddressType.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/listener/arraylistener/KafkaListenerType.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListener.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBootstrap.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBroker.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfiguration.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/listener/KafkaListenerAuthentication.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationCustom.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationOAuth.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationScramSha512.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationTls.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/status/Condition.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/status/KafkaBridgeStatus.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/status/KafkaConnectStatus.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/status/KafkaMirrorMaker2Status.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/status/KafkaMirrorMakerStatus.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/status/KafkaRebalanceStatus.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/status/KafkaUserStatus.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/status/ListenerAddress.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/status/HasStatus.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/status/KafkaConnectorStatus.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/status/KafkaStatus.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/status/KafkaTopicStatus.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/status/Status.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/status/ListenerStatus.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/status/StrimziPodSetStatus.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/storage/PersistentClaimStorageOverride.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/storage/PersistentClaimStorage.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/storage/EphemeralStorage.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/storage/JbodStorage.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/storage/SingleVolumeStorage.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/storage/Storage.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/ContainerTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/ExternalTrafficPolicy.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/JmxTransOutputDefinitionTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/KafkaUserTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/PodManagementPolicy.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/StatefulSetTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/DeploymentStrategy.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/DeploymentTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/InternalServiceTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/IpFamily.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/IpFamilyPolicy.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/MetadataTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/PodDisruptionBudgetTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/PodTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/ResourceTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/BuildConfigTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/CruiseControlTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/EntityOperatorTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/JmxTransQueryTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/JmxTransTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/KafkaBridgeTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/KafkaClusterTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/KafkaConnectTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/KafkaExporterTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2Template.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/tracing/JaegerTracing.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/tracing/Tracing.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/AbstractConnectorSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/ExternalConfigurationReference.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/JmxPrometheusExporterMetrics.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaAuthorizationCustom.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaBridgeAdminClientSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaUserTlsExternalClientAuthentication.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/PasswordSource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/StrimziPodSet.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/StrimziPodSetSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/Crds.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/StrimziPodSetList.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaStatusFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaStatusFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaStatusBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/ListenerStatusFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/ListenerStatusFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/ListenerStatusBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/ConditionFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/ConditionFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/ConditionBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaBridgeStatusFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaBridgeStatusFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaBridgeStatusBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaMirrorMaker2StatusFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaMirrorMaker2StatusFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaMirrorMaker2StatusBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaRebalanceStatusFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaRebalanceStatusFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaRebalanceStatusBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaUserStatusFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaUserStatusFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaUserStatusBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaConnectorStatusFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaConnectorStatusFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaConnectorStatusBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/StatusFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/StatusFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaTopicStatusFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaTopicStatusFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaTopicStatusBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaMirrorMakerStatusFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaMirrorMakerStatusFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaMirrorMakerStatusBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/ListenerAddressFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/ListenerAddressFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/ListenerAddressBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaConnectStatusFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaConnectStatusFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaConnectStatusBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/StrimziPodSetStatusFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/StrimziPodSetStatusFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/StrimziPodSetStatusBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ProbeFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ProbeFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ProbeBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMaker2SpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMaker2SpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMaker2SpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/TgzArtifactFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/TgzArtifactFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/TgzArtifactBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/BuildFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/BuildFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/BuildBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/DockerOutputFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/DockerOutputFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/DockerOutputBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/MavenArtifactFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/MavenArtifactFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/MavenArtifactBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/DownloadableArtifactFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/DownloadableArtifactFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/JarArtifactFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/JarArtifactFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/JarArtifactBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/OtherArtifactFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/OtherArtifactFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/OtherArtifactBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/ZipArtifactFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/ZipArtifactFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/ZipArtifactBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/ImageStreamOutputFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/ImageStreamOutputFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/ImageStreamOutputBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/PluginFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/PluginFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/PluginBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/ExternalConfigurationVolumeSourceFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/ExternalConfigurationVolumeSourceFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/ExternalConfigurationVolumeSourceBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/ConnectorPluginFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/ConnectorPluginFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/ConnectorPluginBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnvVarSourceFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnvVarSourceFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnvVarSourceBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/ExternalConfigurationFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/ExternalConfigurationFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/ExternalConfigurationBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnvFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnvFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnvBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationScramSha512Fluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationScramSha512FluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationScramSha512Builder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationOAuthFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationOAuthFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationOAuthBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBootstrapFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBootstrapFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBootstrapBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBrokerFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBrokerFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBrokerBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationCustomFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationCustomFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationCustomBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationTlsFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationTlsFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationTlsBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserAuthorizationSimpleFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserAuthorizationSimpleFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserAuthorizationSimpleBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/StrimziPodSetSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/StrimziPodSetSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/StrimziPodSetSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/EntityOperatorTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/EntityOperatorTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/EntityOperatorTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/PodTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/PodTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/PodTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/PodDisruptionBudgetTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/PodDisruptionBudgetTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/PodDisruptionBudgetTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/JmxTransOutputDefinitionTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/JmxTransOutputDefinitionTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/JmxTransOutputDefinitionTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/StatefulSetTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/StatefulSetTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/StatefulSetTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/CruiseControlTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/MetadataTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/MetadataTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/MetadataTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaExporterTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaExporterTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaExporterTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaClusterTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/ContainerTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/ContainerTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/ContainerTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaConnectTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaUserTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaUserTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaUserTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/JmxTransQueryTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/JmxTransQueryTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/JmxTransQueryTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/InternalServiceTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/InternalServiceTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/InternalServiceTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/ResourceTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/ResourceTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/ResourceTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/JmxTransTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/JmxTransTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/JmxTransTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/DeploymentTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/DeploymentTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/DeploymentTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/BuildConfigTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/BuildConfigTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/BuildConfigTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaJmxOptionsFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaJmxOptionsFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaJmxOptionsBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/storage/JbodStorageFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/storage/JbodStorageFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/storage/JbodStorageBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/storage/PersistentClaimStorageOverrideFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/storage/PersistentClaimStorageOverrideFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/storage/PersistentClaimStorageOverrideBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/storage/EphemeralStorageFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/storage/EphemeralStorageFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/storage/EphemeralStorageBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/storage/PersistentClaimStorageFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/storage/PersistentClaimStorageFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/storage/PersistentClaimStorageBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMakerFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMakerFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMakerBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/RackFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/RackFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/RackBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/TlsClientAuthenticationFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/TlsClientAuthenticationFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/TlsClientAuthenticationBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AclRuleFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AclRuleFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AclRuleBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMaker2Fluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMaker2FluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMaker2Builder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ClientTlsFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ClientTlsFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ClientTlsBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaConnectorSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaConnectorSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaConnectorSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/EntityUserOperatorSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/EntityUserOperatorSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/EntityUserOperatorSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaTopicFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaTopicFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaTopicBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserTlsClientAuthenticationFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserTlsClientAuthenticationFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserTlsClientAuthenticationBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeHttpConfigFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeHttpConfigFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeHttpConfigBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/JmxPrometheusExporterMetricsFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/JmxPrometheusExporterMetricsFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/JmxPrometheusExporterMetricsBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/SystemPropertyFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/SystemPropertyFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/SystemPropertyBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaTopicSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaTopicSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaTopicSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/CruiseControlSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/CruiseControlSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/CruiseControlSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserTlsExternalClientAuthenticationFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserTlsExternalClientAuthenticationFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserTlsExternalClientAuthenticationBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ZookeeperClusterSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/StrimziPodSetFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/StrimziPodSetFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/StrimziPodSetBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMaker2ConnectorSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMaker2ConnectorSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMaker2ConnectorSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/balancing/BrokerCapacityFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/balancing/BrokerCapacityFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/balancing/BrokerCapacityBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/PasswordSourceFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/tracing/JaegerTracingFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/tracing/JaegerTracingFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/tracing/JaegerTracingBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/PasswordSourceFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/PasswordSourceBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaRebalanceSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaRebalanceSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaRebalanceSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaJmxAuthenticationPasswordFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaJmxAuthenticationPasswordFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaJmxAuthenticationPasswordBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMaker2MirrorSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMaker2MirrorSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMaker2MirrorSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha256Fluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha256FluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha256Builder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationOAuthFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationOAuthFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationOAuthBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha512Fluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha512FluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha512Builder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationPlainFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationPlainFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationPlainBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationTlsFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationTlsFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationTlsBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/TlsSidecarFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/TlsSidecarFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/TlsSidecarBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/CertificateAuthorityFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/CertificateAuthorityFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/CertificateAuthorityBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeConsumerSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeConsumerSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeConsumerSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaConnectSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaConnectSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaConnectSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AclRuleTopicResourceFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AclRuleTopicResourceFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AclRuleTopicResourceBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AclRuleGroupResourceFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AclRuleGroupResourceFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AclRuleGroupResourceBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AbstractConnectorSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AbstractConnectorSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/EntityOperatorSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/EntityOperatorSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/EntityOperatorSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeHttpCorsFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeHttpCorsFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeHttpCorsBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/EntityTopicOperatorSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/EntityTopicOperatorSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/EntityTopicOperatorSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/SidecarFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/SidecarFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/SidecarBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/JvmOptionsFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/JvmOptionsFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/JvmOptionsBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaRebalanceFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaRebalanceFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaRebalanceBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaAuthorizationOpaFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaAuthorizationOpaFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaAuthorizationOpaBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeProducerSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeProducerSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeProducerSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ExternalLoggingFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ExternalLoggingFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ExternalLoggingBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/PasswordSecretSourceFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/PasswordSecretSourceFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/PasswordSecretSourceBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaAuthorizationCustomFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaAuthorizationCustomFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaAuthorizationCustomBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/PasswordFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/PasswordFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/PasswordBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserScramSha512ClientAuthenticationFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserScramSha512ClientAuthenticationFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserScramSha512ClientAuthenticationBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/CertAndKeySecretSourceFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/CertAndKeySecretSourceFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/CertAndKeySecretSourceBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMakerConsumerSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMakerConsumerSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMakerConsumerSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/CertSecretSourceFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/CertSecretSourceFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/CertSecretSourceBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/InlineLoggingFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/InlineLoggingFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/InlineLoggingBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserQuotasFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserQuotasFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserQuotasBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ExternalConfigurationReferenceFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ExternalConfigurationReferenceFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ExternalConfigurationReferenceBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaAuthorizationSimpleFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaAuthorizationSimpleFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaAuthorizationSimpleBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AclRuleTransactionalIdResourceFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AclRuleTransactionalIdResourceFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AclRuleTransactionalIdResourceBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMakerProducerSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMakerProducerSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMakerProducerSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/GenericSecretSourceFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/GenericSecretSourceFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/GenericSecretSourceBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/JmxTransSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/JmxTransSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/JmxTransSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaClusterSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaConnectorFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaConnectorFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaConnectorBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/SpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/SpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeClientSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeClientSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ContainerEnvVarFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ContainerEnvVarFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ContainerEnvVarBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaAuthorizationKeycloakFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaAuthorizationKeycloakFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaAuthorizationKeycloakBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeAdminClientSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeAdminClientSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeAdminClientSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaExporterSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaExporterSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaExporterSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AclRuleClusterResourceFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AclRuleClusterResourceFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AclRuleClusterResourceBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaConnectFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaConnectFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaConnectBuilder.java...
Constructing Javadoc information...
Standard Doclet version 11.0.11
Building tree for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListener.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfiguration.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBootstrap.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBootstrapBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBootstrapFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBootstrapFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBroker.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBrokerBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBrokerFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBrokerFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationFluent.BootstrapNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationFluent.BrokerCertChainAndKeyNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationFluent.BrokersNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationFluentImpl.BootstrapNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationFluentImpl.BrokerCertChainAndKeyNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationFluentImpl.BrokersNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerFluent.ConfigurationNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerFluent.KafkaListenerAuthenticationCustomAuthNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerFluent.KafkaListenerAuthenticationOAuthNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerFluent.KafkaListenerAuthenticationScramSha512AuthNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerFluent.KafkaListenerAuthenticationTlsAuthNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerFluentImpl.ConfigurationNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerFluentImpl.KafkaListenerAuthenticationCustomAuthNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerFluentImpl.KafkaListenerAuthenticationOAuthNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerFluentImpl.KafkaListenerAuthenticationScramSha512AuthNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerFluentImpl.KafkaListenerAuthenticationTlsAuthNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/KafkaListenerType.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthentication.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationOAuth.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationOAuthBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationOAuthFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationOAuthFluent.AccessTokenNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationOAuthFluent.ClientSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationOAuthFluent.RefreshTokenNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationOAuthFluent.TlsTrustedCertificatesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationOAuthFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationOAuthFluentImpl.AccessTokenNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationOAuthFluentImpl.ClientSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationOAuthFluentImpl.RefreshTokenNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationOAuthFluentImpl.TlsTrustedCertificatesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationPlain.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationPlainBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationPlainFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationPlainFluent.PasswordSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationPlainFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationPlainFluentImpl.PasswordSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScram.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha256.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha256Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha256Fluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha256Fluent.PasswordSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha256FluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha256FluentImpl.PasswordSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha512.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha512Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha512Fluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha512Fluent.PasswordSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha512FluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha512FluentImpl.PasswordSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationTls.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationTlsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationTlsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationTlsFluent.CertificateAndKeyNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationTlsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationTlsFluentImpl.CertificateAndKeyNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/balancing/BrokerCapacity.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/balancing/BrokerCapacityBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/balancing/BrokerCapacityFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/balancing/BrokerCapacityFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/balancing/KafkaRebalanceAnnotation.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/balancing/KafkaRebalanceState.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/Artifact.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/Build.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/BuildBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/BuildFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/BuildFluent.DockerOutputNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/BuildFluent.ImageStreamOutputNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/BuildFluent.PluginsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/BuildFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/BuildFluentImpl.DockerOutputNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/BuildFluentImpl.ImageStreamOutputNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/BuildFluentImpl.PluginsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/DockerOutput.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/DockerOutputBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/DockerOutputFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/DockerOutputFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/DownloadableArtifact.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/DownloadableArtifactFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/DownloadableArtifactFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/ImageStreamOutput.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/ImageStreamOutputBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/ImageStreamOutputFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/ImageStreamOutputFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/JarArtifact.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/JarArtifactBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/JarArtifactFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/JarArtifactFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/MavenArtifact.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/MavenArtifactBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/MavenArtifactFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/MavenArtifactFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/OtherArtifact.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/OtherArtifactBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/OtherArtifactFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/OtherArtifactFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/Output.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/Plugin.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/PluginBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/PluginFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/PluginFluent.JarArtifactArtifactsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/PluginFluent.MavenArtifactArtifactsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/PluginFluent.OtherArtifactArtifactsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/PluginFluent.TgzArtifactArtifactsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/PluginFluent.ZipArtifactArtifactsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/PluginFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/PluginFluentImpl.JarArtifactArtifactsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/PluginFluentImpl.MavenArtifactArtifactsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/PluginFluentImpl.OtherArtifactArtifactsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/PluginFluentImpl.TgzArtifactArtifactsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/PluginFluentImpl.ZipArtifactArtifactsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/TgzArtifact.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/TgzArtifactBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/TgzArtifactFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/TgzArtifactFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/ZipArtifact.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/ZipArtifactBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/ZipArtifactFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/ZipArtifactFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ConnectorPlugin.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ConnectorPluginBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ConnectorPluginFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ConnectorPluginFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfiguration.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnv.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnvBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnvFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnvFluent.ValueFromNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnvFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnvFluentImpl.ValueFromNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnvVarSource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnvVarSourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnvVarSourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnvVarSourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationFluent.EnvNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationFluent.VolumesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationFluentImpl.EnvNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationFluentImpl.VolumesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationVolumeSource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationVolumeSourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationVolumeSourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationVolumeSourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/Crds.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/KafkaBridgeList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/KafkaConnectList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/KafkaConnectorList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/KafkaList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/KafkaMirrorMaker2List.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/KafkaMirrorMakerList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/KafkaRebalanceList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/KafkaTopicList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/KafkaUserList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/StrimziPodSetList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthentication.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationCustom.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationCustomBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationCustomFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationCustomFluent.SecretsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationCustomFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationCustomFluentImpl.SecretsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationOAuth.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationOAuthBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationOAuthFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationOAuthFluent.ClientSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationOAuthFluent.TlsTrustedCertificatesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationOAuthFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationOAuthFluentImpl.ClientSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationOAuthFluentImpl.TlsTrustedCertificatesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationScramSha512.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationScramSha512Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationScramSha512Fluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationScramSha512FluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationTls.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationTlsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationTlsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationTlsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/NodeAddressType.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractConnectorSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractConnectorSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractConnectorSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluent.ExternalConfigurationNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluent.ExternalLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluent.InlineLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluent.JaegerTracingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluent.JmxOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluent.JmxPrometheusExporterMetricsConfigNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluent.JvmOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluentImpl.ExternalConfigurationNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluentImpl.ExternalLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluentImpl.InlineLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluentImpl.JaegerTracingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluentImpl.JmxOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluentImpl.JmxPrometheusExporterMetricsConfigNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluentImpl.JvmOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclOperation.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclResourcePatternType.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRule.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleClusterResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleClusterResourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleClusterResourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleClusterResourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleFluent.AclRuleClusterResourceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleFluent.AclRuleGroupResourceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleFluent.AclRuleTopicResourceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleFluent.AclRuleTransactionalIdResourceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleFluentImpl.AclRuleClusterResourceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleFluentImpl.AclRuleGroupResourceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleFluentImpl.AclRuleTopicResourceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleFluentImpl.AclRuleTransactionalIdResourceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleGroupResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleGroupResourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleGroupResourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleGroupResourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleTopicResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleTopicResourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleTopicResourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleTopicResourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleTransactionalIdResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleTransactionalIdResourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleTransactionalIdResourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleTransactionalIdResourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleType.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CertAndKeySecretSource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CertAndKeySecretSourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CertAndKeySecretSourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CertAndKeySecretSourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CertificateAuthority.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CertificateAuthorityBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CertificateAuthorityFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CertificateAuthorityFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CertificateExpirationPolicy.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CertSecretSource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CertSecretSourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CertSecretSourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CertSecretSourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ClientTls.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ClientTlsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ClientTlsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ClientTlsFluent.TrustedCertificatesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ClientTlsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ClientTlsFluentImpl.TrustedCertificatesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/Constants.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ContainerEnvVar.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ContainerEnvVarBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ContainerEnvVarFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ContainerEnvVarFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlResources.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluent.BrokerCapacityNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluent.ExternalLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluent.InlineLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluent.JmxPrometheusExporterMetricsConfigNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluent.JvmOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluent.TlsSidecarNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluentImpl.BrokerCapacityNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluentImpl.ExternalLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluentImpl.InlineLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluentImpl.JmxPrometheusExporterMetricsConfigNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluentImpl.JvmOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluentImpl.TlsSidecarNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityOperatorSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityOperatorSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityOperatorSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityOperatorSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityOperatorSpecFluent.TlsSidecarNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityOperatorSpecFluent.TopicOperatorNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityOperatorSpecFluent.UserOperatorNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityOperatorSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityOperatorSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityOperatorSpecFluentImpl.TlsSidecarNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityOperatorSpecFluentImpl.TopicOperatorNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityOperatorSpecFluentImpl.UserOperatorNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityTopicOperatorSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityTopicOperatorSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityTopicOperatorSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityTopicOperatorSpecFluent.ExternalLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityTopicOperatorSpecFluent.InlineLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityTopicOperatorSpecFluent.JvmOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityTopicOperatorSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityTopicOperatorSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityTopicOperatorSpecFluent.StartupProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityTopicOperatorSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityTopicOperatorSpecFluentImpl.ExternalLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityTopicOperatorSpecFluentImpl.InlineLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityTopicOperatorSpecFluentImpl.JvmOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityTopicOperatorSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityTopicOperatorSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityTopicOperatorSpecFluentImpl.StartupProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityUserOperatorSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityUserOperatorSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityUserOperatorSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityUserOperatorSpecFluent.ExternalLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityUserOperatorSpecFluent.InlineLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityUserOperatorSpecFluent.JvmOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityUserOperatorSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityUserOperatorSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityUserOperatorSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityUserOperatorSpecFluentImpl.ExternalLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityUserOperatorSpecFluentImpl.InlineLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityUserOperatorSpecFluentImpl.JvmOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityUserOperatorSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityUserOperatorSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ExternalConfigurationReference.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ExternalConfigurationReferenceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ExternalConfigurationReferenceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ExternalConfigurationReferenceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ExternalLogging.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ExternalLoggingBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ExternalLoggingFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ExternalLoggingFluent.ValueFromNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ExternalLoggingFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ExternalLoggingFluentImpl.ValueFromNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/GenericSecretSource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/GenericSecretSourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/GenericSecretSourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/GenericSecretSourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/HasConfigurableMetrics.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/HasSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/InlineLogging.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/InlineLoggingBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/InlineLoggingFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/InlineLoggingFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxPrometheusExporterMetrics.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxPrometheusExporterMetricsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxPrometheusExporterMetricsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxPrometheusExporterMetricsFluent.ValueFromNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxPrometheusExporterMetricsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxPrometheusExporterMetricsFluentImpl.ValueFromNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxTransResources.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxTransSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxTransSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxTransSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxTransSpecFluent.KafkaQueriesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxTransSpecFluent.OutputDefinitionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxTransSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxTransSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxTransSpecFluentImpl.KafkaQueriesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxTransSpecFluentImpl.OutputDefinitionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxTransSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JvmOptions.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JvmOptionsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JvmOptionsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JvmOptionsFluent.JavaSystemPropertiesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JvmOptionsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JvmOptionsFluentImpl.JavaSystemPropertiesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/Kafka.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorization.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationCustom.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationCustomBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationCustomFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationCustomFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationKeycloak.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationKeycloakBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationKeycloakFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationKeycloakFluent.TlsTrustedCertificatesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationKeycloakFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationKeycloakFluentImpl.TlsTrustedCertificatesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationOpa.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationOpaBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationOpaFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationOpaFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationSimple.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationSimpleBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationSimpleFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationSimpleFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridge.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeAdminClientSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeAdminClientSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeAdminClientSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeAdminClientSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeClientSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeClientSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeClientSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeConsumerSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeConsumerSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeConsumerSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeConsumerSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeHttpConfig.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeHttpConfigBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeHttpConfigFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeHttpConfigFluent.CorsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeHttpConfigFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeHttpConfigFluentImpl.CorsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeHttpCors.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeHttpCorsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeHttpCorsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeHttpCorsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeProducerSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeProducerSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeProducerSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeProducerSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeResources.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.AdminClientNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.ConsumerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.ExternalLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.HttpNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.InlineLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.JaegerTracingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.JvmOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.KafkaClientAuthenticationOAuthNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.KafkaClientAuthenticationPlainNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.KafkaClientAuthenticationScramSha256Nested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.KafkaClientAuthenticationScramSha512Nested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.KafkaClientAuthenticationTlsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.ProducerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.TlsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.AdminClientNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.ConsumerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.ExternalLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.HttpNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.InlineLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.JaegerTracingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.JvmOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.KafkaClientAuthenticationOAuthNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.KafkaClientAuthenticationPlainNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.KafkaClientAuthenticationScramSha256NestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.KafkaClientAuthenticationScramSha512NestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.KafkaClientAuthenticationTlsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.ProducerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.TlsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.EphemeralStorageNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.ExternalLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.InlineLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.JbodStorageNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.JmxOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.JmxPrometheusExporterMetricsConfigNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.JvmOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.KafkaAuthorizationCustomNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.KafkaAuthorizationKeycloakNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.KafkaAuthorizationOpaNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.KafkaAuthorizationSimpleNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.ListenersNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.PersistentClaimStorageNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.RackNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.EphemeralStorageNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.ExternalLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.InlineLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.JbodStorageNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.JmxOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.JmxPrometheusExporterMetricsConfigNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.JvmOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.KafkaAuthorizationCustomNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.KafkaAuthorizationKeycloakNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.KafkaAuthorizationOpaNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.KafkaAuthorizationSimpleNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.ListenersNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.PersistentClaimStorageNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.RackNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnect.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnector.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectorBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectorFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectorFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectorFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectorFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectorFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectorFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectorFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectorFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectorSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectorSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectorSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectorSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectResources.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluent.BuildNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluent.KafkaClientAuthenticationOAuthNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluent.KafkaClientAuthenticationPlainNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluent.KafkaClientAuthenticationScramSha256Nested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluent.KafkaClientAuthenticationScramSha512Nested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluent.KafkaClientAuthenticationTlsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluent.RackNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluent.TlsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluentImpl.BuildNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluentImpl.KafkaClientAuthenticationOAuthNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluentImpl.KafkaClientAuthenticationPlainNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluentImpl.KafkaClientAuthenticationScramSha256NestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluentImpl.KafkaClientAuthenticationScramSha512NestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluentImpl.KafkaClientAuthenticationTlsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluentImpl.RackNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluentImpl.TlsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaExporterResources.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaExporterSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaExporterSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaExporterSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaExporterSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaExporterSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaExporterSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaExporterSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaExporterSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaExporterSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaExporterSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaJmxAuthentication.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaJmxAuthenticationPassword.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaJmxAuthenticationPasswordBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaJmxAuthenticationPasswordFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaJmxAuthenticationPasswordFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaJmxOptions.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaJmxOptionsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaJmxOptionsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaJmxOptionsFluent.KafkaJmxAuthenticationPasswordNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaJmxOptionsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaJmxOptionsFluentImpl.KafkaJmxAuthenticationPasswordNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecFluent.KafkaClientAuthenticationOAuthNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecFluent.KafkaClientAuthenticationPlainNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecFluent.KafkaClientAuthenticationScramSha256Nested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecFluent.KafkaClientAuthenticationScramSha512Nested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecFluent.KafkaClientAuthenticationTlsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecFluent.TlsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecFluentImpl.KafkaClientAuthenticationOAuthNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecFluentImpl.KafkaClientAuthenticationPlainNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecFluentImpl.KafkaClientAuthenticationScramSha256NestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecFluentImpl.KafkaClientAuthenticationScramSha512NestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecFluentImpl.KafkaClientAuthenticationTlsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecFluentImpl.TlsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ConnectorSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ConnectorSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ConnectorSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ConnectorSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2Fluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2Fluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2Fluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2Fluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2FluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2FluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2FluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2FluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2MirrorSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2MirrorSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2MirrorSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2MirrorSpecFluent.CheckpointConnectorNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2MirrorSpecFluent.HeartbeatConnectorNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2MirrorSpecFluent.SourceConnectorNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2MirrorSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2MirrorSpecFluentImpl.CheckpointConnectorNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2MirrorSpecFluentImpl.HeartbeatConnectorNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2MirrorSpecFluentImpl.SourceConnectorNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2Resources.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2Spec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2SpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2SpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2SpecFluent.ClustersNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2SpecFluent.MirrorsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2SpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2SpecFluentImpl.ClustersNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2SpecFluentImpl.MirrorsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecFluent.KafkaClientAuthenticationOAuthNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecFluent.KafkaClientAuthenticationPlainNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecFluent.KafkaClientAuthenticationScramSha256Nested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecFluent.KafkaClientAuthenticationScramSha512Nested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecFluent.KafkaClientAuthenticationTlsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecFluent.TlsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecFluentImpl.KafkaClientAuthenticationOAuthNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecFluentImpl.KafkaClientAuthenticationPlainNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecFluentImpl.KafkaClientAuthenticationScramSha256NestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecFluentImpl.KafkaClientAuthenticationScramSha512NestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecFluentImpl.KafkaClientAuthenticationTlsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecFluentImpl.TlsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerConsumerSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerConsumerSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerConsumerSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerConsumerSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerProducerSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerProducerSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerProducerSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerProducerSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerResources.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluent.ConsumerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluent.ExternalLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluent.InlineLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluent.JaegerTracingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluent.JmxPrometheusExporterMetricsConfigNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluent.JvmOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluent.ProducerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluentImpl.ConsumerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluentImpl.ExternalLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluentImpl.InlineLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluentImpl.JaegerTracingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluentImpl.JmxPrometheusExporterMetricsConfigNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluentImpl.JvmOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluentImpl.ProducerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaRebalance.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaRebalanceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaRebalanceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaRebalanceFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaRebalanceFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaRebalanceFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaRebalanceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaRebalanceFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaRebalanceFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaRebalanceFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaRebalanceSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaRebalanceSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaRebalanceSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaRebalanceSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaResources.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluent.ClientsCaNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluent.ClusterCaNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluent.CruiseControlNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluent.EntityOperatorNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluent.JmxTransNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluent.KafkaExporterNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluent.KafkaNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluent.ZookeeperNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluentImpl.ClientsCaNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluentImpl.ClusterCaNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluentImpl.CruiseControlNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluentImpl.EntityOperatorNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluentImpl.JmxTransNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluentImpl.KafkaExporterNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluentImpl.KafkaNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluentImpl.ZookeeperNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaTopic.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaTopicBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaTopicFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaTopicFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaTopicFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaTopicFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaTopicFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaTopicFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaTopicFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaTopicFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaTopicSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaTopicSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaTopicSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaTopicSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUser.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserAuthentication.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserAuthorization.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserAuthorizationSimple.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserAuthorizationSimpleBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserAuthorizationSimpleFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserAuthorizationSimpleFluent.AclsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserAuthorizationSimpleFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserAuthorizationSimpleFluentImpl.AclsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserQuotas.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserQuotasBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserQuotasFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserQuotasFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserScramSha512ClientAuthentication.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserScramSha512ClientAuthenticationBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserScramSha512ClientAuthenticationFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserScramSha512ClientAuthenticationFluent.PasswordNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserScramSha512ClientAuthenticationFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserScramSha512ClientAuthenticationFluentImpl.PasswordNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserSpecFluent.KafkaUserAuthorizationSimpleNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserSpecFluent.KafkaUserScramSha512ClientAuthenticationNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserSpecFluent.KafkaUserTlsClientAuthenticationNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserSpecFluent.KafkaUserTlsExternalClientAuthenticationNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserSpecFluent.QuotasNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserSpecFluentImpl.KafkaUserAuthorizationSimpleNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserSpecFluentImpl.KafkaUserScramSha512ClientAuthenticationNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserSpecFluentImpl.KafkaUserTlsClientAuthenticationNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserSpecFluentImpl.KafkaUserTlsExternalClientAuthenticationNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserSpecFluentImpl.QuotasNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserTlsClientAuthentication.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserTlsClientAuthenticationBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserTlsClientAuthenticationFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserTlsClientAuthenticationFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserTlsExternalClientAuthentication.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserTlsExternalClientAuthenticationBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserTlsExternalClientAuthenticationFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserTlsExternalClientAuthenticationFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/Logging.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/MetricsConfig.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/Password.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/PasswordBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/PasswordFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/PasswordFluent.ValueFromNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/PasswordFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/PasswordFluentImpl.ValueFromNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/PasswordSecretSource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/PasswordSecretSourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/PasswordSecretSourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/PasswordSecretSourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/PasswordSource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/PasswordSourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/PasswordSourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/PasswordSourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/Probe.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ProbeBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ProbeFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ProbeFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/Rack.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/RackBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/RackFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/RackFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/Sidecar.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/SidecarBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/SidecarFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/SidecarFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/Spec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/SpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/SpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/StrimziPodSet.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/StrimziPodSetBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/StrimziPodSetFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/StrimziPodSetFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/StrimziPodSetFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/StrimziPodSetFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/StrimziPodSetFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/StrimziPodSetFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/StrimziPodSetFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/StrimziPodSetFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/StrimziPodSetSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/StrimziPodSetSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/StrimziPodSetSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/StrimziPodSetSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/SystemProperty.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/SystemPropertyBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/SystemPropertyFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/SystemPropertyFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/TlsClientAuthentication.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/TlsClientAuthenticationBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/TlsClientAuthenticationFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/TlsClientAuthenticationFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/TlsSidecar.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/TlsSidecarBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/TlsSidecarFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/TlsSidecarFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/TlsSidecarFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/TlsSidecarFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/TlsSidecarFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/TlsSidecarFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/TlsSidecarLogLevel.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/UnknownPropertyPreserving.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluent.EphemeralStorageNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluent.ExternalLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluent.InlineLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluent.JmxOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluent.JmxPrometheusExporterMetricsConfigNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluent.JvmOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluent.PersistentClaimStorageNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluentImpl.EphemeralStorageNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluentImpl.ExternalLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluentImpl.InlineLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluentImpl.JmxOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluentImpl.JmxPrometheusExporterMetricsConfigNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluentImpl.JvmOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluentImpl.PersistentClaimStorageNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/Condition.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/ConditionBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/ConditionFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/ConditionFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/HasStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaBridgeStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaBridgeStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaBridgeStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaBridgeStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaConnectorStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaConnectorStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaConnectorStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaConnectorStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaConnectStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaConnectStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaConnectStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaConnectStatusFluent.ConnectorPluginsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaConnectStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaConnectStatusFluentImpl.ConnectorPluginsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaMirrorMaker2Status.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaMirrorMaker2StatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaMirrorMaker2StatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaMirrorMaker2StatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaMirrorMakerStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaMirrorMakerStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaMirrorMakerStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaMirrorMakerStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaRebalanceStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaRebalanceStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaRebalanceStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaRebalanceStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaStatusFluent.ListenersNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaStatusFluentImpl.ListenersNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaTopicStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaTopicStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaTopicStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaTopicStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaUserStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaUserStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaUserStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaUserStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/ListenerAddress.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/ListenerAddressBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/ListenerAddressFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/ListenerAddressFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/ListenerStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/ListenerStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/ListenerStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/ListenerStatusFluent.AddressesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/ListenerStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/ListenerStatusFluentImpl.AddressesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/Status.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/StatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/StatusFluent.ConditionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/StatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/StatusFluentImpl.ConditionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/StrimziPodSetStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/StrimziPodSetStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/StrimziPodSetStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/StrimziPodSetStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/EphemeralStorage.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/EphemeralStorageBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/EphemeralStorageFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/EphemeralStorageFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/JbodStorage.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/JbodStorageBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/JbodStorageFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/JbodStorageFluent.EphemeralStorageVolumesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/JbodStorageFluent.PersistentClaimStorageVolumesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/JbodStorageFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/JbodStorageFluentImpl.EphemeralStorageVolumesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/JbodStorageFluentImpl.PersistentClaimStorageVolumesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/PersistentClaimStorage.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/PersistentClaimStorageBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/PersistentClaimStorageFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/PersistentClaimStorageFluent.OverridesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/PersistentClaimStorageFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/PersistentClaimStorageFluentImpl.OverridesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/PersistentClaimStorageOverride.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/PersistentClaimStorageOverrideBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/PersistentClaimStorageOverrideFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/PersistentClaimStorageOverrideFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/SingleVolumeStorage.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/Storage.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/BuildConfigTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/BuildConfigTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/BuildConfigTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/BuildConfigTemplateFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/BuildConfigTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/BuildConfigTemplateFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ContainerTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ContainerTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ContainerTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ContainerTemplateFluent.EnvNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ContainerTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ContainerTemplateFluentImpl.EnvNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluent.ApiServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluent.CruiseControlContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluent.DeploymentNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluent.PodDisruptionBudgetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluent.TlsSidecarContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluentImpl.ApiServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluentImpl.CruiseControlContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluentImpl.DeploymentNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluentImpl.PodDisruptionBudgetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluentImpl.TlsSidecarContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/DeploymentStrategy.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/DeploymentTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/DeploymentTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/DeploymentTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/DeploymentTemplateFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/DeploymentTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/DeploymentTemplateFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/EntityOperatorTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/EntityOperatorTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/EntityOperatorTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/EntityOperatorTemplateFluent.DeploymentNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/EntityOperatorTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/EntityOperatorTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/EntityOperatorTemplateFluent.TlsSidecarContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/EntityOperatorTemplateFluent.TopicOperatorContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/EntityOperatorTemplateFluent.UserOperatorContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/EntityOperatorTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/EntityOperatorTemplateFluentImpl.DeploymentNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/EntityOperatorTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/EntityOperatorTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/EntityOperatorTemplateFluentImpl.TlsSidecarContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/EntityOperatorTemplateFluentImpl.TopicOperatorContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/EntityOperatorTemplateFluentImpl.UserOperatorContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ExternalTrafficPolicy.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/InternalServiceTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/InternalServiceTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/InternalServiceTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/InternalServiceTemplateFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/InternalServiceTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/InternalServiceTemplateFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/IpFamily.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/IpFamilyPolicy.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransOutputDefinitionTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransOutputDefinitionTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransOutputDefinitionTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransOutputDefinitionTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransQueryTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransQueryTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransQueryTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransQueryTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransTemplateFluent.ContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransTemplateFluent.DeploymentNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransTemplateFluentImpl.ContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransTemplateFluentImpl.DeploymentNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaBridgeTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateFluent.ApiServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateFluent.BridgeContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateFluent.DeploymentNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateFluent.PodDisruptionBudgetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateFluentImpl.ApiServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateFluentImpl.BridgeContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateFluentImpl.DeploymentNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateFluentImpl.PodDisruptionBudgetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.BootstrapServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.BrokersServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.ClusterCaCertNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.ClusterRoleBindingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.ExternalBootstrapIngressNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.ExternalBootstrapRouteNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.ExternalBootstrapServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.InitContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.JmxSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.KafkaContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.PerPodIngressNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.PerPodRouteNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.PerPodServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.PersistentVolumeClaimNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.PodDisruptionBudgetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.PodSetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.StatefulsetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.BootstrapServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.BrokersServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.ClusterCaCertNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.ClusterRoleBindingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.ExternalBootstrapIngressNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.ExternalBootstrapRouteNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.ExternalBootstrapServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.InitContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.JmxSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.KafkaContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.PerPodIngressNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.PerPodRouteNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.PerPodServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.PersistentVolumeClaimNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.PodDisruptionBudgetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.PodSetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.StatefulsetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluent.ApiServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluent.BuildConfigNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluent.BuildContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluent.BuildPodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluent.BuildServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluent.ClusterRoleBindingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluent.ConnectContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluent.DeploymentNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluent.InitContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluent.JmxSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluent.PodDisruptionBudgetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluentImpl.ApiServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluentImpl.BuildConfigNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluentImpl.BuildContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluentImpl.BuildPodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluentImpl.BuildServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluentImpl.ClusterRoleBindingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluentImpl.ConnectContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluentImpl.DeploymentNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluentImpl.InitContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluentImpl.JmxSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluentImpl.PodDisruptionBudgetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaExporterTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaExporterTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaExporterTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaExporterTemplateFluent.ContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaExporterTemplateFluent.DeploymentNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaExporterTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaExporterTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaExporterTemplateFluent.ServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaExporterTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaExporterTemplateFluentImpl.ContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaExporterTemplateFluentImpl.DeploymentNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaExporterTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaExporterTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaExporterTemplateFluentImpl.ServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2Template.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluent.ApiServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluent.DeploymentNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluent.JmxSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluent.MirrorMaker2ContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluent.PodDisruptionBudgetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluentImpl.ApiServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluentImpl.DeploymentNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluentImpl.JmxSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluentImpl.MirrorMaker2ContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluentImpl.PodDisruptionBudgetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplateFluent.DeploymentNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplateFluent.MirrorMakerContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplateFluent.PodDisruptionBudgetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplateFluentImpl.DeploymentNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplateFluentImpl.MirrorMakerContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplateFluentImpl.PodDisruptionBudgetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaUserTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaUserTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaUserTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaUserTemplateFluent.SecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaUserTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaUserTemplateFluentImpl.SecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/MetadataTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/MetadataTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/MetadataTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/MetadataTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/PodDisruptionBudgetTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/PodDisruptionBudgetTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/PodDisruptionBudgetTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/PodDisruptionBudgetTemplateFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/PodDisruptionBudgetTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/PodDisruptionBudgetTemplateFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/PodManagementPolicy.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/PodTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/PodTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/PodTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/PodTemplateFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/PodTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/PodTemplateFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ResourceTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ResourceTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ResourceTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ResourceTemplateFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ResourceTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ResourceTemplateFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/StatefulSetTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/StatefulSetTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/StatefulSetTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/StatefulSetTemplateFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/StatefulSetTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/StatefulSetTemplateFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluent.ClientServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluent.JmxSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluent.NodesServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluent.PersistentVolumeClaimNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluent.PodDisruptionBudgetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluent.PodSetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluent.StatefulsetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluent.ZookeeperContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluentImpl.ClientServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluentImpl.JmxSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluentImpl.NodesServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluentImpl.PersistentVolumeClaimNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluentImpl.PodDisruptionBudgetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluentImpl.PodSetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluentImpl.StatefulsetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluentImpl.ZookeeperContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/tracing/JaegerTracing.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/tracing/JaegerTracingBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/tracing/JaegerTracingFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/tracing/JaegerTracingFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/tracing/Tracing.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/balancing/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/balancing/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/tracing/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/tracing/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/constant-values.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/serialized-form.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/class-use/KafkaBridgeList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/class-use/KafkaConnectList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/class-use/KafkaConnectorList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/class-use/KafkaList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/class-use/KafkaMirrorMaker2List.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/class-use/KafkaMirrorMakerList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/class-use/KafkaRebalanceList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/class-use/KafkaTopicList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/class-use/KafkaUserList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleClusterResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlResources.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclOperation.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclResourcePatternType.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleGroupResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleTopicResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleTransactionalIdResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ExternalLogging.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleType.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CertAndKeySecretSource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CertSecretSource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CertificateExpirationPolicy.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnect.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ContainerEnvVar.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityOperatorSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityTopicOperatorSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityUserOperatorSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/HasConfigurableMetrics.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/InlineLogging.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JvmOptions.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/GenericSecretSource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ClientTls.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxTransResources.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxTransSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorization.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationKeycloak.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationOpa.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationSimple.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridge.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/Probe.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeClientSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeConsumerSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeHttpConfig.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeHttpCors.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeProducerSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeResources.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnector.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/Constants.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectResources.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/Spec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRule.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CertificateAuthority.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaExporterSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaJmxAuthentication.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectorSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaExporterResources.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaJmxAuthenticationPassword.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaJmxOptions.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ClusterSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2Spec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ConnectorSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2MirrorSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2Resources.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaRebalance.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/HasSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerClientSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerConsumerSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerProducerSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerResources.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaResources.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaRebalanceSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaTopic.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUser.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaTopicSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserAuthentication.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserAuthorization.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserAuthorizationSimple.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserQuotas.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserScramSha512ClientAuthentication.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserTlsClientAuthentication.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/MetricsConfig.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/Password.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/Logging.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/PasswordSecretSource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/Rack.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/Sidecar.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/SystemProperty.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/TlsClientAuthentication.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/TlsSidecar.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/TlsSidecarLogLevel.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/Kafka.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/UnknownPropertyPreserving.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthentication.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationOAuth.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationPlain.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationScram.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationScramSha256.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationScramSha512.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationTls.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/balancing/class-use/KafkaRebalanceAnnotation.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/balancing/class-use/KafkaRebalanceState.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/balancing/class-use/BrokerCapacity.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ConnectorPlugin.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationEnv.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationVolumeSource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationEnvVarSource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/Build.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/Plugin.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/Artifact.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/DockerOutput.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/DownloadableArtifact.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/ImageStreamOutput.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/JarArtifact.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/MavenArtifact.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/OtherArtifact.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/Output.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/TgzArtifact.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/ZipArtifact.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfiguration.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/NodeAddressType.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/KafkaListenerType.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListener.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationBootstrap.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationBroker.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfiguration.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthentication.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationCustom.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationOAuth.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationScramSha512.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationTls.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/Condition.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaBridgeStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaConnectStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaMirrorMaker2Status.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaMirrorMakerStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaRebalanceStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaUserStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/ListenerAddress.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/HasStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaConnectorStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaTopicStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/Status.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/ListenerStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/StrimziPodSetStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/PersistentClaimStorageOverride.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/PersistentClaimStorage.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/EphemeralStorage.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/JbodStorage.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/SingleVolumeStorage.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/Storage.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ContainerTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ExternalTrafficPolicy.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransOutputDefinitionTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaUserTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/PodManagementPolicy.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/StatefulSetTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/DeploymentStrategy.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/DeploymentTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/InternalServiceTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/IpFamily.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/IpFamilyPolicy.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMakerTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/MetadataTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/PodDisruptionBudgetTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/PodTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ResourceTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/BuildConfigTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/EntityOperatorTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransQueryTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaBridgeTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaExporterTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2Template.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/tracing/class-use/JaegerTracing.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/tracing/class-use/Tracing.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractConnectorSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ExternalConfigurationReference.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxPrometheusExporterMetrics.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationCustom.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeAdminClientSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserTlsExternalClientAuthentication.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/PasswordSource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/StrimziPodSet.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/StrimziPodSetSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/class-use/Crds.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/class-use/StrimziPodSetList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaStatusFluent.ListenersNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaStatusFluentImpl.ListenersNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/ListenerStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/ListenerStatusFluent.AddressesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/ListenerStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/ListenerStatusFluentImpl.AddressesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/ListenerStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/ConditionFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/ConditionFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/ConditionBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaBridgeStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaBridgeStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaBridgeStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaMirrorMaker2StatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaMirrorMaker2StatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaMirrorMaker2StatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaRebalanceStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaRebalanceStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaRebalanceStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaUserStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaUserStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaUserStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaConnectorStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaConnectorStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaConnectorStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/StatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/StatusFluent.ConditionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/StatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/StatusFluentImpl.ConditionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaTopicStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaTopicStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaTopicStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaMirrorMakerStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaMirrorMakerStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaMirrorMakerStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/ListenerAddressFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/ListenerAddressFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/ListenerAddressBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaConnectStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaConnectStatusFluent.ConnectorPluginsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaConnectStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaConnectStatusFluentImpl.ConnectorPluginsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaConnectStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/StrimziPodSetStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/StrimziPodSetStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/StrimziPodSetStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ProbeFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ProbeFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ProbeBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2SpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2SpecFluent.MirrorsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2SpecFluent.ClustersNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2SpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2SpecFluentImpl.MirrorsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2SpecFluentImpl.ClustersNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2SpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/TgzArtifactFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/TgzArtifactFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/TgzArtifactBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/BuildFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/BuildFluent.PluginsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/BuildFluent.DockerOutputNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/BuildFluent.ImageStreamOutputNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/BuildFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/BuildFluentImpl.PluginsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/BuildFluentImpl.DockerOutputNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/BuildFluentImpl.ImageStreamOutputNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/BuildBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/DockerOutputFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/DockerOutputFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/DockerOutputBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/MavenArtifactFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/MavenArtifactFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/MavenArtifactBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/DownloadableArtifactFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/DownloadableArtifactFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/JarArtifactFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/JarArtifactFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/JarArtifactBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/OtherArtifactFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/OtherArtifactFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/OtherArtifactBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/ZipArtifactFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/ZipArtifactFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/ZipArtifactBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/ImageStreamOutputFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/ImageStreamOutputFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/ImageStreamOutputBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/PluginFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/PluginFluent.TgzArtifactArtifactsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/PluginFluent.JarArtifactArtifactsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/PluginFluent.OtherArtifactArtifactsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/PluginFluent.ZipArtifactArtifactsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/PluginFluent.MavenArtifactArtifactsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/PluginFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/PluginFluentImpl.TgzArtifactArtifactsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/PluginFluentImpl.JarArtifactArtifactsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/PluginFluentImpl.OtherArtifactArtifactsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/PluginFluentImpl.ZipArtifactArtifactsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/PluginFluentImpl.MavenArtifactArtifactsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/PluginBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationVolumeSourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationVolumeSourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationVolumeSourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ConnectorPluginFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ConnectorPluginFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ConnectorPluginBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationEnvVarSourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationEnvVarSourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationEnvVarSourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationFluent.VolumesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationFluent.EnvNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationFluentImpl.VolumesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationFluentImpl.EnvNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationEnvFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationEnvFluent.ValueFromNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationEnvFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationEnvFluentImpl.ValueFromNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationEnvBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationScramSha512Fluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationScramSha512FluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationScramSha512Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationOAuthFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationOAuthFluent.TlsTrustedCertificatesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationOAuthFluent.ClientSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationOAuthFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationOAuthFluentImpl.TlsTrustedCertificatesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationOAuthFluentImpl.ClientSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationOAuthBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationFluent.BrokersNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationFluent.BootstrapNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationFluent.BrokerCertChainAndKeyNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationFluentImpl.BrokersNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationFluentImpl.BootstrapNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationFluentImpl.BrokerCertChainAndKeyNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationBootstrapFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationBootstrapFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationBootstrapBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationBrokerFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationBrokerFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationBrokerBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerFluent.ConfigurationNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerFluent.KafkaListenerAuthenticationCustomAuthNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerFluent.KafkaListenerAuthenticationScramSha512AuthNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerFluent.KafkaListenerAuthenticationTlsAuthNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerFluent.KafkaListenerAuthenticationOAuthNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerFluentImpl.ConfigurationNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerFluentImpl.KafkaListenerAuthenticationCustomAuthNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerFluentImpl.KafkaListenerAuthenticationScramSha512AuthNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerFluentImpl.KafkaListenerAuthenticationTlsAuthNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerFluentImpl.KafkaListenerAuthenticationOAuthNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationCustomFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationCustomFluent.SecretsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationCustomFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationCustomFluentImpl.SecretsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationCustomBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationTlsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationTlsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationTlsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.JaegerTracingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.InlineLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.ExternalLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.JvmOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.AdminClientNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.ProducerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.ConsumerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.KafkaClientAuthenticationTlsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.KafkaClientAuthenticationOAuthNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.KafkaClientAuthenticationPlainNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.KafkaClientAuthenticationScramSha256Nested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.KafkaClientAuthenticationScramSha512Nested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.TlsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.HttpNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.JaegerTracingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.InlineLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.ExternalLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.JvmOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.AdminClientNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.ProducerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.ConsumerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.KafkaClientAuthenticationTlsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.KafkaClientAuthenticationOAuthNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.KafkaClientAuthenticationPlainNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.KafkaClientAuthenticationScramSha256NestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.KafkaClientAuthenticationScramSha512NestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.TlsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.HttpNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserAuthorizationSimpleFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserAuthorizationSimpleFluent.AclsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserAuthorizationSimpleFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserAuthorizationSimpleFluentImpl.AclsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserAuthorizationSimpleBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/StrimziPodSetSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/StrimziPodSetSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/StrimziPodSetSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/EntityOperatorTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/EntityOperatorTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/EntityOperatorTemplateFluent.TlsSidecarContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/EntityOperatorTemplateFluent.UserOperatorContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/EntityOperatorTemplateFluent.TopicOperatorContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/EntityOperatorTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/EntityOperatorTemplateFluent.DeploymentNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/EntityOperatorTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/EntityOperatorTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/EntityOperatorTemplateFluentImpl.TlsSidecarContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/EntityOperatorTemplateFluentImpl.UserOperatorContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/EntityOperatorTemplateFluentImpl.TopicOperatorContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/EntityOperatorTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/EntityOperatorTemplateFluentImpl.DeploymentNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/EntityOperatorTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/PodTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/PodTemplateFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/PodTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/PodTemplateFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/PodTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/PodDisruptionBudgetTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/PodDisruptionBudgetTemplateFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/PodDisruptionBudgetTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/PodDisruptionBudgetTemplateFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/PodDisruptionBudgetTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransOutputDefinitionTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransOutputDefinitionTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransOutputDefinitionTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/StatefulSetTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/StatefulSetTemplateFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/StatefulSetTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/StatefulSetTemplateFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/StatefulSetTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateFluent.TlsSidecarContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateFluent.CruiseControlContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateFluent.PodDisruptionBudgetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateFluent.ApiServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateFluent.DeploymentNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateFluentImpl.TlsSidecarContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateFluentImpl.CruiseControlContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateFluentImpl.PodDisruptionBudgetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateFluentImpl.ApiServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateFluentImpl.DeploymentNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/MetadataTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/MetadataTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/MetadataTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaExporterTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaExporterTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaExporterTemplateFluent.ContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaExporterTemplateFluent.ServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaExporterTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaExporterTemplateFluent.DeploymentNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaExporterTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaExporterTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaExporterTemplateFluentImpl.ContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaExporterTemplateFluentImpl.ServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaExporterTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaExporterTemplateFluentImpl.DeploymentNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaExporterTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.JmxSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.ClusterRoleBindingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.InitContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.KafkaContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.PodDisruptionBudgetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.ClusterCaCertNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.PersistentVolumeClaimNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.PerPodIngressNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.ExternalBootstrapIngressNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.PerPodRouteNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.ExternalBootstrapRouteNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.PerPodServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.ExternalBootstrapServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.BrokersServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.BootstrapServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.PodSetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.StatefulsetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.JmxSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.ClusterRoleBindingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.InitContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.KafkaContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.PodDisruptionBudgetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.ClusterCaCertNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.PersistentVolumeClaimNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.PerPodIngressNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.ExternalBootstrapIngressNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.PerPodRouteNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.ExternalBootstrapRouteNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.PerPodServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.ExternalBootstrapServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.BrokersServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.BootstrapServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.PodSetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.StatefulsetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMakerTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMakerTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMakerTemplateFluent.MirrorMakerContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMakerTemplateFluent.PodDisruptionBudgetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMakerTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMakerTemplateFluent.DeploymentNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMakerTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMakerTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMakerTemplateFluentImpl.MirrorMakerContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMakerTemplateFluentImpl.PodDisruptionBudgetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMakerTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMakerTemplateFluentImpl.DeploymentNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMakerTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ContainerTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ContainerTemplateFluent.EnvNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ContainerTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ContainerTemplateFluentImpl.EnvNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ContainerTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluent.JmxSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluent.BuildServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluent.ClusterRoleBindingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluent.BuildConfigNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluent.BuildContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluent.InitContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluent.ConnectContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluent.PodDisruptionBudgetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluent.ApiServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluent.BuildPodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluent.DeploymentNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluentImpl.JmxSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluentImpl.BuildServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluentImpl.ClusterRoleBindingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluentImpl.BuildConfigNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluentImpl.BuildContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluentImpl.InitContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluentImpl.ConnectContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluentImpl.PodDisruptionBudgetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluentImpl.ApiServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluentImpl.BuildPodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluentImpl.DeploymentNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaUserTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaUserTemplateFluent.SecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaUserTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaUserTemplateFluentImpl.SecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaUserTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransQueryTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransQueryTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransQueryTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaBridgeTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaBridgeTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaBridgeTemplateFluent.BridgeContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaBridgeTemplateFluent.PodDisruptionBudgetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaBridgeTemplateFluent.ApiServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaBridgeTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaBridgeTemplateFluent.DeploymentNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaBridgeTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaBridgeTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaBridgeTemplateFluentImpl.BridgeContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaBridgeTemplateFluentImpl.PodDisruptionBudgetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaBridgeTemplateFluentImpl.ApiServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaBridgeTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaBridgeTemplateFluentImpl.DeploymentNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaBridgeTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/InternalServiceTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/InternalServiceTemplateFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/InternalServiceTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/InternalServiceTemplateFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/InternalServiceTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ResourceTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ResourceTemplateFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ResourceTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ResourceTemplateFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ResourceTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransTemplateFluent.ContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransTemplateFluent.DeploymentNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransTemplateFluentImpl.ContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransTemplateFluentImpl.DeploymentNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluent.JmxSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluent.ZookeeperContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluent.PodDisruptionBudgetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluent.PersistentVolumeClaimNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluent.NodesServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluent.ClientServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluent.PodSetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluent.StatefulsetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluentImpl.JmxSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluentImpl.ZookeeperContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluentImpl.PodDisruptionBudgetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluentImpl.PersistentVolumeClaimNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluentImpl.NodesServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluentImpl.ClientServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluentImpl.PodSetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluentImpl.StatefulsetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/DeploymentTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/DeploymentTemplateFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/DeploymentTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/DeploymentTemplateFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/DeploymentTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/BuildConfigTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/BuildConfigTemplateFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/BuildConfigTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/BuildConfigTemplateFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/BuildConfigTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateFluent.JmxSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateFluent.MirrorMaker2ContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateFluent.PodDisruptionBudgetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateFluent.ApiServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateFluent.DeploymentNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateFluentImpl.JmxSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateFluentImpl.MirrorMaker2ContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateFluentImpl.PodDisruptionBudgetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateFluentImpl.ApiServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateFluentImpl.DeploymentNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserSpecFluent.QuotasNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserSpecFluent.KafkaUserAuthorizationSimpleNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserSpecFluent.KafkaUserScramSha512ClientAuthenticationNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserSpecFluent.KafkaUserTlsExternalClientAuthenticationNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserSpecFluent.KafkaUserTlsClientAuthenticationNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserSpecFluentImpl.QuotasNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserSpecFluentImpl.KafkaUserAuthorizationSimpleNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserSpecFluentImpl.KafkaUserScramSha512ClientAuthenticationNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserSpecFluentImpl.KafkaUserTlsExternalClientAuthenticationNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserSpecFluentImpl.KafkaUserTlsClientAuthenticationNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaJmxOptionsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaJmxOptionsFluent.KafkaJmxAuthenticationPasswordNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaJmxOptionsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaJmxOptionsFluentImpl.KafkaJmxAuthenticationPasswordNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaJmxOptionsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/JbodStorageFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/JbodStorageFluent.EphemeralStorageVolumesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/JbodStorageFluent.PersistentClaimStorageVolumesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/JbodStorageFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/JbodStorageFluentImpl.EphemeralStorageVolumesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/JbodStorageFluentImpl.PersistentClaimStorageVolumesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/JbodStorageBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/PersistentClaimStorageOverrideFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/PersistentClaimStorageOverrideFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/PersistentClaimStorageOverrideBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/EphemeralStorageFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/EphemeralStorageFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/EphemeralStorageBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/PersistentClaimStorageFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/PersistentClaimStorageFluent.OverridesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/PersistentClaimStorageFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/PersistentClaimStorageFluentImpl.OverridesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/PersistentClaimStorageBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/RackFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/RackFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/RackBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/TlsClientAuthenticationFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/TlsClientAuthenticationFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/TlsClientAuthenticationBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleFluent.AclRuleClusterResourceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleFluent.AclRuleTransactionalIdResourceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleFluent.AclRuleTopicResourceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleFluent.AclRuleGroupResourceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleFluentImpl.AclRuleClusterResourceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleFluentImpl.AclRuleTransactionalIdResourceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleFluentImpl.AclRuleTopicResourceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleFluentImpl.AclRuleGroupResourceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2Fluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2Fluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2Fluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2Fluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2FluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2FluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2FluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2FluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ClientTlsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ClientTlsFluent.TrustedCertificatesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ClientTlsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ClientTlsFluentImpl.TrustedCertificatesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ClientTlsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectorSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectorSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectorSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityUserOperatorSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityUserOperatorSpecFluent.JvmOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityUserOperatorSpecFluent.InlineLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityUserOperatorSpecFluent.ExternalLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityUserOperatorSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityUserOperatorSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityUserOperatorSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityUserOperatorSpecFluentImpl.JvmOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityUserOperatorSpecFluentImpl.InlineLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityUserOperatorSpecFluentImpl.ExternalLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityUserOperatorSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityUserOperatorSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityUserOperatorSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaTopicFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaTopicFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaTopicFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaTopicFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaTopicFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaTopicFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaTopicFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaTopicFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaTopicBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserTlsClientAuthenticationFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserTlsClientAuthenticationFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserTlsClientAuthenticationBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeHttpConfigFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeHttpConfigFluent.CorsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeHttpConfigFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeHttpConfigFluentImpl.CorsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeHttpConfigBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxPrometheusExporterMetricsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxPrometheusExporterMetricsFluent.ValueFromNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxPrometheusExporterMetricsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxPrometheusExporterMetricsFluentImpl.ValueFromNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxPrometheusExporterMetricsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/SystemPropertyFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/SystemPropertyFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/SystemPropertyBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaTopicSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaTopicSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaTopicSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluent.JmxPrometheusExporterMetricsConfigNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluent.BrokerCapacityNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluent.InlineLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluent.ExternalLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluent.JvmOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluent.TlsSidecarNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluentImpl.JmxPrometheusExporterMetricsConfigNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluentImpl.BrokerCapacityNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluentImpl.InlineLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluentImpl.ExternalLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluentImpl.JvmOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluentImpl.TlsSidecarNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserTlsExternalClientAuthenticationFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserTlsExternalClientAuthenticationFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserTlsExternalClientAuthenticationBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluent.JmxPrometheusExporterMetricsConfigNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluent.JmxOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluent.JvmOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluent.InlineLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluent.ExternalLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluent.EphemeralStorageNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluent.PersistentClaimStorageNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluentImpl.JmxPrometheusExporterMetricsConfigNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluentImpl.JmxOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluentImpl.JvmOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluentImpl.InlineLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluentImpl.ExternalLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluentImpl.EphemeralStorageNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluentImpl.PersistentClaimStorageNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/StrimziPodSetFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/StrimziPodSetFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/StrimziPodSetFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/StrimziPodSetFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/StrimziPodSetFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/StrimziPodSetFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/StrimziPodSetFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/StrimziPodSetFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/StrimziPodSetBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ConnectorSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ConnectorSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ConnectorSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/balancing/class-use/BrokerCapacityFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/balancing/class-use/BrokerCapacityFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/balancing/class-use/BrokerCapacityBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/PasswordSourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/tracing/class-use/JaegerTracingFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/tracing/class-use/JaegerTracingFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/tracing/class-use/JaegerTracingBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/PasswordSourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/PasswordSourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaRebalanceSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaRebalanceSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaRebalanceSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluent.ExternalConfigurationNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluent.JaegerTracingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluent.JmxPrometheusExporterMetricsConfigNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluent.JvmOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluent.JmxOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluent.InlineLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluent.ExternalLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluentImpl.ExternalConfigurationNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluentImpl.JaegerTracingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluentImpl.JmxPrometheusExporterMetricsConfigNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluentImpl.JvmOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluentImpl.JmxOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluentImpl.InlineLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluentImpl.ExternalLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaJmxAuthenticationPasswordFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaJmxAuthenticationPasswordFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaJmxAuthenticationPasswordBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2MirrorSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2MirrorSpecFluent.HeartbeatConnectorNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2MirrorSpecFluent.CheckpointConnectorNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2MirrorSpecFluent.SourceConnectorNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2MirrorSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2MirrorSpecFluentImpl.HeartbeatConnectorNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2MirrorSpecFluentImpl.CheckpointConnectorNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2MirrorSpecFluentImpl.SourceConnectorNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2MirrorSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationScramSha256Fluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationScramSha256Fluent.PasswordSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationScramSha256FluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationScramSha256FluentImpl.PasswordSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationScramSha256Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationOAuthFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationOAuthFluent.TlsTrustedCertificatesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationOAuthFluent.RefreshTokenNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationOAuthFluent.AccessTokenNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationOAuthFluent.ClientSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationOAuthFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationOAuthFluentImpl.TlsTrustedCertificatesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationOAuthFluentImpl.RefreshTokenNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationOAuthFluentImpl.AccessTokenNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationOAuthFluentImpl.ClientSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationOAuthBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationScramSha512Fluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationScramSha512Fluent.PasswordSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationScramSha512FluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationScramSha512FluentImpl.PasswordSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationScramSha512Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationPlainFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationPlainFluent.PasswordSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationPlainFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationPlainFluentImpl.PasswordSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationPlainBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationTlsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationTlsFluent.CertificateAndKeyNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationTlsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationTlsFluentImpl.CertificateAndKeyNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationTlsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/TlsSidecarFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/TlsSidecarFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/TlsSidecarFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/TlsSidecarFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/TlsSidecarFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/TlsSidecarFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/TlsSidecarBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CertificateAuthorityFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CertificateAuthorityFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CertificateAuthorityBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeConsumerSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeConsumerSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeConsumerSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluent.BuildNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluent.KafkaClientAuthenticationTlsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluent.KafkaClientAuthenticationOAuthNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluent.KafkaClientAuthenticationPlainNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluent.KafkaClientAuthenticationScramSha256Nested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluent.KafkaClientAuthenticationScramSha512Nested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluent.TlsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluent.RackNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluentImpl.BuildNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluentImpl.KafkaClientAuthenticationTlsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluentImpl.KafkaClientAuthenticationOAuthNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluentImpl.KafkaClientAuthenticationPlainNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluentImpl.KafkaClientAuthenticationScramSha256NestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluentImpl.KafkaClientAuthenticationScramSha512NestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluentImpl.TlsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluentImpl.RackNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleTopicResourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleTopicResourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleTopicResourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleGroupResourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleGroupResourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleGroupResourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractConnectorSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractConnectorSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityOperatorSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityOperatorSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityOperatorSpecFluent.TlsSidecarNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityOperatorSpecFluent.UserOperatorNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityOperatorSpecFluent.TopicOperatorNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityOperatorSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityOperatorSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityOperatorSpecFluentImpl.TlsSidecarNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityOperatorSpecFluentImpl.UserOperatorNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityOperatorSpecFluentImpl.TopicOperatorNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityOperatorSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeHttpCorsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeHttpCorsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeHttpCorsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityTopicOperatorSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityTopicOperatorSpecFluent.JvmOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityTopicOperatorSpecFluent.InlineLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityTopicOperatorSpecFluent.ExternalLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityTopicOperatorSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityTopicOperatorSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityTopicOperatorSpecFluent.StartupProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityTopicOperatorSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityTopicOperatorSpecFluentImpl.JvmOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityTopicOperatorSpecFluentImpl.InlineLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityTopicOperatorSpecFluentImpl.ExternalLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityTopicOperatorSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityTopicOperatorSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityTopicOperatorSpecFluentImpl.StartupProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityTopicOperatorSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/SidecarFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/SidecarFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/SidecarBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JvmOptionsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JvmOptionsFluent.JavaSystemPropertiesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JvmOptionsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JvmOptionsFluentImpl.JavaSystemPropertiesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JvmOptionsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaRebalanceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaRebalanceFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaRebalanceFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaRebalanceFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaRebalanceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaRebalanceFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaRebalanceFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaRebalanceFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaRebalanceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ClusterSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ClusterSpecFluent.KafkaClientAuthenticationTlsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ClusterSpecFluent.KafkaClientAuthenticationOAuthNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ClusterSpecFluent.KafkaClientAuthenticationPlainNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ClusterSpecFluent.KafkaClientAuthenticationScramSha256Nested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ClusterSpecFluent.KafkaClientAuthenticationScramSha512Nested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ClusterSpecFluent.TlsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ClusterSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ClusterSpecFluentImpl.KafkaClientAuthenticationTlsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ClusterSpecFluentImpl.KafkaClientAuthenticationOAuthNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ClusterSpecFluentImpl.KafkaClientAuthenticationPlainNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ClusterSpecFluentImpl.KafkaClientAuthenticationScramSha256NestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ClusterSpecFluentImpl.KafkaClientAuthenticationScramSha512NestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ClusterSpecFluentImpl.TlsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ClusterSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationOpaFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationOpaFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationOpaBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeProducerSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeProducerSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeProducerSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ExternalLoggingFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ExternalLoggingFluent.ValueFromNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ExternalLoggingFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ExternalLoggingFluentImpl.ValueFromNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ExternalLoggingBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/PasswordSecretSourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/PasswordSecretSourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/PasswordSecretSourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationCustomFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationCustomFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationCustomBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/PasswordFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/PasswordFluent.ValueFromNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/PasswordFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/PasswordFluentImpl.ValueFromNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/PasswordBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerClientSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerClientSpecFluent.KafkaClientAuthenticationTlsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerClientSpecFluent.KafkaClientAuthenticationOAuthNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerClientSpecFluent.KafkaClientAuthenticationPlainNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerClientSpecFluent.KafkaClientAuthenticationScramSha256Nested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerClientSpecFluent.KafkaClientAuthenticationScramSha512Nested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerClientSpecFluent.TlsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerClientSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerClientSpecFluentImpl.KafkaClientAuthenticationTlsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerClientSpecFluentImpl.KafkaClientAuthenticationOAuthNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerClientSpecFluentImpl.KafkaClientAuthenticationPlainNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerClientSpecFluentImpl.KafkaClientAuthenticationScramSha256NestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerClientSpecFluentImpl.KafkaClientAuthenticationScramSha512NestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerClientSpecFluentImpl.TlsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerClientSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserScramSha512ClientAuthenticationFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserScramSha512ClientAuthenticationFluent.PasswordNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserScramSha512ClientAuthenticationFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserScramSha512ClientAuthenticationFluentImpl.PasswordNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserScramSha512ClientAuthenticationBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CertAndKeySecretSourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CertAndKeySecretSourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CertAndKeySecretSourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerConsumerSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluent.ClientsCaNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluent.CruiseControlNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluent.KafkaExporterNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluent.JmxTransNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluent.ClusterCaNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluent.EntityOperatorNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluent.ZookeeperNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluent.KafkaNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerConsumerSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerConsumerSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CertSecretSourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CertSecretSourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CertSecretSourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluentImpl.ClientsCaNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluentImpl.CruiseControlNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluentImpl.KafkaExporterNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluentImpl.JmxTransNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluentImpl.ClusterCaNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluentImpl.EntityOperatorNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluentImpl.ZookeeperNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluentImpl.KafkaNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/InlineLoggingFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/InlineLoggingFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/InlineLoggingBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserQuotasFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserQuotasFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserQuotasBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ExternalConfigurationReferenceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ExternalConfigurationReferenceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ExternalConfigurationReferenceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationSimpleFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationSimpleFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationSimpleBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleTransactionalIdResourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleTransactionalIdResourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleTransactionalIdResourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerProducerSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerProducerSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerProducerSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/GenericSecretSourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/GenericSecretSourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/GenericSecretSourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxTransSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxTransSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxTransSpecFluent.KafkaQueriesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxTransSpecFluent.OutputDefinitionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxTransSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxTransSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxTransSpecFluentImpl.KafkaQueriesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxTransSpecFluentImpl.OutputDefinitionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxTransSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.KafkaAuthorizationSimpleNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.KafkaAuthorizationKeycloakNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.KafkaAuthorizationCustomNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.KafkaAuthorizationOpaNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.ListenersNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.JmxPrometheusExporterMetricsConfigNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.JmxOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.JvmOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.InlineLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.ExternalLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.RackNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.JbodStorageNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.EphemeralStorageNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.PersistentClaimStorageNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.KafkaAuthorizationSimpleNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.KafkaAuthorizationKeycloakNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.KafkaAuthorizationCustomNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.KafkaAuthorizationOpaNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.ListenersNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.JmxPrometheusExporterMetricsConfigNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.JmxOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.JvmOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.InlineLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.ExternalLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.RackNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.JbodStorageNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.EphemeralStorageNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.PersistentClaimStorageNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectorFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectorFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectorFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectorFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectorFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectorFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectorFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectorFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectorBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/SpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/SpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeClientSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeClientSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ContainerEnvVarFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ContainerEnvVarFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ContainerEnvVarBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationKeycloakFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationKeycloakFluent.TlsTrustedCertificatesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationKeycloakFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationKeycloakFluentImpl.TlsTrustedCertificatesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationKeycloakBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeAdminClientSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeAdminClientSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeAdminClientSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaExporterSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaExporterSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaExporterSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaExporterSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaExporterSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaExporterSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaExporterSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaExporterSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaExporterSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleClusterResourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleClusterResourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleClusterResourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluent.JaegerTracingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluent.JmxPrometheusExporterMetricsConfigNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluent.InlineLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluent.ExternalLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluent.JvmOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluent.ProducerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluent.ConsumerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluentImpl.JaegerTracingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluentImpl.JmxPrometheusExporterMetricsConfigNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluentImpl.InlineLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluentImpl.ExternalLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluentImpl.JvmOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluentImpl.ProducerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluentImpl.ConsumerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/balancing/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/tracing/package-use.html...
Building index for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/overview-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/index-all.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/allclasses-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/allpackages-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/deprecated-list.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/index.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/overview-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/help-doc.html...
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/api/target/api-0.29.0-SNAPSHOT-javadoc.jar
[INFO] 
[INFO] --- maven-jar-plugin:3.1.0:test-jar (default) @ api ---
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:integration-test (default) @ api ---
[WARNING] useSystemClassLoader setting has no effect when not forking
[WARNING] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:verify (default) @ api ---
[INFO] 
[INFO] --- maven-dependency-plugin:3.1.1:analyze-only (analyze) @ api ---
[INFO] No dependency problems found
[INFO] 
[INFO] ------------------------< io.strimzi:mockkube >-------------------------
[INFO] Building mockkube 0.29.0-SNAPSHOT                                 [6/10]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ mockkube ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ mockkube ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ mockkube ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/mockkube/target/jacoco.exec
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ mockkube ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/cloud-user/strimzi-kafka-operator/mockkube/src/main/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:compile (default-compile) @ mockkube ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ mockkube ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 1 resource
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:testCompile (default-testCompile) @ mockkube ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:3.0.0-M5:test (default-test) @ mockkube ---
[INFO] 
[INFO] -------------------------------------------------------
[INFO]  T E S T S
[INFO] -------------------------------------------------------
[INFO] 
[INFO] Results:
[INFO] 
[INFO] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[INFO] 
[INFO] 
[INFO] --- maven-jar-plugin:3.1.0:jar (default-jar) @ mockkube ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:report (report) @ mockkube ---
[INFO] Skipping JaCoCo execution due to missing execution data file.
[INFO] 
[INFO] >>> maven-source-plugin:3.0.1:jar (attach-sources) > generate-sources @ mockkube >>>
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ mockkube ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ mockkube ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ mockkube ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/mockkube/target/jacoco.exec
[INFO] 
[INFO] <<< maven-source-plugin:3.0.1:jar (attach-sources) < generate-sources @ mockkube <<<
[INFO] 
[INFO] 
[INFO] --- maven-source-plugin:3.0.1:jar (attach-sources) @ mockkube ---
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/mockkube/target/mockkube-0.29.0-SNAPSHOT-sources.jar
[INFO] 
[INFO] --- maven-javadoc-plugin:3.1.0:jar (attach-javadocs) @ mockkube ---
[INFO] 
Loading source file /home/cloud-user/strimzi-kafka-operator/mockkube/src/main/java/io/strimzi/test/mockkube/Observer.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/mockkube/src/main/java/io/strimzi/test/mockkube/PredicatedWatcher.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/mockkube/src/main/java/io/strimzi/test/mockkube/CustomResourceMockBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/mockkube/src/main/java/io/strimzi/test/mockkube/ServiceMockBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/mockkube/src/main/java/io/strimzi/test/mockkube/DeploymentMockBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/mockkube/src/main/java/io/strimzi/test/mockkube/MockBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/mockkube/src/main/java/io/strimzi/test/mockkube/MockKube.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/mockkube/src/main/java/io/strimzi/test/mockkube/StatefulSetMockBuilder.java...
Constructing Javadoc information...
Standard Doclet version 11.0.11
Building tree for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/io/strimzi/test/mockkube/MockKube.html...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/io/strimzi/test/mockkube/MockKube.MockedCrd.html...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/io/strimzi/test/mockkube/Observer.html...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/io/strimzi/test/mockkube/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/io/strimzi/test/mockkube/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/constant-values.html...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/io/strimzi/test/mockkube/class-use/Observer.html...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/io/strimzi/test/mockkube/class-use/MockKube.html...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/io/strimzi/test/mockkube/class-use/MockKube.MockedCrd.html...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/io/strimzi/test/mockkube/package-use.html...
Building index for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/overview-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/index-all.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/allclasses-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/allpackages-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/deprecated-list.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/index.html...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/help-doc.html...
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/mockkube/target/mockkube-0.29.0-SNAPSHOT-javadoc.jar
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:integration-test (default) @ mockkube ---
[WARNING] useSystemClassLoader setting has no effect when not forking
[WARNING] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:verify (default) @ mockkube ---
[INFO] 
[INFO] --- maven-dependency-plugin:3.1.1:analyze-only (analyze) @ mockkube ---
[INFO] No dependency problems found
[INFO] 
[INFO] ----------------------< io.strimzi:config-model >-----------------------
[INFO] Building config-model 0.29.0-SNAPSHOT                             [7/10]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ config-model ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ config-model ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ config-model ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/config-model/target/jacoco.exec
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ config-model ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/cloud-user/strimzi-kafka-operator/config-model/src/main/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:compile (default-compile) @ config-model ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ config-model ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/cloud-user/strimzi-kafka-operator/config-model/src/test/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:testCompile (default-testCompile) @ config-model ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:3.0.0-M5:test (default-test) @ config-model ---
[INFO] 
[INFO] -------------------------------------------------------
[INFO]  T E S T S
[INFO] -------------------------------------------------------
[INFO] 
[INFO] Results:
[INFO] 
[INFO] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[INFO] 
[INFO] 
[INFO] --- maven-jar-plugin:3.1.0:jar (default-jar) @ config-model ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:report (report) @ config-model ---
[INFO] Skipping JaCoCo execution due to missing execution data file.
[INFO] 
[INFO] >>> maven-source-plugin:3.0.1:jar (attach-sources) > generate-sources @ config-model >>>
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ config-model ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ config-model ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ config-model ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/config-model/target/jacoco.exec
[INFO] 
[INFO] <<< maven-source-plugin:3.0.1:jar (attach-sources) < generate-sources @ config-model <<<
[INFO] 
[INFO] 
[INFO] --- maven-source-plugin:3.0.1:jar (attach-sources) @ config-model ---
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/config-model/target/config-model-0.29.0-SNAPSHOT-sources.jar
[INFO] 
[INFO] --- maven-javadoc-plugin:3.1.0:jar (attach-javadocs) @ config-model ---
[INFO] 
Loading source file /home/cloud-user/strimzi-kafka-operator/config-model/src/main/java/io/strimzi/kafka/config/model/ConfigModel.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/config-model/src/main/java/io/strimzi/kafka/config/model/ConfigModels.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/config-model/src/main/java/io/strimzi/kafka/config/model/Scope.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/config-model/src/main/java/io/strimzi/kafka/config/model/Type.java...
Constructing Javadoc information...
Standard Doclet version 11.0.11
Building tree for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/io/strimzi/kafka/config/model/ConfigModel.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/io/strimzi/kafka/config/model/ConfigModels.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/io/strimzi/kafka/config/model/Scope.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/io/strimzi/kafka/config/model/Type.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/io/strimzi/kafka/config/model/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/io/strimzi/kafka/config/model/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/constant-values.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/io/strimzi/kafka/config/model/class-use/ConfigModel.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/io/strimzi/kafka/config/model/class-use/ConfigModels.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/io/strimzi/kafka/config/model/class-use/Scope.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/io/strimzi/kafka/config/model/class-use/Type.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/io/strimzi/kafka/config/model/package-use.html...
Building index for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/overview-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/index-all.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/allclasses-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/allpackages-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/deprecated-list.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/index.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/help-doc.html...
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/config-model/target/config-model-0.29.0-SNAPSHOT-javadoc.jar
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:integration-test (default) @ config-model ---
[WARNING] useSystemClassLoader setting has no effect when not forking
[WARNING] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:verify (default) @ config-model ---
[INFO] 
[INFO] --- maven-dependency-plugin:3.1.1:analyze-only (analyze) @ config-model ---
[INFO] No dependency problems found
[INFO] 
[INFO] -------------------< io.strimzi:certificate-manager >-------------------
[INFO] Building certificate-manager 0.29.0-SNAPSHOT                      [8/10]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ certificate-manager ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ certificate-manager ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ certificate-manager ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/certificate-manager/target/jacoco.exec
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ certificate-manager ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 1 resource
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:compile (default-compile) @ certificate-manager ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ certificate-manager ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 1 resource
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:testCompile (default-testCompile) @ certificate-manager ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:3.0.0-M5:test (default-test) @ certificate-manager ---
[INFO] 
[INFO] -------------------------------------------------------
[INFO]  T E S T S
[INFO] -------------------------------------------------------
[INFO] 
[INFO] Results:
[INFO] 
[INFO] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[INFO] 
[INFO] 
[INFO] --- maven-jar-plugin:3.1.0:jar (default-jar) @ certificate-manager ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:report (report) @ certificate-manager ---
[INFO] Skipping JaCoCo execution due to missing execution data file.
[INFO] 
[INFO] >>> maven-source-plugin:3.0.1:jar (attach-sources) > generate-sources @ certificate-manager >>>
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ certificate-manager ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ certificate-manager ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ certificate-manager ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/certificate-manager/target/jacoco.exec
[INFO] 
[INFO] <<< maven-source-plugin:3.0.1:jar (attach-sources) < generate-sources @ certificate-manager <<<
[INFO] 
[INFO] 
[INFO] --- maven-source-plugin:3.0.1:jar (attach-sources) @ certificate-manager ---
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/certificate-manager-0.29.0-SNAPSHOT-sources.jar
[INFO] 
[INFO] --- maven-javadoc-plugin:3.1.0:jar (attach-javadocs) @ certificate-manager ---
[INFO] 
Loading source file /home/cloud-user/strimzi-kafka-operator/certificate-manager/src/main/java/io/strimzi/certs/CertAndKey.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/certificate-manager/src/main/java/io/strimzi/certs/CertManager.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/certificate-manager/src/main/java/io/strimzi/certs/OpenSslCertManager.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/certificate-manager/src/main/java/io/strimzi/certs/SecretCertProvider.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/certificate-manager/src/main/java/io/strimzi/certs/Subject.java...
Constructing Javadoc information...
Standard Doclet version 11.0.11
Building tree for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/io/strimzi/certs/CertAndKey.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/io/strimzi/certs/CertManager.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/io/strimzi/certs/OpenSslCertManager.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/io/strimzi/certs/SecretCertProvider.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/io/strimzi/certs/Subject.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/io/strimzi/certs/Subject.Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/io/strimzi/certs/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/io/strimzi/certs/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/constant-values.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/io/strimzi/certs/class-use/CertAndKey.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/io/strimzi/certs/class-use/CertManager.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/io/strimzi/certs/class-use/OpenSslCertManager.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/io/strimzi/certs/class-use/SecretCertProvider.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/io/strimzi/certs/class-use/Subject.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/io/strimzi/certs/class-use/Subject.Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/io/strimzi/certs/package-use.html...
Building index for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/overview-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/index-all.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/allclasses-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/allpackages-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/deprecated-list.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/index.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/help-doc.html...
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/certificate-manager-0.29.0-SNAPSHOT-javadoc.jar
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:integration-test (default) @ certificate-manager ---
[WARNING] useSystemClassLoader setting has no effect when not forking
[WARNING] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:verify (default) @ certificate-manager ---
[INFO] 
[INFO] --- maven-dependency-plugin:3.1.1:analyze-only (analyze) @ certificate-manager ---
[INFO] No dependency problems found
[INFO] 
[INFO] ---------------------< io.strimzi:operator-common >---------------------
[INFO] Building operator-common 0.29.0-SNAPSHOT                          [9/10]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ operator-common ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ operator-common ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ operator-common ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/operator-common/target/jacoco.exec
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ operator-common ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:compile (default-compile) @ operator-common ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ operator-common ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 9 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:testCompile (default-testCompile) @ operator-common ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:3.0.0-M5:test (default-test) @ operator-common ---
[INFO] 
[INFO] -------------------------------------------------------
[INFO]  T E S T S
[INFO] -------------------------------------------------------
[INFO] 
[INFO] Results:
[INFO] 
[INFO] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[INFO] 
[INFO] 
[INFO] --- maven-jar-plugin:3.1.0:jar (default-jar) @ operator-common ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:report (report) @ operator-common ---
[INFO] Skipping JaCoCo execution due to missing execution data file.
[INFO] 
[INFO] >>> maven-source-plugin:3.0.1:jar (attach-sources) > generate-sources @ operator-common >>>
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ operator-common ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ operator-common ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ operator-common ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/operator-common/target/jacoco.exec
[INFO] 
[INFO] <<< maven-source-plugin:3.0.1:jar (attach-sources) < generate-sources @ operator-common <<<
[INFO] 
[INFO] 
[INFO] --- maven-source-plugin:3.0.1:jar (attach-sources) @ operator-common ---
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/operator-common/target/operator-common-0.29.0-SNAPSHOT-sources.jar
[INFO] 
[INFO] --- maven-javadoc-plugin:3.1.0:jar (attach-javadocs) @ operator-common ---
[INFO] 
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/cluster/model/InvalidResourceException.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/cluster/model/NoSuchResourceException.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/cluster/model/Ca.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/cluster/model/ClientsCa.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/cluster/model/NodeUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/cluster/model/StatusDiff.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/cluster/operator/resource/cruisecontrol/CruiseControlEndpoints.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/cluster/operator/resource/cruisecontrol/CruiseControlUserTaskStatus.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/cluster/operator/resource/cruisecontrol/CruiseControlGoals.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/cluster/operator/resource/cruisecontrol/CruiseControlConfigurationParameters.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/cluster/operator/resource/cruisecontrol/CruiseControlLoadParameters.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/cluster/operator/resource/cruisecontrol/CruiseControlParameters.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/cluster/operator/resource/cruisecontrol/CruiseControlRebalanceKeys.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/AdminClientProvider.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/BackOff.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/InvalidConfigurationException.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/MaxAttemptsExceededException.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/MetricsProvider.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/PasswordGenerator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/model/NamespaceAndName.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/model/Labels.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/model/OrderedProperties.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/model/ResourceVisitor.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/model/ValidationVisitor.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/DeploymentOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/IngressV1Beta1Operator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/PodDisruptionBudgetOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/PvcOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/ResourceSupport.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/ServiceAccountOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/StatusUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/AbstractNonNamespacedResourceOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/AbstractScalableResourceOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/ClusterRoleOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/ReconcileResult.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/CrdOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/TimeoutException.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/AbstractJsonDiff.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/AbstractResourceOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/AbstractWatchableStatusedResourceOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/BuildOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/EndpointOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/ImageStreamOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/NetworkPolicyOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/NoStackTraceTimeoutException.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/RoleBindingOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/RoleOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/SecretOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/BuildConfigOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/ClusterRoleBindingOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/ConfigMapOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/DeploymentConfigOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/IngressOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/NodeOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/PodOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/ResourceDiff.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/RouteOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/ServiceOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/StorageClassOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/AbstractReadyResourceOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/AbstractWatchableResourceOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/PodDisruptionBudgetV1Beta1Operator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/process/ProcessHelper.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/MetricsAndLogging.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/ReconciliationException.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/AbstractOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/Annotations.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/DefaultAdminClientProvider.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/InvalidConfigParameterException.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/MicrometerMetricsProvider.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/Operator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/OperatorWatcher.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/Reconciliation.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/ReconciliationLogger.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/Util.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/KubernetesVersion.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/PlatformFeaturesAvailability.java...
Constructing Javadoc information...
Standard Doclet version 11.0.11
Building tree for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/AbstractOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/AdminClientProvider.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/Annotations.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/BackOff.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/DefaultAdminClientProvider.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/InvalidConfigParameterException.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/InvalidConfigurationException.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/MaxAttemptsExceededException.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/MetricsAndLogging.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/MetricsProvider.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/MicrometerMetricsProvider.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/Operator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/PasswordGenerator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/Reconciliation.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/ReconciliationException.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/ReconciliationLogger.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/Util.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/CruiseControlConfigurationParameters.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/CruiseControlEndpoints.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/CruiseControlGoals.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/CruiseControlLoadParameters.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/CruiseControlParameters.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/CruiseControlRebalanceKeys.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/CruiseControlUserTaskStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/model/Ca.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/model/ClientsCa.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/model/InvalidResourceException.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/model/NodeUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/model/NoSuchResourceException.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/model/StatusDiff.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/Labels.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/NamespaceAndName.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/OrderedProperties.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/ResourceVisitor.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/ResourceVisitor.Property.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/ResourceVisitor.Visitor.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/ValidationVisitor.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/KubernetesVersion.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/PlatformFeaturesAvailability.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/process/ProcessHelper.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/process/ProcessHelper.ProcessResult.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/AbstractJsonDiff.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/AbstractNonNamespacedResourceOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/AbstractReadyResourceOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/AbstractResourceOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/AbstractScalableResourceOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/AbstractWatchableResourceOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/AbstractWatchableStatusedResourceOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/BuildConfigOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/BuildOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/ClusterRoleBindingOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/ClusterRoleOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/ConfigMapOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/CrdOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/DeploymentConfigOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/DeploymentOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/EndpointOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/ImageStreamOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/IngressOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/IngressV1Beta1Operator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/NetworkPolicyOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/NodeOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/NoStackTraceTimeoutException.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/PodDisruptionBudgetOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/PodDisruptionBudgetV1Beta1Operator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/PodOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/PvcOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/ReconcileResult.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/ReconcileResult.Created.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/ReconcileResult.Noop.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/ReconcileResult.Patched.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/ResourceSupport.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/RoleBindingOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/RoleOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/RouteOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/SecretOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/ServiceAccountOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/ServiceOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/StatusUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/StorageClassOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/TimeoutException.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/model/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/model/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/process/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/process/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/constant-values.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/serialized-form.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/model/class-use/InvalidResourceException.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/model/class-use/NoSuchResourceException.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/model/class-use/Ca.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/model/class-use/ClientsCa.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/model/class-use/NodeUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/model/class-use/StatusDiff.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/class-use/CruiseControlEndpoints.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/class-use/CruiseControlUserTaskStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/class-use/CruiseControlGoals.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/class-use/CruiseControlConfigurationParameters.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/class-use/CruiseControlLoadParameters.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/class-use/CruiseControlParameters.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/class-use/CruiseControlRebalanceKeys.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/AdminClientProvider.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/BackOff.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/InvalidConfigurationException.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/MaxAttemptsExceededException.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/MetricsProvider.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/PasswordGenerator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/class-use/NamespaceAndName.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/class-use/Labels.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/class-use/OrderedProperties.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/class-use/ResourceVisitor.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/class-use/ResourceVisitor.Property.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/class-use/ResourceVisitor.Visitor.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/class-use/ValidationVisitor.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/DeploymentOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/IngressV1Beta1Operator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/PodDisruptionBudgetOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/PvcOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/ResourceSupport.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/ServiceAccountOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/StatusUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/AbstractNonNamespacedResourceOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/AbstractScalableResourceOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/ClusterRoleOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/ReconcileResult.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/ReconcileResult.Patched.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/ReconcileResult.Created.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/ReconcileResult.Noop.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/CrdOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/TimeoutException.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/AbstractJsonDiff.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/AbstractResourceOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/AbstractWatchableStatusedResourceOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/BuildOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/EndpointOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/ImageStreamOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/NetworkPolicyOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/NoStackTraceTimeoutException.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/RoleBindingOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/RoleOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/SecretOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/BuildConfigOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/ClusterRoleBindingOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/ConfigMapOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/DeploymentConfigOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/IngressOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/NodeOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/PodOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/RouteOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/ServiceOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/StorageClassOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/AbstractReadyResourceOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/AbstractWatchableResourceOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/PodDisruptionBudgetV1Beta1Operator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/process/class-use/ProcessHelper.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/process/class-use/ProcessHelper.ProcessResult.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/MetricsAndLogging.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/ReconciliationException.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/AbstractOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/Annotations.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/DefaultAdminClientProvider.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/InvalidConfigParameterException.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/MicrometerMetricsProvider.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/Operator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/Reconciliation.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/ReconciliationLogger.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/Util.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/class-use/KubernetesVersion.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/class-use/PlatformFeaturesAvailability.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/model/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/process/package-use.html...
Building index for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/overview-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/index-all.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/allclasses-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/allpackages-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/deprecated-list.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/index.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/overview-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/help-doc.html...
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/operator-common/target/operator-common-0.29.0-SNAPSHOT-javadoc.jar
[INFO] 
[INFO] --- maven-jar-plugin:3.1.0:test-jar (default) @ operator-common ---
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:integration-test (default) @ operator-common ---
[WARNING] useSystemClassLoader setting has no effect when not forking
[WARNING] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:verify (default) @ operator-common ---
[INFO] 
[INFO] --- maven-dependency-plugin:3.1.1:analyze-only (analyze) @ operator-common ---
[INFO] No dependency problems found
[INFO] 
[INFO] -----------------------< io.strimzi:systemtest >------------------------
[INFO] Building systemtest 0.29.0-SNAPSHOT                              [10/10]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ systemtest ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ systemtest ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ systemtest ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/systemtest/target/jacoco.exec
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ systemtest ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 3 resources
[INFO] Copying 2 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:compile (default-compile) @ systemtest ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ systemtest ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 32 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:testCompile (default-testCompile) @ systemtest ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:3.0.0-M5:test (default-test) @ systemtest ---
[INFO] 
[INFO] -------------------------------------------------------
[INFO]  T E S T S
[INFO] -------------------------------------------------------
[INFO] 
[INFO] Results:
[INFO] 
[INFO] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[INFO] 
[WARNING] Corrupted STDOUT by directly writing to native stream in forked JVM 1. See FAQ web page and the dump file /home/cloud-user/strimzi-kafka-operator/systemtest/target/surefire-reports/2022-03-28T14-53-51_221-jvmRun1.dumpstream
[INFO] 
[INFO] --- maven-jar-plugin:3.1.0:jar (default-jar) @ systemtest ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:report (report) @ systemtest ---
[INFO] Skipping JaCoCo execution due to missing execution data file.
[INFO] 
[INFO] >>> maven-source-plugin:3.0.1:jar (attach-sources) > generate-sources @ systemtest >>>
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ systemtest ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ systemtest ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ systemtest ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/systemtest/target/jacoco.exec
[INFO] 
[INFO] <<< maven-source-plugin:3.0.1:jar (attach-sources) < generate-sources @ systemtest <<<
[INFO] 
[INFO] 
[INFO] --- maven-source-plugin:3.0.1:jar (attach-sources) @ systemtest ---
[INFO] 
[INFO] --- maven-javadoc-plugin:3.1.0:jar (attach-javadocs) @ systemtest ---
[INFO] 
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/annotations/MultiNodeClusterOnly.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/annotations/MultiNodeClusterOnlyCondition.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/annotations/OpenShiftOnly.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/annotations/RequiredMinKubeApiVersion.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/annotations/RequiredMinKubeApiVersionCondition.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/annotations/IsolatedTest.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/annotations/ParallelNamespaceTest.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/annotations/IsolatedSuite.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/annotations/OpenShiftOnlyCondition.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/annotations/ParallelSuite.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/annotations/ParallelTest.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/annotations/StatefulSetTest.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/annotations/StatefulSetTestCondition.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/annotations/StrimziPodSetTest.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/annotations/StrimziPodSetTestCondition.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/cli/KafkaCmdClient.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/enums/DefaultNetworkPolicy.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/enums/ClusterOperatorInstallType.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/enums/CustomResourceStatus.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/enums/OlmInstallationStrategy.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/enums/ClusterOperatorRBACType.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/enums/DeploymentTypes.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/k8s/Events.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/clientproperties/ProducerProperties.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/clientproperties/AbstractKafkaClientProperties.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/clientproperties/ConsumerProperties.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/AbstractKafkaClient.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/KafkaClientOperations.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/clients/ClientArgument.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/clients/ClientArgumentMap.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/clients/ClientType.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/clients/InternalKafkaClient.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/clients/VerifiableClient.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/externalClients/ExternalKafkaClient.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/internalClients/AdminClientOperations.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/internalClients/BaseClients.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/internalClients/BridgeClients.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/internalClients/KafkaAdminClients.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/internalClients/KafkaClients.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/internalClients/KafkaOauthClients.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/internalClients/KafkaTracingClients.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/keycloak/KeycloakInstance.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/listeners/ExecutionListener.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/listeners/OrderTestSuites.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/logs/LogCollector.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/logs/TestExecutionWatcher.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/matchers/HasAllOfReasons.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/matchers/HasAnyOfReasons.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/matchers/HasNoneOfReasons.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/matchers/Matchers.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/matchers/LogHasNoUnexpectedErrors.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/crd/KafkaBridgeResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/crd/KafkaConnectResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/crd/KafkaConnectorResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/crd/KafkaMirrorMaker2Resource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/crd/KafkaTopicResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/crd/KafkaClientsResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/crd/KafkaMirrorMakerResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/crd/KafkaRebalanceResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/crd/KafkaResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/crd/KafkaUserResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/crd/StrimziPodSetResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/operator/BundleResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/operator/specific/HelmResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/operator/specific/SpecificResourceType.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/operator/specific/OlmResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/operator/SetupClusterOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/ResourceCondition.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/ResourceItem.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/ResourceType.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/ThrowableRunner.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/kubernetes/ServiceResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/kubernetes/ClusterOperatorCustomResourceDefinition.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/kubernetes/ClusterRoleBindingResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/kubernetes/ClusterRoleResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/kubernetes/ConfigMapResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/kubernetes/JobResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/kubernetes/NetworkPolicyResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/kubernetes/RoleBindingResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/kubernetes/RoleResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/kubernetes/SecretResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/kubernetes/ServiceAccountResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/kubernetes/ValidatingWebhookConfigurationResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/kubernetes/DeploymentResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/ComponentType.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/ResourceManager.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/draincleaner/DrainCleanerResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/draincleaner/SetupDrainCleaner.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/ResourceOperation.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/security/CertAndKeyFiles.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/security/SystemTestCertAndKey.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/security/SystemTestCertAndKeyBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/security/SystemTestCertManager.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaBridgeUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaConnectUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaConnectorUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaMirrorMaker2Utils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaMirrorMakerUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaRebalanceUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUserUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaTopicUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kubeUtils/controllers/ConfigMapUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kubeUtils/controllers/DeploymentUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kubeUtils/controllers/JobUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kubeUtils/controllers/StatefulSetUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kubeUtils/controllers/StrimziPodSetUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kubeUtils/objects/NamespaceUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kubeUtils/objects/NodeUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kubeUtils/objects/PersistentVolumeClaimUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kubeUtils/objects/PodUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kubeUtils/objects/SecretUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kubeUtils/objects/ServiceUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/specific/BridgeUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/specific/CruiseControlUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/specific/JmxUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/specific/OlmUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/specific/TracingUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/specific/KeycloakUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/ClientUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/FileUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/HttpUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/RollingUpdateUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/StUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/TestKafkaVersion.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/interfaces/IndicativeSentences.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/templates/crd/KafkaBridgeTemplates.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/templates/crd/KafkaClientsTemplates.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/templates/crd/KafkaConnectTemplates.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/templates/crd/KafkaConnectorTemplates.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/templates/crd/KafkaMirrorMaker2Templates.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/templates/crd/KafkaMirrorMakerTemplates.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/templates/crd/KafkaRebalanceTemplates.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/templates/crd/KafkaTemplates.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/templates/crd/KafkaTopicTemplates.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/templates/crd/KafkaUserTemplates.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/templates/kubernetes/ServiceTemplates.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/templates/kubernetes/ClusterRoleBindingTemplates.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/templates/kubernetes/NetworkPolicyTemplates.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/templates/specific/ScraperTemplates.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/BeforeAllOnce.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/metrics/MetricsCollector.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/parallel/SuiteThreadController.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/parallel/TestSuiteNamespaceManager.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/storage/TestStorage.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/Constants.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/Environment.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/KafkaOauthClientsFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/KafkaOauthClientsFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/KafkaOauthClientsBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/BridgeClientsFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/BridgeClientsFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/BridgeClientsBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/KafkaTracingClientsFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/KafkaTracingClientsFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/KafkaTracingClientsBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/BaseClientsFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/BaseClientsFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/KafkaClientsFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/KafkaClientsFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/KafkaClientsBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/KafkaAdminClientsFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/KafkaAdminClientsFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/KafkaAdminClientsBuilder.java...
Constructing Javadoc information...
Standard Doclet version 11.0.11
Building tree for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/IsolatedSuite.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/IsolatedTest.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/MultiNodeClusterOnly.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/MultiNodeClusterOnlyCondition.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/OpenShiftOnly.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/OpenShiftOnlyCondition.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/ParallelNamespaceTest.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/ParallelSuite.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/ParallelTest.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/RequiredMinKubeApiVersion.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/RequiredMinKubeApiVersionCondition.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/StatefulSetTest.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/StatefulSetTestCondition.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/StrimziPodSetTest.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/StrimziPodSetTestCondition.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/cli/KafkaCmdClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clientproperties/AbstractKafkaClientProperties.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clientproperties/AbstractKafkaClientProperties.KafkaClientPropertiesBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clientproperties/ConsumerProperties.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clientproperties/ConsumerProperties.ConsumerPropertiesBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clientproperties/ProducerProperties.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clientproperties/ProducerProperties.ProducerPropertiesBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/ClientArgument.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/ClientArgumentMap.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/ClientType.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/InternalKafkaClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/InternalKafkaClient.Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/VerifiableClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/VerifiableClient.VerifiableClientBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/controllers/ConfigMapUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/controllers/DeploymentUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/controllers/JobUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/controllers/StatefulSetUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/controllers/StrimziPodSetUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/KafkaBridgeResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/KafkaClientsResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/KafkaConnectorResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/KafkaConnectResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/KafkaMirrorMaker2Resource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/KafkaMirrorMakerResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/KafkaRebalanceResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/KafkaResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/KafkaTopicResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/KafkaUserResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/StrimziPodSetResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/KafkaBridgeTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/KafkaClientsTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/KafkaConnectorTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/KafkaConnectTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/KafkaMirrorMaker2Templates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/KafkaMirrorMakerTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/KafkaRebalanceTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/KafkaTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/KafkaTopicTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/KafkaUserTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/draincleaner/DrainCleanerResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/draincleaner/SetupDrainCleaner.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/enums/ClusterOperatorInstallType.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/enums/ClusterOperatorRBACType.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/enums/CustomResourceStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/enums/DefaultNetworkPolicy.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/enums/DeploymentTypes.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/enums/OlmInstallationStrategy.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/externalClients/ExternalKafkaClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/externalClients/ExternalKafkaClient.Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/interfaces/IndicativeSentences.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/AdminClientOperations.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/BaseClients.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/BaseClientsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/BaseClientsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/BridgeClients.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/BridgeClientsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/BridgeClientsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/BridgeClientsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/KafkaAdminClients.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/KafkaAdminClientsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/KafkaAdminClientsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/KafkaAdminClientsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/KafkaClients.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/KafkaClientsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/KafkaClientsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/KafkaClientsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/KafkaOauthClients.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/KafkaOauthClientsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/KafkaOauthClientsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/KafkaOauthClientsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/KafkaTracingClients.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/KafkaTracingClientsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/KafkaTracingClientsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/KafkaTracingClientsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/k8s/Events.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/AbstractKafkaClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/AbstractKafkaClient.Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/KafkaClientOperations.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/KafkaBridgeUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/KafkaConnectorUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/KafkaConnectUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/KafkaMirrorMaker2Utils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/KafkaMirrorMakerUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/KafkaRebalanceUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/KafkaTopicUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/KafkaUserUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/keycloak/KeycloakInstance.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/ClusterOperatorCustomResourceDefinition.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/ClusterRoleBindingResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/ClusterRoleResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/ConfigMapResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/DeploymentResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/JobResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/NetworkPolicyResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/RoleBindingResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/RoleResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/SecretResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/ServiceAccountResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/ServiceResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/ValidatingWebhookConfigurationResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/kubernetes/ClusterRoleBindingTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/kubernetes/NetworkPolicyTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/kubernetes/ServiceTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/listeners/ExecutionListener.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/listeners/OrderTestSuites.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/logs/LogCollector.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/logs/TestExecutionWatcher.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/matchers/HasAllOfReasons.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/matchers/HasAnyOfReasons.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/matchers/HasNoneOfReasons.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/matchers/LogHasNoUnexpectedErrors.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/matchers/Matchers.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/metrics/MetricsCollector.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/metrics/MetricsCollector.Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/objects/NamespaceUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/objects/NodeUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/objects/PersistentVolumeClaimUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/objects/PodUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/objects/SecretUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/objects/ServiceUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/BundleResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/BundleResource.BundleResourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/SetupClusterOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/SetupClusterOperator.SetupClusterOperatorBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/parallel/SuiteThreadController.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/parallel/TestSuiteNamespaceManager.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/ComponentType.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/ResourceCondition.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/ResourceItem.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/ResourceManager.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/ResourceOperation.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/ResourceType.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/ThrowableRunner.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/security/CertAndKeyFiles.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/security/SystemTestCertAndKey.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/security/SystemTestCertAndKeyBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/security/SystemTestCertManager.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/specific/HelmResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/specific/OlmResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/specific/SpecificResourceType.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/specific/ScraperTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/BridgeUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/CruiseControlUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/CruiseControlUtils.SupportedHttpMethods.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/CruiseControlUtils.SupportedSchemes.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/JmxUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/KeycloakUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/OlmUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/TracingUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/storage/TestStorage.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/BeforeAllOnce.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/Constants.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/Environment.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/ClientUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/FileUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/HttpUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/RollingUpdateUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/StUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/TestKafkaVersion.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/cli/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/cli/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/enums/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/enums/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/interfaces/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/interfaces/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/k8s/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/k8s/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clientproperties/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clientproperties/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/externalClients/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/externalClients/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/keycloak/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/keycloak/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/listeners/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/listeners/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/logs/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/logs/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/matchers/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/matchers/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/metrics/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/metrics/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/parallel/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/parallel/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/draincleaner/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/draincleaner/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/specific/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/specific/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/security/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/security/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/storage/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/storage/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/kubernetes/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/kubernetes/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/specific/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/specific/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/controllers/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/controllers/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/objects/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/objects/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/constant-values.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/class-use/MultiNodeClusterOnly.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/class-use/MultiNodeClusterOnlyCondition.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/class-use/OpenShiftOnly.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/class-use/RequiredMinKubeApiVersion.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/class-use/RequiredMinKubeApiVersionCondition.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/class-use/IsolatedTest.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/class-use/ParallelNamespaceTest.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/class-use/IsolatedSuite.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/class-use/OpenShiftOnlyCondition.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/class-use/ParallelSuite.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/class-use/ParallelTest.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/class-use/StatefulSetTest.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/class-use/StatefulSetTestCondition.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/class-use/StrimziPodSetTest.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/class-use/StrimziPodSetTestCondition.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/cli/class-use/KafkaCmdClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/enums/class-use/DefaultNetworkPolicy.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/enums/class-use/ClusterOperatorInstallType.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/enums/class-use/CustomResourceStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/enums/class-use/OlmInstallationStrategy.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/enums/class-use/ClusterOperatorRBACType.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/enums/class-use/DeploymentTypes.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/k8s/class-use/Events.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clientproperties/class-use/ProducerProperties.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clientproperties/class-use/ProducerProperties.ProducerPropertiesBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clientproperties/class-use/AbstractKafkaClientProperties.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clientproperties/class-use/AbstractKafkaClientProperties.KafkaClientPropertiesBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clientproperties/class-use/ConsumerProperties.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clientproperties/class-use/ConsumerProperties.ConsumerPropertiesBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/class-use/AbstractKafkaClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/class-use/AbstractKafkaClient.Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/class-use/KafkaClientOperations.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/class-use/ClientArgument.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/class-use/ClientArgumentMap.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/class-use/ClientType.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/class-use/InternalKafkaClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/class-use/InternalKafkaClient.Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/class-use/VerifiableClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/class-use/VerifiableClient.VerifiableClientBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/externalClients/class-use/ExternalKafkaClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/externalClients/class-use/ExternalKafkaClient.Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/AdminClientOperations.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/BaseClients.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/BridgeClients.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/KafkaAdminClients.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/KafkaClients.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/KafkaOauthClients.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/KafkaTracingClients.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/keycloak/class-use/KeycloakInstance.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/listeners/class-use/ExecutionListener.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/listeners/class-use/OrderTestSuites.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/logs/class-use/LogCollector.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/logs/class-use/TestExecutionWatcher.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/matchers/class-use/HasAllOfReasons.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/matchers/class-use/HasAnyOfReasons.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/matchers/class-use/HasNoneOfReasons.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/matchers/class-use/Matchers.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/matchers/class-use/LogHasNoUnexpectedErrors.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/class-use/KafkaBridgeResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/class-use/KafkaConnectResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/class-use/KafkaConnectorResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/class-use/KafkaMirrorMaker2Resource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/class-use/KafkaTopicResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/class-use/KafkaClientsResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/class-use/KafkaMirrorMakerResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/class-use/KafkaRebalanceResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/class-use/KafkaResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/class-use/KafkaUserResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/class-use/StrimziPodSetResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/class-use/BundleResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/class-use/BundleResource.BundleResourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/specific/class-use/HelmResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/specific/class-use/SpecificResourceType.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/specific/class-use/OlmResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/class-use/SetupClusterOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/class-use/SetupClusterOperator.SetupClusterOperatorBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/class-use/ResourceCondition.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/class-use/ResourceItem.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/class-use/ResourceType.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/class-use/ThrowableRunner.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/class-use/ServiceResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/class-use/ClusterOperatorCustomResourceDefinition.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/class-use/ClusterRoleBindingResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/class-use/ClusterRoleResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/class-use/ConfigMapResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/class-use/JobResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/class-use/NetworkPolicyResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/class-use/RoleBindingResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/class-use/RoleResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/class-use/SecretResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/class-use/ServiceAccountResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/class-use/ValidatingWebhookConfigurationResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/class-use/DeploymentResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/class-use/ComponentType.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/class-use/ResourceManager.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/draincleaner/class-use/DrainCleanerResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/draincleaner/class-use/SetupDrainCleaner.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/class-use/ResourceOperation.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/security/class-use/CertAndKeyFiles.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/security/class-use/SystemTestCertAndKey.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/security/class-use/SystemTestCertAndKeyBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/security/class-use/SystemTestCertManager.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/class-use/KafkaBridgeUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/class-use/KafkaConnectUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/class-use/KafkaConnectorUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/class-use/KafkaMirrorMaker2Utils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/class-use/KafkaMirrorMakerUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/class-use/KafkaRebalanceUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/class-use/KafkaUserUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/class-use/KafkaUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/class-use/KafkaTopicUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/controllers/class-use/ConfigMapUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/controllers/class-use/DeploymentUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/controllers/class-use/JobUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/controllers/class-use/StatefulSetUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/controllers/class-use/StrimziPodSetUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/objects/class-use/NamespaceUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/objects/class-use/NodeUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/objects/class-use/PersistentVolumeClaimUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/objects/class-use/PodUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/objects/class-use/SecretUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/objects/class-use/ServiceUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/class-use/BridgeUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/class-use/CruiseControlUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/class-use/CruiseControlUtils.SupportedSchemes.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/class-use/CruiseControlUtils.SupportedHttpMethods.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/class-use/JmxUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/class-use/OlmUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/class-use/TracingUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/class-use/KeycloakUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/class-use/ClientUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/class-use/FileUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/class-use/HttpUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/class-use/RollingUpdateUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/class-use/StUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/class-use/TestKafkaVersion.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/interfaces/class-use/IndicativeSentences.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/class-use/KafkaBridgeTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/class-use/KafkaClientsTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/class-use/KafkaConnectTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/class-use/KafkaConnectorTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/class-use/KafkaMirrorMaker2Templates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/class-use/KafkaMirrorMakerTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/class-use/KafkaRebalanceTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/class-use/KafkaTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/class-use/KafkaTopicTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/class-use/KafkaUserTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/kubernetes/class-use/ServiceTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/kubernetes/class-use/ClusterRoleBindingTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/kubernetes/class-use/NetworkPolicyTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/specific/class-use/ScraperTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/class-use/BeforeAllOnce.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/metrics/class-use/MetricsCollector.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/metrics/class-use/MetricsCollector.Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/parallel/class-use/SuiteThreadController.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/parallel/class-use/TestSuiteNamespaceManager.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/storage/class-use/TestStorage.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/class-use/Constants.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/class-use/Environment.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/KafkaOauthClientsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/KafkaOauthClientsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/KafkaOauthClientsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/BridgeClientsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/BridgeClientsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/BridgeClientsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/KafkaTracingClientsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/KafkaTracingClientsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/KafkaTracingClientsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/BaseClientsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/BaseClientsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/KafkaClientsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/KafkaClientsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/KafkaClientsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/KafkaAdminClientsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/KafkaAdminClientsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/KafkaAdminClientsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/cli/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/enums/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/interfaces/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/k8s/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clientproperties/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/externalClients/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/keycloak/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/listeners/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/logs/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/matchers/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/metrics/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/parallel/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/draincleaner/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/specific/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/security/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/storage/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/kubernetes/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/specific/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/controllers/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/objects/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/package-use.html...
Building index for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/overview-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/index-all.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/allclasses-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/allpackages-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/deprecated-list.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/index.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/overview-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/help-doc.html...
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/systemtest/target/systemtest-0.29.0-SNAPSHOT-javadoc.jar
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:integration-test (default) @ systemtest ---
[WARNING] useSystemClassLoader setting has no effect when not forking
[WARNING] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
2022-03-28 18:55:04 [main] INFO  [TestExecutionListener:29] =======================================================================
2022-03-28 18:55:04 [main] INFO  [TestExecutionListener:30] =======================================================================
2022-03-28 18:55:04 [main] INFO  [TestExecutionListener:31]                         Test run started
2022-03-28 18:55:04 [main] INFO  [TestExecutionListener:32] =======================================================================
2022-03-28 18:55:04 [main] INFO  [TestExecutionListener:33] =======================================================================
2022-03-28 18:55:04 [main] INFO  [TestExecutionListener:48] Following testclasses are selected for run:
2022-03-28 18:55:04 [main] INFO  [TestExecutionListener:51] -> io.strimzi.systemtest.bridge.HttpBridgeScramShaST
2022-03-28 18:55:04 [main] INFO  [TestExecutionListener:51] -> io.strimzi.systemtest.bridge.HttpBridgeTlsST
2022-03-28 18:55:04 [main] INFO  [TestExecutionListener:51] -> io.strimzi.systemtest.operators.topic.ThrottlingQuotaST
2022-03-28 18:55:04 [main] INFO  [TestExecutionListener:51] -> io.strimzi.systemtest.operators.topic.TopicST
2022-03-28 18:55:04 [main] INFO  [TestExecutionListener:51] -> io.strimzi.systemtest.operators.user.UserST
2022-03-28 18:55:04 [main] INFO  [TestExecutionListener:51] -> io.strimzi.systemtest.operators.ReconciliationST
2022-03-28 18:55:04 [main] INFO  [TestExecutionListener:51] -> io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST
2022-03-28 18:55:04 [main] INFO  [TestExecutionListener:52] =======================================================================
2022-03-28 18:55:04 [main] INFO  [TestExecutionListener:53] =======================================================================
[INFO] Running io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST
[INFO] Running io.strimzi.systemtest.bridge.HttpBridgeTlsST
[INFO] Running io.strimzi.systemtest.bridge.HttpBridgeScramShaST
[INFO] Running io.strimzi.systemtest.operators.ReconciliationST
[INFO] Running io.strimzi.systemtest.operators.topic.ThrottlingQuotaST
[INFO] Running io.strimzi.systemtest.operators.topic.TopicST
[INFO] Running io.strimzi.systemtest.operators.user.UserST
2022-03-28 18:55:04 [ForkJoinPool-1-worker-31] DEBUG [Environment:271] Json configuration is not provided or cannot be processed!
2022-03-28 18:55:04 [ForkJoinPool-1-worker-31] INFO  [Environment:219] Used environment variables:
2022-03-28 18:55:04 [ForkJoinPool-1-worker-31] INFO  [Environment:220] CONFIG: /home/cloud-user/strimzi-kafka-operator/systemtest/config.json
2022-03-28 18:55:04 [ForkJoinPool-1-worker-31] INFO  [Environment:221] STRIMZI_RBAC_SCOPE: CLUSTER
2022-03-28 18:55:04 [ForkJoinPool-1-worker-31] INFO  [Environment:221] OLM_APP_BUNDLE_PREFIX: strimzi-cluster-operator
2022-03-28 18:55:04 [ForkJoinPool-1-worker-31] INFO  [Environment:221] TEST_CLIENTS_VERSION: 0.2.0
2022-03-28 18:55:04 [ForkJoinPool-1-worker-31] INFO  [Environment:221] OLM_SOURCE_NAMESPACE: openshift-marketplace
2022-03-28 18:55:04 [ForkJoinPool-1-worker-31] INFO  [Environment:221] CLUSTER_OPERATOR_INSTALL_TYPE: BUNDLE
2022-03-28 18:55:04 [ForkJoinPool-1-worker-31] INFO  [Environment:221] STRIMZI_COMPONENTS_LOG_LEVEL: INFO
2022-03-28 18:55:04 [ForkJoinPool-1-worker-31] INFO  [Environment:221] SKIP_TEARDOWN: false
2022-03-28 18:55:04 [ForkJoinPool-1-worker-31] INFO  [Environment:221] LB_FINALIZERS: false
2022-03-28 18:55:04 [ForkJoinPool-1-worker-31] INFO  [Environment:221] OLM_OPERATOR_DEPLOYMENT_NAME: strimzi-cluster-operator
2022-03-28 18:55:04 [ForkJoinPool-1-worker-31] INFO  [Environment:221] DOCKER_ORG: strimzi
2022-03-28 18:55:04 [ForkJoinPool-1-worker-31] INFO  [Environment:221] TEST_LOG_DIR: /home/cloud-user/strimzi-kafka-operator/systemtest/../systemtest/target/logs/
2022-03-28 18:55:04 [ForkJoinPool-1-worker-31] INFO  [Environment:221] COMPONENTS_IMAGE_PULL_POLICY: IfNotPresent
2022-03-28 18:55:04 [ForkJoinPool-1-worker-31] INFO  [Environment:221] DOCKER_REGISTRY: quay.io
2022-03-28 18:55:04 [ForkJoinPool-1-worker-31] INFO  [Environment:221] TEST_CLIENT_IMAGE: quay.io/strimzi/test-client:latest-kafka-3.1.0
2022-03-28 18:55:04 [ForkJoinPool-1-worker-31] INFO  [Environment:221] SYSTEM_TEST_STRIMZI_IMAGE_PULL_SECRET: 
2022-03-28 18:55:04 [ForkJoinPool-1-worker-31] INFO  [Environment:221] TEST_ADMIN_IMAGE: quay.io/strimzi-test-clients/test-client-kafka-admin:0.2.0-kafka-3.1.0
2022-03-28 18:55:04 [ForkJoinPool-1-worker-31] INFO  [Environment:221] TEST_HTTP_PRODUCER_IMAGE: quay.io/strimzi-test-clients/test-client-http-producer:0.2.0
2022-03-28 18:55:04 [ForkJoinPool-1-worker-31] INFO  [Environment:221] OLM_OPERATOR_NAME: strimzi-kafka-operator
2022-03-28 18:55:04 [ForkJoinPool-1-worker-31] INFO  [Environment:221] DOCKER_TAG: latest
2022-03-28 18:55:04 [ForkJoinPool-1-worker-31] INFO  [Environment:221] OLM_SOURCE_NAME: community-operators
2022-03-28 18:55:04 [ForkJoinPool-1-worker-31] INFO  [Environment:221] STRIMZI_FEATURE_GATES: 
2022-03-28 18:55:04 [ForkJoinPool-1-worker-31] INFO  [Environment:221] CLIENTS_KAFKA_VERSION: 3.1.0
2022-03-28 18:55:04 [ForkJoinPool-1-worker-31] INFO  [Environment:221] TEST_HTTP_CONSUMER_IMAGE: quay.io/strimzi-test-clients/test-client-http-consumer:0.2.0
2022-03-28 18:55:04 [ForkJoinPool-1-worker-31] INFO  [Environment:221] STRIMZI_LOG_LEVEL: DEBUG
2022-03-28 18:55:04 [ForkJoinPool-1-worker-31] INFO  [Environment:221] ST_KAFKA_VERSION: 3.1.0
2022-03-28 18:55:04 [ForkJoinPool-1-worker-31] INFO  [Environment:221] OPERATOR_IMAGE_PULL_POLICY: Always
2022-03-28 18:55:04 [ForkJoinPool-1-worker-31] INFO  [Environment:221] DEFAULT_TO_DENY_NETWORK_POLICIES: true
2022-03-28 18:55:04 [ForkJoinPool-1-worker-31] INFO  [Environment:221] TEST_PRODUCER_IMAGE: quay.io/strimzi-test-clients/test-client-kafka-producer:0.2.0-kafka-3.1.0
2022-03-28 18:55:04 [ForkJoinPool-1-worker-31] INFO  [Environment:221] BRIDGE_IMAGE: latest-released
2022-03-28 18:55:04 [ForkJoinPool-1-worker-31] INFO  [Environment:221] TEST_STREAMS_IMAGE: quay.io/strimzi-test-clients/test-client-kafka-streams:0.2.0-kafka-3.1.0
2022-03-28 18:55:04 [ForkJoinPool-1-worker-31] INFO  [Environment:221] TEST_CONSUMER_IMAGE: quay.io/strimzi-test-clients/test-client-kafka-consumer:0.2.0-kafka-3.1.0
2022-03-28 18:55:04 [ForkJoinPool-1-worker-31] INFO  [Environment:221] OLM_OPERATOR_VERSION: 
2022-03-28 18:55:04 [ForkJoinPool-1-worker-27] DEBUG [BeforeAllOnce:51] ============================================================================
2022-03-28 18:55:04 [ForkJoinPool-1-worker-27] DEBUG [BeforeAllOnce:52] [io.strimzi.systemtest.operators.topic.TopicST - Before Suite] - Setup Suite environment
2022-03-28 18:55:04 [ForkJoinPool-1-worker-27] DEBUG [Config:540] Trying to configure client from Kubernetes config...
2022-03-28 18:55:04 [ForkJoinPool-1-worker-27] DEBUG [Config:549] Found for Kubernetes config at: [/home/cloud-user/.kube/config].
2022-03-28 18:55:05 [ForkJoinPool-1-worker-27] DEBUG [Config:540] Trying to configure client from Kubernetes config...
2022-03-28 18:55:05 [ForkJoinPool-1-worker-27] DEBUG [Config:549] Found for Kubernetes config at: [/home/cloud-user/.kube/config].
2022-03-28 18:55:05 [ForkJoinPool-1-worker-27] DEBUG [KubeCluster:80] Cluster minikube is not installed!
2022-03-28 18:55:05 [ForkJoinPool-1-worker-27] DEBUG [KubeCluster:71] Cluster kubectl is installed
2022-03-28 18:55:05 [ForkJoinPool-1-worker-27] TRACE [Exec:248] Running command - kubectl cluster-info
2022-03-28 18:55:06 [ForkJoinPool-1-worker-27] DEBUG [Exec:419] Command: kubectl cluster-info
2022-03-28 18:55:06 [ForkJoinPool-1-worker-27] DEBUG [Exec:419] Return code: 0
2022-03-28 18:55:06 [ForkJoinPool-1-worker-27] TRACE [Exec:248] Running command - kubectl api-resources
2022-03-28 18:55:07 [ForkJoinPool-1-worker-27] DEBUG [Exec:419] Command: kubectl api-resources
2022-03-28 18:55:07 [ForkJoinPool-1-worker-27] DEBUG [Exec:419] Return code: 0
2022-03-28 18:55:07 [ForkJoinPool-1-worker-27] DEBUG [KubeCluster:77] Cluster kubectl is not running!
2022-03-28 18:55:07 [ForkJoinPool-1-worker-27] DEBUG [KubeCluster:71] Cluster oc is installed
2022-03-28 18:55:07 [ForkJoinPool-1-worker-27] TRACE [Exec:248] Running command - oc status -n default
2022-03-28 18:55:08 [ForkJoinPool-1-worker-27] DEBUG [Exec:419] Command: oc status -n default
2022-03-28 18:55:08 [ForkJoinPool-1-worker-27] DEBUG [Exec:419] Return code: 0
2022-03-28 18:55:08 [ForkJoinPool-1-worker-27] TRACE [Exec:248] Running command - oc api-resources
2022-03-28 18:55:09 [ForkJoinPool-1-worker-27] DEBUG [Exec:419] Command: oc api-resources
2022-03-28 18:55:09 [ForkJoinPool-1-worker-27] DEBUG [Exec:419] Return code: 0
2022-03-28 18:55:09 [ForkJoinPool-1-worker-27] DEBUG [KubeCluster:73] Cluster oc is running
2022-03-28 18:55:09 [ForkJoinPool-1-worker-27] INFO  [KubeCluster:87] Using cluster: oc
2022-03-28 18:55:09 [ForkJoinPool-1-worker-27] INFO  [KubeClusterResource:60] Cluster default namespace is 'default'
2022-03-28 18:55:09 [ForkJoinPool-1-worker-27] INFO  [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@1d015e4d
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=*
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-03-28 18:55:09 [ForkJoinPool-1-worker-27] DEBUG [SetupClusterOperator:198] Cluster operator installation configuration:
SetupClusterOperator{cluster=io.strimzi.test.k8s.KubeClusterResource@27ab2a7d, extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@1d015e4d, clusterOperatorName='strimzi-cluster-operator', namespaceInstallTo='infra-namespace', namespaceToWatch='*', bindingsNamespaces=[infra-namespace], operationTimeout=300000, reconciliationInterval=30000, extraEnvVars=[], extraLabels={}, clusterOperatorRBACType=CLUSTER, testClassName='null', testMethodName='null'}
2022-03-28 18:55:09 [ForkJoinPool-1-worker-27] INFO  [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-03-28 18:55:11 [ForkJoinPool-1-worker-27] INFO  [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-03-28 18:55:11 [ForkJoinPool-1-worker-27] DEBUG [TestUtils:125] Waiting for Namespace infra-namespace
2022-03-28 18:55:11 [ForkJoinPool-1-worker-27] TRACE [Exec:248] Running command - oc --namespace default get Namespace infra-namespace -o json
2022-03-28 18:55:11 [ForkJoinPool-1-worker-27] DEBUG [Exec:419] Command: oc --namespace default get Namespace infra-namespace -o json
2022-03-28 18:55:11 [ForkJoinPool-1-worker-27] DEBUG [Exec:419] Return code: 0
2022-03-28 18:55:11 [ForkJoinPool-1-worker-27] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c40,c30",
            "openshift.io/sa.scc.supplemental-groups": "1001620000/10000",
            "openshift.io/sa.scc.uid-range": "1001620000/10000"
        },
        "creationTimestamp": "2022-03-28T18:55:06Z",
        "labels": {
            "kubernetes.io/metadata.name": "infra-namespace"
        },
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T18:55:06Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:labels": {
                            ".": {},
                            "f:kubernetes.io/metadata.name": {}
                        }
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T18:55:06Z"
            }
        ],
        "name": "infra-namespace",
        "resourceVersion": "258537",
        "uid": "315dfcf6-6b1b-414d-91f7-bd3dea80e6c6"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 18:55:11 [ForkJoinPool-1-worker-27] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace]}
2022-03-28 18:55:11 [ForkJoinPool-1-worker-27] INFO  [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-28 18:55:11 [ForkJoinPool-1-worker-27] DEBUG [SetupClusterOperator:478] Installation resource type: ServiceAccount
2022-03-28 18:55:11 [ForkJoinPool-1-worker-27] INFO  [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-28 18:55:11 [ForkJoinPool-1-worker-27] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-28 18:55:11 [ForkJoinPool-1-worker-27] DEBUG [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-28 18:55:11 [ForkJoinPool-1-worker-27] INFO  [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-28 18:55:12 [ForkJoinPool-1-worker-27] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-28 18:55:12 [ForkJoinPool-1-worker-27] DEBUG [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-28 18:55:12 [ForkJoinPool-1-worker-27] INFO  [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-28 18:55:12 [ForkJoinPool-1-worker-27] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-28 18:55:12 [ForkJoinPool-1-worker-27] DEBUG [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-28 18:55:12 [ForkJoinPool-1-worker-27] INFO  [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-28 18:55:12 [ForkJoinPool-1-worker-27] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-28 18:55:12 [ForkJoinPool-1-worker-27] DEBUG [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-28 18:55:12 [ForkJoinPool-1-worker-27] INFO  [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-28 18:55:12 [ForkJoinPool-1-worker-27] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-28 18:55:12 [ForkJoinPool-1-worker-27] DEBUG [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-28 18:55:12 [ForkJoinPool-1-worker-27] INFO  [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-28 18:55:12 [ForkJoinPool-1-worker-27] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-28 18:55:13 [ForkJoinPool-1-worker-27] DEBUG [SetupClusterOperator:478] Installation resource type: Crd
2022-03-28 18:55:13 [ForkJoinPool-1-worker-27] INFO  [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-28 18:55:14 [ForkJoinPool-1-worker-27] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-28 18:55:14 [ForkJoinPool-1-worker-27] DEBUG [SetupClusterOperator:478] Installation resource type: Crd
2022-03-28 18:55:14 [ForkJoinPool-1-worker-27] INFO  [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-28 18:55:15 [ForkJoinPool-1-worker-27] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-28 18:55:15 [ForkJoinPool-1-worker-27] DEBUG [SetupClusterOperator:478] Installation resource type: Crd
2022-03-28 18:55:15 [ForkJoinPool-1-worker-27] INFO  [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-28 18:55:15 [ForkJoinPool-1-worker-27] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-28 18:55:15 [ForkJoinPool-1-worker-27] DEBUG [SetupClusterOperator:478] Installation resource type: Crd
2022-03-28 18:55:15 [ForkJoinPool-1-worker-27] INFO  [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-28 18:55:15 [ForkJoinPool-1-worker-27] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-28 18:55:16 [ForkJoinPool-1-worker-27] DEBUG [SetupClusterOperator:478] Installation resource type: Crd
2022-03-28 18:55:16 [ForkJoinPool-1-worker-27] INFO  [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-28 18:55:16 [ForkJoinPool-1-worker-27] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-28 18:55:16 [ForkJoinPool-1-worker-27] DEBUG [SetupClusterOperator:478] Installation resource type: Crd
2022-03-28 18:55:16 [ForkJoinPool-1-worker-27] INFO  [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-28 18:55:16 [ForkJoinPool-1-worker-27] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-28 18:55:17 [ForkJoinPool-1-worker-27] DEBUG [SetupClusterOperator:478] Installation resource type: Crd
2022-03-28 18:55:17 [ForkJoinPool-1-worker-27] INFO  [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-28 18:55:17 [ForkJoinPool-1-worker-27] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-28 18:55:17 [ForkJoinPool-1-worker-27] DEBUG [SetupClusterOperator:478] Installation resource type: Crd
2022-03-28 18:55:17 [ForkJoinPool-1-worker-27] INFO  [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-28 18:55:17 [ForkJoinPool-1-worker-27] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-28 18:55:17 [ForkJoinPool-1-worker-27] DEBUG [SetupClusterOperator:478] Installation resource type: Crd
2022-03-28 18:55:17 [ForkJoinPool-1-worker-27] INFO  [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-28 18:55:18 [ForkJoinPool-1-worker-27] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-28 18:55:18 [ForkJoinPool-1-worker-27] DEBUG [SetupClusterOperator:478] Installation resource type: Crd
2022-03-28 18:55:18 [ForkJoinPool-1-worker-27] INFO  [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-28 18:55:18 [ForkJoinPool-1-worker-27] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-28 18:55:19 [ForkJoinPool-1-worker-27] DEBUG [SetupClusterOperator:478] Installation resource type: ConfigMap
2022-03-28 18:55:19 [ForkJoinPool-1-worker-27] INFO  [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-28 18:55:19 [ForkJoinPool-1-worker-27] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-28 18:55:19 [ForkJoinPool-1-worker-27] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=infra-namespace, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 18:55:19 [ForkJoinPool-1-worker-27] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-03-28 18:55:19 [ForkJoinPool-1-worker-27] INFO  [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/cloud-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-28 18:55:19 [ForkJoinPool-1-worker-27] INFO  [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-28 18:55:19 [ForkJoinPool-1-worker-27] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-28 18:55:19 [ForkJoinPool-1-worker-27] INFO  [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/cloud-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-03-28 18:55:19 [ForkJoinPool-1-worker-27] INFO  [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-28 18:55:19 [ForkJoinPool-1-worker-27] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-28 18:55:19 [ForkJoinPool-1-worker-27] INFO  [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/cloud-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-03-28 18:55:19 [ForkJoinPool-1-worker-27] INFO  [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-28 18:55:20 [ForkJoinPool-1-worker-27] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-28 18:55:20 [ForkJoinPool-1-worker-27] INFO  [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/cloud-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-28 18:55:20 [ForkJoinPool-1-worker-27] INFO  [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-28 18:55:20 [ForkJoinPool-1-worker-27] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-28 18:55:20 [ForkJoinPool-1-worker-27] INFO  [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/cloud-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-03-28 18:55:20 [ForkJoinPool-1-worker-27] INFO  [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-28 18:55:20 [ForkJoinPool-1-worker-27] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-28 18:55:20 [ForkJoinPool-1-worker-27] INFO  [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-03-28 18:55:20 [ForkJoinPool-1-worker-27] INFO  [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-03-28 18:55:20 [ForkJoinPool-1-worker-27] INFO  [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-28 18:55:20 [ForkJoinPool-1-worker-27] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-namespaced
2022-03-28 18:55:20 [ForkJoinPool-1-worker-27] INFO  [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-03-28 18:55:20 [ForkJoinPool-1-worker-27] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-entity-operator
2022-03-28 18:55:21 [ForkJoinPool-1-worker-27] INFO  [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-28 18:55:21 [ForkJoinPool-1-worker-27] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-28 18:55:21 [ForkJoinPool-1-worker-27] INFO  [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-28 18:55:21 [ForkJoinPool-1-worker-27] DEBUG [TestUtils:125] Waiting for Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-28 18:55:22 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (479900ms till timeout)
2022-03-28 18:55:23 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (478797ms till timeout)
2022-03-28 18:55:24 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (477694ms till timeout)
2022-03-28 18:55:25 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (476593ms till timeout)
2022-03-28 18:55:26 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (475491ms till timeout)
2022-03-28 18:55:27 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (474388ms till timeout)
2022-03-28 18:55:28 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (473286ms till timeout)
2022-03-28 18:55:29 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (472186ms till timeout)
2022-03-28 18:55:30 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (471086ms till timeout)
2022-03-28 18:55:31 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (469982ms till timeout)
2022-03-28 18:55:33 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (468868ms till timeout)
2022-03-28 18:55:34 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (467767ms till timeout)
2022-03-28 18:55:35 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (466662ms till timeout)
2022-03-28 18:55:36 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (465559ms till timeout)
2022-03-28 18:55:37 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (464459ms till timeout)
2022-03-28 18:55:38 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (463359ms till timeout)
2022-03-28 18:55:39 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (462259ms till timeout)
2022-03-28 18:55:40 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (461160ms till timeout)
2022-03-28 18:55:41 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (460059ms till timeout)
2022-03-28 18:55:42 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (458958ms till timeout)
2022-03-28 18:55:44 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (457857ms till timeout)
2022-03-28 18:55:45 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (456756ms till timeout)
2022-03-28 18:55:46 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (455656ms till timeout)
2022-03-28 18:55:47 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (454556ms till timeout)
2022-03-28 18:55:48 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (453456ms till timeout)
2022-03-28 18:55:49 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (452356ms till timeout)
2022-03-28 18:55:50 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (451255ms till timeout)
2022-03-28 18:55:51 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (450154ms till timeout)
2022-03-28 18:55:52 [ForkJoinPool-1-worker-27] INFO  [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-03-28 18:55:52 [ForkJoinPool-1-worker-27] INFO  [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-03-28 18:55:52 [ForkJoinPool-1-worker-27] DEBUG [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready
2022-03-28 18:55:53 [ForkJoinPool-1-worker-27] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-5dc4cd9447-4npc2 not ready: strimzi-cluster-operator)
2022-03-28 18:55:53 [ForkJoinPool-1-worker-27] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-5dc4cd9447-4npc2 are ready
2022-03-28 18:55:53 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599882ms till timeout)
2022-03-28 18:55:54 [ForkJoinPool-1-worker-27] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-5dc4cd9447-4npc2 not ready: strimzi-cluster-operator)
2022-03-28 18:55:54 [ForkJoinPool-1-worker-27] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-5dc4cd9447-4npc2 are ready
2022-03-28 18:55:54 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598782ms till timeout)
2022-03-28 18:55:55 [ForkJoinPool-1-worker-27] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-5dc4cd9447-4npc2 not ready: strimzi-cluster-operator)
2022-03-28 18:55:55 [ForkJoinPool-1-worker-27] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-5dc4cd9447-4npc2 are ready
2022-03-28 18:55:55 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597679ms till timeout)
2022-03-28 18:55:56 [ForkJoinPool-1-worker-27] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-5dc4cd9447-4npc2 not ready: strimzi-cluster-operator)
2022-03-28 18:55:56 [ForkJoinPool-1-worker-27] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-5dc4cd9447-4npc2 are ready
2022-03-28 18:55:56 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596573ms till timeout)
2022-03-28 18:55:57 [ForkJoinPool-1-worker-27] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-5dc4cd9447-4npc2 not ready: strimzi-cluster-operator)
2022-03-28 18:55:57 [ForkJoinPool-1-worker-27] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-5dc4cd9447-4npc2 are ready
2022-03-28 18:55:57 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595471ms till timeout)
2022-03-28 18:55:58 [ForkJoinPool-1-worker-27] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-5dc4cd9447-4npc2 not ready: strimzi-cluster-operator)
2022-03-28 18:55:58 [ForkJoinPool-1-worker-27] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-5dc4cd9447-4npc2 are ready
2022-03-28 18:55:58 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594370ms till timeout)
2022-03-28 18:55:59 [ForkJoinPool-1-worker-27] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-5dc4cd9447-4npc2 not ready: strimzi-cluster-operator)
2022-03-28 18:55:59 [ForkJoinPool-1-worker-27] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-5dc4cd9447-4npc2 are ready
2022-03-28 18:55:59 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593268ms till timeout)
2022-03-28 18:56:00 [ForkJoinPool-1-worker-27] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-5dc4cd9447-4npc2 not ready: strimzi-cluster-operator)
2022-03-28 18:56:00 [ForkJoinPool-1-worker-27] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-5dc4cd9447-4npc2 are ready
2022-03-28 18:56:00 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592165ms till timeout)
2022-03-28 18:56:01 [ForkJoinPool-1-worker-27] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-5dc4cd9447-4npc2 not ready: strimzi-cluster-operator)
2022-03-28 18:56:01 [ForkJoinPool-1-worker-27] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-5dc4cd9447-4npc2 are ready
2022-03-28 18:56:01 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591064ms till timeout)
2022-03-28 18:56:03 [ForkJoinPool-1-worker-27] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-5dc4cd9447-4npc2 not ready: strimzi-cluster-operator)
2022-03-28 18:56:03 [ForkJoinPool-1-worker-27] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-5dc4cd9447-4npc2 are ready
2022-03-28 18:56:03 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (589963ms till timeout)
2022-03-28 18:56:04 [ForkJoinPool-1-worker-27] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-5dc4cd9447-4npc2 not ready: strimzi-cluster-operator)
2022-03-28 18:56:04 [ForkJoinPool-1-worker-27] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-5dc4cd9447-4npc2 are ready
2022-03-28 18:56:04 [ForkJoinPool-1-worker-27] INFO  [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-03-28 18:56:04 [ForkJoinPool-1-worker-31] DEBUG [AbstractST:666] ============================================================================
2022-03-28 18:56:04 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:666] ============================================================================
2022-03-28 18:56:04 [ForkJoinPool-1-worker-23] DEBUG [AbstractST:666] ============================================================================
2022-03-28 18:56:04 [ForkJoinPool-1-worker-27] DEBUG [AbstractST:666] ============================================================================
2022-03-28 18:56:04 [ForkJoinPool-1-worker-9] DEBUG [AbstractST:666] ============================================================================
2022-03-28 18:56:04 [ForkJoinPool-1-worker-27] DEBUG [AbstractST:667] [operators.topic.TopicST - Before All] - Setup test suite environment
2022-03-28 18:56:04 [ForkJoinPool-1-worker-31] DEBUG [AbstractST:667] [operators.user.UserST - Before All] - Setup test suite environment
2022-03-28 18:56:04 [ForkJoinPool-1-worker-9] DEBUG [AbstractST:667] [operators.topic.ThrottlingQuotaST - Before All] - Setup test suite environment
2022-03-28 18:56:04 [ForkJoinPool-1-worker-31] DEBUG [SuiteThreadController:69] [operators.user.UserST] - Adding parallel suite: UserST
2022-03-28 18:56:04 [ForkJoinPool-1-worker-27] DEBUG [SuiteThreadController:69] [operators.topic.TopicST] - Adding parallel suite: TopicST
2022-03-28 18:56:04 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:69] [operators.topic.ThrottlingQuotaST] - Adding parallel suite: ThrottlingQuotaST
2022-03-28 18:56:04 [ForkJoinPool-1-worker-27] DEBUG [SuiteThreadController:73] [operators.topic.TopicST] - Parallel suites count: 2
2022-03-28 18:56:04 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:73] [operators.topic.ThrottlingQuotaST] - Parallel suites count: 3
2022-03-28 18:56:04 [ForkJoinPool-1-worker-27] DEBUG [SuiteThreadController:184] TopicST suite now can proceed its execution
2022-03-28 18:56:04 [ForkJoinPool-1-worker-5] DEBUG [AbstractST:666] ============================================================================
2022-03-28 18:56:04 [ForkJoinPool-1-worker-5] DEBUG [AbstractST:667] [bridge.HttpBridgeScramShaST - Before All] - Setup test suite environment
2022-03-28 18:56:04 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:184] ThrottlingQuotaST suite now can proceed its execution
2022-03-28 18:56:04 [ForkJoinPool-1-worker-31] DEBUG [SuiteThreadController:73] [operators.user.UserST] - Parallel suites count: 1
2022-03-28 18:56:04 [ForkJoinPool-1-worker-23] DEBUG [AbstractST:667] [bridge.HttpBridgeTlsST - Before All] - Setup test suite environment
2022-03-28 18:56:04 [ForkJoinPool-1-worker-31] DEBUG [SuiteThreadController:184] UserST suite now can proceed its execution
2022-03-28 18:56:04 [ForkJoinPool-1-worker-17] DEBUG [AbstractST:666] ============================================================================
2022-03-28 18:56:04 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:69] [bridge.HttpBridgeScramShaST] - Adding parallel suite: HttpBridgeScramShaST
2022-03-28 18:56:04 [ForkJoinPool-1-worker-23] DEBUG [SuiteThreadController:69] [bridge.HttpBridgeTlsST] - Adding parallel suite: HttpBridgeTlsST
2022-03-28 18:56:04 [ForkJoinPool-1-worker-27] DEBUG [TestSuiteNamespaceManager:128] Content of the test suite namespaces map:
{HttpBridgeTlsST=[http-bridge-tls-st], CruiseControlST=[cruise-control-st], AbstractST=[abstract-st], OpaIntegrationST=[opa-integration-st], CruiseControlApiST=[cruise-control-api-st], HttpBridgeScramShaST=[http-bridge-scram-sha-st], ReconciliationST=[reconciliation-st], DynamicConfST=[dynamic-conf-st], AbstractUpgradeST=[abstract-upgrade-st], QuotasST=[quotas-st], DynamicConfSharedST=[dynamic-conf-shared-st], RollingUpdateST=[rolling-update-st], OlmAbstractST=[olm-abstract-st], CustomAuthorizerST=[custom-authorizer-st], AlternativeReconcileTriggersST=[alternative-reconcile-triggers-st], LogSettingST=[log-setting-st], ListenersST=[listeners-st], UserST=[user-st], TopicST=[topic-st], SecurityST=[security-st], TracingST=[tracing-st], AbstractNamespaceST=[abstract-namespace-st], CruiseControlConfigurationST=[cruise-control-configuration-st], HttpBridgeCorsST=[http-bridge-cors-st], ConfigProviderST=[config-provider-st], KafkaST=[kafka-st], LoggingChangeST=[logging-change-st], HttpBridgeKafkaExternalListenersST=[http-bridge-kafka-external-listeners-st], OauthAbstractST=[oauth-abstract-st], ThrottlingQuotaST=[throttling-quota-st], MultipleListenersST=[multiple-listeners-st]}
2022-03-28 18:56:04 [ForkJoinPool-1-worker-31] DEBUG [TestSuiteNamespaceManager:128] Content of the test suite namespaces map:
{HttpBridgeTlsST=[http-bridge-tls-st], CruiseControlST=[cruise-control-st], AbstractST=[abstract-st], OpaIntegrationST=[opa-integration-st], CruiseControlApiST=[cruise-control-api-st], HttpBridgeScramShaST=[http-bridge-scram-sha-st], ReconciliationST=[reconciliation-st], DynamicConfST=[dynamic-conf-st], AbstractUpgradeST=[abstract-upgrade-st], QuotasST=[quotas-st], DynamicConfSharedST=[dynamic-conf-shared-st], RollingUpdateST=[rolling-update-st], OlmAbstractST=[olm-abstract-st], CustomAuthorizerST=[custom-authorizer-st], AlternativeReconcileTriggersST=[alternative-reconcile-triggers-st], LogSettingST=[log-setting-st], ListenersST=[listeners-st], UserST=[user-st], TopicST=[topic-st], SecurityST=[security-st], TracingST=[tracing-st], AbstractNamespaceST=[abstract-namespace-st], CruiseControlConfigurationST=[cruise-control-configuration-st], HttpBridgeCorsST=[http-bridge-cors-st], ConfigProviderST=[config-provider-st], KafkaST=[kafka-st], LoggingChangeST=[logging-change-st], HttpBridgeKafkaExternalListenersST=[http-bridge-kafka-external-listeners-st], OauthAbstractST=[oauth-abstract-st], ThrottlingQuotaST=[throttling-quota-st], MultipleListenersST=[multiple-listeners-st]}
2022-03-28 18:56:04 [ForkJoinPool-1-worker-23] DEBUG [SuiteThreadController:73] [bridge.HttpBridgeTlsST] - Parallel suites count: 5
2022-03-28 18:56:04 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:73] [bridge.HttpBridgeScramShaST] - Parallel suites count: 4
2022-03-28 18:56:04 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:667] [operators.ReconciliationST - Before All] - Setup test suite environment
2022-03-28 18:56:04 [ForkJoinPool-1-worker-23] DEBUG [SuiteThreadController:184] HttpBridgeTlsST suite now can proceed its execution
2022-03-28 18:56:04 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:184] HttpBridgeScramShaST suite now can proceed its execution
2022-03-28 18:56:04 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:69] [operators.ReconciliationST] - Adding parallel suite: ReconciliationST
2022-03-28 18:56:04 [ForkJoinPool-1-worker-9] DEBUG [TestSuiteNamespaceManager:128] Content of the test suite namespaces map:
{HttpBridgeTlsST=[http-bridge-tls-st], CruiseControlST=[cruise-control-st], AbstractST=[abstract-st], OpaIntegrationST=[opa-integration-st], CruiseControlApiST=[cruise-control-api-st], HttpBridgeScramShaST=[http-bridge-scram-sha-st], ReconciliationST=[reconciliation-st], DynamicConfST=[dynamic-conf-st], AbstractUpgradeST=[abstract-upgrade-st], QuotasST=[quotas-st], DynamicConfSharedST=[dynamic-conf-shared-st], RollingUpdateST=[rolling-update-st], OlmAbstractST=[olm-abstract-st], CustomAuthorizerST=[custom-authorizer-st], AlternativeReconcileTriggersST=[alternative-reconcile-triggers-st], LogSettingST=[log-setting-st], ListenersST=[listeners-st], UserST=[user-st], TopicST=[topic-st], SecurityST=[security-st], TracingST=[tracing-st], AbstractNamespaceST=[abstract-namespace-st], CruiseControlConfigurationST=[cruise-control-configuration-st], HttpBridgeCorsST=[http-bridge-cors-st], ConfigProviderST=[config-provider-st], KafkaST=[kafka-st], LoggingChangeST=[logging-change-st], HttpBridgeKafkaExternalListenersST=[http-bridge-kafka-external-listeners-st], OauthAbstractST=[oauth-abstract-st], ThrottlingQuotaST=[throttling-quota-st], MultipleListenersST=[multiple-listeners-st]}
2022-03-28 18:56:04 [ForkJoinPool-1-worker-17] DEBUG [AbstractST:667] [cruisecontrol.CruiseControlConfigurationST - Before All] - Setup test suite environment
2022-03-28 18:56:04 [ForkJoinPool-1-worker-9] DEBUG [TestSuiteNamespaceManager:129] Test suite `ThrottlingQuotaST` creates these additional namespaces:[throttling-quota-st]
2022-03-28 18:56:04 [ForkJoinPool-1-worker-23] DEBUG [TestSuiteNamespaceManager:128] Content of the test suite namespaces map:
{HttpBridgeTlsST=[http-bridge-tls-st], CruiseControlST=[cruise-control-st], AbstractST=[abstract-st], OpaIntegrationST=[opa-integration-st], CruiseControlApiST=[cruise-control-api-st], HttpBridgeScramShaST=[http-bridge-scram-sha-st], ReconciliationST=[reconciliation-st], DynamicConfST=[dynamic-conf-st], AbstractUpgradeST=[abstract-upgrade-st], QuotasST=[quotas-st], DynamicConfSharedST=[dynamic-conf-shared-st], RollingUpdateST=[rolling-update-st], OlmAbstractST=[olm-abstract-st], CustomAuthorizerST=[custom-authorizer-st], AlternativeReconcileTriggersST=[alternative-reconcile-triggers-st], LogSettingST=[log-setting-st], ListenersST=[listeners-st], UserST=[user-st], TopicST=[topic-st], SecurityST=[security-st], TracingST=[tracing-st], AbstractNamespaceST=[abstract-namespace-st], CruiseControlConfigurationST=[cruise-control-configuration-st], HttpBridgeCorsST=[http-bridge-cors-st], ConfigProviderST=[config-provider-st], KafkaST=[kafka-st], LoggingChangeST=[logging-change-st], HttpBridgeKafkaExternalListenersST=[http-bridge-kafka-external-listeners-st], OauthAbstractST=[oauth-abstract-st], ThrottlingQuotaST=[throttling-quota-st], MultipleListenersST=[multiple-listeners-st]}
2022-03-28 18:56:04 [ForkJoinPool-1-worker-27] DEBUG [TestSuiteNamespaceManager:129] Test suite `TopicST` creates these additional namespaces:[topic-st]
2022-03-28 18:56:04 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:73] [operators.ReconciliationST] - Parallel suites count: 6
2022-03-28 18:56:04 [ForkJoinPool-1-worker-31] DEBUG [TestSuiteNamespaceManager:129] Test suite `UserST` creates these additional namespaces:[user-st]
2022-03-28 18:56:04 [ForkJoinPool-1-worker-23] DEBUG [TestSuiteNamespaceManager:129] Test suite `HttpBridgeTlsST` creates these additional namespaces:[http-bridge-tls-st]
2022-03-28 18:56:04 [ForkJoinPool-1-worker-17] DEBUG [SuiteThreadController:69] [cruisecontrol.CruiseControlConfigurationST] - Adding parallel suite: CruiseControlConfigurationST
2022-03-28 18:56:04 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:184] ReconciliationST suite now can proceed its execution
2022-03-28 18:56:04 [ForkJoinPool-1-worker-17] DEBUG [SuiteThreadController:73] [cruisecontrol.CruiseControlConfigurationST] - Parallel suites count: 7
2022-03-28 18:56:04 [ForkJoinPool-1-worker-5] DEBUG [TestSuiteNamespaceManager:128] Content of the test suite namespaces map:
{HttpBridgeTlsST=[http-bridge-tls-st], CruiseControlST=[cruise-control-st], AbstractST=[abstract-st], OpaIntegrationST=[opa-integration-st], CruiseControlApiST=[cruise-control-api-st], HttpBridgeScramShaST=[http-bridge-scram-sha-st], ReconciliationST=[reconciliation-st], DynamicConfST=[dynamic-conf-st], AbstractUpgradeST=[abstract-upgrade-st], QuotasST=[quotas-st], DynamicConfSharedST=[dynamic-conf-shared-st], RollingUpdateST=[rolling-update-st], OlmAbstractST=[olm-abstract-st], CustomAuthorizerST=[custom-authorizer-st], AlternativeReconcileTriggersST=[alternative-reconcile-triggers-st], LogSettingST=[log-setting-st], ListenersST=[listeners-st], UserST=[user-st], TopicST=[topic-st], SecurityST=[security-st], TracingST=[tracing-st], AbstractNamespaceST=[abstract-namespace-st], CruiseControlConfigurationST=[cruise-control-configuration-st], HttpBridgeCorsST=[http-bridge-cors-st], ConfigProviderST=[config-provider-st], KafkaST=[kafka-st], LoggingChangeST=[logging-change-st], HttpBridgeKafkaExternalListenersST=[http-bridge-kafka-external-listeners-st], OauthAbstractST=[oauth-abstract-st], ThrottlingQuotaST=[throttling-quota-st], MultipleListenersST=[multiple-listeners-st]}
2022-03-28 18:56:04 [ForkJoinPool-1-worker-17] DEBUG [SuiteThreadController:184] CruiseControlConfigurationST suite now can proceed its execution
2022-03-28 18:56:04 [ForkJoinPool-1-worker-13] DEBUG [TestSuiteNamespaceManager:128] Content of the test suite namespaces map:
{HttpBridgeTlsST=[http-bridge-tls-st], CruiseControlST=[cruise-control-st], AbstractST=[abstract-st], OpaIntegrationST=[opa-integration-st], CruiseControlApiST=[cruise-control-api-st], HttpBridgeScramShaST=[http-bridge-scram-sha-st], ReconciliationST=[reconciliation-st], DynamicConfST=[dynamic-conf-st], AbstractUpgradeST=[abstract-upgrade-st], QuotasST=[quotas-st], DynamicConfSharedST=[dynamic-conf-shared-st], RollingUpdateST=[rolling-update-st], OlmAbstractST=[olm-abstract-st], CustomAuthorizerST=[custom-authorizer-st], AlternativeReconcileTriggersST=[alternative-reconcile-triggers-st], LogSettingST=[log-setting-st], ListenersST=[listeners-st], UserST=[user-st], TopicST=[topic-st], SecurityST=[security-st], TracingST=[tracing-st], AbstractNamespaceST=[abstract-namespace-st], CruiseControlConfigurationST=[cruise-control-configuration-st], HttpBridgeCorsST=[http-bridge-cors-st], ConfigProviderST=[config-provider-st], KafkaST=[kafka-st], LoggingChangeST=[logging-change-st], HttpBridgeKafkaExternalListenersST=[http-bridge-kafka-external-listeners-st], OauthAbstractST=[oauth-abstract-st], ThrottlingQuotaST=[throttling-quota-st], MultipleListenersST=[multiple-listeners-st]}
2022-03-28 18:56:04 [ForkJoinPool-1-worker-17] DEBUG [TestSuiteNamespaceManager:128] Content of the test suite namespaces map:
{HttpBridgeTlsST=[http-bridge-tls-st], CruiseControlST=[cruise-control-st], AbstractST=[abstract-st], OpaIntegrationST=[opa-integration-st], CruiseControlApiST=[cruise-control-api-st], HttpBridgeScramShaST=[http-bridge-scram-sha-st], ReconciliationST=[reconciliation-st], DynamicConfST=[dynamic-conf-st], AbstractUpgradeST=[abstract-upgrade-st], QuotasST=[quotas-st], DynamicConfSharedST=[dynamic-conf-shared-st], RollingUpdateST=[rolling-update-st], OlmAbstractST=[olm-abstract-st], CustomAuthorizerST=[custom-authorizer-st], AlternativeReconcileTriggersST=[alternative-reconcile-triggers-st], LogSettingST=[log-setting-st], ListenersST=[listeners-st], UserST=[user-st], TopicST=[topic-st], SecurityST=[security-st], TracingST=[tracing-st], AbstractNamespaceST=[abstract-namespace-st], CruiseControlConfigurationST=[cruise-control-configuration-st], HttpBridgeCorsST=[http-bridge-cors-st], ConfigProviderST=[config-provider-st], KafkaST=[kafka-st], LoggingChangeST=[logging-change-st], HttpBridgeKafkaExternalListenersST=[http-bridge-kafka-external-listeners-st], OauthAbstractST=[oauth-abstract-st], ThrottlingQuotaST=[throttling-quota-st], MultipleListenersST=[multiple-listeners-st]}
2022-03-28 18:56:04 [ForkJoinPool-1-worker-5] DEBUG [TestSuiteNamespaceManager:129] Test suite `HttpBridgeScramShaST` creates these additional namespaces:[http-bridge-scram-sha-st]
2022-03-28 18:56:04 [ForkJoinPool-1-worker-13] DEBUG [TestSuiteNamespaceManager:129] Test suite `ReconciliationST` creates these additional namespaces:[reconciliation-st]
2022-03-28 18:56:04 [ForkJoinPool-1-worker-17] DEBUG [TestSuiteNamespaceManager:129] Test suite `CruiseControlConfigurationST` creates these additional namespaces:[cruise-control-configuration-st]
2022-03-28 18:56:04 [ForkJoinPool-1-worker-31] INFO  [KubeClusterResource:156] Creating Namespace: user-st
2022-03-28 18:56:04 [ForkJoinPool-1-worker-13] INFO  [KubeClusterResource:156] Creating Namespace: reconciliation-st
2022-03-28 18:56:04 [ForkJoinPool-1-worker-9] INFO  [KubeClusterResource:156] Creating Namespace: throttling-quota-st
2022-03-28 18:56:04 [ForkJoinPool-1-worker-23] INFO  [KubeClusterResource:156] Creating Namespace: http-bridge-tls-st
2022-03-28 18:56:04 [ForkJoinPool-1-worker-5] INFO  [KubeClusterResource:156] Creating Namespace: http-bridge-scram-sha-st
2022-03-28 18:56:04 [ForkJoinPool-1-worker-17] INFO  [KubeClusterResource:156] Creating Namespace: cruise-control-configuration-st
2022-03-28 18:56:04 [ForkJoinPool-1-worker-27] INFO  [KubeClusterResource:156] Creating Namespace: topic-st
2022-03-28 18:56:04 [ForkJoinPool-1-worker-31] DEBUG [TestUtils:125] Waiting for Namespace user-st
2022-03-28 18:56:04 [ForkJoinPool-1-worker-31] TRACE [Exec:248] Running command - oc --namespace infra-namespace get Namespace user-st -o json
2022-03-28 18:56:04 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Namespace reconciliation-st
2022-03-28 18:56:04 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace infra-namespace get Namespace reconciliation-st -o json
2022-03-28 18:56:04 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for Namespace http-bridge-tls-st
2022-03-28 18:56:04 [ForkJoinPool-1-worker-23] TRACE [Exec:248] Running command - oc --namespace infra-namespace get Namespace http-bridge-tls-st -o json
2022-03-28 18:56:04 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Namespace http-bridge-scram-sha-st
2022-03-28 18:56:04 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace infra-namespace get Namespace http-bridge-scram-sha-st -o json
2022-03-28 18:56:04 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Namespace throttling-quota-st
2022-03-28 18:56:04 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace infra-namespace get Namespace throttling-quota-st -o json
2022-03-28 18:56:04 [ForkJoinPool-1-worker-27] DEBUG [TestUtils:125] Waiting for Namespace topic-st
2022-03-28 18:56:04 [ForkJoinPool-1-worker-27] TRACE [Exec:248] Running command - oc --namespace infra-namespace get Namespace topic-st -o json
2022-03-28 18:56:04 [ForkJoinPool-1-worker-17] DEBUG [TestUtils:125] Waiting for Namespace cruise-control-configuration-st
2022-03-28 18:56:04 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace infra-namespace get Namespace cruise-control-configuration-st -o json
2022-03-28 18:56:04 [ForkJoinPool-1-worker-31] DEBUG [Exec:419] Command: oc --namespace infra-namespace get Namespace user-st -o json
2022-03-28 18:56:04 [ForkJoinPool-1-worker-31] DEBUG [Exec:419] Return code: 0
2022-03-28 18:56:04 [ForkJoinPool-1-worker-31] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c40,c35",
            "openshift.io/sa.scc.supplemental-groups": "1001630000/10000",
            "openshift.io/sa.scc.uid-range": "1001630000/10000"
        },
        "creationTimestamp": "2022-03-28T18:55:59Z",
        "labels": {
            "kubernetes.io/metadata.name": "user-st"
        },
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T18:55:59Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:labels": {
                            ".": {},
                            "f:kubernetes.io/metadata.name": {}
                        }
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T18:55:59Z"
            }
        ],
        "name": "user-st",
        "resourceVersion": "258970",
        "uid": "bbbec725-c5e4-41e1-a7b4-3a18391926bf"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 18:56:04 [ForkJoinPool-1-worker-31] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st]}
2022-03-28 18:56:04 [ForkJoinPool-1-worker-31] INFO  [KubeClusterResource:82] Client use Namespace: user-st
2022-03-28 18:56:04 [ForkJoinPool-1-worker-31] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=user-st, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 18:56:04 [ForkJoinPool-1-worker-31] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: user-st
2022-03-28 18:56:05 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace infra-namespace get Namespace reconciliation-st -o json
2022-03-28 18:56:05 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 18:56:05 [ForkJoinPool-1-worker-13] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c41,c0",
            "openshift.io/sa.scc.supplemental-groups": "1001640000/10000",
            "openshift.io/sa.scc.uid-range": "1001640000/10000"
        },
        "creationTimestamp": "2022-03-28T18:55:59Z",
        "labels": {
            "kubernetes.io/metadata.name": "reconciliation-st"
        },
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T18:55:59Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:labels": {
                            ".": {},
                            "f:kubernetes.io/metadata.name": {}
                        }
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T18:55:59Z"
            }
        ],
        "name": "reconciliation-st",
        "resourceVersion": "258998",
        "uid": "2b1f1b01-3410-4c7f-9730-7f0bbf1e252f"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 18:56:05 [ForkJoinPool-1-worker-13] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st]}
2022-03-28 18:56:05 [ForkJoinPool-1-worker-13] INFO  [KubeClusterResource:82] Client use Namespace: reconciliation-st
2022-03-28 18:56:05 [ForkJoinPool-1-worker-13] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=reconciliation-st, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 18:56:05 [ForkJoinPool-1-worker-13] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: reconciliation-st
2022-03-28 18:56:05 [ForkJoinPool-1-worker-25] INFO  [TestSeparator:23] ############################################################################
2022-03-28 18:56:05 [ForkJoinPool-1-worker-7] INFO  [TestSeparator:23] ############################################################################
2022-03-28 18:56:05 [ForkJoinPool-1-worker-25] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.ReconciliationST.testPauseReconciliationInKafkaAndKafkaConnectWithConnector-STARTED
2022-03-28 18:56:05 [ForkJoinPool-1-worker-7] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.ReconciliationST.testPauseReconciliationInKafkaRebalanceAndTopic-STARTED
2022-03-28 18:56:05 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:658] ============================================================================
2022-03-28 18:56:05 [ForkJoinPool-1-worker-25] DEBUG [AbstractST:658] ============================================================================
2022-03-28 18:56:05 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:659] [operators.ReconciliationST - Before Each] - Setup test case environment
2022-03-28 18:56:05 [ForkJoinPool-1-worker-25] DEBUG [AbstractST:659] [operators.ReconciliationST - Before Each] - Setup test case environment
2022-03-28 18:56:05 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:77] [operators.ReconciliationST] - Adding parallel test: testPauseReconciliationInKafkaRebalanceAndTopic
2022-03-28 18:56:05 [ForkJoinPool-1-worker-25] DEBUG [SuiteThreadController:77] [operators.ReconciliationST] - Adding parallel test: testPauseReconciliationInKafkaAndKafkaConnectWithConnector
2022-03-28 18:56:05 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:81] [operators.ReconciliationST] - Parallel test count: 1
2022-03-28 18:56:05 [ForkJoinPool-1-worker-25] DEBUG [SuiteThreadController:81] [operators.ReconciliationST] - Parallel test count: 2
2022-03-28 18:56:05 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:230] testPauseReconciliationInKafkaRebalanceAndTopic test now can proceed its execution
2022-03-28 18:56:05 [ForkJoinPool-1-worker-25] DEBUG [SuiteThreadController:230] testPauseReconciliationInKafkaAndKafkaConnectWithConnector test now can proceed its execution
2022-03-28 18:56:05 [ForkJoinPool-1-worker-7] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 18:56:05 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace infra-namespace get Namespace http-bridge-scram-sha-st -o json
2022-03-28 18:56:05 [ForkJoinPool-1-worker-7] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-959c6ed5}
2022-03-28 18:56:05 [ForkJoinPool-1-worker-7] TRACE [AbstractST:607] USERS_NAME_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-user-2104248162-45187051}
2022-03-28 18:56:05 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 18:56:05 [ForkJoinPool-1-worker-7] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-634125597-1746493046}
2022-03-28 18:56:05 [ForkJoinPool-1-worker-5] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c41,c10",
            "openshift.io/sa.scc.supplemental-groups": "1001660000/10000",
            "openshift.io/sa.scc.uid-range": "1001660000/10000"
        },
        "creationTimestamp": "2022-03-28T18:55:59Z",
        "labels": {
            "kubernetes.io/metadata.name": "http-bridge-scram-sha-st"
        },
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:labels": {
                            ".": {},
                            "f:kubernetes.io/metadata.name": {}
                        }
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T18:55:59Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T18:56:00Z"
            }
        ],
        "name": "http-bridge-scram-sha-st",
        "resourceVersion": "259071",
        "uid": "d58dbe62-d7d0-4c8b-8d33-0e07ed62e383"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 18:56:05 [ForkJoinPool-1-worker-7] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-959c6ed5-kafka-clients}
2022-03-28 18:56:05 [ForkJoinPool-1-worker-5] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[http-bridge-scram-sha-st], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st]}
2022-03-28 18:56:05 [ForkJoinPool-1-worker-7] INFO  [TestSuiteNamespaceManager:163] Creating namespace:namespace-0 for test case:testPauseReconciliationInKafkaRebalanceAndTopic
2022-03-28 18:56:05 [ForkJoinPool-1-worker-5] INFO  [KubeClusterResource:82] Client use Namespace: http-bridge-scram-sha-st
2022-03-28 18:56:05 [ForkJoinPool-1-worker-5] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=http-bridge-scram-sha-st, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 18:56:05 [ForkJoinPool-1-worker-5] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: http-bridge-scram-sha-st
2022-03-28 18:56:05 [ForkJoinPool-1-worker-5] INFO  [HttpBridgeScramShaST:123] Deploy Kafka and KafkaBridge before tests
2022-03-28 18:56:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace infra-namespace get Namespace throttling-quota-st -o json
2022-03-28 18:56:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 18:56:05 [ForkJoinPool-1-worker-9] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c41,c15",
            "openshift.io/sa.scc.supplemental-groups": "1001670000/10000",
            "openshift.io/sa.scc.uid-range": "1001670000/10000"
        },
        "creationTimestamp": "2022-03-28T18:55:59Z",
        "labels": {
            "kubernetes.io/metadata.name": "throttling-quota-st"
        },
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:labels": {
                            ".": {},
                            "f:kubernetes.io/metadata.name": {}
                        }
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T18:55:59Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T18:56:00Z"
            }
        ],
        "name": "throttling-quota-st",
        "resourceVersion": "259107",
        "uid": "e7d14372-3ff5-49d2-8f6b-8c3400db6101"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 18:56:05 [ForkJoinPool-1-worker-23] DEBUG [Exec:419] Command: oc --namespace infra-namespace get Namespace http-bridge-tls-st -o json
2022-03-28 18:56:05 [ForkJoinPool-1-worker-23] DEBUG [Exec:419] Return code: 0
2022-03-28 18:56:05 [ForkJoinPool-1-worker-23] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c41,c5",
            "openshift.io/sa.scc.supplemental-groups": "1001650000/10000",
            "openshift.io/sa.scc.uid-range": "1001650000/10000"
        },
        "creationTimestamp": "2022-03-28T18:55:59Z",
        "labels": {
            "kubernetes.io/metadata.name": "http-bridge-tls-st"
        },
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:labels": {
                            ".": {},
                            "f:kubernetes.io/metadata.name": {}
                        }
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T18:55:59Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T18:56:00Z"
            }
        ],
        "name": "http-bridge-tls-st",
        "resourceVersion": "259027",
        "uid": "dab89607-ad95-44f0-b8b1-da5189b3f21a"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 18:56:05 [ForkJoinPool-1-worker-9] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[http-bridge-scram-sha-st], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st]}
2022-03-28 18:56:05 [ForkJoinPool-1-worker-9] INFO  [KubeClusterResource:82] Client use Namespace: throttling-quota-st
2022-03-28 18:56:05 [ForkJoinPool-1-worker-23] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[http-bridge-scram-sha-st], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st]}
2022-03-28 18:56:05 [ForkJoinPool-1-worker-23] INFO  [KubeClusterResource:82] Client use Namespace: http-bridge-tls-st
2022-03-28 18:56:05 [ForkJoinPool-1-worker-9] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=throttling-quota-st, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 18:56:05 [ForkJoinPool-1-worker-27] DEBUG [Exec:419] Command: oc --namespace infra-namespace get Namespace topic-st -o json
2022-03-28 18:56:05 [ForkJoinPool-1-worker-23] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=http-bridge-tls-st, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 18:56:05 [ForkJoinPool-1-worker-27] DEBUG [Exec:419] Return code: 0
2022-03-28 18:56:05 [ForkJoinPool-1-worker-9] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: throttling-quota-st
2022-03-28 18:56:05 [ForkJoinPool-1-worker-23] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: http-bridge-tls-st
2022-03-28 18:56:05 [ForkJoinPool-1-worker-27] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c41,c20",
            "openshift.io/sa.scc.supplemental-groups": "1001680000/10000",
            "openshift.io/sa.scc.uid-range": "1001680000/10000"
        },
        "creationTimestamp": "2022-03-28T18:55:59Z",
        "labels": {
            "kubernetes.io/metadata.name": "topic-st"
        },
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:labels": {
                            ".": {},
                            "f:kubernetes.io/metadata.name": {}
                        }
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T18:55:59Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T18:56:00Z"
            }
        ],
        "name": "topic-st",
        "resourceVersion": "259121",
        "uid": "200de35d-ec18-4590-a7e1-8bee8fc8abe4"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 18:56:05 [ForkJoinPool-1-worker-9] INFO  [ThrottlingQuotaST:304] Deploying shared Kafka across all test cases in throttling-quota-st namespace
2022-03-28 18:56:05 [ForkJoinPool-1-worker-23] INFO  [HttpBridgeTlsST:129] Deploy Kafka and KafkaBridge before tests
2022-03-28 18:56:05 [ForkJoinPool-1-worker-27] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[http-bridge-scram-sha-st], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st]}
2022-03-28 18:56:05 [ForkJoinPool-1-worker-27] INFO  [KubeClusterResource:82] Client use Namespace: topic-st
2022-03-28 18:56:05 [ForkJoinPool-1-worker-27] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=topic-st, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 18:56:05 [ForkJoinPool-1-worker-27] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: topic-st
2022-03-28 18:56:05 [ForkJoinPool-1-worker-27] INFO  [TopicST:491] Deploying shared Kafka across all test cases in topic-st namespace
2022-03-28 18:56:05 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace infra-namespace get Namespace cruise-control-configuration-st -o json
2022-03-28 18:56:05 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 18:56:05 [ForkJoinPool-1-worker-17] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c41,c25",
            "openshift.io/sa.scc.supplemental-groups": "1001690000/10000",
            "openshift.io/sa.scc.uid-range": "1001690000/10000"
        },
        "creationTimestamp": "2022-03-28T18:55:59Z",
        "labels": {
            "kubernetes.io/metadata.name": "cruise-control-configuration-st"
        },
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:labels": {
                            ".": {},
                            "f:kubernetes.io/metadata.name": {}
                        }
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T18:55:59Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T18:56:00Z"
            }
        ],
        "name": "cruise-control-configuration-st",
        "resourceVersion": "259149",
        "uid": "f4fdf9c4-7059-4ec0-b525-b37b2c595a6f"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 18:56:05 [ForkJoinPool-1-worker-17] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[http-bridge-scram-sha-st], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st]}
2022-03-28 18:56:05 [ForkJoinPool-1-worker-17] INFO  [KubeClusterResource:82] Client use Namespace: cruise-control-configuration-st
2022-03-28 18:56:05 [ForkJoinPool-1-worker-17] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=cruise-control-configuration-st, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 18:56:05 [ForkJoinPool-1-worker-17] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: cruise-control-configuration-st
2022-03-28 18:56:05 [ForkJoinPool-1-worker-17] INFO  [TestSeparator:23] ############################################################################
2022-03-28 18:56:05 [ForkJoinPool-1-worker-17] INFO  [TestSeparator:24] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testCapacityFile-STARTED
2022-03-28 18:56:05 [ForkJoinPool-1-worker-15] INFO  [TestSeparator:23] ############################################################################
2022-03-28 18:56:05 [ForkJoinPool-1-worker-11] INFO  [TestSeparator:23] ############################################################################
2022-03-28 18:56:05 [ForkJoinPool-1-worker-3] INFO  [TestSeparator:23] ############################################################################
2022-03-28 18:56:05 [ForkJoinPool-1-worker-21] INFO  [TestSeparator:23] ############################################################################
2022-03-28 18:56:05 [ForkJoinPool-1-worker-29] INFO  [TestSeparator:23] ############################################################################
2022-03-28 18:56:05 [ForkJoinPool-1-worker-15] INFO  [TestSeparator:24] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testDeployAndUnDeployCruiseControl-STARTED
2022-03-28 18:56:05 [ForkJoinPool-1-worker-11] INFO  [TestSeparator:24] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testConfigurationPerformanceOptions-STARTED
2022-03-28 18:56:05 [ForkJoinPool-1-worker-3] INFO  [TestSeparator:24] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods-STARTED
2022-03-28 18:56:05 [ForkJoinPool-1-worker-17] DEBUG [AbstractST:658] ============================================================================
2022-03-28 18:56:05 [ForkJoinPool-1-worker-21] INFO  [TestSeparator:24] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testConfigurationReflection-STARTED
2022-03-28 18:56:05 [ForkJoinPool-1-worker-3] DEBUG [AbstractST:658] ============================================================================
2022-03-28 18:56:05 [ForkJoinPool-1-worker-11] DEBUG [AbstractST:658] ============================================================================
2022-03-28 18:56:05 [ForkJoinPool-1-worker-15] DEBUG [AbstractST:658] ============================================================================
2022-03-28 18:56:05 [ForkJoinPool-1-worker-17] DEBUG [AbstractST:659] [cruisecontrol.CruiseControlConfigurationST - Before Each] - Setup test case environment
2022-03-28 18:56:05 [ForkJoinPool-1-worker-29] INFO  [TestSeparator:24] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testConfigurationFileIsCreated-STARTED
2022-03-28 18:56:05 [ForkJoinPool-1-worker-11] DEBUG [AbstractST:659] [cruisecontrol.CruiseControlConfigurationST - Before Each] - Setup test case environment
2022-03-28 18:56:05 [ForkJoinPool-1-worker-21] DEBUG [AbstractST:658] ============================================================================
2022-03-28 18:56:05 [ForkJoinPool-1-worker-17] DEBUG [SuiteThreadController:77] [cruisecontrol.CruiseControlConfigurationST] - Adding parallel test: testCapacityFile
2022-03-28 18:56:05 [ForkJoinPool-1-worker-11] DEBUG [SuiteThreadController:77] [cruisecontrol.CruiseControlConfigurationST] - Adding parallel test: testConfigurationPerformanceOptions
2022-03-28 18:56:05 [ForkJoinPool-1-worker-17] DEBUG [SuiteThreadController:81] [cruisecontrol.CruiseControlConfigurationST] - Parallel test count: 3
2022-03-28 18:56:05 [ForkJoinPool-1-worker-29] DEBUG [AbstractST:658] ============================================================================
2022-03-28 18:56:05 [ForkJoinPool-1-worker-17] DEBUG [SuiteThreadController:230] testCapacityFile test now can proceed its execution
2022-03-28 18:56:05 [ForkJoinPool-1-worker-29] DEBUG [AbstractST:659] [cruisecontrol.CruiseControlConfigurationST - Before Each] - Setup test case environment
2022-03-28 18:56:05 [ForkJoinPool-1-worker-29] DEBUG [SuiteThreadController:77] [cruisecontrol.CruiseControlConfigurationST] - Adding parallel test: testConfigurationFileIsCreated
2022-03-28 18:56:05 [ForkJoinPool-1-worker-29] DEBUG [SuiteThreadController:81] [cruisecontrol.CruiseControlConfigurationST] - Parallel test count: 5
2022-03-28 18:56:05 [ForkJoinPool-1-worker-29] DEBUG [SuiteThreadController:230] testConfigurationFileIsCreated test now can proceed its execution
2022-03-28 18:56:05 [ForkJoinPool-1-worker-11] DEBUG [SuiteThreadController:81] [cruisecontrol.CruiseControlConfigurationST] - Parallel test count: 4
2022-03-28 18:56:05 [ForkJoinPool-1-worker-21] DEBUG [AbstractST:659] [cruisecontrol.CruiseControlConfigurationST - Before Each] - Setup test case environment
2022-03-28 18:56:05 [ForkJoinPool-1-worker-15] DEBUG [AbstractST:659] [cruisecontrol.CruiseControlConfigurationST - Before Each] - Setup test case environment
2022-03-28 18:56:05 [ForkJoinPool-1-worker-3] DEBUG [AbstractST:659] [cruisecontrol.CruiseControlConfigurationST - Before Each] - Setup test case environment
2022-03-28 18:56:05 [ForkJoinPool-1-worker-21] DEBUG [SuiteThreadController:77] [cruisecontrol.CruiseControlConfigurationST] - Adding parallel test: testConfigurationReflection
2022-03-28 18:56:05 [ForkJoinPool-1-worker-11] DEBUG [SuiteThreadController:230] testConfigurationPerformanceOptions test now can proceed its execution
2022-03-28 18:56:05 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:77] [cruisecontrol.CruiseControlConfigurationST] - Adding parallel test: testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods
2022-03-28 18:56:05 [ForkJoinPool-1-worker-15] DEBUG [SuiteThreadController:77] [cruisecontrol.CruiseControlConfigurationST] - Adding parallel test: testDeployAndUnDeployCruiseControl
2022-03-28 18:56:05 [ForkJoinPool-1-worker-21] DEBUG [SuiteThreadController:81] [cruisecontrol.CruiseControlConfigurationST] - Parallel test count: 6
2022-03-28 18:56:05 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:81] [cruisecontrol.CruiseControlConfigurationST] - Parallel test count: 7
2022-03-28 18:56:05 [ForkJoinPool-1-worker-15] DEBUG [SuiteThreadController:81] [cruisecontrol.CruiseControlConfigurationST] - Parallel test count: 8
2022-03-28 18:56:05 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:230] testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods test now can proceed its execution
2022-03-28 18:56:05 [ForkJoinPool-1-worker-15] DEBUG [SuiteThreadController:230] testDeployAndUnDeployCruiseControl test now can proceed its execution
2022-03-28 18:56:05 [ForkJoinPool-1-worker-21] DEBUG [SuiteThreadController:230] testConfigurationReflection test now can proceed its execution
2022-03-28 18:56:05 [ForkJoinPool-1-worker-7] INFO  [KubeClusterResource:156] Creating Namespace: namespace-0
2022-03-28 18:56:05 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Namespace namespace-0
2022-03-28 18:56:05 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace cruise-control-configuration-st get Namespace namespace-0 -o json
2022-03-28 18:56:05 [ForkJoinPool-1-worker-31] INFO  [ResourceManager:155] Create/Update Kafka user-cluster-name in namespace user-st
2022-03-28 18:56:05 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:155] Create/Update Kafka http-bridge-scram-sha-cluster-name in namespace http-bridge-scram-sha-st
2022-03-28 18:56:05 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:155] Create/Update Kafka quota-cluster in namespace throttling-quota-st
2022-03-28 18:56:05 [ForkJoinPool-1-worker-27] INFO  [ResourceManager:155] Create/Update Kafka topic-cluster-name in namespace topic-st
2022-03-28 18:56:05 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:155] Create/Update Kafka http-bridge-tls-cluster-name in namespace http-bridge-tls-st
2022-03-28 18:56:05 [ForkJoinPool-1-worker-27] WARN  [VersionUsageUtils:60] The client is using resource type 'kafkas' with unstable version 'v1beta2'
2022-03-28 18:56:05 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace cruise-control-configuration-st get Namespace namespace-0 -o json
2022-03-28 18:56:05 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 18:56:05 [ForkJoinPool-1-worker-7] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c41,c30",
            "openshift.io/sa.scc.supplemental-groups": "1001700000/10000",
            "openshift.io/sa.scc.uid-range": "1001700000/10000"
        },
        "creationTimestamp": "2022-03-28T18:56:00Z",
        "labels": {
            "kubernetes.io/metadata.name": "namespace-0"
        },
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T18:56:00Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:labels": {
                            ".": {},
                            "f:kubernetes.io/metadata.name": {}
                        }
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T18:56:00Z"
            }
        ],
        "name": "namespace-0",
        "resourceVersion": "259217",
        "uid": "d60b12ce-6362-46a4-84fc-7fa45f93be59"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 18:56:05 [ForkJoinPool-1-worker-7] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[http-bridge-scram-sha-st], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[namespace-0]}
2022-03-28 18:56:05 [ForkJoinPool-1-worker-7] INFO  [KubeClusterResource:82] Client use Namespace: namespace-0
2022-03-28 18:56:05 [ForkJoinPool-1-worker-7] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-0, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 18:56:05 [ForkJoinPool-1-worker-7] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-0
2022-03-28 18:56:05 [ForkJoinPool-1-worker-21] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 18:56:05 [ForkJoinPool-1-worker-21] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-959c6ed5, testConfigurationReflection=my-cluster-188382d8}
2022-03-28 18:56:05 [ForkJoinPool-1-worker-21] TRACE [AbstractST:607] USERS_NAME_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-user-2104248162-45187051, testConfigurationReflection=my-user-1890969183-1754686912}
2022-03-28 18:56:05 [ForkJoinPool-1-worker-21] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-634125597-1746493046, testConfigurationReflection=my-topic-1394577332-1936226438}
2022-03-28 18:56:05 [ForkJoinPool-1-worker-21] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-959c6ed5-kafka-clients, testConfigurationReflection=my-cluster-188382d8-kafka-clients}
2022-03-28 18:56:05 [ForkJoinPool-1-worker-21] INFO  [TestSuiteNamespaceManager:163] Creating namespace:namespace-1 for test case:testConfigurationReflection
2022-03-28 18:56:05 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:quota-cluster
2022-03-28 18:56:05 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update Kafka my-cluster-959c6ed5 in namespace namespace-0
2022-03-28 18:56:05 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:164] Using Namespace: namespace-0
2022-03-28 18:56:05 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:http-bridge-scram-sha-cluster-name
2022-03-28 18:56:05 [ForkJoinPool-1-worker-31] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:user-cluster-name
2022-03-28 18:56:05 [ForkJoinPool-1-worker-21] INFO  [KubeClusterResource:156] Creating Namespace: namespace-1
2022-03-28 18:56:05 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:http-bridge-tls-cluster-name
2022-03-28 18:56:05 [ForkJoinPool-1-worker-27] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:topic-cluster-name
2022-03-28 18:56:06 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:433] Wait for Kafka: quota-cluster will have desired state: Ready
2022-03-28 18:56:06 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Kafka: quota-cluster will have desired state: Ready
2022-03-28 18:56:06 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-959c6ed5
2022-03-28 18:56:06 [ForkJoinPool-1-worker-27] INFO  [ResourceManager:433] Wait for Kafka: topic-cluster-name will have desired state: Ready
2022-03-28 18:56:06 [ForkJoinPool-1-worker-31] INFO  [ResourceManager:433] Wait for Kafka: user-cluster-name will have desired state: Ready
2022-03-28 18:56:06 [ForkJoinPool-1-worker-31] DEBUG [TestUtils:125] Waiting for Kafka: user-cluster-name will have desired state: Ready
2022-03-28 18:56:06 [ForkJoinPool-1-worker-27] DEBUG [TestUtils:125] Waiting for Kafka: topic-cluster-name will have desired state: Ready
2022-03-28 18:56:06 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:433] Wait for Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready
2022-03-28 18:56:06 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready
2022-03-28 18:56:06 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:433] Wait for Kafka: http-bridge-tls-cluster-name will have desired state: Ready
2022-03-28 18:56:06 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for Kafka: http-bridge-tls-cluster-name will have desired state: Ready
2022-03-28 18:56:06 [ForkJoinPool-1-worker-21] DEBUG [TestUtils:125] Waiting for Namespace namespace-1
2022-03-28 18:56:06 [ForkJoinPool-1-worker-21] TRACE [Exec:248] Running command - oc --namespace namespace-0 get Namespace namespace-1 -o json
2022-03-28 18:56:06 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (839817ms till timeout)
2022-03-28 18:56:06 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:433] Wait for Kafka: my-cluster-959c6ed5 will have desired state: Ready
2022-03-28 18:56:06 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Kafka: my-cluster-959c6ed5 will have desired state: Ready
2022-03-28 18:56:06 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (839813ms till timeout)
2022-03-28 18:56:06 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (839811ms till timeout)
2022-03-28 18:56:06 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (839810ms till timeout)
2022-03-28 18:56:06 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (839810ms till timeout)
2022-03-28 18:56:06 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1319810ms till timeout)
2022-03-28 18:56:06 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Command: oc --namespace namespace-0 get Namespace namespace-1 -o json
2022-03-28 18:56:06 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Return code: 0
2022-03-28 18:56:06 [ForkJoinPool-1-worker-21] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c41,c35",
            "openshift.io/sa.scc.supplemental-groups": "1001710000/10000",
            "openshift.io/sa.scc.uid-range": "1001710000/10000"
        },
        "creationTimestamp": "2022-03-28T18:56:01Z",
        "labels": {
            "kubernetes.io/metadata.name": "namespace-1"
        },
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T18:56:01Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:labels": {
                            ".": {},
                            "f:kubernetes.io/metadata.name": {}
                        }
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T18:56:01Z"
            }
        ],
        "name": "namespace-1",
        "resourceVersion": "259258",
        "uid": "c72cd2ba-6f83-4695-886f-648d73a11ed7"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 18:56:06 [ForkJoinPool-1-worker-21] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[http-bridge-scram-sha-st], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[namespace-0], io.strimzi.test.logs.CollectorElement@c4c0ea0=[namespace-1]}
2022-03-28 18:56:06 [ForkJoinPool-1-worker-21] INFO  [KubeClusterResource:82] Client use Namespace: namespace-1
2022-03-28 18:56:06 [ForkJoinPool-1-worker-21] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-1, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 18:56:06 [ForkJoinPool-1-worker-21] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-1
2022-03-28 18:56:06 [ForkJoinPool-1-worker-15] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 18:56:06 [ForkJoinPool-1-worker-15] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-959c6ed5, testDeployAndUnDeployCruiseControl=my-cluster-5dfd606f, testConfigurationReflection=my-cluster-188382d8}
2022-03-28 18:56:06 [ForkJoinPool-1-worker-15] TRACE [AbstractST:607] USERS_NAME_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-user-2104248162-45187051, testDeployAndUnDeployCruiseControl=my-user-506268651-1309117986, testConfigurationReflection=my-user-1890969183-1754686912}
2022-03-28 18:56:06 [ForkJoinPool-1-worker-15] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-634125597-1746493046, testDeployAndUnDeployCruiseControl=my-topic-465377127-800134460, testConfigurationReflection=my-topic-1394577332-1936226438}
2022-03-28 18:56:06 [ForkJoinPool-1-worker-15] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-959c6ed5-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-5dfd606f-kafka-clients, testConfigurationReflection=my-cluster-188382d8-kafka-clients}
2022-03-28 18:56:06 [ForkJoinPool-1-worker-15] INFO  [TestSuiteNamespaceManager:163] Creating namespace:namespace-2 for test case:testDeployAndUnDeployCruiseControl
2022-03-28 18:56:06 [ForkJoinPool-1-worker-21] INFO  [ResourceManager:155] Create/Update Kafka my-cluster-188382d8 in namespace namespace-1
2022-03-28 18:56:06 [ForkJoinPool-1-worker-21] INFO  [ResourceManager:164] Using Namespace: namespace-1
2022-03-28 18:56:06 [ForkJoinPool-1-worker-15] INFO  [KubeClusterResource:156] Creating Namespace: namespace-2
2022-03-28 18:56:06 [ForkJoinPool-1-worker-21] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-188382d8
2022-03-28 18:56:06 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Namespace namespace-2
2022-03-28 18:56:06 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-1 get Namespace namespace-2 -o json
2022-03-28 18:56:06 [ForkJoinPool-1-worker-21] INFO  [ResourceManager:433] Wait for Kafka: my-cluster-188382d8 will have desired state: Ready
2022-03-28 18:56:06 [ForkJoinPool-1-worker-21] DEBUG [TestUtils:125] Waiting for Kafka: my-cluster-188382d8 will have desired state: Ready
2022-03-28 18:56:07 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1319901ms till timeout)
2022-03-28 18:56:07 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (838716ms till timeout)
2022-03-28 18:56:07 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-1 get Namespace namespace-2 -o json
2022-03-28 18:56:07 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 18:56:07 [ForkJoinPool-1-worker-15] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c41,c40",
            "openshift.io/sa.scc.supplemental-groups": "1001720000/10000",
            "openshift.io/sa.scc.uid-range": "1001720000/10000"
        },
        "creationTimestamp": "2022-03-28T18:56:02Z",
        "labels": {
            "kubernetes.io/metadata.name": "namespace-2"
        },
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T18:56:02Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:labels": {
                            ".": {},
                            "f:kubernetes.io/metadata.name": {}
                        }
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T18:56:02Z"
            }
        ],
        "name": "namespace-2",
        "resourceVersion": "259301",
        "uid": "f57f1c90-fa0a-4c7d-bcfd-d06cc1a5c1bb"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 18:56:07 [ForkJoinPool-1-worker-15] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[http-bridge-scram-sha-st], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[namespace-0], io.strimzi.test.logs.CollectorElement@f4405b3b=[namespace-2], io.strimzi.test.logs.CollectorElement@c4c0ea0=[namespace-1]}
2022-03-28 18:56:07 [ForkJoinPool-1-worker-15] INFO  [KubeClusterResource:82] Client use Namespace: namespace-2
2022-03-28 18:56:07 [ForkJoinPool-1-worker-15] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-2, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 18:56:07 [ForkJoinPool-1-worker-15] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-2
2022-03-28 18:56:07 [ForkJoinPool-1-worker-3] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 18:56:07 [ForkJoinPool-1-worker-3] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-959c6ed5, testDeployAndUnDeployCruiseControl=my-cluster-5dfd606f, testConfigurationReflection=my-cluster-188382d8, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-e6343cec}
2022-03-28 18:56:07 [ForkJoinPool-1-worker-3] TRACE [AbstractST:607] USERS_NAME_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-user-2104248162-45187051, testDeployAndUnDeployCruiseControl=my-user-506268651-1309117986, testConfigurationReflection=my-user-1890969183-1754686912, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-2069267102-581538560}
2022-03-28 18:56:07 [ForkJoinPool-1-worker-3] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-634125597-1746493046, testDeployAndUnDeployCruiseControl=my-topic-465377127-800134460, testConfigurationReflection=my-topic-1394577332-1936226438, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-293775067-432059493}
2022-03-28 18:56:07 [ForkJoinPool-1-worker-3] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-959c6ed5-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-5dfd606f-kafka-clients, testConfigurationReflection=my-cluster-188382d8-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-e6343cec-kafka-clients}
2022-03-28 18:56:07 [ForkJoinPool-1-worker-3] INFO  [TestSuiteNamespaceManager:163] Creating namespace:namespace-3 for test case:testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods
2022-03-28 18:56:07 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:155] Create/Update Kafka my-cluster-5dfd606f in namespace namespace-2
2022-03-28 18:56:07 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:164] Using Namespace: namespace-2
2022-03-28 18:56:07 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (838715ms till timeout)
2022-03-28 18:56:07 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (838648ms till timeout)
2022-03-28 18:56:07 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (838649ms till timeout)
2022-03-28 18:56:07 [ForkJoinPool-1-worker-3] INFO  [KubeClusterResource:156] Creating Namespace: namespace-3
2022-03-28 18:56:07 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (838647ms till timeout)
2022-03-28 18:56:07 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-5dfd606f
2022-03-28 18:56:07 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1318620ms till timeout)
2022-03-28 18:56:07 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Namespace namespace-3
2022-03-28 18:56:07 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-2 get Namespace namespace-3 -o json
2022-03-28 18:56:07 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:433] Wait for Kafka: my-cluster-5dfd606f will have desired state: Ready
2022-03-28 18:56:07 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Kafka: my-cluster-5dfd606f will have desired state: Ready
2022-03-28 18:56:07 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1319901ms till timeout)
2022-03-28 18:56:08 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-2 get Namespace namespace-3 -o json
2022-03-28 18:56:08 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 18:56:08 [ForkJoinPool-1-worker-3] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c42,c4",
            "openshift.io/sa.scc.supplemental-groups": "1001730000/10000",
            "openshift.io/sa.scc.uid-range": "1001730000/10000"
        },
        "creationTimestamp": "2022-03-28T18:56:03Z",
        "labels": {
            "kubernetes.io/metadata.name": "namespace-3"
        },
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:labels": {
                            ".": {},
                            "f:kubernetes.io/metadata.name": {}
                        }
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T18:56:02Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T18:56:03Z"
            }
        ],
        "name": "namespace-3",
        "resourceVersion": "259345",
        "uid": "a7a1c04c-a0c5-4002-b66d-348d676f601e"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 18:56:08 [ForkJoinPool-1-worker-3] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@3881d5f2=[namespace-3], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[http-bridge-scram-sha-st], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[namespace-0], io.strimzi.test.logs.CollectorElement@f4405b3b=[namespace-2], io.strimzi.test.logs.CollectorElement@c4c0ea0=[namespace-1]}
2022-03-28 18:56:08 [ForkJoinPool-1-worker-3] INFO  [KubeClusterResource:82] Client use Namespace: namespace-3
2022-03-28 18:56:08 [ForkJoinPool-1-worker-3] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-3, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 18:56:08 [ForkJoinPool-1-worker-3] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-3
2022-03-28 18:56:08 [ForkJoinPool-1-worker-11] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 18:56:08 [ForkJoinPool-1-worker-11] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-959c6ed5, testDeployAndUnDeployCruiseControl=my-cluster-5dfd606f, testConfigurationPerformanceOptions=my-cluster-08515847, testConfigurationReflection=my-cluster-188382d8, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-e6343cec}
2022-03-28 18:56:08 [ForkJoinPool-1-worker-11] TRACE [AbstractST:607] USERS_NAME_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-user-2104248162-45187051, testDeployAndUnDeployCruiseControl=my-user-506268651-1309117986, testConfigurationPerformanceOptions=my-user-98362545-1476587236, testConfigurationReflection=my-user-1890969183-1754686912, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-2069267102-581538560}
2022-03-28 18:56:08 [ForkJoinPool-1-worker-11] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-634125597-1746493046, testDeployAndUnDeployCruiseControl=my-topic-465377127-800134460, testConfigurationPerformanceOptions=my-topic-696588651-1198599652, testConfigurationReflection=my-topic-1394577332-1936226438, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-293775067-432059493}
2022-03-28 18:56:08 [ForkJoinPool-1-worker-11] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-959c6ed5-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-5dfd606f-kafka-clients, testConfigurationPerformanceOptions=my-cluster-08515847-kafka-clients, testConfigurationReflection=my-cluster-188382d8-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-e6343cec-kafka-clients}
2022-03-28 18:56:08 [ForkJoinPool-1-worker-11] INFO  [TestSuiteNamespaceManager:163] Creating namespace:namespace-4 for test case:testConfigurationPerformanceOptions
2022-03-28 18:56:08 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:155] Create/Update Kafka my-cluster-e6343cec in namespace namespace-3
2022-03-28 18:56:08 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:164] Using Namespace: namespace-3
2022-03-28 18:56:08 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1318802ms till timeout)
2022-03-28 18:56:08 [ForkJoinPool-1-worker-11] INFO  [KubeClusterResource:156] Creating Namespace: namespace-4
2022-03-28 18:56:08 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-e6343cec
2022-03-28 18:56:08 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Namespace namespace-4
2022-03-28 18:56:08 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace namespace-3 get Namespace namespace-4 -o json
2022-03-28 18:56:08 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:433] Wait for Kafka: my-cluster-e6343cec will have desired state: Ready
2022-03-28 18:56:08 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Kafka: my-cluster-e6343cec will have desired state: Ready
2022-03-28 18:56:08 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (837585ms till timeout)
2022-03-28 18:56:08 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (837581ms till timeout)
2022-03-28 18:56:08 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (837483ms till timeout)
2022-03-28 18:56:08 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1319808ms till timeout)
2022-03-28 18:56:08 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (837483ms till timeout)
2022-03-28 18:56:08 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (837481ms till timeout)
2022-03-28 18:56:08 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1317481ms till timeout)
2022-03-28 18:56:08 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace namespace-3 get Namespace namespace-4 -o json
2022-03-28 18:56:08 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 18:56:08 [ForkJoinPool-1-worker-11] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c42,c9",
            "openshift.io/sa.scc.supplemental-groups": "1001740000/10000",
            "openshift.io/sa.scc.uid-range": "1001740000/10000"
        },
        "creationTimestamp": "2022-03-28T18:56:03Z",
        "labels": {
            "kubernetes.io/metadata.name": "namespace-4"
        },
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T18:56:03Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:labels": {
                            ".": {},
                            "f:kubernetes.io/metadata.name": {}
                        }
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T18:56:03Z"
            }
        ],
        "name": "namespace-4",
        "resourceVersion": "259382",
        "uid": "883bb468-f7c2-419a-b6aa-153be0a6452f"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 18:56:08 [ForkJoinPool-1-worker-11] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[http-bridge-scram-sha-st], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c4c0ea0=[namespace-1], io.strimzi.test.logs.CollectorElement@f851b6c3=[namespace-4], io.strimzi.test.logs.CollectorElement@3881d5f2=[namespace-3], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[namespace-0], io.strimzi.test.logs.CollectorElement@f4405b3b=[namespace-2]}
2022-03-28 18:56:08 [ForkJoinPool-1-worker-11] INFO  [KubeClusterResource:82] Client use Namespace: namespace-4
2022-03-28 18:56:08 [ForkJoinPool-1-worker-11] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-4, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 18:56:08 [ForkJoinPool-1-worker-11] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-4
2022-03-28 18:56:08 [ForkJoinPool-1-worker-29] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 18:56:08 [ForkJoinPool-1-worker-29] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-959c6ed5, testDeployAndUnDeployCruiseControl=my-cluster-5dfd606f, testConfigurationFileIsCreated=my-cluster-17fc585b, testConfigurationPerformanceOptions=my-cluster-08515847, testConfigurationReflection=my-cluster-188382d8, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-e6343cec}
2022-03-28 18:56:08 [ForkJoinPool-1-worker-29] TRACE [AbstractST:607] USERS_NAME_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-user-2104248162-45187051, testDeployAndUnDeployCruiseControl=my-user-506268651-1309117986, testConfigurationFileIsCreated=my-user-355815510-1316268151, testConfigurationPerformanceOptions=my-user-98362545-1476587236, testConfigurationReflection=my-user-1890969183-1754686912, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-2069267102-581538560}
2022-03-28 18:56:08 [ForkJoinPool-1-worker-29] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-634125597-1746493046, testDeployAndUnDeployCruiseControl=my-topic-465377127-800134460, testConfigurationFileIsCreated=my-topic-423850169-855699454, testConfigurationPerformanceOptions=my-topic-696588651-1198599652, testConfigurationReflection=my-topic-1394577332-1936226438, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-293775067-432059493}
2022-03-28 18:56:08 [ForkJoinPool-1-worker-29] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-959c6ed5-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-5dfd606f-kafka-clients, testConfigurationFileIsCreated=my-cluster-17fc585b-kafka-clients, testConfigurationPerformanceOptions=my-cluster-08515847-kafka-clients, testConfigurationReflection=my-cluster-188382d8-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-e6343cec-kafka-clients}
2022-03-28 18:56:08 [ForkJoinPool-1-worker-29] INFO  [TestSuiteNamespaceManager:163] Creating namespace:namespace-5 for test case:testConfigurationFileIsCreated
2022-03-28 18:56:08 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:155] Create/Update Kafka my-cluster-08515847 in namespace namespace-4
2022-03-28 18:56:08 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:164] Using Namespace: namespace-4
2022-03-28 18:56:08 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1318797ms till timeout)
2022-03-28 18:56:08 [ForkJoinPool-1-worker-29] INFO  [KubeClusterResource:156] Creating Namespace: namespace-5
2022-03-28 18:56:08 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-08515847
2022-03-28 18:56:09 [ForkJoinPool-1-worker-29] DEBUG [TestUtils:125] Waiting for Namespace namespace-5
2022-03-28 18:56:09 [ForkJoinPool-1-worker-29] TRACE [Exec:248] Running command - oc --namespace namespace-4 get Namespace namespace-5 -o json
2022-03-28 18:56:09 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:433] Wait for Kafka: my-cluster-08515847 will have desired state: Ready
2022-03-28 18:56:09 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Kafka: my-cluster-08515847 will have desired state: Ready
2022-03-28 18:56:09 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1317676ms till timeout)
2022-03-28 18:56:09 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1319807ms till timeout)
2022-03-28 18:56:09 [ForkJoinPool-1-worker-29] DEBUG [Exec:419] Command: oc --namespace namespace-4 get Namespace namespace-5 -o json
2022-03-28 18:56:09 [ForkJoinPool-1-worker-29] DEBUG [Exec:419] Return code: 0
2022-03-28 18:56:09 [ForkJoinPool-1-worker-29] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c42,c14",
            "openshift.io/sa.scc.supplemental-groups": "1001750000/10000",
            "openshift.io/sa.scc.uid-range": "1001750000/10000"
        },
        "creationTimestamp": "2022-03-28T18:56:04Z",
        "labels": {
            "kubernetes.io/metadata.name": "namespace-5"
        },
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T18:56:04Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:labels": {
                            ".": {},
                            "f:kubernetes.io/metadata.name": {}
                        }
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T18:56:04Z"
            }
        ],
        "name": "namespace-5",
        "resourceVersion": "259421",
        "uid": "977e940b-8f78-4514-9556-e4201f9bbc63"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 18:56:09 [ForkJoinPool-1-worker-29] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[http-bridge-scram-sha-st], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c4c0ea0=[namespace-1], io.strimzi.test.logs.CollectorElement@aec142ef=[namespace-5], io.strimzi.test.logs.CollectorElement@f851b6c3=[namespace-4], io.strimzi.test.logs.CollectorElement@3881d5f2=[namespace-3], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[namespace-0], io.strimzi.test.logs.CollectorElement@f4405b3b=[namespace-2]}
2022-03-28 18:56:09 [ForkJoinPool-1-worker-29] INFO  [KubeClusterResource:82] Client use Namespace: namespace-5
2022-03-28 18:56:09 [ForkJoinPool-1-worker-29] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-5, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 18:56:09 [ForkJoinPool-1-worker-29] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-5
2022-03-28 18:56:09 [ForkJoinPool-1-worker-17] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 18:56:09 [ForkJoinPool-1-worker-17] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-959c6ed5, testDeployAndUnDeployCruiseControl=my-cluster-5dfd606f, testConfigurationFileIsCreated=my-cluster-17fc585b, testCapacityFile=my-cluster-855b9105, testConfigurationPerformanceOptions=my-cluster-08515847, testConfigurationReflection=my-cluster-188382d8, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-e6343cec}
2022-03-28 18:56:09 [ForkJoinPool-1-worker-17] TRACE [AbstractST:607] USERS_NAME_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-user-2104248162-45187051, testDeployAndUnDeployCruiseControl=my-user-506268651-1309117986, testConfigurationFileIsCreated=my-user-355815510-1316268151, testCapacityFile=my-user-1460643829-103298436, testConfigurationPerformanceOptions=my-user-98362545-1476587236, testConfigurationReflection=my-user-1890969183-1754686912, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-2069267102-581538560}
2022-03-28 18:56:09 [ForkJoinPool-1-worker-17] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-634125597-1746493046, testDeployAndUnDeployCruiseControl=my-topic-465377127-800134460, testConfigurationFileIsCreated=my-topic-423850169-855699454, testCapacityFile=my-topic-549312506-649708453, testConfigurationPerformanceOptions=my-topic-696588651-1198599652, testConfigurationReflection=my-topic-1394577332-1936226438, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-293775067-432059493}
2022-03-28 18:56:09 [ForkJoinPool-1-worker-17] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-959c6ed5-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-5dfd606f-kafka-clients, testConfigurationFileIsCreated=my-cluster-17fc585b-kafka-clients, testCapacityFile=my-cluster-855b9105-kafka-clients, testConfigurationPerformanceOptions=my-cluster-08515847-kafka-clients, testConfigurationReflection=my-cluster-188382d8-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-e6343cec-kafka-clients}
2022-03-28 18:56:09 [ForkJoinPool-1-worker-17] INFO  [TestSuiteNamespaceManager:163] Creating namespace:namespace-6 for test case:testCapacityFile
2022-03-28 18:56:09 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (836466ms till timeout)
2022-03-28 18:56:09 [ForkJoinPool-1-worker-29] INFO  [ResourceManager:155] Create/Update Kafka my-cluster-17fc585b in namespace namespace-5
2022-03-28 18:56:09 [ForkJoinPool-1-worker-29] INFO  [ResourceManager:164] Using Namespace: namespace-5
2022-03-28 18:56:09 [ForkJoinPool-1-worker-17] INFO  [KubeClusterResource:156] Creating Namespace: namespace-6
2022-03-28 18:56:09 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (836386ms till timeout)
2022-03-28 18:56:09 [ForkJoinPool-1-worker-29] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-17fc585b
2022-03-28 18:56:09 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (836294ms till timeout)
2022-03-28 18:56:09 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1318618ms till timeout)
2022-03-28 18:56:09 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (836290ms till timeout)
2022-03-28 18:56:09 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (836290ms till timeout)
2022-03-28 18:56:09 [ForkJoinPool-1-worker-17] DEBUG [TestUtils:125] Waiting for Namespace namespace-6
2022-03-28 18:56:09 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-5 get Namespace namespace-6 -o json
2022-03-28 18:56:09 [ForkJoinPool-1-worker-29] INFO  [ResourceManager:433] Wait for Kafka: my-cluster-17fc585b will have desired state: Ready
2022-03-28 18:56:09 [ForkJoinPool-1-worker-29] DEBUG [TestUtils:125] Waiting for Kafka: my-cluster-17fc585b will have desired state: Ready
2022-03-28 18:56:09 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1316290ms till timeout)
2022-03-28 18:56:10 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1317673ms till timeout)
2022-03-28 18:56:10 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1319811ms till timeout)
2022-03-28 18:56:10 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-5 get Namespace namespace-6 -o json
2022-03-28 18:56:10 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 18:56:10 [ForkJoinPool-1-worker-17] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c42,c19",
            "openshift.io/sa.scc.supplemental-groups": "1001760000/10000",
            "openshift.io/sa.scc.uid-range": "1001760000/10000"
        },
        "creationTimestamp": "2022-03-28T18:56:05Z",
        "labels": {
            "kubernetes.io/metadata.name": "namespace-6"
        },
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T18:56:05Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:labels": {
                            ".": {},
                            "f:kubernetes.io/metadata.name": {}
                        }
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T18:56:05Z"
            }
        ],
        "name": "namespace-6",
        "resourceVersion": "259469",
        "uid": "56cafa53-5332-4016-a6e8-3930f3ad8152"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 18:56:10 [ForkJoinPool-1-worker-17] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[http-bridge-scram-sha-st], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c4c0ea0=[namespace-1], io.strimzi.test.logs.CollectorElement@aec142ef=[namespace-5], io.strimzi.test.logs.CollectorElement@f851b6c3=[namespace-4], io.strimzi.test.logs.CollectorElement@3881d5f2=[namespace-3], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[namespace-0], io.strimzi.test.logs.CollectorElement@f4405b3b=[namespace-2], io.strimzi.test.logs.CollectorElement@3e255cd9=[namespace-6]}
2022-03-28 18:56:10 [ForkJoinPool-1-worker-17] INFO  [KubeClusterResource:82] Client use Namespace: namespace-6
2022-03-28 18:56:10 [ForkJoinPool-1-worker-17] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-6, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 18:56:10 [ForkJoinPool-1-worker-17] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-6
2022-03-28 18:56:10 [ForkJoinPool-1-worker-25] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 18:56:10 [ForkJoinPool-1-worker-25] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-959c6ed5, testDeployAndUnDeployCruiseControl=my-cluster-5dfd606f, testConfigurationFileIsCreated=my-cluster-17fc585b, testCapacityFile=my-cluster-855b9105, testConfigurationPerformanceOptions=my-cluster-08515847, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-afe273c9, testConfigurationReflection=my-cluster-188382d8, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-e6343cec}
2022-03-28 18:56:10 [ForkJoinPool-1-worker-25] TRACE [AbstractST:607] USERS_NAME_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-user-2104248162-45187051, testDeployAndUnDeployCruiseControl=my-user-506268651-1309117986, testConfigurationFileIsCreated=my-user-355815510-1316268151, testCapacityFile=my-user-1460643829-103298436, testConfigurationPerformanceOptions=my-user-98362545-1476587236, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-user-8456079-650483351, testConfigurationReflection=my-user-1890969183-1754686912, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-2069267102-581538560}
2022-03-28 18:56:10 [ForkJoinPool-1-worker-25] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-634125597-1746493046, testDeployAndUnDeployCruiseControl=my-topic-465377127-800134460, testConfigurationFileIsCreated=my-topic-423850169-855699454, testCapacityFile=my-topic-549312506-649708453, testConfigurationPerformanceOptions=my-topic-696588651-1198599652, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-topic-1687496771-142091227, testConfigurationReflection=my-topic-1394577332-1936226438, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-293775067-432059493}
2022-03-28 18:56:10 [ForkJoinPool-1-worker-25] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-959c6ed5-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-5dfd606f-kafka-clients, testConfigurationFileIsCreated=my-cluster-17fc585b-kafka-clients, testCapacityFile=my-cluster-855b9105-kafka-clients, testConfigurationPerformanceOptions=my-cluster-08515847-kafka-clients, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-afe273c9-kafka-clients, testConfigurationReflection=my-cluster-188382d8-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-e6343cec-kafka-clients}
2022-03-28 18:56:10 [ForkJoinPool-1-worker-25] INFO  [TestSuiteNamespaceManager:163] Creating namespace:namespace-7 for test case:testPauseReconciliationInKafkaAndKafkaConnectWithConnector
2022-03-28 18:56:10 [ForkJoinPool-1-worker-17] INFO  [ResourceManager:155] Create/Update Kafka my-cluster-855b9105 in namespace namespace-6
2022-03-28 18:56:10 [ForkJoinPool-1-worker-17] INFO  [ResourceManager:164] Using Namespace: namespace-6
2022-03-28 18:56:10 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1316576ms till timeout)
2022-03-28 18:56:10 [ForkJoinPool-1-worker-25] INFO  [KubeClusterResource:156] Creating Namespace: namespace-7
2022-03-28 18:56:10 [ForkJoinPool-1-worker-17] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-855b9105
2022-03-28 18:56:10 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1318616ms till timeout)
2022-03-28 18:56:10 [ForkJoinPool-1-worker-17] INFO  [ResourceManager:433] Wait for Kafka: my-cluster-855b9105 will have desired state: Ready
2022-03-28 18:56:10 [ForkJoinPool-1-worker-25] DEBUG [TestUtils:125] Waiting for Namespace namespace-7
2022-03-28 18:56:10 [ForkJoinPool-1-worker-25] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace namespace-7 -o json
2022-03-28 18:56:10 [ForkJoinPool-1-worker-17] DEBUG [TestUtils:125] Waiting for Kafka: my-cluster-855b9105 will have desired state: Ready
2022-03-28 18:56:10 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (835270ms till timeout)
2022-03-28 18:56:10 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1319814ms till timeout)
2022-03-28 18:56:10 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (835266ms till timeout)
2022-03-28 18:56:10 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (835169ms till timeout)
2022-03-28 18:56:10 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1317494ms till timeout)
2022-03-28 18:56:10 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (835170ms till timeout)
2022-03-28 18:56:10 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (835169ms till timeout)
2022-03-28 18:56:11 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1315171ms till timeout)
2022-03-28 18:56:11 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace namespace-7 -o json
2022-03-28 18:56:11 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Return code: 0
2022-03-28 18:56:11 [ForkJoinPool-1-worker-25] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c42,c24",
            "openshift.io/sa.scc.supplemental-groups": "1001770000/10000",
            "openshift.io/sa.scc.uid-range": "1001770000/10000"
        },
        "creationTimestamp": "2022-03-28T18:56:06Z",
        "labels": {
            "kubernetes.io/metadata.name": "namespace-7"
        },
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T18:56:06Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:labels": {
                            ".": {},
                            "f:kubernetes.io/metadata.name": {}
                        }
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T18:56:06Z"
            }
        ],
        "name": "namespace-7",
        "resourceVersion": "259514",
        "uid": "084e8053-a248-4c4b-b605-85869ebb7524"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 18:56:11 [ForkJoinPool-1-worker-25] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[http-bridge-scram-sha-st], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c4c0ea0=[namespace-1], io.strimzi.test.logs.CollectorElement@aec142ef=[namespace-5], io.strimzi.test.logs.CollectorElement@24b97ba9=[namespace-7], io.strimzi.test.logs.CollectorElement@f851b6c3=[namespace-4], io.strimzi.test.logs.CollectorElement@3881d5f2=[namespace-3], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[namespace-0], io.strimzi.test.logs.CollectorElement@f4405b3b=[namespace-2], io.strimzi.test.logs.CollectorElement@3e255cd9=[namespace-6]}
2022-03-28 18:56:11 [ForkJoinPool-1-worker-25] INFO  [KubeClusterResource:82] Client use Namespace: namespace-7
2022-03-28 18:56:11 [ForkJoinPool-1-worker-25] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-7, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 18:56:11 [ForkJoinPool-1-worker-25] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-7
2022-03-28 18:56:11 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1316553ms till timeout)
2022-03-28 18:56:11 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:155] Create/Update Kafka my-cluster-afe273c9 in namespace namespace-7
2022-03-28 18:56:11 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:164] Using Namespace: namespace-7
2022-03-28 18:56:11 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1318690ms till timeout)
2022-03-28 18:56:11 [ForkJoinPool-1-worker-25] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-afe273c9
2022-03-28 18:56:11 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:433] Wait for Kafka: my-cluster-afe273c9 will have desired state: Ready
2022-03-28 18:56:11 [ForkJoinPool-1-worker-25] DEBUG [TestUtils:125] Waiting for Kafka: my-cluster-afe273c9 will have desired state: Ready
2022-03-28 18:56:11 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1315419ms till timeout)
2022-03-28 18:56:11 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (839807ms till timeout)
2022-03-28 18:56:11 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1317458ms till timeout)
2022-03-28 18:56:11 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (834172ms till timeout)
2022-03-28 18:56:11 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1318715ms till timeout)
2022-03-28 18:56:12 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (834071ms till timeout)
2022-03-28 18:56:12 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (833973ms till timeout)
2022-03-28 18:56:12 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1316298ms till timeout)
2022-03-28 18:56:12 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (833973ms till timeout)
2022-03-28 18:56:12 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1314066ms till timeout)
2022-03-28 18:56:12 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (833972ms till timeout)
2022-03-28 18:56:12 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1315449ms till timeout)
2022-03-28 18:56:12 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1317586ms till timeout)
2022-03-28 18:56:12 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1314320ms till timeout)
2022-03-28 18:56:12 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (838707ms till timeout)
2022-03-28 18:56:12 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1316354ms till timeout)
2022-03-28 18:56:12 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (833072ms till timeout)
2022-03-28 18:56:13 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1317614ms till timeout)
2022-03-28 18:56:13 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (832968ms till timeout)
2022-03-28 18:56:13 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (832870ms till timeout)
2022-03-28 18:56:13 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1315195ms till timeout)
2022-03-28 18:56:13 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (832869ms till timeout)
2022-03-28 18:56:13 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (832870ms till timeout)
2022-03-28 18:56:13 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1312873ms till timeout)
2022-03-28 18:56:13 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1314346ms till timeout)
2022-03-28 18:56:13 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1316485ms till timeout)
2022-03-28 18:56:13 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1313220ms till timeout)
2022-03-28 18:56:13 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (837607ms till timeout)
2022-03-28 18:56:13 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1315252ms till timeout)
2022-03-28 18:56:14 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (831971ms till timeout)
2022-03-28 18:56:14 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1316511ms till timeout)
2022-03-28 18:56:14 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (831866ms till timeout)
2022-03-28 18:56:14 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (831759ms till timeout)
2022-03-28 18:56:14 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (831759ms till timeout)
2022-03-28 18:56:14 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1314084ms till timeout)
2022-03-28 18:56:14 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (831760ms till timeout)
2022-03-28 18:56:14 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1313229ms till timeout)
2022-03-28 18:56:14 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1311752ms till timeout)
2022-03-28 18:56:14 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1315367ms till timeout)
2022-03-28 18:56:14 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1312121ms till timeout)
2022-03-28 18:56:14 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (836505ms till timeout)
2022-03-28 18:56:15 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1314153ms till timeout)
2022-03-28 18:56:15 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (830873ms till timeout)
2022-03-28 18:56:15 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1315413ms till timeout)
2022-03-28 18:56:15 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (830768ms till timeout)
2022-03-28 18:56:15 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (830660ms till timeout)
2022-03-28 18:56:15 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (830565ms till timeout)
2022-03-28 18:56:15 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1312887ms till timeout)
2022-03-28 18:56:15 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (830563ms till timeout)
2022-03-28 18:56:15 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1310564ms till timeout)
2022-03-28 18:56:15 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1312040ms till timeout)
2022-03-28 18:56:15 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1314179ms till timeout)
2022-03-28 18:56:15 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1311018ms till timeout)
2022-03-28 18:56:16 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (835406ms till timeout)
2022-03-28 18:56:16 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1313053ms till timeout)
2022-03-28 18:56:16 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (829773ms till timeout)
2022-03-28 18:56:16 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1314314ms till timeout)
2022-03-28 18:56:16 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (829667ms till timeout)
2022-03-28 18:56:16 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (829558ms till timeout)
2022-03-28 18:56:16 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (829461ms till timeout)
2022-03-28 18:56:16 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1311691ms till timeout)
2022-03-28 18:56:16 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1309460ms till timeout)
2022-03-28 18:56:16 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (829367ms till timeout)
2022-03-28 18:56:16 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1310936ms till timeout)
2022-03-28 18:56:16 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1313074ms till timeout)
2022-03-28 18:56:17 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1309916ms till timeout)
2022-03-28 18:56:17 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (834294ms till timeout)
2022-03-28 18:56:17 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1311938ms till timeout)
2022-03-28 18:56:17 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (828672ms till timeout)
2022-03-28 18:56:17 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1313214ms till timeout)
2022-03-28 18:56:17 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (828567ms till timeout)
2022-03-28 18:56:17 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (828458ms till timeout)
2022-03-28 18:56:17 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (828360ms till timeout)
2022-03-28 18:56:17 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1310590ms till timeout)
2022-03-28 18:56:17 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1309834ms till timeout)
2022-03-28 18:56:17 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (828264ms till timeout)
2022-03-28 18:56:17 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1308357ms till timeout)
2022-03-28 18:56:17 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1311975ms till timeout)
2022-03-28 18:56:18 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1308777ms till timeout)
2022-03-28 18:56:18 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (833168ms till timeout)
2022-03-28 18:56:18 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1310818ms till timeout)
2022-03-28 18:56:18 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (827573ms till timeout)
2022-03-28 18:56:18 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1312116ms till timeout)
2022-03-28 18:56:18 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (827465ms till timeout)
2022-03-28 18:56:18 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (827358ms till timeout)
2022-03-28 18:56:18 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (827262ms till timeout)
2022-03-28 18:56:18 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1309492ms till timeout)
2022-03-28 18:56:19 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1308642ms till timeout)
2022-03-28 18:56:19 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (827072ms till timeout)
2022-03-28 18:56:19 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1307165ms till timeout)
2022-03-28 18:56:19 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1310874ms till timeout)
2022-03-28 18:56:19 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1307677ms till timeout)
2022-03-28 18:56:19 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (832065ms till timeout)
2022-03-28 18:56:19 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1309715ms till timeout)
2022-03-28 18:56:19 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (826469ms till timeout)
2022-03-28 18:56:19 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1311012ms till timeout)
2022-03-28 18:56:19 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (826366ms till timeout)
2022-03-28 18:56:19 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (826260ms till timeout)
2022-03-28 18:56:19 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (826163ms till timeout)
2022-03-28 18:56:20 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1308393ms till timeout)
2022-03-28 18:56:20 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1307543ms till timeout)
2022-03-28 18:56:20 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (825879ms till timeout)
2022-03-28 18:56:20 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1305971ms till timeout)
2022-03-28 18:56:20 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1309681ms till timeout)
2022-03-28 18:56:20 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1306575ms till timeout)
2022-03-28 18:56:20 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (830962ms till timeout)
2022-03-28 18:56:20 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1308614ms till timeout)
2022-03-28 18:56:20 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (825367ms till timeout)
2022-03-28 18:56:20 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1309910ms till timeout)
2022-03-28 18:56:20 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (825267ms till timeout)
2022-03-28 18:56:20 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (825160ms till timeout)
2022-03-28 18:56:21 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (825064ms till timeout)
2022-03-28 18:56:21 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1307294ms till timeout)
2022-03-28 18:56:21 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1306445ms till timeout)
2022-03-28 18:56:21 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (824779ms till timeout)
2022-03-28 18:56:21 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1308486ms till timeout)
2022-03-28 18:56:21 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1304776ms till timeout)
2022-03-28 18:56:21 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1305421ms till timeout)
2022-03-28 18:56:21 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (829808ms till timeout)
2022-03-28 18:56:21 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1307457ms till timeout)
2022-03-28 18:56:21 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (824209ms till timeout)
2022-03-28 18:56:21 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1308752ms till timeout)
2022-03-28 18:56:22 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (824109ms till timeout)
2022-03-28 18:56:22 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (824011ms till timeout)
2022-03-28 18:56:22 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (823916ms till timeout)
2022-03-28 18:56:22 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1306146ms till timeout)
2022-03-28 18:56:22 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1305297ms till timeout)
2022-03-28 18:56:22 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (823632ms till timeout)
2022-03-28 18:56:22 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1303630ms till timeout)
2022-03-28 18:56:22 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1307340ms till timeout)
2022-03-28 18:56:22 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1304274ms till timeout)
2022-03-28 18:56:22 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (828661ms till timeout)
2022-03-28 18:56:22 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1306312ms till timeout)
2022-03-28 18:56:22 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (823065ms till timeout)
2022-03-28 18:56:23 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1307608ms till timeout)
2022-03-28 18:56:23 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (822963ms till timeout)
2022-03-28 18:56:23 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (822864ms till timeout)
2022-03-28 18:56:23 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (822769ms till timeout)
2022-03-28 18:56:23 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1304994ms till timeout)
2022-03-28 18:56:23 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1304146ms till timeout)
2022-03-28 18:56:23 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (822479ms till timeout)
2022-03-28 18:56:23 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1306188ms till timeout)
2022-03-28 18:56:23 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1302478ms till timeout)
2022-03-28 18:56:23 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1303123ms till timeout)
2022-03-28 18:56:23 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (827509ms till timeout)
2022-03-28 18:56:24 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1305159ms till timeout)
2022-03-28 18:56:24 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (821913ms till timeout)
2022-03-28 18:56:24 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1306456ms till timeout)
2022-03-28 18:56:24 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (821814ms till timeout)
2022-03-28 18:56:24 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (821716ms till timeout)
2022-03-28 18:56:24 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (821622ms till timeout)
2022-03-28 18:56:24 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1303852ms till timeout)
2022-03-28 18:56:24 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1303002ms till timeout)
2022-03-28 18:56:24 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (821337ms till timeout)
2022-03-28 18:56:24 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1301335ms till timeout)
2022-03-28 18:56:24 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1305045ms till timeout)
2022-03-28 18:56:24 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1301971ms till timeout)
2022-03-28 18:56:25 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (826352ms till timeout)
2022-03-28 18:56:25 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1303991ms till timeout)
2022-03-28 18:56:25 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (820744ms till timeout)
2022-03-28 18:56:25 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1305287ms till timeout)
2022-03-28 18:56:25 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (820646ms till timeout)
2022-03-28 18:56:25 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (820547ms till timeout)
2022-03-28 18:56:25 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (820453ms till timeout)
2022-03-28 18:56:25 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1302683ms till timeout)
2022-03-28 18:56:25 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1301834ms till timeout)
2022-03-28 18:56:25 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (820169ms till timeout)
2022-03-28 18:56:26 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1303877ms till timeout)
2022-03-28 18:56:26 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1300167ms till timeout)
2022-03-28 18:56:26 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1300812ms till timeout)
2022-03-28 18:56:26 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (825199ms till timeout)
2022-03-28 18:56:26 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1302850ms till timeout)
2022-03-28 18:56:26 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (819604ms till timeout)
2022-03-28 18:56:26 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1304147ms till timeout)
2022-03-28 18:56:26 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (819505ms till timeout)
2022-03-28 18:56:26 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (819408ms till timeout)
2022-03-28 18:56:26 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (819313ms till timeout)
2022-03-28 18:56:26 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1301542ms till timeout)
2022-03-28 18:56:26 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1300690ms till timeout)
2022-03-28 18:56:27 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (819024ms till timeout)
2022-03-28 18:56:27 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1299022ms till timeout)
2022-03-28 18:56:27 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1302732ms till timeout)
2022-03-28 18:56:27 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1299666ms till timeout)
2022-03-28 18:56:27 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (824054ms till timeout)
2022-03-28 18:56:27 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1301704ms till timeout)
2022-03-28 18:56:27 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (818458ms till timeout)
2022-03-28 18:56:27 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1303001ms till timeout)
2022-03-28 18:56:27 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (818358ms till timeout)
2022-03-28 18:56:27 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (818261ms till timeout)
2022-03-28 18:56:27 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (818166ms till timeout)
2022-03-28 18:56:28 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1300396ms till timeout)
2022-03-28 18:56:28 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1299547ms till timeout)
2022-03-28 18:56:28 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (817881ms till timeout)
2022-03-28 18:56:28 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1301589ms till timeout)
2022-03-28 18:56:28 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1297879ms till timeout)
2022-03-28 18:56:28 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1298524ms till timeout)
2022-03-28 18:56:28 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (822910ms till timeout)
2022-03-28 18:56:28 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1300561ms till timeout)
2022-03-28 18:56:28 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (817315ms till timeout)
2022-03-28 18:56:28 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1301855ms till timeout)
2022-03-28 18:56:28 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (817213ms till timeout)
2022-03-28 18:56:28 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (817116ms till timeout)
2022-03-28 18:56:29 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (817012ms till timeout)
2022-03-28 18:56:29 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1299233ms till timeout)
2022-03-28 18:56:29 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1298382ms till timeout)
2022-03-28 18:56:29 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (816715ms till timeout)
2022-03-28 18:56:29 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1300422ms till timeout)
2022-03-28 18:56:29 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1296712ms till timeout)
2022-03-28 18:56:29 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1297357ms till timeout)
2022-03-28 18:56:29 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (821744ms till timeout)
2022-03-28 18:56:29 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1299395ms till timeout)
2022-03-28 18:56:29 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (816149ms till timeout)
2022-03-28 18:56:29 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1300692ms till timeout)
2022-03-28 18:56:30 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (816050ms till timeout)
2022-03-28 18:56:30 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (815949ms till timeout)
2022-03-28 18:56:30 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (815854ms till timeout)
2022-03-28 18:56:30 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1298083ms till timeout)
2022-03-28 18:56:30 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1297234ms till timeout)
2022-03-28 18:56:30 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (815563ms till timeout)
2022-03-28 18:56:30 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1295545ms till timeout)
2022-03-28 18:56:30 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1299247ms till timeout)
2022-03-28 18:56:30 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1296185ms till timeout)
2022-03-28 18:56:30 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (820561ms till timeout)
2022-03-28 18:56:30 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1298212ms till timeout)
2022-03-28 18:56:31 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (814965ms till timeout)
2022-03-28 18:56:31 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1299507ms till timeout)
2022-03-28 18:56:31 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (814864ms till timeout)
2022-03-28 18:56:31 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (814764ms till timeout)
2022-03-28 18:56:31 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (814668ms till timeout)
2022-03-28 18:56:31 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1296898ms till timeout)
2022-03-28 18:56:31 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1296049ms till timeout)
2022-03-28 18:56:31 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (814384ms till timeout)
2022-03-28 18:56:31 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1298092ms till timeout)
2022-03-28 18:56:31 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1294381ms till timeout)
2022-03-28 18:56:31 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1295027ms till timeout)
2022-03-28 18:56:32 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (819413ms till timeout)
2022-03-28 18:56:32 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1297063ms till timeout)
2022-03-28 18:56:32 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (813818ms till timeout)
2022-03-28 18:56:32 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1298360ms till timeout)
2022-03-28 18:56:32 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (813717ms till timeout)
2022-03-28 18:56:32 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (813618ms till timeout)
2022-03-28 18:56:32 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (813523ms till timeout)
2022-03-28 18:56:32 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1295753ms till timeout)
2022-03-28 18:56:32 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1294904ms till timeout)
2022-03-28 18:56:32 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (813236ms till timeout)
2022-03-28 18:56:32 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1293230ms till timeout)
2022-03-28 18:56:32 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1296940ms till timeout)
2022-03-28 18:56:33 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1293875ms till timeout)
2022-03-28 18:56:33 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (818256ms till timeout)
2022-03-28 18:56:33 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1295907ms till timeout)
2022-03-28 18:56:33 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (812661ms till timeout)
2022-03-28 18:56:33 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1297203ms till timeout)
2022-03-28 18:56:33 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (812560ms till timeout)
2022-03-28 18:56:33 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (812462ms till timeout)
2022-03-28 18:56:33 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (812368ms till timeout)
2022-03-28 18:56:33 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1294598ms till timeout)
2022-03-28 18:56:33 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1293748ms till timeout)
2022-03-28 18:56:34 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (812083ms till timeout)
2022-03-28 18:56:34 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1295791ms till timeout)
2022-03-28 18:56:34 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1292079ms till timeout)
2022-03-28 18:56:34 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1292725ms till timeout)
2022-03-28 18:56:34 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (817113ms till timeout)
2022-03-28 18:56:34 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1294761ms till timeout)
2022-03-28 18:56:34 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (811515ms till timeout)
2022-03-28 18:56:34 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1296058ms till timeout)
2022-03-28 18:56:34 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (811415ms till timeout)
2022-03-28 18:56:34 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (811318ms till timeout)
2022-03-28 18:56:34 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (811222ms till timeout)
2022-03-28 18:56:34 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1293447ms till timeout)
2022-03-28 18:56:35 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1292595ms till timeout)
2022-03-28 18:56:35 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (810920ms till timeout)
2022-03-28 18:56:35 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1294621ms till timeout)
2022-03-28 18:56:35 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1290895ms till timeout)
2022-03-28 18:56:35 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1291543ms till timeout)
2022-03-28 18:56:35 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (815915ms till timeout)
2022-03-28 18:56:35 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (810410ms till timeout)
2022-03-28 18:56:35 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1293553ms till timeout)
2022-03-28 18:56:35 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1294931ms till timeout)
2022-03-28 18:56:35 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (810271ms till timeout)
2022-03-28 18:56:35 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (810171ms till timeout)
2022-03-28 18:56:36 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (810076ms till timeout)
2022-03-28 18:56:36 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1292304ms till timeout)
2022-03-28 18:56:36 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1291456ms till timeout)
2022-03-28 18:56:36 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (809767ms till timeout)
2022-03-28 18:56:36 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1289764ms till timeout)
2022-03-28 18:56:36 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1293473ms till timeout)
2022-03-28 18:56:36 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1290408ms till timeout)
2022-03-28 18:56:36 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (814793ms till timeout)
2022-03-28 18:56:36 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (809292ms till timeout)
2022-03-28 18:56:36 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1292443ms till timeout)
2022-03-28 18:56:36 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1293833ms till timeout)
2022-03-28 18:56:36 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (809171ms till timeout)
2022-03-28 18:56:37 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (809070ms till timeout)
2022-03-28 18:56:37 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (808975ms till timeout)
2022-03-28 18:56:37 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1291204ms till timeout)
2022-03-28 18:56:37 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1290349ms till timeout)
2022-03-28 18:56:37 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (808665ms till timeout)
2022-03-28 18:56:37 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1288663ms till timeout)
2022-03-28 18:56:37 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1292373ms till timeout)
2022-03-28 18:56:37 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1289307ms till timeout)
2022-03-28 18:56:37 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (813693ms till timeout)
2022-03-28 18:56:37 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (808193ms till timeout)
2022-03-28 18:56:37 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1291344ms till timeout)
2022-03-28 18:56:37 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1292736ms till timeout)
2022-03-28 18:56:38 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (808073ms till timeout)
2022-03-28 18:56:38 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (807972ms till timeout)
2022-03-28 18:56:38 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (807877ms till timeout)
2022-03-28 18:56:38 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1290094ms till timeout)
2022-03-28 18:56:38 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1289242ms till timeout)
2022-03-28 18:56:38 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (807568ms till timeout)
2022-03-28 18:56:38 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1287563ms till timeout)
2022-03-28 18:56:38 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1291273ms till timeout)
2022-03-28 18:56:38 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1288208ms till timeout)
2022-03-28 18:56:38 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (812595ms till timeout)
2022-03-28 18:56:38 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1290238ms till timeout)
2022-03-28 18:56:38 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (807085ms till timeout)
2022-03-28 18:56:39 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1291628ms till timeout)
2022-03-28 18:56:39 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (806975ms till timeout)
2022-03-28 18:56:39 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (806876ms till timeout)
2022-03-28 18:56:39 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (806777ms till timeout)
2022-03-28 18:56:39 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1288985ms till timeout)
2022-03-28 18:56:39 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1288124ms till timeout)
2022-03-28 18:56:39 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (806458ms till timeout)
2022-03-28 18:56:39 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1286456ms till timeout)
2022-03-28 18:56:39 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1290166ms till timeout)
2022-03-28 18:56:39 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1287101ms till timeout)
2022-03-28 18:56:39 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (811487ms till timeout)
2022-03-28 18:56:40 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1289139ms till timeout)
2022-03-28 18:56:40 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (805892ms till timeout)
2022-03-28 18:56:40 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1290436ms till timeout)
2022-03-28 18:56:40 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (805794ms till timeout)
2022-03-28 18:56:40 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (805679ms till timeout)
2022-03-28 18:56:40 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (805557ms till timeout)
2022-03-28 18:56:40 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1287779ms till timeout)
2022-03-28 18:56:40 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1286929ms till timeout)
2022-03-28 18:56:40 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (805356ms till timeout)
2022-03-28 18:56:40 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1289065ms till timeout)
2022-03-28 18:56:40 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1285355ms till timeout)
2022-03-28 18:56:40 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1285999ms till timeout)
2022-03-28 18:56:41 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (810389ms till timeout)
2022-03-28 18:56:41 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1288039ms till timeout)
2022-03-28 18:56:41 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (804793ms till timeout)
2022-03-28 18:56:41 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1289336ms till timeout)
2022-03-28 18:56:41 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (804692ms till timeout)
2022-03-28 18:56:41 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (804581ms till timeout)
2022-03-28 18:56:41 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (804460ms till timeout)
2022-03-28 18:56:41 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1286682ms till timeout)
2022-03-28 18:56:41 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1285831ms till timeout)
2022-03-28 18:56:41 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (804166ms till timeout)
2022-03-28 18:56:42 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1284163ms till timeout)
2022-03-28 18:56:42 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1287873ms till timeout)
2022-03-28 18:56:42 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1284806ms till timeout)
2022-03-28 18:56:42 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (809288ms till timeout)
2022-03-28 18:56:42 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1286938ms till timeout)
2022-03-28 18:56:42 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (803691ms till timeout)
2022-03-28 18:56:42 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1288235ms till timeout)
2022-03-28 18:56:42 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (803592ms till timeout)
2022-03-28 18:56:42 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (803483ms till timeout)
2022-03-28 18:56:42 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (803362ms till timeout)
2022-03-28 18:56:42 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1285584ms till timeout)
2022-03-28 18:56:42 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1284730ms till timeout)
2022-03-28 18:56:43 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (803069ms till timeout)
2022-03-28 18:56:43 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1283066ms till timeout)
2022-03-28 18:56:43 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1286681ms till timeout)
2022-03-28 18:56:43 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (808097ms till timeout)
2022-03-28 18:56:43 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1283613ms till timeout)
2022-03-28 18:56:43 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1285747ms till timeout)
2022-03-28 18:56:43 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (802494ms till timeout)
2022-03-28 18:56:43 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (802490ms till timeout)
2022-03-28 18:56:43 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1287037ms till timeout)
2022-03-28 18:56:43 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (802386ms till timeout)
2022-03-28 18:56:43 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (802265ms till timeout)
2022-03-28 18:56:43 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1284485ms till timeout)
2022-03-28 18:56:44 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1283632ms till timeout)
2022-03-28 18:56:44 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (801966ms till timeout)
2022-03-28 18:56:44 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1281964ms till timeout)
2022-03-28 18:56:44 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1285579ms till timeout)
2022-03-28 18:56:44 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (806996ms till timeout)
2022-03-28 18:56:44 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1282514ms till timeout)
2022-03-28 18:56:44 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1284647ms till timeout)
2022-03-28 18:56:44 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (801397ms till timeout)
2022-03-28 18:56:44 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (801392ms till timeout)
2022-03-28 18:56:44 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1285843ms till timeout)
2022-03-28 18:56:44 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (801194ms till timeout)
2022-03-28 18:56:45 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (801096ms till timeout)
2022-03-28 18:56:45 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1283326ms till timeout)
2022-03-28 18:56:45 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1282477ms till timeout)
2022-03-28 18:56:45 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (800812ms till timeout)
2022-03-28 18:56:45 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1280809ms till timeout)
2022-03-28 18:56:45 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1284424ms till timeout)
2022-03-28 18:56:45 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1281359ms till timeout)
2022-03-28 18:56:45 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (805841ms till timeout)
2022-03-28 18:56:45 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1283492ms till timeout)
2022-03-28 18:56:45 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (800246ms till timeout)
2022-03-28 18:56:45 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (800242ms till timeout)
2022-03-28 18:56:45 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1284693ms till timeout)
2022-03-28 18:56:46 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (800049ms till timeout)
2022-03-28 18:56:46 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (799954ms till timeout)
2022-03-28 18:56:46 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1282184ms till timeout)
2022-03-28 18:56:46 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1281335ms till timeout)
2022-03-28 18:56:46 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (799669ms till timeout)
2022-03-28 18:56:46 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1279667ms till timeout)
2022-03-28 18:56:46 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1283283ms till timeout)
2022-03-28 18:56:46 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1280218ms till timeout)
2022-03-28 18:56:46 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (804699ms till timeout)
2022-03-28 18:56:46 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1282351ms till timeout)
2022-03-28 18:56:46 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (799105ms till timeout)
2022-03-28 18:56:47 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (799098ms till timeout)
2022-03-28 18:56:47 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1283542ms till timeout)
2022-03-28 18:56:47 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (798893ms till timeout)
2022-03-28 18:56:47 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (798798ms till timeout)
2022-03-28 18:56:47 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1281028ms till timeout)
2022-03-28 18:56:47 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1280179ms till timeout)
2022-03-28 18:56:47 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (798514ms till timeout)
2022-03-28 18:56:47 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1278512ms till timeout)
2022-03-28 18:56:47 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1282127ms till timeout)
2022-03-28 18:56:47 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1279060ms till timeout)
2022-03-28 18:56:47 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (803541ms till timeout)
2022-03-28 18:56:47 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1281193ms till timeout)
2022-03-28 18:56:48 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (797946ms till timeout)
2022-03-28 18:56:48 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (797942ms till timeout)
2022-03-28 18:56:48 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1282395ms till timeout)
2022-03-28 18:56:48 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (797751ms till timeout)
2022-03-28 18:56:48 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (797656ms till timeout)
2022-03-28 18:56:48 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1279885ms till timeout)
2022-03-28 18:56:48 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1279036ms till timeout)
2022-03-28 18:56:48 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (797371ms till timeout)
2022-03-28 18:56:48 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1277369ms till timeout)
2022-03-28 18:56:48 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1280982ms till timeout)
2022-03-28 18:56:49 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1277916ms till timeout)
2022-03-28 18:56:49 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (802398ms till timeout)
2022-03-28 18:56:49 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1280049ms till timeout)
2022-03-28 18:56:49 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (796803ms till timeout)
2022-03-28 18:56:49 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (796799ms till timeout)
2022-03-28 18:56:49 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1281251ms till timeout)
2022-03-28 18:56:49 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (796607ms till timeout)
2022-03-28 18:56:49 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (796511ms till timeout)
2022-03-28 18:56:49 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1278741ms till timeout)
2022-03-28 18:56:49 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1277892ms till timeout)
2022-03-28 18:56:49 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (796227ms till timeout)
2022-03-28 18:56:49 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1276224ms till timeout)
2022-03-28 18:56:50 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1279839ms till timeout)
2022-03-28 18:56:50 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (801256ms till timeout)
2022-03-28 18:56:50 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1276774ms till timeout)
2022-03-28 18:56:50 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1278906ms till timeout)
2022-03-28 18:56:50 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (795660ms till timeout)
2022-03-28 18:56:50 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (795648ms till timeout)
2022-03-28 18:56:50 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1280100ms till timeout)
2022-03-28 18:56:50 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (795454ms till timeout)
2022-03-28 18:56:50 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (795359ms till timeout)
2022-03-28 18:56:50 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1277589ms till timeout)
2022-03-28 18:56:50 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1276740ms till timeout)
2022-03-28 18:56:51 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (795074ms till timeout)
2022-03-28 18:56:51 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1275073ms till timeout)
2022-03-28 18:56:51 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1278687ms till timeout)
2022-03-28 18:56:51 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (800104ms till timeout)
2022-03-28 18:56:51 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1275621ms till timeout)
2022-03-28 18:56:51 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1277755ms till timeout)
2022-03-28 18:56:51 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (794508ms till timeout)
2022-03-28 18:56:51 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (794500ms till timeout)
2022-03-28 18:56:51 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1278956ms till timeout)
2022-03-28 18:56:51 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (794312ms till timeout)
2022-03-28 18:56:51 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (794217ms till timeout)
2022-03-28 18:56:51 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1276446ms till timeout)
2022-03-28 18:56:52 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1275596ms till timeout)
2022-03-28 18:56:52 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (793931ms till timeout)
2022-03-28 18:56:52 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1273928ms till timeout)
2022-03-28 18:56:52 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1277544ms till timeout)
2022-03-28 18:56:52 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (798960ms till timeout)
2022-03-28 18:56:52 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1274478ms till timeout)
2022-03-28 18:56:52 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1276612ms till timeout)
2022-03-28 18:56:52 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (793365ms till timeout)
2022-03-28 18:56:52 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (793360ms till timeout)
2022-03-28 18:56:52 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1277812ms till timeout)
2022-03-28 18:56:52 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (793169ms till timeout)
2022-03-28 18:56:53 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (793073ms till timeout)
2022-03-28 18:56:53 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1275301ms till timeout)
2022-03-28 18:56:53 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1274452ms till timeout)
2022-03-28 18:56:53 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (792776ms till timeout)
2022-03-28 18:56:53 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1272773ms till timeout)
2022-03-28 18:56:53 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1276386ms till timeout)
2022-03-28 18:56:53 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (797803ms till timeout)
2022-03-28 18:56:53 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1273319ms till timeout)
2022-03-28 18:56:53 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1275454ms till timeout)
2022-03-28 18:56:53 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (792206ms till timeout)
2022-03-28 18:56:53 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (792202ms till timeout)
2022-03-28 18:56:54 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1276654ms till timeout)
2022-03-28 18:56:54 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (792008ms till timeout)
2022-03-28 18:56:54 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (791911ms till timeout)
2022-03-28 18:56:54 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1274140ms till timeout)
2022-03-28 18:56:54 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1273290ms till timeout)
2022-03-28 18:56:54 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (791625ms till timeout)
2022-03-28 18:56:54 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1271621ms till timeout)
2022-03-28 18:56:54 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1275234ms till timeout)
2022-03-28 18:56:54 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (796650ms till timeout)
2022-03-28 18:56:54 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1272168ms till timeout)
2022-03-28 18:56:54 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1274300ms till timeout)
2022-03-28 18:56:54 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (791055ms till timeout)
2022-03-28 18:56:55 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (791050ms till timeout)
2022-03-28 18:56:55 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1275496ms till timeout)
2022-03-28 18:56:55 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (790851ms till timeout)
2022-03-28 18:56:55 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (790756ms till timeout)
2022-03-28 18:56:55 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1272986ms till timeout)
2022-03-28 18:56:55 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1272137ms till timeout)
2022-03-28 18:56:55 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (790472ms till timeout)
2022-03-28 18:56:55 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1270470ms till timeout)
2022-03-28 18:56:55 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1274080ms till timeout)
2022-03-28 18:56:55 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1271015ms till timeout)
2022-03-28 18:56:55 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (795497ms till timeout)
2022-03-28 18:56:56 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1273144ms till timeout)
2022-03-28 18:56:56 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (789897ms till timeout)
2022-03-28 18:56:56 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (789893ms till timeout)
2022-03-28 18:56:56 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1274344ms till timeout)
2022-03-28 18:56:56 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (789700ms till timeout)
2022-03-28 18:56:56 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (789605ms till timeout)
2022-03-28 18:56:56 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1271835ms till timeout)
2022-03-28 18:56:56 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1270985ms till timeout)
2022-03-28 18:56:56 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (789320ms till timeout)
2022-03-28 18:56:56 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1269319ms till timeout)
2022-03-28 18:56:56 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1272933ms till timeout)
2022-03-28 18:56:57 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1269868ms till timeout)
2022-03-28 18:56:57 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (794350ms till timeout)
2022-03-28 18:56:57 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1272001ms till timeout)
2022-03-28 18:56:57 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (788753ms till timeout)
2022-03-28 18:56:57 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (788743ms till timeout)
2022-03-28 18:56:57 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1273200ms till timeout)
2022-03-28 18:56:57 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (788556ms till timeout)
2022-03-28 18:56:57 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (788461ms till timeout)
2022-03-28 18:56:57 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1270691ms till timeout)
2022-03-28 18:56:57 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1269842ms till timeout)
2022-03-28 18:56:57 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (788177ms till timeout)
2022-03-28 18:56:58 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1268174ms till timeout)
2022-03-28 18:56:58 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1271787ms till timeout)
2022-03-28 18:56:58 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (793204ms till timeout)
2022-03-28 18:56:58 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1268722ms till timeout)
2022-03-28 18:56:58 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1270855ms till timeout)
2022-03-28 18:56:58 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (787608ms till timeout)
2022-03-28 18:56:58 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (787604ms till timeout)
2022-03-28 18:56:58 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1272056ms till timeout)
2022-03-28 18:56:58 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (787413ms till timeout)
2022-03-28 18:56:58 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (787317ms till timeout)
2022-03-28 18:56:58 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1269547ms till timeout)
2022-03-28 18:56:58 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1268696ms till timeout)
2022-03-28 18:56:59 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (787031ms till timeout)
2022-03-28 18:56:59 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1267028ms till timeout)
2022-03-28 18:56:59 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1270642ms till timeout)
2022-03-28 18:56:59 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1267576ms till timeout)
2022-03-28 18:56:59 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (792057ms till timeout)
2022-03-28 18:56:59 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1269708ms till timeout)
2022-03-28 18:56:59 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (786462ms till timeout)
2022-03-28 18:56:59 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (786458ms till timeout)
2022-03-28 18:56:59 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1270910ms till timeout)
2022-03-28 18:56:59 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (786265ms till timeout)
2022-03-28 18:56:59 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (786170ms till timeout)
2022-03-28 18:57:00 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1268400ms till timeout)
2022-03-28 18:57:00 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1267550ms till timeout)
2022-03-28 18:57:00 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (785884ms till timeout)
2022-03-28 18:57:00 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1265882ms till timeout)
2022-03-28 18:57:00 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1269497ms till timeout)
2022-03-28 18:57:00 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1266431ms till timeout)
2022-03-28 18:57:00 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (790913ms till timeout)
2022-03-28 18:57:00 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1268564ms till timeout)
2022-03-28 18:57:00 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (785313ms till timeout)
2022-03-28 18:57:00 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (785309ms till timeout)
2022-03-28 18:57:00 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1269746ms till timeout)
2022-03-28 18:57:01 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (785102ms till timeout)
2022-03-28 18:57:01 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (785007ms till timeout)
2022-03-28 18:57:01 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1267232ms till timeout)
2022-03-28 18:57:01 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1266387ms till timeout)
2022-03-28 18:57:01 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (784723ms till timeout)
2022-03-28 18:57:01 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1264720ms till timeout)
2022-03-28 18:57:01 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1268336ms till timeout)
2022-03-28 18:57:01 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (789752ms till timeout)
2022-03-28 18:57:01 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1265270ms till timeout)
2022-03-28 18:57:01 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1267404ms till timeout)
2022-03-28 18:57:01 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (784157ms till timeout)
2022-03-28 18:57:01 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (784152ms till timeout)
2022-03-28 18:57:02 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1268604ms till timeout)
2022-03-28 18:57:02 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (783960ms till timeout)
2022-03-28 18:57:02 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (783865ms till timeout)
2022-03-28 18:57:02 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1266093ms till timeout)
2022-03-28 18:57:02 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1265245ms till timeout)
2022-03-28 18:57:02 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (783577ms till timeout)
2022-03-28 18:57:02 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1263574ms till timeout)
2022-03-28 18:57:02 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1267187ms till timeout)
2022-03-28 18:57:02 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (788604ms till timeout)
2022-03-28 18:57:02 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1264122ms till timeout)
2022-03-28 18:57:02 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1266254ms till timeout)
2022-03-28 18:57:03 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (783009ms till timeout)
2022-03-28 18:57:03 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (783005ms till timeout)
2022-03-28 18:57:03 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1267456ms till timeout)
2022-03-28 18:57:03 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (782812ms till timeout)
2022-03-28 18:57:03 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (782715ms till timeout)
2022-03-28 18:57:03 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1264945ms till timeout)
2022-03-28 18:57:03 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1264089ms till timeout)
2022-03-28 18:57:03 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (782423ms till timeout)
2022-03-28 18:57:03 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1262420ms till timeout)
2022-03-28 18:57:03 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1266029ms till timeout)
2022-03-28 18:57:03 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1262957ms till timeout)
2022-03-28 18:57:03 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (787439ms till timeout)
2022-03-28 18:57:04 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1265080ms till timeout)
2022-03-28 18:57:04 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (781834ms till timeout)
2022-03-28 18:57:04 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (781829ms till timeout)
2022-03-28 18:57:04 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1266281ms till timeout)
2022-03-28 18:57:04 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (781636ms till timeout)
2022-03-28 18:57:04 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (781540ms till timeout)
2022-03-28 18:57:04 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1263769ms till timeout)
2022-03-28 18:57:04 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1262917ms till timeout)
2022-03-28 18:57:04 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (781246ms till timeout)
2022-03-28 18:57:04 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1261244ms till timeout)
2022-03-28 18:57:05 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1264858ms till timeout)
2022-03-28 18:57:05 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (786275ms till timeout)
2022-03-28 18:57:05 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1261793ms till timeout)
2022-03-28 18:57:05 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1263926ms till timeout)
2022-03-28 18:57:05 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (780679ms till timeout)
2022-03-28 18:57:05 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (780675ms till timeout)
2022-03-28 18:57:05 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1265124ms till timeout)
2022-03-28 18:57:05 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (780479ms till timeout)
2022-03-28 18:57:05 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (780384ms till timeout)
2022-03-28 18:57:05 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1262614ms till timeout)
2022-03-28 18:57:05 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1261765ms till timeout)
2022-03-28 18:57:06 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (780097ms till timeout)
2022-03-28 18:57:06 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1260062ms till timeout)
2022-03-28 18:57:06 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1263668ms till timeout)
2022-03-28 18:57:06 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1260602ms till timeout)
2022-03-28 18:57:06 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (785083ms till timeout)
2022-03-28 18:57:06 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1262734ms till timeout)
2022-03-28 18:57:06 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (779488ms till timeout)
2022-03-28 18:57:06 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (779483ms till timeout)
2022-03-28 18:57:06 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1263935ms till timeout)
2022-03-28 18:57:06 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (779291ms till timeout)
2022-03-28 18:57:06 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (779196ms till timeout)
2022-03-28 18:57:07 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1261424ms till timeout)
2022-03-28 18:57:07 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1260575ms till timeout)
2022-03-28 18:57:07 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (778908ms till timeout)
2022-03-28 18:57:07 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1258906ms till timeout)
2022-03-28 18:57:07 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1262521ms till timeout)
2022-03-28 18:57:07 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1259455ms till timeout)
2022-03-28 18:57:07 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (783935ms till timeout)
2022-03-28 18:57:07 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1261587ms till timeout)
2022-03-28 18:57:07 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (778341ms till timeout)
2022-03-28 18:57:07 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (778337ms till timeout)
2022-03-28 18:57:07 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1262788ms till timeout)
2022-03-28 18:57:07 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (778145ms till timeout)
2022-03-28 18:57:08 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (778048ms till timeout)
2022-03-28 18:57:08 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1260279ms till timeout)
2022-03-28 18:57:08 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1259429ms till timeout)
2022-03-28 18:57:08 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (777761ms till timeout)
2022-03-28 18:57:08 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1257759ms till timeout)
2022-03-28 18:57:08 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1261375ms till timeout)
2022-03-28 18:57:08 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1258308ms till timeout)
2022-03-28 18:57:08 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (782788ms till timeout)
2022-03-28 18:57:08 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1260438ms till timeout)
2022-03-28 18:57:08 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (777190ms till timeout)
2022-03-28 18:57:08 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (777179ms till timeout)
2022-03-28 18:57:09 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1261631ms till timeout)
2022-03-28 18:57:09 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (776986ms till timeout)
2022-03-28 18:57:09 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (776891ms till timeout)
2022-03-28 18:57:09 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1259121ms till timeout)
2022-03-28 18:57:09 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1258271ms till timeout)
2022-03-28 18:57:09 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (776606ms till timeout)
2022-03-28 18:57:09 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1256603ms till timeout)
2022-03-28 18:57:09 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1260219ms till timeout)
2022-03-28 18:57:09 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (781633ms till timeout)
2022-03-28 18:57:09 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1257151ms till timeout)
2022-03-28 18:57:09 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1259276ms till timeout)
2022-03-28 18:57:09 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (776030ms till timeout)
2022-03-28 18:57:10 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (776025ms till timeout)
2022-03-28 18:57:10 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1260467ms till timeout)
2022-03-28 18:57:10 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (775822ms till timeout)
2022-03-28 18:57:10 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (775727ms till timeout)
2022-03-28 18:57:10 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1257958ms till timeout)
2022-03-28 18:57:10 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1257108ms till timeout)
2022-03-28 18:57:10 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (775443ms till timeout)
2022-03-28 18:57:10 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1255440ms till timeout)
2022-03-28 18:57:10 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1259053ms till timeout)
2022-03-28 18:57:10 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1255988ms till timeout)
2022-03-28 18:57:10 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (780470ms till timeout)
2022-03-28 18:57:11 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1258124ms till timeout)
2022-03-28 18:57:11 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (774878ms till timeout)
2022-03-28 18:57:11 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (774873ms till timeout)
2022-03-28 18:57:11 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1259325ms till timeout)
2022-03-28 18:57:11 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (774681ms till timeout)
2022-03-28 18:57:11 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (774585ms till timeout)
2022-03-28 18:57:11 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1256814ms till timeout)
2022-03-28 18:57:11 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1255965ms till timeout)
2022-03-28 18:57:11 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (774300ms till timeout)
2022-03-28 18:57:11 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1254294ms till timeout)
2022-03-28 18:57:12 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1257908ms till timeout)
2022-03-28 18:57:12 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (779325ms till timeout)
2022-03-28 18:57:12 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1254842ms till timeout)
2022-03-28 18:57:12 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1256976ms till timeout)
2022-03-28 18:57:12 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (773729ms till timeout)
2022-03-28 18:57:12 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (773724ms till timeout)
2022-03-28 18:57:12 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1258173ms till timeout)
2022-03-28 18:57:12 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (773525ms till timeout)
2022-03-28 18:57:12 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (773420ms till timeout)
2022-03-28 18:57:12 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1255647ms till timeout)
2022-03-28 18:57:12 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1254792ms till timeout)
2022-03-28 18:57:13 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (773108ms till timeout)
2022-03-28 18:57:13 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1253105ms till timeout)
2022-03-28 18:57:13 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1256719ms till timeout)
2022-03-28 18:57:13 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (778135ms till timeout)
2022-03-28 18:57:13 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1253653ms till timeout)
2022-03-28 18:57:13 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1255736ms till timeout)
2022-03-28 18:57:13 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (772503ms till timeout)
2022-03-28 18:57:13 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (772594ms till timeout)
2022-03-28 18:57:13 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1257038ms till timeout)
2022-03-28 18:57:13 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (772390ms till timeout)
2022-03-28 18:57:13 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (772292ms till timeout)
2022-03-28 18:57:13 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1254522ms till timeout)
2022-03-28 18:57:14 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1253673ms till timeout)
2022-03-28 18:57:14 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (772008ms till timeout)
2022-03-28 18:57:14 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1252006ms till timeout)
2022-03-28 18:57:14 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1255621ms till timeout)
2022-03-28 18:57:14 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (777037ms till timeout)
2022-03-28 18:57:14 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1252459ms till timeout)
2022-03-28 18:57:14 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1254592ms till timeout)
2022-03-28 18:57:14 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (771342ms till timeout)
2022-03-28 18:57:14 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (771433ms till timeout)
2022-03-28 18:57:14 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1255884ms till timeout)
2022-03-28 18:57:14 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (771240ms till timeout)
2022-03-28 18:57:14 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (771144ms till timeout)
2022-03-28 18:57:15 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1253374ms till timeout)
2022-03-28 18:57:15 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1252524ms till timeout)
2022-03-28 18:57:15 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (770859ms till timeout)
2022-03-28 18:57:15 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1250857ms till timeout)
2022-03-28 18:57:15 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1254472ms till timeout)
2022-03-28 18:57:15 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (775888ms till timeout)
2022-03-28 18:57:15 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1251310ms till timeout)
2022-03-28 18:57:15 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1253442ms till timeout)
2022-03-28 18:57:15 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (770286ms till timeout)
2022-03-28 18:57:15 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (770195ms till timeout)
2022-03-28 18:57:15 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1254735ms till timeout)
2022-03-28 18:57:16 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (770088ms till timeout)
2022-03-28 18:57:16 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (769993ms till timeout)
2022-03-28 18:57:16 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1252221ms till timeout)
2022-03-28 18:57:16 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1251371ms till timeout)
2022-03-28 18:57:16 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (769707ms till timeout)
2022-03-28 18:57:16 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1249704ms till timeout)
2022-03-28 18:57:16 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1253315ms till timeout)
2022-03-28 18:57:16 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (774733ms till timeout)
2022-03-28 18:57:16 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1250154ms till timeout)
2022-03-28 18:57:16 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1252284ms till timeout)
2022-03-28 18:57:16 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (769123ms till timeout)
2022-03-28 18:57:16 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (769032ms till timeout)
2022-03-28 18:57:17 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1253571ms till timeout)
2022-03-28 18:57:17 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (768925ms till timeout)
2022-03-28 18:57:17 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (768829ms till timeout)
2022-03-28 18:57:17 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1251057ms till timeout)
2022-03-28 18:57:17 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1250206ms till timeout)
2022-03-28 18:57:17 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (768541ms till timeout)
2022-03-28 18:57:17 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1248531ms till timeout)
2022-03-28 18:57:17 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1252146ms till timeout)
2022-03-28 18:57:17 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (773559ms till timeout)
2022-03-28 18:57:17 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1248977ms till timeout)
2022-03-28 18:57:18 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1251109ms till timeout)
2022-03-28 18:57:18 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (767862ms till timeout)
2022-03-28 18:57:18 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (767953ms till timeout)
2022-03-28 18:57:18 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1252405ms till timeout)
2022-03-28 18:57:18 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (767760ms till timeout)
2022-03-28 18:57:18 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (767665ms till timeout)
2022-03-28 18:57:18 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1249895ms till timeout)
2022-03-28 18:57:18 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1249045ms till timeout)
2022-03-28 18:57:18 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (767379ms till timeout)
2022-03-28 18:57:18 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1247378ms till timeout)
2022-03-28 18:57:18 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1250992ms till timeout)
2022-03-28 18:57:19 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (772409ms till timeout)
2022-03-28 18:57:19 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1247831ms till timeout)
2022-03-28 18:57:19 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1249964ms till timeout)
2022-03-28 18:57:19 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (766808ms till timeout)
2022-03-28 18:57:19 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (766717ms till timeout)
2022-03-28 18:57:19 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1251259ms till timeout)
2022-03-28 18:57:19 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (766614ms till timeout)
2022-03-28 18:57:19 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (766518ms till timeout)
2022-03-28 18:57:19 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1248748ms till timeout)
2022-03-28 18:57:19 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1247894ms till timeout)
2022-03-28 18:57:19 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (766229ms till timeout)
2022-03-28 18:57:19 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1246227ms till timeout)
2022-03-28 18:57:20 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1249842ms till timeout)
2022-03-28 18:57:20 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (771259ms till timeout)
2022-03-28 18:57:20 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1246681ms till timeout)
2022-03-28 18:57:20 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1248814ms till timeout)
2022-03-28 18:57:20 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (765568ms till timeout)
2022-03-28 18:57:20 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (765659ms till timeout)
2022-03-28 18:57:20 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1250111ms till timeout)
2022-03-28 18:57:20 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (765466ms till timeout)
2022-03-28 18:57:20 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (765371ms till timeout)
2022-03-28 18:57:20 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1247600ms till timeout)
2022-03-28 18:57:20 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1246751ms till timeout)
2022-03-28 18:57:21 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (765085ms till timeout)
2022-03-28 18:57:21 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1245083ms till timeout)
2022-03-28 18:57:21 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1248697ms till timeout)
2022-03-28 18:57:21 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (770113ms till timeout)
2022-03-28 18:57:21 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1245536ms till timeout)
2022-03-28 18:57:21 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1247669ms till timeout)
2022-03-28 18:57:21 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (764423ms till timeout)
2022-03-28 18:57:21 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (764513ms till timeout)
2022-03-28 18:57:21 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1248962ms till timeout)
2022-03-28 18:57:21 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (764312ms till timeout)
2022-03-28 18:57:21 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (764216ms till timeout)
2022-03-28 18:57:21 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1246446ms till timeout)
2022-03-28 18:57:22 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1245597ms till timeout)
2022-03-28 18:57:22 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (763932ms till timeout)
2022-03-28 18:57:22 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1243929ms till timeout)
2022-03-28 18:57:22 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1247544ms till timeout)
2022-03-28 18:57:22 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (768959ms till timeout)
2022-03-28 18:57:22 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1244373ms till timeout)
2022-03-28 18:57:22 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1246504ms till timeout)
2022-03-28 18:57:22 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (763258ms till timeout)
2022-03-28 18:57:22 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (763349ms till timeout)
2022-03-28 18:57:22 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1247801ms till timeout)
2022-03-28 18:57:22 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (763157ms till timeout)
2022-03-28 18:57:23 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (763062ms till timeout)
2022-03-28 18:57:23 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1245292ms till timeout)
2022-03-28 18:57:23 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1244442ms till timeout)
2022-03-28 18:57:23 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (762777ms till timeout)
2022-03-28 18:57:23 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1242775ms till timeout)
2022-03-28 18:57:23 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1246388ms till timeout)
2022-03-28 18:57:23 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (767805ms till timeout)
2022-03-28 18:57:23 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1243227ms till timeout)
2022-03-28 18:57:23 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1245360ms till timeout)
2022-03-28 18:57:23 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (762204ms till timeout)
2022-03-28 18:57:23 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (762112ms till timeout)
2022-03-28 18:57:24 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1246655ms till timeout)
2022-03-28 18:57:24 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (762011ms till timeout)
2022-03-28 18:57:24 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (761916ms till timeout)
2022-03-28 18:57:24 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1244146ms till timeout)
2022-03-28 18:57:24 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1243294ms till timeout)
2022-03-28 18:57:24 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (761629ms till timeout)
2022-03-28 18:57:24 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1241627ms till timeout)
2022-03-28 18:57:24 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1245242ms till timeout)
2022-03-28 18:57:24 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (766658ms till timeout)
2022-03-28 18:57:24 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1242080ms till timeout)
2022-03-28 18:57:24 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1244213ms till timeout)
2022-03-28 18:57:25 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (761058ms till timeout)
2022-03-28 18:57:25 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (760966ms till timeout)
2022-03-28 18:57:25 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1245509ms till timeout)
2022-03-28 18:57:25 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (760865ms till timeout)
2022-03-28 18:57:25 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (760769ms till timeout)
2022-03-28 18:57:25 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1242999ms till timeout)
2022-03-28 18:57:25 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1242148ms till timeout)
2022-03-28 18:57:25 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (760482ms till timeout)
2022-03-28 18:57:25 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1240481ms till timeout)
2022-03-28 18:57:25 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1244093ms till timeout)
2022-03-28 18:57:25 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (765505ms till timeout)
2022-03-28 18:57:26 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1240926ms till timeout)
2022-03-28 18:57:26 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1243059ms till timeout)
2022-03-28 18:57:26 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (759904ms till timeout)
2022-03-28 18:57:26 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (759813ms till timeout)
2022-03-28 18:57:26 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1244356ms till timeout)
2022-03-28 18:57:26 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (759708ms till timeout)
2022-03-28 18:57:26 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (759611ms till timeout)
2022-03-28 18:57:26 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1241841ms till timeout)
2022-03-28 18:57:26 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1240971ms till timeout)
2022-03-28 18:57:26 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (759306ms till timeout)
2022-03-28 18:57:26 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1239302ms till timeout)
2022-03-28 18:57:27 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1242916ms till timeout)
2022-03-28 18:57:27 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (764332ms till timeout)
2022-03-28 18:57:27 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1239755ms till timeout)
2022-03-28 18:57:27 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1241887ms till timeout)
2022-03-28 18:57:27 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (758732ms till timeout)
2022-03-28 18:57:27 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (758641ms till timeout)
2022-03-28 18:57:27 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1243184ms till timeout)
2022-03-28 18:57:27 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (758539ms till timeout)
2022-03-28 18:57:27 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (758444ms till timeout)
2022-03-28 18:57:27 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1240674ms till timeout)
2022-03-28 18:57:27 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1239826ms till timeout)
2022-03-28 18:57:27 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (758160ms till timeout)
2022-03-28 18:57:28 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1238158ms till timeout)
2022-03-28 18:57:28 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1241773ms till timeout)
2022-03-28 18:57:28 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (763190ms till timeout)
2022-03-28 18:57:28 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1238612ms till timeout)
2022-03-28 18:57:28 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1240746ms till timeout)
2022-03-28 18:57:28 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (757499ms till timeout)
2022-03-28 18:57:28 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (757589ms till timeout)
2022-03-28 18:57:28 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1242041ms till timeout)
2022-03-28 18:57:28 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (757397ms till timeout)
2022-03-28 18:57:28 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (757302ms till timeout)
2022-03-28 18:57:28 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1239532ms till timeout)
2022-03-28 18:57:29 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1238683ms till timeout)
2022-03-28 18:57:29 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (757016ms till timeout)
2022-03-28 18:57:29 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1237014ms till timeout)
2022-03-28 18:57:29 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1240629ms till timeout)
2022-03-28 18:57:29 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (762045ms till timeout)
2022-03-28 18:57:29 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1237469ms till timeout)
2022-03-28 18:57:29 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1239602ms till timeout)
2022-03-28 18:57:29 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (756446ms till timeout)
2022-03-28 18:57:29 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (756355ms till timeout)
2022-03-28 18:57:29 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1240898ms till timeout)
2022-03-28 18:57:29 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (756254ms till timeout)
2022-03-28 18:57:29 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (756158ms till timeout)
2022-03-28 18:57:30 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1238388ms till timeout)
2022-03-28 18:57:30 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1237539ms till timeout)
2022-03-28 18:57:30 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (755874ms till timeout)
2022-03-28 18:57:30 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1235872ms till timeout)
2022-03-28 18:57:30 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1239486ms till timeout)
2022-03-28 18:57:30 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (760901ms till timeout)
2022-03-28 18:57:30 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1236324ms till timeout)
2022-03-28 18:57:30 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1238456ms till timeout)
2022-03-28 18:57:30 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (755301ms till timeout)
2022-03-28 18:57:30 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (755210ms till timeout)
2022-03-28 18:57:30 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1239753ms till timeout)
2022-03-28 18:57:31 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (755108ms till timeout)
2022-03-28 18:57:31 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (755011ms till timeout)
2022-03-28 18:57:31 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1237241ms till timeout)
2022-03-28 18:57:31 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1236393ms till timeout)
2022-03-28 18:57:31 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (754724ms till timeout)
2022-03-28 18:57:31 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1234721ms till timeout)
2022-03-28 18:57:31 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1238336ms till timeout)
2022-03-28 18:57:31 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (759752ms till timeout)
2022-03-28 18:57:31 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1235175ms till timeout)
2022-03-28 18:57:31 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1237307ms till timeout)
2022-03-28 18:57:31 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (754061ms till timeout)
2022-03-28 18:57:31 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (754152ms till timeout)
2022-03-28 18:57:32 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1238603ms till timeout)
2022-03-28 18:57:32 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (753959ms till timeout)
2022-03-28 18:57:32 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (753863ms till timeout)
2022-03-28 18:57:32 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1236093ms till timeout)
2022-03-28 18:57:32 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1235244ms till timeout)
2022-03-28 18:57:32 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (753579ms till timeout)
2022-03-28 18:57:32 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1233577ms till timeout)
2022-03-28 18:57:32 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1237192ms till timeout)
2022-03-28 18:57:32 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (758609ms till timeout)
2022-03-28 18:57:32 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1234031ms till timeout)
2022-03-28 18:57:33 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1236165ms till timeout)
2022-03-28 18:57:33 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (752919ms till timeout)
2022-03-28 18:57:33 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (753009ms till timeout)
2022-03-28 18:57:33 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1237461ms till timeout)
2022-03-28 18:57:33 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (752817ms till timeout)
2022-03-28 18:57:33 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (752722ms till timeout)
2022-03-28 18:57:33 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1234952ms till timeout)
2022-03-28 18:57:33 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1234102ms till timeout)
2022-03-28 18:57:33 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (752437ms till timeout)
2022-03-28 18:57:33 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1232432ms till timeout)
2022-03-28 18:57:33 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1236047ms till timeout)
2022-03-28 18:57:33 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (757463ms till timeout)
2022-03-28 18:57:34 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1232886ms till timeout)
2022-03-28 18:57:34 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1235019ms till timeout)
2022-03-28 18:57:34 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (751864ms till timeout)
2022-03-28 18:57:34 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (751773ms till timeout)
2022-03-28 18:57:34 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1236316ms till timeout)
2022-03-28 18:57:34 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (751672ms till timeout)
2022-03-28 18:57:34 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (751576ms till timeout)
2022-03-28 18:57:34 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1233806ms till timeout)
2022-03-28 18:57:34 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1232956ms till timeout)
2022-03-28 18:57:34 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (751291ms till timeout)
2022-03-28 18:57:34 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1231289ms till timeout)
2022-03-28 18:57:35 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1234904ms till timeout)
2022-03-28 18:57:35 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (756321ms till timeout)
2022-03-28 18:57:35 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1231744ms till timeout)
2022-03-28 18:57:35 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1233877ms till timeout)
2022-03-28 18:57:35 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (750631ms till timeout)
2022-03-28 18:57:35 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (750722ms till timeout)
2022-03-28 18:57:35 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1235174ms till timeout)
2022-03-28 18:57:35 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (750530ms till timeout)
2022-03-28 18:57:35 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (750434ms till timeout)
2022-03-28 18:57:35 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1232664ms till timeout)
2022-03-28 18:57:35 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1231815ms till timeout)
2022-03-28 18:57:35 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (750150ms till timeout)
2022-03-28 18:57:36 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1230145ms till timeout)
2022-03-28 18:57:36 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1233761ms till timeout)
2022-03-28 18:57:36 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (755177ms till timeout)
2022-03-28 18:57:36 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1230600ms till timeout)
2022-03-28 18:57:36 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1232733ms till timeout)
2022-03-28 18:57:36 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (749486ms till timeout)
2022-03-28 18:57:36 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (749576ms till timeout)
2022-03-28 18:57:36 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1234028ms till timeout)
2022-03-28 18:57:36 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (749380ms till timeout)
2022-03-28 18:57:36 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (749283ms till timeout)
2022-03-28 18:57:36 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1231513ms till timeout)
2022-03-28 18:57:37 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1230664ms till timeout)
2022-03-28 18:57:37 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (748996ms till timeout)
2022-03-28 18:57:37 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1228992ms till timeout)
2022-03-28 18:57:37 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1232606ms till timeout)
2022-03-28 18:57:37 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (754024ms till timeout)
2022-03-28 18:57:37 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1229446ms till timeout)
2022-03-28 18:57:37 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1231579ms till timeout)
2022-03-28 18:57:37 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (748333ms till timeout)
2022-03-28 18:57:37 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (748423ms till timeout)
2022-03-28 18:57:37 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1232875ms till timeout)
2022-03-28 18:57:37 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (748231ms till timeout)
2022-03-28 18:57:37 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (748135ms till timeout)
2022-03-28 18:57:38 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1230365ms till timeout)
2022-03-28 18:57:38 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1229512ms till timeout)
2022-03-28 18:57:38 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (747847ms till timeout)
2022-03-28 18:57:38 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1227844ms till timeout)
2022-03-28 18:57:38 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1231460ms till timeout)
2022-03-28 18:57:38 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (752876ms till timeout)
2022-03-28 18:57:38 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1228300ms till timeout)
2022-03-28 18:57:38 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1230433ms till timeout)
2022-03-28 18:57:38 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (747186ms till timeout)
2022-03-28 18:57:38 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (747277ms till timeout)
2022-03-28 18:57:38 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1231730ms till timeout)
2022-03-28 18:57:39 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (747070ms till timeout)
2022-03-28 18:57:39 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (746968ms till timeout)
2022-03-28 18:57:39 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1229196ms till timeout)
2022-03-28 18:57:39 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1228346ms till timeout)
2022-03-28 18:57:39 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (746681ms till timeout)
2022-03-28 18:57:39 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1226679ms till timeout)
2022-03-28 18:57:39 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1230293ms till timeout)
2022-03-28 18:57:39 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (751710ms till timeout)
2022-03-28 18:57:39 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1227132ms till timeout)
2022-03-28 18:57:39 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1229265ms till timeout)
2022-03-28 18:57:40 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (746019ms till timeout)
2022-03-28 18:57:40 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (746110ms till timeout)
2022-03-28 18:57:40 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1230562ms till timeout)
2022-03-28 18:57:40 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (745918ms till timeout)
2022-03-28 18:57:40 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (745822ms till timeout)
2022-03-28 18:57:40 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1228049ms till timeout)
2022-03-28 18:57:40 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1227200ms till timeout)
2022-03-28 18:57:40 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (745535ms till timeout)
2022-03-28 18:57:40 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1225532ms till timeout)
2022-03-28 18:57:40 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1229147ms till timeout)
2022-03-28 18:57:40 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (750564ms till timeout)
2022-03-28 18:57:40 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1225987ms till timeout)
2022-03-28 18:57:41 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1228123ms till timeout)
2022-03-28 18:57:41 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (744964ms till timeout)
2022-03-28 18:57:41 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (744873ms till timeout)
2022-03-28 18:57:41 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1229415ms till timeout)
2022-03-28 18:57:41 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (744769ms till timeout)
2022-03-28 18:57:41 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (744674ms till timeout)
2022-03-28 18:57:41 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1226904ms till timeout)
2022-03-28 18:57:41 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1226055ms till timeout)
2022-03-28 18:57:41 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (744390ms till timeout)
2022-03-28 18:57:41 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1224388ms till timeout)
2022-03-28 18:57:41 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1228002ms till timeout)
2022-03-28 18:57:42 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (749419ms till timeout)
2022-03-28 18:57:42 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1224841ms till timeout)
2022-03-28 18:57:42 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1226974ms till timeout)
2022-03-28 18:57:42 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (743723ms till timeout)
2022-03-28 18:57:42 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (743814ms till timeout)
2022-03-28 18:57:42 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1228263ms till timeout)
2022-03-28 18:57:42 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (743619ms till timeout)
2022-03-28 18:57:42 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (743524ms till timeout)
2022-03-28 18:57:42 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1225733ms till timeout)
2022-03-28 18:57:42 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1224884ms till timeout)
2022-03-28 18:57:42 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (743219ms till timeout)
2022-03-28 18:57:43 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1223206ms till timeout)
2022-03-28 18:57:43 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1226821ms till timeout)
2022-03-28 18:57:43 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (748237ms till timeout)
2022-03-28 18:57:43 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1223660ms till timeout)
2022-03-28 18:57:43 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1225794ms till timeout)
2022-03-28 18:57:43 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (742639ms till timeout)
2022-03-28 18:57:43 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (742548ms till timeout)
2022-03-28 18:57:43 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1227091ms till timeout)
2022-03-28 18:57:43 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (742447ms till timeout)
2022-03-28 18:57:43 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (742352ms till timeout)
2022-03-28 18:57:43 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1224578ms till timeout)
2022-03-28 18:57:43 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1223729ms till timeout)
2022-03-28 18:57:44 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (742064ms till timeout)
2022-03-28 18:57:44 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1222062ms till timeout)
2022-03-28 18:57:44 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1225676ms till timeout)
2022-03-28 18:57:44 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (747092ms till timeout)
2022-03-28 18:57:44 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1222514ms till timeout)
2022-03-28 18:57:44 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1224648ms till timeout)
2022-03-28 18:57:44 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (741402ms till timeout)
2022-03-28 18:57:44 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (741493ms till timeout)
2022-03-28 18:57:44 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1225945ms till timeout)
2022-03-28 18:57:44 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (741301ms till timeout)
2022-03-28 18:57:44 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (741205ms till timeout)
2022-03-28 18:57:45 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1223435ms till timeout)
2022-03-28 18:57:45 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1222586ms till timeout)
2022-03-28 18:57:45 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (740921ms till timeout)
2022-03-28 18:57:45 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1220908ms till timeout)
2022-03-28 18:57:45 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1224523ms till timeout)
2022-03-28 18:57:45 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (745939ms till timeout)
2022-03-28 18:57:45 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1221346ms till timeout)
2022-03-28 18:57:45 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1223479ms till timeout)
2022-03-28 18:57:45 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (740232ms till timeout)
2022-03-28 18:57:45 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (740322ms till timeout)
2022-03-28 18:57:45 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1224774ms till timeout)
2022-03-28 18:57:45 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (740129ms till timeout)
2022-03-28 18:57:46 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (740030ms till timeout)
2022-03-28 18:57:46 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1222259ms till timeout)
2022-03-28 18:57:46 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1221410ms till timeout)
2022-03-28 18:57:46 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (739744ms till timeout)
2022-03-28 18:57:46 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1219742ms till timeout)
2022-03-28 18:57:46 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1223357ms till timeout)
2022-03-28 18:57:46 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (744773ms till timeout)
2022-03-28 18:57:46 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1220195ms till timeout)
2022-03-28 18:57:46 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1222329ms till timeout)
2022-03-28 18:57:46 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (739173ms till timeout)
2022-03-28 18:57:46 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (739082ms till timeout)
2022-03-28 18:57:47 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1223624ms till timeout)
2022-03-28 18:57:47 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (738979ms till timeout)
2022-03-28 18:57:47 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (738883ms till timeout)
2022-03-28 18:57:47 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1221113ms till timeout)
2022-03-28 18:57:47 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1220264ms till timeout)
2022-03-28 18:57:47 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (738597ms till timeout)
2022-03-28 18:57:47 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1218595ms till timeout)
2022-03-28 18:57:47 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1222209ms till timeout)
2022-03-28 18:57:47 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (743626ms till timeout)
2022-03-28 18:57:47 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1219048ms till timeout)
2022-03-28 18:57:47 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1221180ms till timeout)
2022-03-28 18:57:48 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (738025ms till timeout)
2022-03-28 18:57:48 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (737934ms till timeout)
2022-03-28 18:57:48 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1222478ms till timeout)
2022-03-28 18:57:48 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (737833ms till timeout)
2022-03-28 18:57:48 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (737738ms till timeout)
2022-03-28 18:57:48 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1219967ms till timeout)
2022-03-28 18:57:48 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1219118ms till timeout)
2022-03-28 18:57:48 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (737449ms till timeout)
2022-03-28 18:57:48 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1217446ms till timeout)
2022-03-28 18:57:48 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1221061ms till timeout)
2022-03-28 18:57:48 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (742478ms till timeout)
2022-03-28 18:57:49 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1217892ms till timeout)
2022-03-28 18:57:49 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1220026ms till timeout)
2022-03-28 18:57:49 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (736870ms till timeout)
2022-03-28 18:57:49 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (736779ms till timeout)
2022-03-28 18:57:49 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1221322ms till timeout)
2022-03-28 18:57:49 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (736678ms till timeout)
2022-03-28 18:57:49 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (736582ms till timeout)
2022-03-28 18:57:49 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1218812ms till timeout)
2022-03-28 18:57:49 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1217963ms till timeout)
2022-03-28 18:57:49 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (736295ms till timeout)
2022-03-28 18:57:49 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1216293ms till timeout)
2022-03-28 18:57:50 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1219908ms till timeout)
2022-03-28 18:57:50 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (741325ms till timeout)
2022-03-28 18:57:50 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1216747ms till timeout)
2022-03-28 18:57:50 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1218878ms till timeout)
2022-03-28 18:57:50 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (735631ms till timeout)
2022-03-28 18:57:50 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (735722ms till timeout)
2022-03-28 18:57:50 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1220174ms till timeout)
2022-03-28 18:57:50 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (735528ms till timeout)
2022-03-28 18:57:50 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (735429ms till timeout)
2022-03-28 18:57:50 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1217662ms till timeout)
2022-03-28 18:57:50 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1216809ms till timeout)
2022-03-28 18:57:50 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (735144ms till timeout)
2022-03-28 18:57:51 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1215141ms till timeout)
2022-03-28 18:57:51 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1218755ms till timeout)
2022-03-28 18:57:51 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (740172ms till timeout)
2022-03-28 18:57:51 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1215593ms till timeout)
2022-03-28 18:57:51 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1217726ms till timeout)
2022-03-28 18:57:51 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (734570ms till timeout)
2022-03-28 18:57:51 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (734479ms till timeout)
2022-03-28 18:57:51 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1219016ms till timeout)
2022-03-28 18:57:51 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (734370ms till timeout)
2022-03-28 18:57:51 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (734274ms till timeout)
2022-03-28 18:57:51 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1216504ms till timeout)
2022-03-28 18:57:52 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1215654ms till timeout)
2022-03-28 18:57:52 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (733989ms till timeout)
2022-03-28 18:57:52 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1213982ms till timeout)
2022-03-28 18:57:52 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1217596ms till timeout)
2022-03-28 18:57:52 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (739013ms till timeout)
2022-03-28 18:57:52 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1214434ms till timeout)
2022-03-28 18:57:52 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1216567ms till timeout)
2022-03-28 18:57:52 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (733412ms till timeout)
2022-03-28 18:57:52 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (733321ms till timeout)
2022-03-28 18:57:52 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1217863ms till timeout)
2022-03-28 18:57:52 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (733217ms till timeout)
2022-03-28 18:57:52 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (733122ms till timeout)
2022-03-28 18:57:53 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1215351ms till timeout)
2022-03-28 18:57:53 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1214502ms till timeout)
2022-03-28 18:57:53 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (732820ms till timeout)
2022-03-28 18:57:53 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1212812ms till timeout)
2022-03-28 18:57:53 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1216426ms till timeout)
2022-03-28 18:57:53 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (737843ms till timeout)
2022-03-28 18:57:53 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1213265ms till timeout)
2022-03-28 18:57:53 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1215395ms till timeout)
2022-03-28 18:57:53 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (732239ms till timeout)
2022-03-28 18:57:53 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (732148ms till timeout)
2022-03-28 18:57:53 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1216691ms till timeout)
2022-03-28 18:57:54 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (732047ms till timeout)
2022-03-28 18:57:54 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (731952ms till timeout)
2022-03-28 18:57:54 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1214181ms till timeout)
2022-03-28 18:57:54 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1213332ms till timeout)
2022-03-28 18:57:54 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (731666ms till timeout)
2022-03-28 18:57:54 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1211663ms till timeout)
2022-03-28 18:57:54 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1215278ms till timeout)
2022-03-28 18:57:54 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (736695ms till timeout)
2022-03-28 18:57:54 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1212118ms till timeout)
2022-03-28 18:57:54 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1214250ms till timeout)
2022-03-28 18:57:55 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (731091ms till timeout)
2022-03-28 18:57:55 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (730996ms till timeout)
2022-03-28 18:57:55 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1215537ms till timeout)
2022-03-28 18:57:55 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (730893ms till timeout)
2022-03-28 18:57:55 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (730798ms till timeout)
2022-03-28 18:57:55 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1213028ms till timeout)
2022-03-28 18:57:55 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1212179ms till timeout)
2022-03-28 18:57:55 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (730513ms till timeout)
2022-03-28 18:57:55 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1210511ms till timeout)
2022-03-28 18:57:55 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1214126ms till timeout)
2022-03-28 18:57:55 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (735542ms till timeout)
2022-03-28 18:57:55 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1210964ms till timeout)
2022-03-28 18:57:56 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1213094ms till timeout)
2022-03-28 18:57:56 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (729940ms till timeout)
2022-03-28 18:57:56 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (729848ms till timeout)
2022-03-28 18:57:56 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1214390ms till timeout)
2022-03-28 18:57:56 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (729746ms till timeout)
2022-03-28 18:57:56 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (729650ms till timeout)
2022-03-28 18:57:56 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1211880ms till timeout)
2022-03-28 18:57:56 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1211031ms till timeout)
2022-03-28 18:57:56 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (729366ms till timeout)
2022-03-28 18:57:56 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1209363ms till timeout)
2022-03-28 18:57:56 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1212978ms till timeout)
2022-03-28 18:57:57 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (734393ms till timeout)
2022-03-28 18:57:57 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1209816ms till timeout)
2022-03-28 18:57:57 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1211949ms till timeout)
2022-03-28 18:57:57 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (728793ms till timeout)
2022-03-28 18:57:57 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (728702ms till timeout)
2022-03-28 18:57:57 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1213245ms till timeout)
2022-03-28 18:57:57 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (728601ms till timeout)
2022-03-28 18:57:57 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (728506ms till timeout)
2022-03-28 18:57:57 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1210735ms till timeout)
2022-03-28 18:57:57 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1209886ms till timeout)
2022-03-28 18:57:57 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (728220ms till timeout)
2022-03-28 18:57:57 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1208218ms till timeout)
2022-03-28 18:57:58 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1211832ms till timeout)
2022-03-28 18:57:58 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (733249ms till timeout)
2022-03-28 18:57:58 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1208671ms till timeout)
2022-03-28 18:57:58 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1210804ms till timeout)
2022-03-28 18:57:58 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (727558ms till timeout)
2022-03-28 18:57:58 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (727649ms till timeout)
2022-03-28 18:57:58 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1212099ms till timeout)
2022-03-28 18:57:58 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (727455ms till timeout)
2022-03-28 18:57:58 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (727359ms till timeout)
2022-03-28 18:57:58 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1209590ms till timeout)
2022-03-28 18:57:58 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1208733ms till timeout)
2022-03-28 18:57:59 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (727067ms till timeout)
2022-03-28 18:57:59 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1207065ms till timeout)
2022-03-28 18:57:59 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1210680ms till timeout)
2022-03-28 18:57:59 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (732096ms till timeout)
2022-03-28 18:57:59 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1207519ms till timeout)
2022-03-28 18:57:59 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1209651ms till timeout)
2022-03-28 18:57:59 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (726404ms till timeout)
2022-03-28 18:57:59 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (726495ms till timeout)
2022-03-28 18:57:59 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1210947ms till timeout)
2022-03-28 18:57:59 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (726302ms till timeout)
2022-03-28 18:57:59 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (726208ms till timeout)
2022-03-28 18:58:00 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1208438ms till timeout)
2022-03-28 18:58:00 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1207588ms till timeout)
2022-03-28 18:58:00 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (725922ms till timeout)
2022-03-28 18:58:00 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1205920ms till timeout)
2022-03-28 18:58:00 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1209534ms till timeout)
2022-03-28 18:58:00 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (730951ms till timeout)
2022-03-28 18:58:00 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1206358ms till timeout)
2022-03-28 18:58:00 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1208491ms till timeout)
2022-03-28 18:58:00 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (725335ms till timeout)
2022-03-28 18:58:00 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (725244ms till timeout)
2022-03-28 18:58:00 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1209786ms till timeout)
2022-03-28 18:58:00 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (725142ms till timeout)
2022-03-28 18:58:01 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (725047ms till timeout)
2022-03-28 18:58:01 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1207271ms till timeout)
2022-03-28 18:58:01 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1206422ms till timeout)
2022-03-28 18:58:01 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (724757ms till timeout)
2022-03-28 18:58:01 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1204755ms till timeout)
2022-03-28 18:58:01 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1208359ms till timeout)
2022-03-28 18:58:01 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (729775ms till timeout)
2022-03-28 18:58:01 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1205198ms till timeout)
2022-03-28 18:58:01 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1207330ms till timeout)
2022-03-28 18:58:01 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (724175ms till timeout)
2022-03-28 18:58:01 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (724084ms till timeout)
2022-03-28 18:58:02 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1208627ms till timeout)
2022-03-28 18:58:02 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (723983ms till timeout)
2022-03-28 18:58:02 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (723885ms till timeout)
2022-03-28 18:58:02 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1206114ms till timeout)
2022-03-28 18:58:02 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1205265ms till timeout)
2022-03-28 18:58:02 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (723600ms till timeout)
2022-03-28 18:58:02 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1203598ms till timeout)
2022-03-28 18:58:02 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1207212ms till timeout)
2022-03-28 18:58:02 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (728628ms till timeout)
2022-03-28 18:58:02 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1204051ms till timeout)
2022-03-28 18:58:02 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1206182ms till timeout)
2022-03-28 18:58:03 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (723027ms till timeout)
2022-03-28 18:58:03 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (722936ms till timeout)
2022-03-28 18:58:03 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1207479ms till timeout)
2022-03-28 18:58:03 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (722835ms till timeout)
2022-03-28 18:58:03 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (722739ms till timeout)
2022-03-28 18:58:03 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1204969ms till timeout)
2022-03-28 18:58:03 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1204121ms till timeout)
2022-03-28 18:58:03 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (722455ms till timeout)
2022-03-28 18:58:03 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1202453ms till timeout)
2022-03-28 18:58:03 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1206068ms till timeout)
2022-03-28 18:58:03 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (727484ms till timeout)
2022-03-28 18:58:04 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1202906ms till timeout)
2022-03-28 18:58:04 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1205039ms till timeout)
2022-03-28 18:58:04 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (721884ms till timeout)
2022-03-28 18:58:04 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (721792ms till timeout)
2022-03-28 18:58:04 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1206336ms till timeout)
2022-03-28 18:58:04 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (721691ms till timeout)
2022-03-28 18:58:04 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (721596ms till timeout)
2022-03-28 18:58:04 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1203826ms till timeout)
2022-03-28 18:58:04 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1202977ms till timeout)
2022-03-28 18:58:04 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (721311ms till timeout)
2022-03-28 18:58:04 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1201309ms till timeout)
2022-03-28 18:58:04 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1204924ms till timeout)
2022-03-28 18:58:05 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (726340ms till timeout)
2022-03-28 18:58:05 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1201764ms till timeout)
2022-03-28 18:58:05 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1203896ms till timeout)
2022-03-28 18:58:05 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (720650ms till timeout)
2022-03-28 18:58:05 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (720740ms till timeout)
2022-03-28 18:58:05 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1205193ms till timeout)
2022-03-28 18:58:05 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (720549ms till timeout)
2022-03-28 18:58:05 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (720453ms till timeout)
2022-03-28 18:58:05 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1202683ms till timeout)
2022-03-28 18:58:05 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1201834ms till timeout)
2022-03-28 18:58:05 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (720169ms till timeout)
2022-03-28 18:58:06 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1200167ms till timeout)
2022-03-28 18:58:06 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1203781ms till timeout)
2022-03-28 18:58:06 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (725197ms till timeout)
2022-03-28 18:58:06 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1200620ms till timeout)
2022-03-28 18:58:06 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1202753ms till timeout)
2022-03-28 18:58:06 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (719598ms till timeout)
2022-03-28 18:58:06 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (719506ms till timeout)
2022-03-28 18:58:06 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1204050ms till timeout)
2022-03-28 18:58:06 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (719399ms till timeout)
2022-03-28 18:58:06 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (719304ms till timeout)
2022-03-28 18:58:06 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1201534ms till timeout)
2022-03-28 18:58:07 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1200685ms till timeout)
2022-03-28 18:58:07 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (719013ms till timeout)
2022-03-28 18:58:07 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1199011ms till timeout)
2022-03-28 18:58:07 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1202626ms till timeout)
2022-03-28 18:58:07 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (724042ms till timeout)
2022-03-28 18:58:07 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1199465ms till timeout)
2022-03-28 18:58:07 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1201597ms till timeout)
2022-03-28 18:58:07 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (718351ms till timeout)
2022-03-28 18:58:07 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (718442ms till timeout)
2022-03-28 18:58:07 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1202892ms till timeout)
2022-03-28 18:58:07 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (718247ms till timeout)
2022-03-28 18:58:07 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (718153ms till timeout)
2022-03-28 18:58:08 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1200382ms till timeout)
2022-03-28 18:58:08 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1199533ms till timeout)
2022-03-28 18:58:08 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (717868ms till timeout)
2022-03-28 18:58:08 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1197866ms till timeout)
2022-03-28 18:58:08 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1201481ms till timeout)
2022-03-28 18:58:08 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (722898ms till timeout)
2022-03-28 18:58:08 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1198319ms till timeout)
2022-03-28 18:58:08 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1200452ms till timeout)
2022-03-28 18:58:08 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (717206ms till timeout)
2022-03-28 18:58:08 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (717296ms till timeout)
2022-03-28 18:58:08 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1201748ms till timeout)
2022-03-28 18:58:09 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (717104ms till timeout)
2022-03-28 18:58:09 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (717009ms till timeout)
2022-03-28 18:58:09 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1199239ms till timeout)
2022-03-28 18:58:09 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1198388ms till timeout)
2022-03-28 18:58:09 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (716723ms till timeout)
2022-03-28 18:58:09 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1196722ms till timeout)
2022-03-28 18:58:09 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1200336ms till timeout)
2022-03-28 18:58:09 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (721753ms till timeout)
2022-03-28 18:58:09 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1197176ms till timeout)
2022-03-28 18:58:09 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1199309ms till timeout)
2022-03-28 18:58:09 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (716062ms till timeout)
2022-03-28 18:58:09 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (716153ms till timeout)
2022-03-28 18:58:10 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1200605ms till timeout)
2022-03-28 18:58:10 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (715961ms till timeout)
2022-03-28 18:58:10 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (715865ms till timeout)
2022-03-28 18:58:10 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1198095ms till timeout)
2022-03-28 18:58:10 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1197246ms till timeout)
2022-03-28 18:58:10 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (715581ms till timeout)
2022-03-28 18:58:10 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1195578ms till timeout)
2022-03-28 18:58:10 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1199193ms till timeout)
2022-03-28 18:58:10 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (720610ms till timeout)
2022-03-28 18:58:10 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1196033ms till timeout)
2022-03-28 18:58:11 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1198169ms till timeout)
2022-03-28 18:58:11 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (715014ms till timeout)
2022-03-28 18:58:11 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (714923ms till timeout)
2022-03-28 18:58:11 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1199466ms till timeout)
2022-03-28 18:58:11 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (714819ms till timeout)
2022-03-28 18:58:11 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (714724ms till timeout)
2022-03-28 18:58:11 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1196945ms till timeout)
2022-03-28 18:58:11 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1196096ms till timeout)
2022-03-28 18:58:11 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (714430ms till timeout)
2022-03-28 18:58:11 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1194425ms till timeout)
2022-03-28 18:58:11 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1198040ms till timeout)
2022-03-28 18:58:11 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (719456ms till timeout)
2022-03-28 18:58:12 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1194878ms till timeout)
2022-03-28 18:58:12 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1197011ms till timeout)
2022-03-28 18:58:12 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (713765ms till timeout)
2022-03-28 18:58:12 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (713855ms till timeout)
2022-03-28 18:58:12 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1198307ms till timeout)
2022-03-28 18:58:12 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (713663ms till timeout)
2022-03-28 18:58:12 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (713568ms till timeout)
2022-03-28 18:58:12 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1195797ms till timeout)
2022-03-28 18:58:12 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1194946ms till timeout)
2022-03-28 18:58:12 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (713279ms till timeout)
2022-03-28 18:58:12 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1193276ms till timeout)
2022-03-28 18:58:13 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1196876ms till timeout)
2022-03-28 18:58:13 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (718293ms till timeout)
2022-03-28 18:58:13 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1193715ms till timeout)
2022-03-28 18:58:13 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1195847ms till timeout)
2022-03-28 18:58:13 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (712601ms till timeout)
2022-03-28 18:58:13 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (712692ms till timeout)
2022-03-28 18:58:13 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1197144ms till timeout)
2022-03-28 18:58:13 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (712500ms till timeout)
2022-03-28 18:58:13 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (712405ms till timeout)
2022-03-28 18:58:13 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1194635ms till timeout)
2022-03-28 18:58:13 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1193785ms till timeout)
2022-03-28 18:58:13 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (712120ms till timeout)
2022-03-28 18:58:14 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1192116ms till timeout)
2022-03-28 18:58:14 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1195731ms till timeout)
2022-03-28 18:58:14 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (717147ms till timeout)
2022-03-28 18:58:14 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1192569ms till timeout)
2022-03-28 18:58:14 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1194702ms till timeout)
2022-03-28 18:58:14 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (711544ms till timeout)
2022-03-28 18:58:14 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (711453ms till timeout)
2022-03-28 18:58:14 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1195995ms till timeout)
2022-03-28 18:58:14 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (711351ms till timeout)
2022-03-28 18:58:14 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (711256ms till timeout)
2022-03-28 18:58:14 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1193485ms till timeout)
2022-03-28 18:58:15 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1192636ms till timeout)
2022-03-28 18:58:15 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (710970ms till timeout)
2022-03-28 18:58:15 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1190968ms till timeout)
2022-03-28 18:58:15 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1194581ms till timeout)
2022-03-28 18:58:15 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (715998ms till timeout)
2022-03-28 18:58:15 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1191421ms till timeout)
2022-03-28 18:58:15 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1193553ms till timeout)
2022-03-28 18:58:15 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (710307ms till timeout)
2022-03-28 18:58:15 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (710398ms till timeout)
2022-03-28 18:58:15 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1194850ms till timeout)
2022-03-28 18:58:15 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (710206ms till timeout)
2022-03-28 18:58:16 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (710110ms till timeout)
2022-03-28 18:58:16 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1192340ms till timeout)
2022-03-28 18:58:16 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1191491ms till timeout)
2022-03-28 18:58:16 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (709826ms till timeout)
2022-03-28 18:58:16 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1189823ms till timeout)
2022-03-28 18:58:16 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1193438ms till timeout)
2022-03-28 18:58:16 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (714855ms till timeout)
2022-03-28 18:58:16 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1190278ms till timeout)
2022-03-28 18:58:16 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1192410ms till timeout)
2022-03-28 18:58:16 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (709163ms till timeout)
2022-03-28 18:58:16 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (709253ms till timeout)
2022-03-28 18:58:16 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1193706ms till timeout)
2022-03-28 18:58:17 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (709061ms till timeout)
2022-03-28 18:58:17 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (708966ms till timeout)
2022-03-28 18:58:17 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1191196ms till timeout)
2022-03-28 18:58:17 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1190347ms till timeout)
2022-03-28 18:58:17 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (708681ms till timeout)
2022-03-28 18:58:17 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1188679ms till timeout)
2022-03-28 18:58:17 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1192294ms till timeout)
2022-03-28 18:58:17 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (713710ms till timeout)
2022-03-28 18:58:17 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1189132ms till timeout)
2022-03-28 18:58:17 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1191265ms till timeout)
2022-03-28 18:58:18 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (708110ms till timeout)
2022-03-28 18:58:18 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (708019ms till timeout)
2022-03-28 18:58:18 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1192561ms till timeout)
2022-03-28 18:58:18 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (707918ms till timeout)
2022-03-28 18:58:18 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (707823ms till timeout)
2022-03-28 18:58:18 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1190053ms till timeout)
2022-03-28 18:58:18 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1189202ms till timeout)
2022-03-28 18:58:18 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (707537ms till timeout)
2022-03-28 18:58:18 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1187534ms till timeout)
2022-03-28 18:58:18 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1191149ms till timeout)
2022-03-28 18:58:18 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (712565ms till timeout)
2022-03-28 18:58:18 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1187988ms till timeout)
2022-03-28 18:58:19 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1190121ms till timeout)
2022-03-28 18:58:19 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (706966ms till timeout)
2022-03-28 18:58:19 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (706873ms till timeout)
2022-03-28 18:58:19 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1191418ms till timeout)
2022-03-28 18:58:19 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (706774ms till timeout)
2022-03-28 18:58:19 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (706679ms till timeout)
2022-03-28 18:58:19 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1188909ms till timeout)
2022-03-28 18:58:19 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1188059ms till timeout)
2022-03-28 18:58:19 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (706394ms till timeout)
2022-03-28 18:58:19 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1186387ms till timeout)
2022-03-28 18:58:19 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1190006ms till timeout)
2022-03-28 18:58:20 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (711423ms till timeout)
2022-03-28 18:58:20 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1186846ms till timeout)
2022-03-28 18:58:20 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1188979ms till timeout)
2022-03-28 18:58:20 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (705733ms till timeout)
2022-03-28 18:58:20 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (705824ms till timeout)
2022-03-28 18:58:20 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1190275ms till timeout)
2022-03-28 18:58:20 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (705631ms till timeout)
2022-03-28 18:58:20 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (705534ms till timeout)
2022-03-28 18:58:20 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1187762ms till timeout)
2022-03-28 18:58:20 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1186911ms till timeout)
2022-03-28 18:58:20 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (705242ms till timeout)
2022-03-28 18:58:20 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1185239ms till timeout)
2022-03-28 18:58:21 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1188851ms till timeout)
2022-03-28 18:58:21 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (710268ms till timeout)
2022-03-28 18:58:21 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1185691ms till timeout)
2022-03-28 18:58:21 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1187822ms till timeout)
2022-03-28 18:58:21 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (704574ms till timeout)
2022-03-28 18:58:21 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (704651ms till timeout)
2022-03-28 18:58:21 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1189094ms till timeout)
2022-03-28 18:58:21 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (704448ms till timeout)
2022-03-28 18:58:21 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (704353ms till timeout)
2022-03-28 18:58:21 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1186582ms till timeout)
2022-03-28 18:58:21 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1185733ms till timeout)
2022-03-28 18:58:22 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (704066ms till timeout)
2022-03-28 18:58:22 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1184059ms till timeout)
2022-03-28 18:58:22 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1187670ms till timeout)
2022-03-28 18:58:22 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (709086ms till timeout)
2022-03-28 18:58:22 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1184507ms till timeout)
2022-03-28 18:58:22 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1186639ms till timeout)
2022-03-28 18:58:22 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (703392ms till timeout)
2022-03-28 18:58:22 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (703483ms till timeout)
2022-03-28 18:58:22 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1187934ms till timeout)
2022-03-28 18:58:22 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (703289ms till timeout)
2022-03-28 18:58:22 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (703195ms till timeout)
2022-03-28 18:58:23 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1185421ms till timeout)
2022-03-28 18:58:23 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1184571ms till timeout)
2022-03-28 18:58:23 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (702906ms till timeout)
2022-03-28 18:58:23 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1182904ms till timeout)
2022-03-28 18:58:23 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1186518ms till timeout)
2022-03-28 18:58:23 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (707934ms till timeout)
2022-03-28 18:58:23 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1183357ms till timeout)
2022-03-28 18:58:23 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1185489ms till timeout)
2022-03-28 18:58:23 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (702327ms till timeout)
2022-03-28 18:58:23 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (702233ms till timeout)
2022-03-28 18:58:23 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1186779ms till timeout)
2022-03-28 18:58:23 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (702134ms till timeout)
2022-03-28 18:58:24 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (702038ms till timeout)
2022-03-28 18:58:24 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1184267ms till timeout)
2022-03-28 18:58:24 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1183418ms till timeout)
2022-03-28 18:58:24 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (701753ms till timeout)
2022-03-28 18:58:24 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1181746ms till timeout)
2022-03-28 18:58:24 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1185358ms till timeout)
2022-03-28 18:58:24 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (706775ms till timeout)
2022-03-28 18:58:24 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1182198ms till timeout)
2022-03-28 18:58:24 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1184331ms till timeout)
2022-03-28 18:58:24 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (701171ms till timeout)
2022-03-28 18:58:24 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (701080ms till timeout)
2022-03-28 18:58:25 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1185610ms till timeout)
2022-03-28 18:58:25 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (700964ms till timeout)
2022-03-28 18:58:25 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (700868ms till timeout)
2022-03-28 18:58:25 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1183098ms till timeout)
2022-03-28 18:58:25 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1182249ms till timeout)
2022-03-28 18:58:25 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (700584ms till timeout)
2022-03-28 18:58:25 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1180580ms till timeout)
2022-03-28 18:58:25 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1184195ms till timeout)
2022-03-28 18:58:25 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (705611ms till timeout)
2022-03-28 18:58:25 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1181034ms till timeout)
2022-03-28 18:58:26 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1183167ms till timeout)
2022-03-28 18:58:26 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (699921ms till timeout)
2022-03-28 18:58:26 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (700011ms till timeout)
2022-03-28 18:58:26 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1184464ms till timeout)
2022-03-28 18:58:26 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (699819ms till timeout)
2022-03-28 18:58:26 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (699724ms till timeout)
2022-03-28 18:58:26 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1181955ms till timeout)
2022-03-28 18:58:26 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1181106ms till timeout)
2022-03-28 18:58:26 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (699437ms till timeout)
2022-03-28 18:58:26 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1179436ms till timeout)
2022-03-28 18:58:26 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1183051ms till timeout)
2022-03-28 18:58:26 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (704468ms till timeout)
2022-03-28 18:58:27 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1179885ms till timeout)
2022-03-28 18:58:27 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1182015ms till timeout)
2022-03-28 18:58:27 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (698768ms till timeout)
2022-03-28 18:58:27 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (698857ms till timeout)
2022-03-28 18:58:27 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1183311ms till timeout)
2022-03-28 18:58:27 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (698666ms till timeout)
2022-03-28 18:58:27 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (698572ms till timeout)
2022-03-28 18:58:27 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1180802ms till timeout)
2022-03-28 18:58:27 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1179952ms till timeout)
2022-03-28 18:58:27 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (698287ms till timeout)
2022-03-28 18:58:27 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1178284ms till timeout)
2022-03-28 18:58:28 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1181898ms till timeout)
2022-03-28 18:58:28 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (703314ms till timeout)
2022-03-28 18:58:28 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1178737ms till timeout)
2022-03-28 18:58:28 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1180870ms till timeout)
2022-03-28 18:58:28 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (697623ms till timeout)
2022-03-28 18:58:28 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (697714ms till timeout)
2022-03-28 18:58:28 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1182167ms till timeout)
2022-03-28 18:58:28 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (697523ms till timeout)
2022-03-28 18:58:28 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (697425ms till timeout)
2022-03-28 18:58:28 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1179650ms till timeout)
2022-03-28 18:58:28 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1178801ms till timeout)
2022-03-28 18:58:28 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (697136ms till timeout)
2022-03-28 18:58:29 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1177134ms till timeout)
2022-03-28 18:58:29 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1180748ms till timeout)
2022-03-28 18:58:29 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (702162ms till timeout)
2022-03-28 18:58:29 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1177585ms till timeout)
2022-03-28 18:58:29 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1179715ms till timeout)
2022-03-28 18:58:29 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (696469ms till timeout)
2022-03-28 18:58:29 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (696560ms till timeout)
2022-03-28 18:58:29 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1181012ms till timeout)
2022-03-28 18:58:29 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (696368ms till timeout)
2022-03-28 18:58:29 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (696272ms till timeout)
2022-03-28 18:58:29 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1178502ms till timeout)
2022-03-28 18:58:30 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1177653ms till timeout)
2022-03-28 18:58:30 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (695988ms till timeout)
2022-03-28 18:58:30 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1175980ms till timeout)
2022-03-28 18:58:30 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1179595ms till timeout)
2022-03-28 18:58:30 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (701012ms till timeout)
2022-03-28 18:58:30 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1176435ms till timeout)
2022-03-28 18:58:30 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1178566ms till timeout)
2022-03-28 18:58:30 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (695320ms till timeout)
2022-03-28 18:58:30 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (695409ms till timeout)
2022-03-28 18:58:30 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1179858ms till timeout)
2022-03-28 18:58:30 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (695213ms till timeout)
2022-03-28 18:58:30 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (695118ms till timeout)
2022-03-28 18:58:31 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1177346ms till timeout)
2022-03-28 18:58:31 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1176496ms till timeout)
2022-03-28 18:58:31 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (694830ms till timeout)
2022-03-28 18:58:31 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1174828ms till timeout)
2022-03-28 18:58:31 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1178442ms till timeout)
2022-03-28 18:58:31 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (699857ms till timeout)
2022-03-28 18:58:31 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1175278ms till timeout)
2022-03-28 18:58:31 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1177412ms till timeout)
2022-03-28 18:58:31 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (694256ms till timeout)
2022-03-28 18:58:31 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (694164ms till timeout)
2022-03-28 18:58:31 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1178707ms till timeout)
2022-03-28 18:58:32 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (694063ms till timeout)
2022-03-28 18:58:32 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (693968ms till timeout)
2022-03-28 18:58:32 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1176196ms till timeout)
2022-03-28 18:58:32 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1175344ms till timeout)
2022-03-28 18:58:32 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (693679ms till timeout)
2022-03-28 18:58:32 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1173677ms till timeout)
2022-03-28 18:58:32 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1177291ms till timeout)
2022-03-28 18:58:32 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (698708ms till timeout)
2022-03-28 18:58:32 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1174130ms till timeout)
2022-03-28 18:58:32 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1176261ms till timeout)
2022-03-28 18:58:33 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (693015ms till timeout)
2022-03-28 18:58:33 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (693106ms till timeout)
2022-03-28 18:58:33 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1177558ms till timeout)
2022-03-28 18:58:33 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (692913ms till timeout)
2022-03-28 18:58:33 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (692819ms till timeout)
2022-03-28 18:58:33 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1175047ms till timeout)
2022-03-28 18:58:33 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1174199ms till timeout)
2022-03-28 18:58:33 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (692533ms till timeout)
2022-03-28 18:58:33 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1172528ms till timeout)
2022-03-28 18:58:33 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1176143ms till timeout)
2022-03-28 18:58:33 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (697561ms till timeout)
2022-03-28 18:58:33 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1172983ms till timeout)
2022-03-28 18:58:34 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1175115ms till timeout)
2022-03-28 18:58:34 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (691867ms till timeout)
2022-03-28 18:58:34 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (691958ms till timeout)
2022-03-28 18:58:34 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1176409ms till timeout)
2022-03-28 18:58:34 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (691765ms till timeout)
2022-03-28 18:58:34 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (691669ms till timeout)
2022-03-28 18:58:34 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1173898ms till timeout)
2022-03-28 18:58:34 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1173050ms till timeout)
2022-03-28 18:58:34 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (691384ms till timeout)
2022-03-28 18:58:34 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1171382ms till timeout)
2022-03-28 18:58:34 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1174997ms till timeout)
2022-03-28 18:58:35 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (696409ms till timeout)
2022-03-28 18:58:35 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1171831ms till timeout)
2022-03-28 18:58:35 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1173964ms till timeout)
2022-03-28 18:58:35 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (690718ms till timeout)
2022-03-28 18:58:35 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (690809ms till timeout)
2022-03-28 18:58:35 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1175260ms till timeout)
2022-03-28 18:58:35 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (690616ms till timeout)
2022-03-28 18:58:35 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (690521ms till timeout)
2022-03-28 18:58:35 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1172750ms till timeout)
2022-03-28 18:58:35 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1171901ms till timeout)
2022-03-28 18:58:35 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (690234ms till timeout)
2022-03-28 18:58:35 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1170232ms till timeout)
2022-03-28 18:58:36 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1173847ms till timeout)
2022-03-28 18:58:36 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (695264ms till timeout)
2022-03-28 18:58:36 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1170686ms till timeout)
2022-03-28 18:58:36 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1172817ms till timeout)
2022-03-28 18:58:36 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (689569ms till timeout)
2022-03-28 18:58:36 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (689660ms till timeout)
2022-03-28 18:58:36 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1174114ms till timeout)
2022-03-28 18:58:36 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (689470ms till timeout)
2022-03-28 18:58:36 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (689375ms till timeout)
2022-03-28 18:58:36 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1171604ms till timeout)
2022-03-28 18:58:36 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1170755ms till timeout)
2022-03-28 18:58:37 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (689090ms till timeout)
2022-03-28 18:58:37 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1169087ms till timeout)
2022-03-28 18:58:37 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1172701ms till timeout)
2022-03-28 18:58:37 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (694117ms till timeout)
2022-03-28 18:58:37 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1169540ms till timeout)
2022-03-28 18:58:37 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1171673ms till timeout)
2022-03-28 18:58:37 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (688426ms till timeout)
2022-03-28 18:58:37 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (688517ms till timeout)
2022-03-28 18:58:37 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1172968ms till timeout)
2022-03-28 18:58:37 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (688324ms till timeout)
2022-03-28 18:58:37 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (688228ms till timeout)
2022-03-28 18:58:37 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1170456ms till timeout)
2022-03-28 18:58:38 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1169607ms till timeout)
2022-03-28 18:58:38 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (687942ms till timeout)
2022-03-28 18:58:38 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1167940ms till timeout)
2022-03-28 18:58:38 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1171554ms till timeout)
2022-03-28 18:58:38 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (692972ms till timeout)
2022-03-28 18:58:38 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1168394ms till timeout)
2022-03-28 18:58:38 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1170527ms till timeout)
2022-03-28 18:58:38 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (687281ms till timeout)
2022-03-28 18:58:38 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (687372ms till timeout)
2022-03-28 18:58:38 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1171820ms till timeout)
2022-03-28 18:58:38 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (687180ms till timeout)
2022-03-28 18:58:39 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (687084ms till timeout)
2022-03-28 18:58:39 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1169314ms till timeout)
2022-03-28 18:58:39 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1168465ms till timeout)
2022-03-28 18:58:39 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (686799ms till timeout)
2022-03-28 18:58:39 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1166796ms till timeout)
2022-03-28 18:58:39 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1170411ms till timeout)
2022-03-28 18:58:39 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (691827ms till timeout)
2022-03-28 18:58:39 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1167250ms till timeout)
2022-03-28 18:58:39 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1169383ms till timeout)
2022-03-28 18:58:39 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (686137ms till timeout)
2022-03-28 18:58:39 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (686227ms till timeout)
2022-03-28 18:58:39 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1170680ms till timeout)
2022-03-28 18:58:40 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (686035ms till timeout)
2022-03-28 18:58:40 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (685940ms till timeout)
2022-03-28 18:58:40 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1168170ms till timeout)
2022-03-28 18:58:40 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1167321ms till timeout)
2022-03-28 18:58:40 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (685656ms till timeout)
2022-03-28 18:58:40 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1165654ms till timeout)
2022-03-28 18:58:40 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1169266ms till timeout)
2022-03-28 18:58:40 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (690682ms till timeout)
2022-03-28 18:58:40 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1166105ms till timeout)
2022-03-28 18:58:40 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1168238ms till timeout)
2022-03-28 18:58:41 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (685084ms till timeout)
2022-03-28 18:58:41 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (684993ms till timeout)
2022-03-28 18:58:41 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1169535ms till timeout)
2022-03-28 18:58:41 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (684891ms till timeout)
2022-03-28 18:58:41 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (684796ms till timeout)
2022-03-28 18:58:41 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1167026ms till timeout)
2022-03-28 18:58:41 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1166176ms till timeout)
2022-03-28 18:58:41 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (684511ms till timeout)
2022-03-28 18:58:41 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1164508ms till timeout)
2022-03-28 18:58:41 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1168119ms till timeout)
2022-03-28 18:58:41 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (689535ms till timeout)
2022-03-28 18:58:41 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1164958ms till timeout)
2022-03-28 18:58:42 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1167086ms till timeout)
2022-03-28 18:58:42 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (683840ms till timeout)
2022-03-28 18:58:42 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (683931ms till timeout)
2022-03-28 18:58:42 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1168383ms till timeout)
2022-03-28 18:58:42 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (683739ms till timeout)
2022-03-28 18:58:42 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (683644ms till timeout)
2022-03-28 18:58:42 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1165874ms till timeout)
2022-03-28 18:58:42 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1165025ms till timeout)
2022-03-28 18:58:42 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (683359ms till timeout)
2022-03-28 18:58:42 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1163356ms till timeout)
2022-03-28 18:58:42 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1166971ms till timeout)
2022-03-28 18:58:43 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (688387ms till timeout)
2022-03-28 18:58:43 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1163787ms till timeout)
2022-03-28 18:58:43 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1165907ms till timeout)
2022-03-28 18:58:43 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (682659ms till timeout)
2022-03-28 18:58:43 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (682749ms till timeout)
2022-03-28 18:58:43 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1167201ms till timeout)
2022-03-28 18:58:43 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (682557ms till timeout)
2022-03-28 18:58:43 [ForkJoinPool-1-worker-31] INFO  [ResourceManager:444] Kafka: user-cluster-name is in desired state: Ready
2022-03-28 18:58:43 [ForkJoinPool-1-worker-1] INFO  [TestSeparator:23] ############################################################################
2022-03-28 18:58:43 [ForkJoinPool-1-worker-31] INFO  [TestSeparator:23] ############################################################################
2022-03-28 18:58:43 [ForkJoinPool-1-worker-1] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.user.UserST.testTlsExternalUserWithQuotas-STARTED
2022-03-28 18:58:43 [ForkJoinPool-1-worker-31] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.user.UserST.testCreatingUsersWithSecretPrefix-STARTED
2022-03-28 18:58:43 [ForkJoinPool-1-worker-31] DEBUG [AbstractST:658] ============================================================================
2022-03-28 18:58:43 [ForkJoinPool-1-worker-1] DEBUG [AbstractST:658] ============================================================================
2022-03-28 18:58:43 [ForkJoinPool-1-worker-31] DEBUG [AbstractST:659] [operators.user.UserST - Before Each] - Setup test case environment
2022-03-28 18:58:43 [ForkJoinPool-1-worker-1] DEBUG [AbstractST:659] [operators.user.UserST - Before Each] - Setup test case environment
2022-03-28 18:58:43 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:77] [operators.user.UserST] - Adding parallel test: testTlsExternalUserWithQuotas
2022-03-28 18:58:43 [ForkJoinPool-1-worker-31] DEBUG [SuiteThreadController:77] [operators.user.UserST] - Adding parallel test: testCreatingUsersWithSecretPrefix
2022-03-28 18:58:43 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:81] [operators.user.UserST] - Parallel test count: 9
2022-03-28 18:58:43 [ForkJoinPool-1-worker-31] DEBUG [SuiteThreadController:81] [operators.user.UserST] - Parallel test count: 10
2022-03-28 18:58:43 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:230] testTlsExternalUserWithQuotas test now can proceed its execution
2022-03-28 18:58:43 [ForkJoinPool-1-worker-31] DEBUG [SuiteThreadController:230] testCreatingUsersWithSecretPrefix test now can proceed its execution
2022-03-28 18:58:43 [ForkJoinPool-1-worker-1] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 18:58:43 [ForkJoinPool-1-worker-1] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-959c6ed5, testTlsExternalUserWithQuotas=my-cluster-100f5461, testDeployAndUnDeployCruiseControl=my-cluster-5dfd606f, testConfigurationFileIsCreated=my-cluster-17fc585b, testCapacityFile=my-cluster-855b9105, testConfigurationPerformanceOptions=my-cluster-08515847, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-afe273c9, testConfigurationReflection=my-cluster-188382d8, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-e6343cec}
2022-03-28 18:58:43 [ForkJoinPool-1-worker-1] TRACE [AbstractST:607] USERS_NAME_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-user-2104248162-45187051, testTlsExternalUserWithQuotas=my-user-1909583883-1106194424, testDeployAndUnDeployCruiseControl=my-user-506268651-1309117986, testConfigurationFileIsCreated=my-user-355815510-1316268151, testCapacityFile=my-user-1460643829-103298436, testConfigurationPerformanceOptions=my-user-98362545-1476587236, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-user-8456079-650483351, testConfigurationReflection=my-user-1890969183-1754686912, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-2069267102-581538560}
2022-03-28 18:58:43 [ForkJoinPool-1-worker-1] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-634125597-1746493046, testTlsExternalUserWithQuotas=my-topic-1986059791-167638004, testDeployAndUnDeployCruiseControl=my-topic-465377127-800134460, testConfigurationFileIsCreated=my-topic-423850169-855699454, testCapacityFile=my-topic-549312506-649708453, testConfigurationPerformanceOptions=my-topic-696588651-1198599652, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-topic-1687496771-142091227, testConfigurationReflection=my-topic-1394577332-1936226438, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-293775067-432059493}
2022-03-28 18:58:43 [ForkJoinPool-1-worker-1] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-959c6ed5-kafka-clients, testTlsExternalUserWithQuotas=my-cluster-100f5461-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-5dfd606f-kafka-clients, testConfigurationFileIsCreated=my-cluster-17fc585b-kafka-clients, testCapacityFile=my-cluster-855b9105-kafka-clients, testConfigurationPerformanceOptions=my-cluster-08515847-kafka-clients, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-afe273c9-kafka-clients, testConfigurationReflection=my-cluster-188382d8-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-e6343cec-kafka-clients}
2022-03-28 18:58:43 [ForkJoinPool-1-worker-31] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 18:58:43 [ForkJoinPool-1-worker-31] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-959c6ed5, testTlsExternalUserWithQuotas=my-cluster-100f5461, testDeployAndUnDeployCruiseControl=my-cluster-5dfd606f, testConfigurationFileIsCreated=my-cluster-17fc585b, testCapacityFile=my-cluster-855b9105, testCreatingUsersWithSecretPrefix=my-cluster-f743fdfd, testConfigurationPerformanceOptions=my-cluster-08515847, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-afe273c9, testConfigurationReflection=my-cluster-188382d8, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-e6343cec}
2022-03-28 18:58:43 [ForkJoinPool-1-worker-31] TRACE [AbstractST:607] USERS_NAME_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-user-2104248162-45187051, testTlsExternalUserWithQuotas=my-user-1909583883-1106194424, testDeployAndUnDeployCruiseControl=my-user-506268651-1309117986, testConfigurationFileIsCreated=my-user-355815510-1316268151, testCapacityFile=my-user-1460643829-103298436, testCreatingUsersWithSecretPrefix=my-user-607975993-1210920737, testConfigurationPerformanceOptions=my-user-98362545-1476587236, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-user-8456079-650483351, testConfigurationReflection=my-user-1890969183-1754686912, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-2069267102-581538560}
2022-03-28 18:58:43 [ForkJoinPool-1-worker-31] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-634125597-1746493046, testTlsExternalUserWithQuotas=my-topic-1986059791-167638004, testDeployAndUnDeployCruiseControl=my-topic-465377127-800134460, testConfigurationFileIsCreated=my-topic-423850169-855699454, testCapacityFile=my-topic-549312506-649708453, testCreatingUsersWithSecretPrefix=my-topic-1727421070-668192795, testConfigurationPerformanceOptions=my-topic-696588651-1198599652, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-topic-1687496771-142091227, testConfigurationReflection=my-topic-1394577332-1936226438, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-293775067-432059493}
2022-03-28 18:58:43 [ForkJoinPool-1-worker-31] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-959c6ed5-kafka-clients, testTlsExternalUserWithQuotas=my-cluster-100f5461-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-5dfd606f-kafka-clients, testConfigurationFileIsCreated=my-cluster-17fc585b-kafka-clients, testCapacityFile=my-cluster-855b9105-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-f743fdfd-kafka-clients, testConfigurationPerformanceOptions=my-cluster-08515847-kafka-clients, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-afe273c9-kafka-clients, testConfigurationReflection=my-cluster-188382d8-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-e6343cec-kafka-clients}
2022-03-28 18:58:43 [ForkJoinPool-1-worker-31] INFO  [TestSuiteNamespaceManager:163] Creating namespace:namespace-8 for test case:testCreatingUsersWithSecretPrefix
2022-03-28 18:58:43 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:155] Create/Update KafkaUser my-user-1909583883-1106194424 in namespace user-st
2022-03-28 18:58:43 [ForkJoinPool-1-worker-1] WARN  [VersionUsageUtils:60] The client is using resource type 'kafkausers' with unstable version 'v1beta2'
2022-03-28 18:58:43 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1164690ms till timeout)
2022-03-28 18:58:43 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1163840ms till timeout)
2022-03-28 18:58:43 [ForkJoinPool-1-worker-31] INFO  [KubeClusterResource:156] Creating Namespace: namespace-8
2022-03-28 18:58:43 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-1909583883-1106194424
2022-03-28 18:58:43 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (682178ms till timeout)
2022-03-28 18:58:44 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:433] Wait for KafkaUser: my-user-1909583883-1106194424 will have desired state: Ready
2022-03-28 18:58:44 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for KafkaUser: my-user-1909583883-1106194424 will have desired state: Ready
2022-03-28 18:58:44 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1162166ms till timeout)
2022-03-28 18:58:44 [ForkJoinPool-1-worker-31] DEBUG [TestUtils:125] Waiting for Namespace namespace-8
2022-03-28 18:58:44 [ForkJoinPool-1-worker-31] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-8 -o json
2022-03-28 18:58:44 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1165778ms till timeout)
2022-03-28 18:58:44 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] KafkaUser: my-user-1909583883-1106194424 will have desired state: Ready not ready, will try again in 1000 ms (179804ms till timeout)
2022-03-28 18:58:44 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (687184ms till timeout)
2022-03-28 18:58:44 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1162613ms till timeout)
2022-03-28 18:58:44 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1164741ms till timeout)
2022-03-28 18:58:44 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (681580ms till timeout)
2022-03-28 18:58:44 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (681489ms till timeout)
2022-03-28 18:58:44 [ForkJoinPool-1-worker-31] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-8 -o json
2022-03-28 18:58:44 [ForkJoinPool-1-worker-31] DEBUG [Exec:419] Return code: 0
2022-03-28 18:58:44 [ForkJoinPool-1-worker-31] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c42,c29",
            "openshift.io/sa.scc.supplemental-groups": "1001780000/10000",
            "openshift.io/sa.scc.uid-range": "1001780000/10000"
        },
        "creationTimestamp": "2022-03-28T18:58:39Z",
        "labels": {
            "kubernetes.io/metadata.name": "namespace-8"
        },
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T18:58:39Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:labels": {
                            ".": {},
                            "f:kubernetes.io/metadata.name": {}
                        }
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T18:58:39Z"
            }
        ],
        "name": "namespace-8",
        "resourceVersion": "263421",
        "uid": "8d3b849e-17ad-4ea2-9a8d-b16fbacf6b20"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 18:58:44 [ForkJoinPool-1-worker-31] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[http-bridge-scram-sha-st], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[namespace-8], io.strimzi.test.logs.CollectorElement@c4c0ea0=[namespace-1], io.strimzi.test.logs.CollectorElement@aec142ef=[namespace-5], io.strimzi.test.logs.CollectorElement@24b97ba9=[namespace-7], io.strimzi.test.logs.CollectorElement@f851b6c3=[namespace-4], io.strimzi.test.logs.CollectorElement@3881d5f2=[namespace-3], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[namespace-0], io.strimzi.test.logs.CollectorElement@f4405b3b=[namespace-2], io.strimzi.test.logs.CollectorElement@3e255cd9=[namespace-6]}
2022-03-28 18:58:44 [ForkJoinPool-1-worker-31] INFO  [KubeClusterResource:82] Client use Namespace: namespace-8
2022-03-28 18:58:44 [ForkJoinPool-1-worker-31] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-8, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 18:58:44 [ForkJoinPool-1-worker-31] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-8
2022-03-28 18:58:44 [ForkJoinPool-1-worker-31] INFO  [ResourceManager:155] Create/Update Kafka my-cluster-f743fdfd in namespace namespace-8
2022-03-28 18:58:44 [ForkJoinPool-1-worker-31] INFO  [ResourceManager:164] Using Namespace: namespace-8
2022-03-28 18:58:44 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1166038ms till timeout)
2022-03-28 18:58:44 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (681436ms till timeout)
2022-03-28 18:58:44 [ForkJoinPool-1-worker-31] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-f743fdfd
2022-03-28 18:58:44 [ForkJoinPool-1-worker-31] INFO  [ResourceManager:433] Wait for Kafka: my-cluster-f743fdfd will have desired state: Ready
2022-03-28 18:58:44 [ForkJoinPool-1-worker-31] DEBUG [TestUtils:125] Waiting for Kafka: my-cluster-f743fdfd will have desired state: Ready
2022-03-28 18:58:44 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1163520ms till timeout)
2022-03-28 18:58:45 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1162671ms till timeout)
2022-03-28 18:58:45 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (839809ms till timeout)
2022-03-28 18:58:45 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (681006ms till timeout)
2022-03-28 18:58:45 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1161004ms till timeout)
2022-03-28 18:58:45 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1164618ms till timeout)
2022-03-28 18:58:45 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] KafkaUser: my-user-1909583883-1106194424 will have desired state: Ready not ready, will try again in 1000 ms (178648ms till timeout)
2022-03-28 18:58:45 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (686036ms till timeout)
2022-03-28 18:58:45 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1161459ms till timeout)
2022-03-28 18:58:45 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1163591ms till timeout)
2022-03-28 18:58:45 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (680436ms till timeout)
2022-03-28 18:58:45 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (680345ms till timeout)
2022-03-28 18:58:45 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1164888ms till timeout)
2022-03-28 18:58:45 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (680242ms till timeout)
2022-03-28 18:58:46 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1162423ms till timeout)
2022-03-28 18:58:46 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1161574ms till timeout)
2022-03-28 18:58:46 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (679909ms till timeout)
2022-03-28 18:58:46 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (838619ms till timeout)
2022-03-28 18:58:46 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1159899ms till timeout)
2022-03-28 18:58:46 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1163514ms till timeout)
2022-03-28 18:58:46 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (684932ms till timeout)
2022-03-28 18:58:46 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:444] KafkaUser: my-user-1909583883-1106194424 is in desired state: Ready
2022-03-28 18:58:46 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1160353ms till timeout)
2022-03-28 18:58:46 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1162485ms till timeout)
2022-03-28 18:58:46 [ForkJoinPool-1-worker-1] DEBUG [UserST:274] Command for kafka-configs.sh bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user CN=my-user-1909583883-1106194424
2022-03-28 18:58:46 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace user-st exec user-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user CN=my-user-1909583883-1106194424
2022-03-28 18:58:46 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (679239ms till timeout)
2022-03-28 18:58:46 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (679323ms till timeout)
2022-03-28 18:58:46 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1163777ms till timeout)
2022-03-28 18:58:46 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (679131ms till timeout)
2022-03-28 18:58:47 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1161325ms till timeout)
2022-03-28 18:58:47 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1160476ms till timeout)
2022-03-28 18:58:47 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (837520ms till timeout)
2022-03-28 18:58:47 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (678809ms till timeout)
2022-03-28 18:58:47 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1158802ms till timeout)
2022-03-28 18:58:47 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1162417ms till timeout)
2022-03-28 18:58:47 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (683825ms till timeout)
2022-03-28 18:58:47 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1159246ms till timeout)
2022-03-28 18:58:47 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1161379ms till timeout)
2022-03-28 18:58:47 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (678224ms till timeout)
2022-03-28 18:58:47 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (678133ms till timeout)
2022-03-28 18:58:47 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1162675ms till timeout)
2022-03-28 18:58:48 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (678027ms till timeout)
2022-03-28 18:58:48 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1160228ms till timeout)
2022-03-28 18:58:48 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1159378ms till timeout)
2022-03-28 18:58:48 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (836422ms till timeout)
2022-03-28 18:58:48 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (677616ms till timeout)
2022-03-28 18:58:48 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1157615ms till timeout)
2022-03-28 18:58:48 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1161229ms till timeout)
2022-03-28 18:58:48 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (682646ms till timeout)
2022-03-28 18:58:48 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1158069ms till timeout)
2022-03-28 18:58:48 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1160202ms till timeout)
2022-03-28 18:58:49 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (677047ms till timeout)
2022-03-28 18:58:49 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (676955ms till timeout)
2022-03-28 18:58:49 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1161499ms till timeout)
2022-03-28 18:58:49 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (676855ms till timeout)
2022-03-28 18:58:49 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1159082ms till timeout)
2022-03-28 18:58:49 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1158235ms till timeout)
2022-03-28 18:58:49 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (835280ms till timeout)
2022-03-28 18:58:49 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (676475ms till timeout)
2022-03-28 18:58:49 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1156473ms till timeout)
2022-03-28 18:58:49 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1160087ms till timeout)
2022-03-28 18:58:49 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (681504ms till timeout)
2022-03-28 18:58:50 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1156927ms till timeout)
2022-03-28 18:58:50 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1159060ms till timeout)
2022-03-28 18:58:50 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (675810ms till timeout)
2022-03-28 18:58:50 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:444] Kafka: http-bridge-tls-cluster-name is in desired state: Ready
2022-03-28 18:58:50 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:155] Create/Update KafkaUser my-user-1679479847-1226530320 in namespace http-bridge-tls-st
2022-03-28 18:58:50 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1160350ms till timeout)
2022-03-28 18:58:50 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (675705ms till timeout)
2022-03-28 18:58:50 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-1679479847-1226530320
2022-03-28 18:58:50 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1157936ms till timeout)
2022-03-28 18:58:50 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1157088ms till timeout)
2022-03-28 18:58:50 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:433] Wait for KafkaUser: my-user-1679479847-1226530320 will have desired state: Ready
2022-03-28 18:58:50 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for KafkaUser: my-user-1679479847-1226530320 will have desired state: Ready
2022-03-28 18:58:50 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (834128ms till timeout)
2022-03-28 18:58:50 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (675323ms till timeout)
2022-03-28 18:58:50 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] KafkaUser: my-user-1679479847-1226530320 will have desired state: Ready not ready, will try again in 1000 ms (179805ms till timeout)
2022-03-28 18:58:50 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1155319ms till timeout)
2022-03-28 18:58:50 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1158933ms till timeout)
2022-03-28 18:58:51 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (680349ms till timeout)
2022-03-28 18:58:51 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1155772ms till timeout)
2022-03-28 18:58:51 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1157905ms till timeout)
2022-03-28 18:58:51 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (674658ms till timeout)
2022-03-28 18:58:51 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1159201ms till timeout)
2022-03-28 18:58:51 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (674557ms till timeout)
2022-03-28 18:58:51 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1156786ms till timeout)
2022-03-28 18:58:51 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1155937ms till timeout)
2022-03-28 18:58:51 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (832982ms till timeout)
2022-03-28 18:58:51 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (674172ms till timeout)
2022-03-28 18:58:51 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] KafkaUser: my-user-1679479847-1226530320 will have desired state: Ready not ready, will try again in 1000 ms (178654ms till timeout)
2022-03-28 18:58:52 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1154168ms till timeout)
2022-03-28 18:58:52 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1157782ms till timeout)
2022-03-28 18:58:52 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (679198ms till timeout)
2022-03-28 18:58:52 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1154621ms till timeout)
2022-03-28 18:58:52 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1156753ms till timeout)
2022-03-28 18:58:52 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (673506ms till timeout)
2022-03-28 18:58:52 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1158048ms till timeout)
2022-03-28 18:58:52 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (673404ms till timeout)
2022-03-28 18:58:52 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1155633ms till timeout)
2022-03-28 18:58:52 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1154785ms till timeout)
2022-03-28 18:58:52 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (831830ms till timeout)
2022-03-28 18:58:53 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (673025ms till timeout)
2022-03-28 18:58:53 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] KafkaUser: my-user-1679479847-1226530320 will have desired state: Ready not ready, will try again in 1000 ms (177508ms till timeout)
2022-03-28 18:58:53 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1153023ms till timeout)
2022-03-28 18:58:53 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1156639ms till timeout)
2022-03-28 18:58:53 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (678055ms till timeout)
2022-03-28 18:58:53 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1153478ms till timeout)
2022-03-28 18:58:53 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1155610ms till timeout)
2022-03-28 18:58:53 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (672364ms till timeout)
2022-03-28 18:58:53 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1156900ms till timeout)
2022-03-28 18:58:53 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (672241ms till timeout)
2022-03-28 18:58:53 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1154465ms till timeout)
2022-03-28 18:58:54 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1153611ms till timeout)
2022-03-28 18:58:54 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (830655ms till timeout)
2022-03-28 18:58:54 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (671841ms till timeout)
2022-03-28 18:58:54 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:444] KafkaUser: my-user-1679479847-1226530320 is in desired state: Ready
2022-03-28 18:58:54 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1151839ms till timeout)
2022-03-28 18:58:54 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1155453ms till timeout)
2022-03-28 18:58:54 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (676865ms till timeout)
2022-03-28 18:58:54 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1152295ms till timeout)
2022-03-28 18:58:54 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1154428ms till timeout)
2022-03-28 18:58:54 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:155] Create/Update Deployment http-bridge-tls-st-kafka-clients in namespace http-bridge-tls-st
2022-03-28 18:58:54 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:444] Kafka: quota-cluster is in desired state: Ready
2022-03-28 18:58:54 [ForkJoinPool-1-worker-9] INFO  [TestSeparator:23] ############################################################################
2022-03-28 18:58:54 [ForkJoinPool-1-worker-9] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.topic.ThrottlingQuotaST.testThrottlingQuotasCreateTopic-STARTED
2022-03-28 18:58:54 [ForkJoinPool-1-worker-9] DEBUG [AbstractST:658] ============================================================================
2022-03-28 18:58:54 [ForkJoinPool-1-worker-9] DEBUG [AbstractST:659] [operators.topic.ThrottlingQuotaST - Before Each] - Setup test case environment
2022-03-28 18:58:54 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:77] [operators.topic.ThrottlingQuotaST] - Adding parallel test: testThrottlingQuotasCreateTopic
2022-03-28 18:58:54 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:81] [operators.topic.ThrottlingQuotaST] - Parallel test count: 11
2022-03-28 18:58:54 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:230] testThrottlingQuotasCreateTopic test now can proceed its execution
2022-03-28 18:58:54 [ForkJoinPool-1-worker-9] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 18:58:54 [ForkJoinPool-1-worker-9] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-959c6ed5, testTlsExternalUserWithQuotas=my-cluster-100f5461, testDeployAndUnDeployCruiseControl=my-cluster-5dfd606f, testConfigurationFileIsCreated=my-cluster-17fc585b, testThrottlingQuotasCreateTopic=my-cluster-e9955717, testCapacityFile=my-cluster-855b9105, testCreatingUsersWithSecretPrefix=my-cluster-f743fdfd, testConfigurationPerformanceOptions=my-cluster-08515847, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-afe273c9, testConfigurationReflection=my-cluster-188382d8, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-e6343cec}
2022-03-28 18:58:54 [ForkJoinPool-1-worker-9] TRACE [AbstractST:607] USERS_NAME_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-user-2104248162-45187051, testTlsExternalUserWithQuotas=my-user-1909583883-1106194424, testDeployAndUnDeployCruiseControl=my-user-506268651-1309117986, testConfigurationFileIsCreated=my-user-355815510-1316268151, testThrottlingQuotasCreateTopic=my-user-1876060546-1887905718, testCapacityFile=my-user-1460643829-103298436, testCreatingUsersWithSecretPrefix=my-user-607975993-1210920737, testConfigurationPerformanceOptions=my-user-98362545-1476587236, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-user-8456079-650483351, testConfigurationReflection=my-user-1890969183-1754686912, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-2069267102-581538560}
2022-03-28 18:58:54 [ForkJoinPool-1-worker-9] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-634125597-1746493046, testTlsExternalUserWithQuotas=my-topic-1986059791-167638004, testDeployAndUnDeployCruiseControl=my-topic-465377127-800134460, testConfigurationFileIsCreated=my-topic-423850169-855699454, testThrottlingQuotasCreateTopic=my-topic-1246597695-228473879, testCapacityFile=my-topic-549312506-649708453, testCreatingUsersWithSecretPrefix=my-topic-1727421070-668192795, testConfigurationPerformanceOptions=my-topic-696588651-1198599652, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-topic-1687496771-142091227, testConfigurationReflection=my-topic-1394577332-1936226438, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-293775067-432059493}
2022-03-28 18:58:54 [ForkJoinPool-1-worker-9] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-959c6ed5-kafka-clients, testTlsExternalUserWithQuotas=my-cluster-100f5461-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-5dfd606f-kafka-clients, testConfigurationFileIsCreated=my-cluster-17fc585b-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-e9955717-kafka-clients, testCapacityFile=my-cluster-855b9105-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-f743fdfd-kafka-clients, testConfigurationPerformanceOptions=my-cluster-08515847-kafka-clients, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-afe273c9-kafka-clients, testConfigurationReflection=my-cluster-188382d8-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-e6343cec-kafka-clients}
2022-03-28 18:58:54 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:155] Create/Update KafkaUser my-user-1876060546-1887905718 in namespace throttling-quota-st
2022-03-28 18:58:54 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1155721ms till timeout)
2022-03-28 18:58:54 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:http-bridge-tls-st-kafka-clients
2022-03-28 18:58:55 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (671080ms till timeout)
2022-03-28 18:58:55 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-1876060546-1887905718
2022-03-28 18:58:55 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1153313ms till timeout)
2022-03-28 18:58:55 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1152464ms till timeout)
2022-03-28 18:58:55 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:433] Wait for KafkaUser: my-user-1876060546-1887905718 will have desired state: Ready
2022-03-28 18:58:55 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for KafkaUser: my-user-1876060546-1887905718 will have desired state: Ready
2022-03-28 18:58:55 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (829509ms till timeout)
2022-03-28 18:58:55 [ForkJoinPool-1-worker-23] INFO  [DeploymentUtils:161] Wait for Deployment: http-bridge-tls-st-kafka-clients will be ready
2022-03-28 18:58:55 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for Wait for Deployment: http-bridge-tls-st-kafka-clients will be ready
2022-03-28 18:58:55 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (670705ms till timeout)
2022-03-28 18:58:55 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] KafkaUser: my-user-1876060546-1887905718 will have desired state: Ready not ready, will try again in 1000 ms (179812ms till timeout)
2022-03-28 18:58:55 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1150703ms till timeout)
2022-03-28 18:58:55 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Wait for Deployment: http-bridge-tls-st-kafka-clients will be ready not ready, will try again in 1000 ms (479812ms till timeout)
2022-03-28 18:58:55 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1154314ms till timeout)
2022-03-28 18:58:55 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (675729ms till timeout)
2022-03-28 18:58:55 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1151152ms till timeout)
2022-03-28 18:58:55 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1153285ms till timeout)
2022-03-28 18:58:56 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1154624ms till timeout)
2022-03-28 18:58:56 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (669976ms till timeout)
2022-03-28 18:58:56 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1152206ms till timeout)
2022-03-28 18:58:56 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1151356ms till timeout)
2022-03-28 18:58:56 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (828401ms till timeout)
2022-03-28 18:58:56 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (669595ms till timeout)
2022-03-28 18:58:56 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] KafkaUser: my-user-1876060546-1887905718 will have desired state: Ready not ready, will try again in 1000 ms (178701ms till timeout)
2022-03-28 18:58:56 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1149594ms till timeout)
2022-03-28 18:58:56 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Wait for Deployment: http-bridge-tls-st-kafka-clients will be ready not ready, will try again in 1000 ms (478703ms till timeout)
2022-03-28 18:58:56 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1153210ms till timeout)
2022-03-28 18:58:56 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (674626ms till timeout)
2022-03-28 18:58:56 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1150049ms till timeout)
2022-03-28 18:58:56 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1152183ms till timeout)
2022-03-28 18:58:57 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1153527ms till timeout)
2022-03-28 18:58:57 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (668878ms till timeout)
2022-03-28 18:58:57 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1151107ms till timeout)
2022-03-28 18:58:57 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1150259ms till timeout)
2022-03-28 18:58:57 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (827303ms till timeout)
2022-03-28 18:58:57 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (668497ms till timeout)
2022-03-28 18:58:57 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1148495ms till timeout)
2022-03-28 18:58:57 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] KafkaUser: my-user-1876060546-1887905718 will have desired state: Ready not ready, will try again in 1000 ms (177509ms till timeout)
2022-03-28 18:58:57 [ForkJoinPool-1-worker-23] INFO  [DeploymentUtils:168] Deployment: http-bridge-tls-st-kafka-clients is ready
2022-03-28 18:58:57 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1152110ms till timeout)
2022-03-28 18:58:57 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (673526ms till timeout)
2022-03-28 18:58:57 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1148950ms till timeout)
2022-03-28 18:58:58 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:155] Create/Update KafkaBridge http-bridge-tls-cluster-name in namespace http-bridge-tls-st
2022-03-28 18:58:58 [ForkJoinPool-1-worker-23] WARN  [VersionUsageUtils:60] The client is using resource type 'kafkabridges' with unstable version 'v1beta2'
2022-03-28 18:58:58 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1151086ms till timeout)
2022-03-28 18:58:58 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaBridge:http-bridge-tls-cluster-name
2022-03-28 18:58:58 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1152346ms till timeout)
2022-03-28 18:58:58 [ForkJoinPool-1-worker-27] INFO  [ResourceManager:444] Kafka: topic-cluster-name is in desired state: Ready
2022-03-28 18:58:58 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:433] Wait for KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready
2022-03-28 18:58:58 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready
2022-03-28 18:58:58 [ForkJoinPool-1-worker-33] INFO  [TestSeparator:23] ############################################################################
2022-03-28 18:58:58 [ForkJoinPool-1-worker-33] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.topic.ThrottlingQuotaST.testThrottlingQuotasCreateAlterPartitions-STARTED
2022-03-28 18:58:58 [ForkJoinPool-1-worker-33] DEBUG [AbstractST:658] ============================================================================
2022-03-28 18:58:58 [ForkJoinPool-1-worker-33] DEBUG [AbstractST:659] [operators.topic.ThrottlingQuotaST - Before Each] - Setup test case environment
2022-03-28 18:58:58 [ForkJoinPool-1-worker-33] DEBUG [SuiteThreadController:77] [operators.topic.ThrottlingQuotaST] - Adding parallel test: testThrottlingQuotasCreateAlterPartitions
2022-03-28 18:58:58 [ForkJoinPool-1-worker-33] DEBUG [SuiteThreadController:81] [operators.topic.ThrottlingQuotaST] - Parallel test count: 12
2022-03-28 18:58:58 [ForkJoinPool-1-worker-33] DEBUG [SuiteThreadController:230] testThrottlingQuotasCreateAlterPartitions test now can proceed its execution
2022-03-28 18:58:58 [ForkJoinPool-1-worker-33] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 18:58:58 [ForkJoinPool-1-worker-33] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-959c6ed5, testTlsExternalUserWithQuotas=my-cluster-100f5461, testDeployAndUnDeployCruiseControl=my-cluster-5dfd606f, testConfigurationFileIsCreated=my-cluster-17fc585b, testThrottlingQuotasCreateTopic=my-cluster-e9955717, testThrottlingQuotasCreateAlterPartitions=my-cluster-ec1b34f8, testCapacityFile=my-cluster-855b9105, testCreatingUsersWithSecretPrefix=my-cluster-f743fdfd, testConfigurationPerformanceOptions=my-cluster-08515847, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-afe273c9, testConfigurationReflection=my-cluster-188382d8, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-e6343cec}
2022-03-28 18:58:58 [ForkJoinPool-1-worker-33] TRACE [AbstractST:607] USERS_NAME_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-user-2104248162-45187051, testTlsExternalUserWithQuotas=my-user-1909583883-1106194424, testDeployAndUnDeployCruiseControl=my-user-506268651-1309117986, testConfigurationFileIsCreated=my-user-355815510-1316268151, testThrottlingQuotasCreateTopic=my-user-1876060546-1887905718, testThrottlingQuotasCreateAlterPartitions=my-user-333986137-1152182125, testCapacityFile=my-user-1460643829-103298436, testCreatingUsersWithSecretPrefix=my-user-607975993-1210920737, testConfigurationPerformanceOptions=my-user-98362545-1476587236, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-user-8456079-650483351, testConfigurationReflection=my-user-1890969183-1754686912, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-2069267102-581538560}
2022-03-28 18:58:58 [ForkJoinPool-1-worker-33] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-634125597-1746493046, testTlsExternalUserWithQuotas=my-topic-1986059791-167638004, testDeployAndUnDeployCruiseControl=my-topic-465377127-800134460, testConfigurationFileIsCreated=my-topic-423850169-855699454, testThrottlingQuotasCreateTopic=my-topic-1246597695-228473879, testThrottlingQuotasCreateAlterPartitions=my-topic-325891726-169328137, testCapacityFile=my-topic-549312506-649708453, testCreatingUsersWithSecretPrefix=my-topic-1727421070-668192795, testConfigurationPerformanceOptions=my-topic-696588651-1198599652, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-topic-1687496771-142091227, testConfigurationReflection=my-topic-1394577332-1936226438, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-293775067-432059493}
2022-03-28 18:58:58 [ForkJoinPool-1-worker-33] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-959c6ed5-kafka-clients, testTlsExternalUserWithQuotas=my-cluster-100f5461-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-5dfd606f-kafka-clients, testConfigurationFileIsCreated=my-cluster-17fc585b-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-e9955717-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-ec1b34f8-kafka-clients, testCapacityFile=my-cluster-855b9105-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-f743fdfd-kafka-clients, testConfigurationPerformanceOptions=my-cluster-08515847-kafka-clients, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-afe273c9-kafka-clients, testConfigurationReflection=my-cluster-188382d8-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-e6343cec-kafka-clients}
2022-03-28 18:58:58 [ForkJoinPool-1-worker-33] INFO  [ResourceManager:155] Create/Update KafkaUser my-user-333986137-1152182125 in namespace throttling-quota-st
2022-03-28 18:58:58 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1149920ms till timeout)
2022-03-28 18:58:58 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1149067ms till timeout)
2022-03-28 18:58:58 [ForkJoinPool-1-worker-33] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-333986137-1152182125
2022-03-28 18:58:58 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (479787ms till timeout)
2022-03-28 18:58:58 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (826117ms till timeout)
2022-03-28 18:58:58 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (667311ms till timeout)
2022-03-28 18:58:58 [ForkJoinPool-1-worker-33] INFO  [ResourceManager:433] Wait for KafkaUser: my-user-333986137-1152182125 will have desired state: Ready
2022-03-28 18:58:58 [ForkJoinPool-1-worker-33] DEBUG [TestUtils:125] Waiting for KafkaUser: my-user-333986137-1152182125 will have desired state: Ready
2022-03-28 18:58:58 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1147308ms till timeout)
2022-03-28 18:58:58 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] KafkaUser: my-user-1876060546-1887905718 will have desired state: Ready not ready, will try again in 1000 ms (176322ms till timeout)
2022-03-28 18:58:59 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1150919ms till timeout)
2022-03-28 18:58:59 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] KafkaUser: my-user-333986137-1152182125 will have desired state: Ready not ready, will try again in 1000 ms (179806ms till timeout)
2022-03-28 18:58:59 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (672333ms till timeout)
2022-03-28 18:58:59 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1147756ms till timeout)
2022-03-28 18:58:59 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1149983ms till timeout)
2022-03-28 18:58:59 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1151245ms till timeout)
2022-03-28 18:58:59 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1148823ms till timeout)
2022-03-28 18:58:59 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1147970ms till timeout)
2022-03-28 18:58:59 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (478600ms till timeout)
2022-03-28 18:58:59 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (825013ms till timeout)
2022-03-28 18:58:59 [ForkJoinPool-1-worker-1] INFO  [Exec:417] Command: oc --namespace user-st exec user-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user CN=my-user-1909583883-1106194424
2022-03-28 18:58:59 [ForkJoinPool-1-worker-1] INFO  [Exec:417] Return code: 0
2022-03-28 18:58:59 [ForkJoinPool-1-worker-35] INFO  [TestSeparator:23] ############################################################################
2022-03-28 18:58:59 [ForkJoinPool-1-worker-35] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.topic.ThrottlingQuotaST.testKafkaAdminTopicOperations-STARTED
2022-03-28 18:58:59 [ForkJoinPool-1-worker-35] DEBUG [AbstractST:658] ============================================================================
2022-03-28 18:58:59 [ForkJoinPool-1-worker-35] DEBUG [AbstractST:659] [operators.topic.ThrottlingQuotaST - Before Each] - Setup test case environment
2022-03-28 18:58:59 [ForkJoinPool-1-worker-35] DEBUG [SuiteThreadController:77] [operators.topic.ThrottlingQuotaST] - Adding parallel test: testKafkaAdminTopicOperations
2022-03-28 18:58:59 [ForkJoinPool-1-worker-35] DEBUG [SuiteThreadController:81] [operators.topic.ThrottlingQuotaST] - Parallel test count: 13
2022-03-28 18:58:59 [ForkJoinPool-1-worker-35] DEBUG [SuiteThreadController:230] testKafkaAdminTopicOperations test now can proceed its execution
2022-03-28 18:58:59 [ForkJoinPool-1-worker-35] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 18:58:59 [ForkJoinPool-1-worker-35] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaAdminTopicOperations=my-cluster-506fda27, testConfigurationFileIsCreated=my-cluster-17fc585b, testThrottlingQuotasCreateAlterPartitions=my-cluster-ec1b34f8, testCreatingUsersWithSecretPrefix=my-cluster-f743fdfd, testConfigurationReflection=my-cluster-188382d8, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-959c6ed5, testTlsExternalUserWithQuotas=my-cluster-100f5461, testDeployAndUnDeployCruiseControl=my-cluster-5dfd606f, testThrottlingQuotasCreateTopic=my-cluster-e9955717, testCapacityFile=my-cluster-855b9105, testConfigurationPerformanceOptions=my-cluster-08515847, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-afe273c9, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-e6343cec}
2022-03-28 18:58:59 [ForkJoinPool-1-worker-35] TRACE [AbstractST:607] USERS_NAME_MAP: {testKafkaAdminTopicOperations=my-user-1780746402-846362125, testConfigurationFileIsCreated=my-user-355815510-1316268151, testThrottlingQuotasCreateAlterPartitions=my-user-333986137-1152182125, testCreatingUsersWithSecretPrefix=my-user-607975993-1210920737, testConfigurationReflection=my-user-1890969183-1754686912, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-2104248162-45187051, testTlsExternalUserWithQuotas=my-user-1909583883-1106194424, testDeployAndUnDeployCruiseControl=my-user-506268651-1309117986, testThrottlingQuotasCreateTopic=my-user-1876060546-1887905718, testCapacityFile=my-user-1460643829-103298436, testConfigurationPerformanceOptions=my-user-98362545-1476587236, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-user-8456079-650483351, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-2069267102-581538560}
2022-03-28 18:58:59 [ForkJoinPool-1-worker-35] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaAdminTopicOperations=my-topic-481231182-953607496, testConfigurationFileIsCreated=my-topic-423850169-855699454, testThrottlingQuotasCreateAlterPartitions=my-topic-325891726-169328137, testCreatingUsersWithSecretPrefix=my-topic-1727421070-668192795, testConfigurationReflection=my-topic-1394577332-1936226438, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-634125597-1746493046, testTlsExternalUserWithQuotas=my-topic-1986059791-167638004, testDeployAndUnDeployCruiseControl=my-topic-465377127-800134460, testThrottlingQuotasCreateTopic=my-topic-1246597695-228473879, testCapacityFile=my-topic-549312506-649708453, testConfigurationPerformanceOptions=my-topic-696588651-1198599652, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-topic-1687496771-142091227, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-293775067-432059493}
2022-03-28 18:58:59 [ForkJoinPool-1-worker-35] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaAdminTopicOperations=my-cluster-506fda27-kafka-clients, testConfigurationFileIsCreated=my-cluster-17fc585b-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-ec1b34f8-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-f743fdfd-kafka-clients, testConfigurationReflection=my-cluster-188382d8-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-959c6ed5-kafka-clients, testTlsExternalUserWithQuotas=my-cluster-100f5461-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-5dfd606f-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-e9955717-kafka-clients, testCapacityFile=my-cluster-855b9105-kafka-clients, testConfigurationPerformanceOptions=my-cluster-08515847-kafka-clients, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-afe273c9-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-e6343cec-kafka-clients}
2022-03-28 18:58:59 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (666209ms till timeout)
2022-03-28 18:58:59 [ForkJoinPool-1-worker-35] INFO  [ResourceManager:155] Create/Update KafkaUser my-user-1780746402-846362125 in namespace throttling-quota-st
2022-03-28 18:59:00 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1146207ms till timeout)
2022-03-28 18:59:00 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] KafkaUser: my-user-1876060546-1887905718 will have desired state: Ready not ready, will try again in 1000 ms (175220ms till timeout)
2022-03-28 18:59:00 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1149820ms till timeout)
2022-03-28 18:59:00 [ForkJoinPool-1-worker-35] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-1780746402-846362125
2022-03-28 18:59:00 [ForkJoinPool-1-worker-1] INFO  [KafkaUserUtils:62] Waiting for KafkaUser deletion my-user-1909583883-1106194424
2022-03-28 18:59:00 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for KafkaUser deletion my-user-1909583883-1106194424
2022-03-28 18:59:00 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] KafkaUser: my-user-333986137-1152182125 will have desired state: Ready not ready, will try again in 1000 ms (178616ms till timeout)
2022-03-28 18:59:00 [ForkJoinPool-1-worker-1] INFO  [KafkaUserUtils:75] KafkaUser my-user-1909583883-1106194424 deleted
2022-03-28 18:59:00 [ForkJoinPool-1-worker-35] INFO  [ResourceManager:433] Wait for KafkaUser: my-user-1780746402-846362125 will have desired state: Ready
2022-03-28 18:59:00 [ForkJoinPool-1-worker-35] DEBUG [TestUtils:125] Waiting for KafkaUser: my-user-1780746402-846362125 will have desired state: Ready
2022-03-28 18:59:00 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (671147ms till timeout)
2022-03-28 18:59:00 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for all KafkaUser CN=my-user-1909583883-1106194424 attributes will be cleaned
2022-03-28 18:59:00 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace user-st exec user-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user CN=my-user-1909583883-1106194424
2022-03-28 18:59:00 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1146563ms till timeout)
2022-03-28 18:59:00 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1148791ms till timeout)
2022-03-28 18:59:00 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] KafkaUser: my-user-1780746402-846362125 will have desired state: Ready not ready, will try again in 1000 ms (179803ms till timeout)
2022-03-28 18:59:00 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1150088ms till timeout)
2022-03-28 18:59:00 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1147726ms till timeout)
2022-03-28 18:59:00 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1146872ms till timeout)
2022-03-28 18:59:00 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (477502ms till timeout)
2022-03-28 18:59:01 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (665113ms till timeout)
2022-03-28 18:59:01 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (823822ms till timeout)
2022-03-28 18:59:01 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1145109ms till timeout)
2022-03-28 18:59:01 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] KafkaUser: my-user-1876060546-1887905718 will have desired state: Ready not ready, will try again in 1000 ms (174028ms till timeout)
2022-03-28 18:59:01 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] KafkaUser: my-user-333986137-1152182125 will have desired state: Ready not ready, will try again in 1000 ms (177516ms till timeout)
2022-03-28 18:59:01 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1148628ms till timeout)
2022-03-28 18:59:01 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (670046ms till timeout)
2022-03-28 18:59:01 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1147695ms till timeout)
2022-03-28 18:59:01 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1145371ms till timeout)
2022-03-28 18:59:01 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] KafkaUser: my-user-1780746402-846362125 will have desired state: Ready not ready, will try again in 1000 ms (178610ms till timeout)
2022-03-28 18:59:01 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1148990ms till timeout)
2022-03-28 18:59:01 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1146627ms till timeout)
2022-03-28 18:59:01 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1145774ms till timeout)
2022-03-28 18:59:02 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (476404ms till timeout)
2022-03-28 18:59:02 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (664014ms till timeout)
2022-03-28 18:59:02 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (822723ms till timeout)
2022-03-28 18:59:02 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1144011ms till timeout)
2022-03-28 18:59:02 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] KafkaUser: my-user-1876060546-1887905718 will have desired state: Ready not ready, will try again in 1000 ms (172931ms till timeout)
2022-03-28 18:59:02 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] KafkaUser: my-user-333986137-1152182125 will have desired state: Ready not ready, will try again in 1000 ms (176419ms till timeout)
2022-03-28 18:59:02 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1147532ms till timeout)
2022-03-28 18:59:02 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (668948ms till timeout)
2022-03-28 18:59:02 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1146595ms till timeout)
2022-03-28 18:59:02 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1144272ms till timeout)
2022-03-28 18:59:02 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] KafkaUser: my-user-1780746402-846362125 will have desired state: Ready not ready, will try again in 1000 ms (177512ms till timeout)
2022-03-28 18:59:02 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1147890ms till timeout)
2022-03-28 18:59:02 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1145530ms till timeout)
2022-03-28 18:59:03 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1144676ms till timeout)
2022-03-28 18:59:03 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (475305ms till timeout)
2022-03-28 18:59:03 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:444] Kafka: http-bridge-scram-sha-cluster-name is in desired state: Ready
2022-03-28 18:59:03 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:155] Create/Update KafkaUser my-user-410252673-1542305576 in namespace http-bridge-scram-sha-st
2022-03-28 18:59:03 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (821622ms till timeout)
2022-03-28 18:59:03 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1142912ms till timeout)
2022-03-28 18:59:03 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:444] KafkaUser: my-user-1876060546-1887905718 is in desired state: Ready
2022-03-28 18:59:03 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-410252673-1542305576
2022-03-28 18:59:03 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] KafkaUser: my-user-333986137-1152182125 will have desired state: Ready not ready, will try again in 1000 ms (175322ms till timeout)
2022-03-28 18:59:03 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1146434ms till timeout)
2022-03-28 18:59:03 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:433] Wait for KafkaUser: my-user-410252673-1542305576 will have desired state: Ready
2022-03-28 18:59:03 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for KafkaUser: my-user-410252673-1542305576 will have desired state: Ready
2022-03-28 18:59:03 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:155] Create/Update Job create-admin-my-cluster-e9955717-kafka-clients in namespace throttling-quota-st
2022-03-28 18:59:03 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (667758ms till timeout)
2022-03-28 18:59:03 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaUser: my-user-410252673-1542305576 will have desired state: Ready not ready, will try again in 1000 ms (179862ms till timeout)
2022-03-28 18:59:03 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1145459ms till timeout)
2022-03-28 18:59:03 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:create-admin-my-cluster-e9955717-kafka-clients
2022-03-28 18:59:03 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1143089ms till timeout)
2022-03-28 18:59:03 [ForkJoinPool-1-worker-35] INFO  [ResourceManager:444] KafkaUser: my-user-1780746402-846362125 is in desired state: Ready
2022-03-28 18:59:03 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1146706ms till timeout)
2022-03-28 18:59:03 [ForkJoinPool-1-worker-9] INFO  [JobUtils:81] Waiting for job: create-admin-my-cluster-e9955717-kafka-clients will be in active state
2022-03-28 18:59:03 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 18:59:04 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1144390ms till timeout)
2022-03-28 18:59:04 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1143540ms till timeout)
2022-03-28 18:59:04 [ForkJoinPool-1-worker-35] INFO  [ResourceManager:155] Create/Update Job create-admin-my-cluster-506fda27-kafka-clients in namespace throttling-quota-st
2022-03-28 18:59:04 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (474170ms till timeout)
2022-03-28 18:59:04 [ForkJoinPool-1-worker-9] INFO  [PodUtils:186] Waiting for message will be in the log
2022-03-28 18:59:04 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Waiting for message will be in the log
2022-03-28 18:59:04 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (820490ms till timeout)
2022-03-28 18:59:04 [ForkJoinPool-1-worker-35] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:create-admin-my-cluster-506fda27-kafka-clients
2022-03-28 18:59:04 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1141779ms till timeout)
2022-03-28 18:59:04 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (299813ms till timeout)
2022-03-28 18:59:04 [ForkJoinPool-1-worker-35] INFO  [JobUtils:81] Waiting for job: create-admin-my-cluster-506fda27-kafka-clients will be in active state
2022-03-28 18:59:04 [ForkJoinPool-1-worker-35] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 18:59:04 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1145299ms till timeout)
2022-03-28 18:59:04 [ForkJoinPool-1-worker-33] INFO  [ResourceManager:444] KafkaUser: my-user-333986137-1152182125 is in desired state: Ready
2022-03-28 18:59:04 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (666619ms till timeout)
2022-03-28 18:59:04 [ForkJoinPool-1-worker-33] INFO  [ResourceManager:155] Create/Update Job create-admin-my-cluster-ec1b34f8-kafka-clients in namespace throttling-quota-st
2022-03-28 18:59:04 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaUser: my-user-410252673-1542305576 will have desired state: Ready not ready, will try again in 1000 ms (178671ms till timeout)
2022-03-28 18:59:04 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1144269ms till timeout)
2022-03-28 18:59:04 [ForkJoinPool-1-worker-35] INFO  [PodUtils:186] Waiting for message will be in the log
2022-03-28 18:59:04 [ForkJoinPool-1-worker-35] DEBUG [TestUtils:125] Waiting for Waiting for message will be in the log
2022-03-28 18:59:05 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1141946ms till timeout)
2022-03-28 18:59:05 [ForkJoinPool-1-worker-33] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:create-admin-my-cluster-ec1b34f8-kafka-clients
2022-03-28 18:59:05 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1145567ms till timeout)
2022-03-28 18:59:05 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (119809ms till timeout)
2022-03-28 18:59:05 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1143244ms till timeout)
2022-03-28 18:59:05 [ForkJoinPool-1-worker-33] INFO  [JobUtils:81] Waiting for job: create-admin-my-cluster-ec1b34f8-kafka-clients will be in active state
2022-03-28 18:59:05 [ForkJoinPool-1-worker-33] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 18:59:05 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1142395ms till timeout)
2022-03-28 18:59:05 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (473025ms till timeout)
2022-03-28 18:59:05 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (819343ms till timeout)
2022-03-28 18:59:05 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1140598ms till timeout)
2022-03-28 18:59:05 [ForkJoinPool-1-worker-33] INFO  [PodUtils:186] Waiting for message will be in the log
2022-03-28 18:59:05 [ForkJoinPool-1-worker-33] DEBUG [TestUtils:125] Waiting for Waiting for message will be in the log
2022-03-28 18:59:05 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (298635ms till timeout)
2022-03-28 18:59:05 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (299872ms till timeout)
2022-03-28 18:59:05 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1144078ms till timeout)
2022-03-28 18:59:05 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (665495ms till timeout)
2022-03-28 18:59:06 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1143145ms till timeout)
2022-03-28 18:59:06 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:444] KafkaUser: my-user-410252673-1542305576 is in desired state: Ready
2022-03-28 18:59:06 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1140822ms till timeout)
2022-03-28 18:59:06 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1144441ms till timeout)
2022-03-28 18:59:06 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (118681ms till timeout)
2022-03-28 18:59:06 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1142122ms till timeout)
2022-03-28 18:59:06 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1141272ms till timeout)
2022-03-28 18:59:06 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (471904ms till timeout)
2022-03-28 18:59:06 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (818223ms till timeout)
2022-03-28 18:59:06 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1139501ms till timeout)
2022-03-28 18:59:06 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:155] Create/Update Deployment http-bridge-scram-sha-st-shared-kafka-clients in namespace http-bridge-scram-sha-st
2022-03-28 18:59:06 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (297447ms till timeout)
2022-03-28 18:59:06 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (298709ms till timeout)
2022-03-28 18:59:06 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:http-bridge-scram-sha-st-shared-kafka-clients
2022-03-28 18:59:06 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1142923ms till timeout)
2022-03-28 18:59:07 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (664340ms till timeout)
2022-03-28 18:59:07 [ForkJoinPool-1-worker-1] INFO  [Exec:417] Command: oc --namespace user-st exec user-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user CN=my-user-1909583883-1106194424
2022-03-28 18:59:07 [ForkJoinPool-1-worker-1] INFO  [Exec:417] Return code: 0
2022-03-28 18:59:07 [ForkJoinPool-1-worker-1] DEBUG [AbstractST:674] ============================================================================
2022-03-28 18:59:07 [ForkJoinPool-1-worker-1] DEBUG [AbstractST:675] [operators.user.UserST - After Each] - Clean up after test
2022-03-28 18:59:07 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:344] ############################################################################
2022-03-28 18:59:07 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:348] Delete all resources for testTlsExternalUserWithQuotas
2022-03-28 18:59:07 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:241] Delete of KafkaUser my-user-1909583883-1106194424 in namespace user-st
2022-03-28 18:59:07 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1141991ms till timeout)
2022-03-28 18:59:07 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-1909583883-1106194424
2022-03-28 18:59:07 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1139667ms till timeout)
2022-03-28 18:59:07 [ForkJoinPool-1-worker-5] INFO  [DeploymentUtils:161] Wait for Deployment: http-bridge-scram-sha-st-shared-kafka-clients will be ready
2022-03-28 18:59:07 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Wait for Deployment: http-bridge-scram-sha-st-shared-kafka-clients will be ready
2022-03-28 18:59:07 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1143290ms till timeout)
2022-03-28 18:59:07 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (117530ms till timeout)
2022-03-28 18:59:07 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:369] ############################################################################
2022-03-28 18:59:07 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:267] testTlsExternalUserWithQuotas - Notifies waiting test cases:[testPauseReconciliationInKafkaRebalanceAndTopic, testPauseReconciliationInKafkaAndKafkaConnectWithConnector, testCapacityFile, testConfigurationFileIsCreated, testConfigurationPerformanceOptions, testConfigurationReflection, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testDeployAndUnDeployCruiseControl, testTlsExternalUserWithQuotas, testCreatingUsersWithSecretPrefix, testThrottlingQuotasCreateTopic, testThrottlingQuotasCreateAlterPartitions, testKafkaAdminTopicOperations] to and randomly select one to start execution
2022-03-28 18:59:07 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:93] [operators.user.UserST] - Removing parallel test: testTlsExternalUserWithQuotas
2022-03-28 18:59:07 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:97] [operators.user.UserST] - Parallel test count: 12
2022-03-28 18:59:07 [ForkJoinPool-1-worker-1] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.user.UserST.testTlsExternalUserWithQuotas-FINISHED
2022-03-28 18:59:07 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1140970ms till timeout)
2022-03-28 18:59:07 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Wait for Deployment: http-bridge-scram-sha-st-shared-kafka-clients will be ready not ready, will try again in 1000 ms (479813ms till timeout)
2022-03-28 18:59:07 [ForkJoinPool-1-worker-1] INFO  [TestSeparator:30] ############################################################################
2022-03-28 18:59:07 [ForkJoinPool-1-worker-1] INFO  [TestSeparator:23] ############################################################################
2022-03-28 18:59:07 [ForkJoinPool-1-worker-1] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.user.UserST.testUserTemplate-STARTED
2022-03-28 18:59:07 [ForkJoinPool-1-worker-1] DEBUG [AbstractST:658] ============================================================================
2022-03-28 18:59:07 [ForkJoinPool-1-worker-1] DEBUG [AbstractST:659] [operators.user.UserST - Before Each] - Setup test case environment
2022-03-28 18:59:07 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:77] [operators.user.UserST] - Adding parallel test: testUserTemplate
2022-03-28 18:59:07 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:81] [operators.user.UserST] - Parallel test count: 13
2022-03-28 18:59:07 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:230] testUserTemplate test now can proceed its execution
2022-03-28 18:59:07 [ForkJoinPool-1-worker-1] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 18:59:07 [ForkJoinPool-1-worker-1] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaAdminTopicOperations=my-cluster-506fda27, testConfigurationFileIsCreated=my-cluster-17fc585b, testThrottlingQuotasCreateAlterPartitions=my-cluster-ec1b34f8, testCreatingUsersWithSecretPrefix=my-cluster-f743fdfd, testConfigurationReflection=my-cluster-188382d8, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-959c6ed5, testTlsExternalUserWithQuotas=my-cluster-100f5461, testDeployAndUnDeployCruiseControl=my-cluster-5dfd606f, testThrottlingQuotasCreateTopic=my-cluster-e9955717, testCapacityFile=my-cluster-855b9105, testConfigurationPerformanceOptions=my-cluster-08515847, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-afe273c9, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-e6343cec, testUserTemplate=my-cluster-927d5a1b}
2022-03-28 18:59:07 [ForkJoinPool-1-worker-1] TRACE [AbstractST:607] USERS_NAME_MAP: {testKafkaAdminTopicOperations=my-user-1780746402-846362125, testConfigurationFileIsCreated=my-user-355815510-1316268151, testThrottlingQuotasCreateAlterPartitions=my-user-333986137-1152182125, testCreatingUsersWithSecretPrefix=my-user-607975993-1210920737, testConfigurationReflection=my-user-1890969183-1754686912, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-2104248162-45187051, testTlsExternalUserWithQuotas=my-user-1909583883-1106194424, testDeployAndUnDeployCruiseControl=my-user-506268651-1309117986, testThrottlingQuotasCreateTopic=my-user-1876060546-1887905718, testCapacityFile=my-user-1460643829-103298436, testConfigurationPerformanceOptions=my-user-98362545-1476587236, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-user-8456079-650483351, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-2069267102-581538560, testUserTemplate=my-user-989633374-940717398}
2022-03-28 18:59:07 [ForkJoinPool-1-worker-1] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaAdminTopicOperations=my-topic-481231182-953607496, testConfigurationFileIsCreated=my-topic-423850169-855699454, testThrottlingQuotasCreateAlterPartitions=my-topic-325891726-169328137, testCreatingUsersWithSecretPrefix=my-topic-1727421070-668192795, testConfigurationReflection=my-topic-1394577332-1936226438, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-634125597-1746493046, testTlsExternalUserWithQuotas=my-topic-1986059791-167638004, testDeployAndUnDeployCruiseControl=my-topic-465377127-800134460, testThrottlingQuotasCreateTopic=my-topic-1246597695-228473879, testCapacityFile=my-topic-549312506-649708453, testConfigurationPerformanceOptions=my-topic-696588651-1198599652, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-topic-1687496771-142091227, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-293775067-432059493, testUserTemplate=my-topic-1542379258-2002911578}
2022-03-28 18:59:07 [ForkJoinPool-1-worker-1] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaAdminTopicOperations=my-cluster-506fda27-kafka-clients, testConfigurationFileIsCreated=my-cluster-17fc585b-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-ec1b34f8-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-f743fdfd-kafka-clients, testConfigurationReflection=my-cluster-188382d8-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-959c6ed5-kafka-clients, testTlsExternalUserWithQuotas=my-cluster-100f5461-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-5dfd606f-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-e9955717-kafka-clients, testCapacityFile=my-cluster-855b9105-kafka-clients, testConfigurationPerformanceOptions=my-cluster-08515847-kafka-clients, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-afe273c9-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-e6343cec-kafka-clients, testUserTemplate=my-cluster-927d5a1b-kafka-clients}
2022-03-28 18:59:07 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:155] Create/Update KafkaUser my-user-989633374-940717398 in namespace user-st
2022-03-28 18:59:07 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1140122ms till timeout)
2022-03-28 18:59:07 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (470744ms till timeout)
2022-03-28 18:59:07 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-989633374-940717398
2022-03-28 18:59:07 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (817070ms till timeout)
2022-03-28 18:59:07 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1138358ms till timeout)
2022-03-28 18:59:07 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:433] Wait for KafkaUser: my-user-989633374-940717398 will have desired state: Ready
2022-03-28 18:59:07 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for KafkaUser: my-user-989633374-940717398 will have desired state: Ready
2022-03-28 18:59:07 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (296303ms till timeout)
2022-03-28 18:59:08 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] KafkaUser: my-user-989633374-940717398 will have desired state: Ready not ready, will try again in 1000 ms (179806ms till timeout)
2022-03-28 18:59:08 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (297562ms till timeout)
2022-03-28 18:59:08 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1141778ms till timeout)
2022-03-28 18:59:08 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (663195ms till timeout)
2022-03-28 18:59:08 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1140845ms till timeout)
2022-03-28 18:59:08 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1138522ms till timeout)
2022-03-28 18:59:08 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1142142ms till timeout)
2022-03-28 18:59:08 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (116375ms till timeout)
2022-03-28 18:59:08 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Wait for Deployment: http-bridge-scram-sha-st-shared-kafka-clients will be ready not ready, will try again in 1000 ms (478666ms till timeout)
2022-03-28 18:59:08 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1139821ms till timeout)
2022-03-28 18:59:08 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1138973ms till timeout)
2022-03-28 18:59:08 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (469601ms till timeout)
2022-03-28 18:59:08 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (815921ms till timeout)
2022-03-28 18:59:09 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1137207ms till timeout)
2022-03-28 18:59:09 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (295150ms till timeout)
2022-03-28 18:59:09 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] KafkaUser: my-user-989633374-940717398 will have desired state: Ready not ready, will try again in 1000 ms (178652ms till timeout)
2022-03-28 18:59:09 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (296410ms till timeout)
2022-03-28 18:59:09 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1140624ms till timeout)
2022-03-28 18:59:09 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (662040ms till timeout)
2022-03-28 18:59:09 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1139690ms till timeout)
2022-03-28 18:59:09 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1137367ms till timeout)
2022-03-28 18:59:09 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1140987ms till timeout)
2022-03-28 18:59:09 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (115227ms till timeout)
2022-03-28 18:59:09 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Wait for Deployment: http-bridge-scram-sha-st-shared-kafka-clients will be ready not ready, will try again in 1000 ms (477511ms till timeout)
2022-03-28 18:59:09 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1138666ms till timeout)
2022-03-28 18:59:09 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1137819ms till timeout)
2022-03-28 18:59:09 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (468448ms till timeout)
2022-03-28 18:59:10 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (814768ms till timeout)
2022-03-28 18:59:10 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1136055ms till timeout)
2022-03-28 18:59:10 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (294002ms till timeout)
2022-03-28 18:59:10 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:444] KafkaUser: my-user-989633374-940717398 is in desired state: Ready
2022-03-28 18:59:10 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (295261ms till timeout)
2022-03-28 18:59:10 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1139475ms till timeout)
2022-03-28 18:59:10 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (660892ms till timeout)
2022-03-28 18:59:10 [ForkJoinPool-1-worker-1] DEBUG [AbstractST:674] ============================================================================
2022-03-28 18:59:10 [ForkJoinPool-1-worker-1] DEBUG [AbstractST:675] [operators.user.UserST - After Each] - Clean up after test
2022-03-28 18:59:10 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:344] ############################################################################
2022-03-28 18:59:10 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:348] Delete all resources for testUserTemplate
2022-03-28 18:59:10 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:241] Delete of KafkaUser my-user-989633374-940717398 in namespace user-st
2022-03-28 18:59:10 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1138541ms till timeout)
2022-03-28 18:59:10 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1136219ms till timeout)
2022-03-28 18:59:10 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-989633374-940717398
2022-03-28 18:59:10 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1139839ms till timeout)
2022-03-28 18:59:10 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (114066ms till timeout)
2022-03-28 18:59:10 [ForkJoinPool-1-worker-5] INFO  [DeploymentUtils:168] Deployment: http-bridge-scram-sha-st-shared-kafka-clients is ready
2022-03-28 18:59:10 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1137517ms till timeout)
2022-03-28 18:59:10 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:369] ############################################################################
2022-03-28 18:59:10 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:267] testUserTemplate - Notifies waiting test cases:[testPauseReconciliationInKafkaRebalanceAndTopic, testPauseReconciliationInKafkaAndKafkaConnectWithConnector, testCapacityFile, testConfigurationFileIsCreated, testConfigurationPerformanceOptions, testConfigurationReflection, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testDeployAndUnDeployCruiseControl, testTlsExternalUserWithQuotas, testCreatingUsersWithSecretPrefix, testThrottlingQuotasCreateTopic, testThrottlingQuotasCreateAlterPartitions, testKafkaAdminTopicOperations, testUserTemplate] to and randomly select one to start execution
2022-03-28 18:59:10 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:93] [operators.user.UserST] - Removing parallel test: testUserTemplate
2022-03-28 18:59:10 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:97] [operators.user.UserST] - Parallel test count: 12
2022-03-28 18:59:10 [ForkJoinPool-1-worker-1] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.user.UserST.testUserTemplate-FINISHED
2022-03-28 18:59:10 [ForkJoinPool-1-worker-1] INFO  [TestSeparator:30] ############################################################################
2022-03-28 18:59:10 [ForkJoinPool-1-worker-1] INFO  [TestSeparator:23] ############################################################################
2022-03-28 18:59:10 [ForkJoinPool-1-worker-1] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.user.UserST.testUpdateUser-STARTED
2022-03-28 18:59:10 [ForkJoinPool-1-worker-1] DEBUG [AbstractST:658] ============================================================================
2022-03-28 18:59:10 [ForkJoinPool-1-worker-1] DEBUG [AbstractST:659] [operators.user.UserST - Before Each] - Setup test case environment
2022-03-28 18:59:10 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:77] [operators.user.UserST] - Adding parallel test: testUpdateUser
2022-03-28 18:59:10 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:81] [operators.user.UserST] - Parallel test count: 13
2022-03-28 18:59:10 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:230] testUpdateUser test now can proceed its execution
2022-03-28 18:59:10 [ForkJoinPool-1-worker-1] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 18:59:10 [ForkJoinPool-1-worker-1] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaAdminTopicOperations=my-cluster-506fda27, testConfigurationFileIsCreated=my-cluster-17fc585b, testThrottlingQuotasCreateAlterPartitions=my-cluster-ec1b34f8, testCreatingUsersWithSecretPrefix=my-cluster-f743fdfd, testUpdateUser=my-cluster-d582eb8c, testConfigurationReflection=my-cluster-188382d8, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-959c6ed5, testTlsExternalUserWithQuotas=my-cluster-100f5461, testDeployAndUnDeployCruiseControl=my-cluster-5dfd606f, testThrottlingQuotasCreateTopic=my-cluster-e9955717, testCapacityFile=my-cluster-855b9105, testConfigurationPerformanceOptions=my-cluster-08515847, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-afe273c9, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-e6343cec, testUserTemplate=my-cluster-927d5a1b}
2022-03-28 18:59:10 [ForkJoinPool-1-worker-1] TRACE [AbstractST:607] USERS_NAME_MAP: {testKafkaAdminTopicOperations=my-user-1780746402-846362125, testConfigurationFileIsCreated=my-user-355815510-1316268151, testThrottlingQuotasCreateAlterPartitions=my-user-333986137-1152182125, testCreatingUsersWithSecretPrefix=my-user-607975993-1210920737, testUpdateUser=my-user-1297536277-778093599, testConfigurationReflection=my-user-1890969183-1754686912, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-2104248162-45187051, testTlsExternalUserWithQuotas=my-user-1909583883-1106194424, testDeployAndUnDeployCruiseControl=my-user-506268651-1309117986, testThrottlingQuotasCreateTopic=my-user-1876060546-1887905718, testCapacityFile=my-user-1460643829-103298436, testConfigurationPerformanceOptions=my-user-98362545-1476587236, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-user-8456079-650483351, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-2069267102-581538560, testUserTemplate=my-user-989633374-940717398}
2022-03-28 18:59:10 [ForkJoinPool-1-worker-1] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaAdminTopicOperations=my-topic-481231182-953607496, testConfigurationFileIsCreated=my-topic-423850169-855699454, testThrottlingQuotasCreateAlterPartitions=my-topic-325891726-169328137, testCreatingUsersWithSecretPrefix=my-topic-1727421070-668192795, testUpdateUser=my-topic-1325563869-1324403074, testConfigurationReflection=my-topic-1394577332-1936226438, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-634125597-1746493046, testTlsExternalUserWithQuotas=my-topic-1986059791-167638004, testDeployAndUnDeployCruiseControl=my-topic-465377127-800134460, testThrottlingQuotasCreateTopic=my-topic-1246597695-228473879, testCapacityFile=my-topic-549312506-649708453, testConfigurationPerformanceOptions=my-topic-696588651-1198599652, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-topic-1687496771-142091227, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-293775067-432059493, testUserTemplate=my-topic-1542379258-2002911578}
2022-03-28 18:59:10 [ForkJoinPool-1-worker-1] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaAdminTopicOperations=my-cluster-506fda27-kafka-clients, testConfigurationFileIsCreated=my-cluster-17fc585b-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-ec1b34f8-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-f743fdfd-kafka-clients, testUpdateUser=my-cluster-d582eb8c-kafka-clients, testConfigurationReflection=my-cluster-188382d8-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-959c6ed5-kafka-clients, testTlsExternalUserWithQuotas=my-cluster-100f5461-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-5dfd606f-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-e9955717-kafka-clients, testCapacityFile=my-cluster-855b9105-kafka-clients, testConfigurationPerformanceOptions=my-cluster-08515847-kafka-clients, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-afe273c9-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-e6343cec-kafka-clients, testUserTemplate=my-cluster-927d5a1b-kafka-clients}
2022-03-28 18:59:10 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:155] Create/Update KafkaUser my-user-1297536277-778093599 in namespace user-st
2022-03-28 18:59:11 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1136674ms till timeout)
2022-03-28 18:59:11 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (467298ms till timeout)
2022-03-28 18:59:11 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-1297536277-778093599
2022-03-28 18:59:11 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:155] Create/Update KafkaBridge http-bridge-scram-sha-cluster-name in namespace http-bridge-scram-sha-st
2022-03-28 18:59:11 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (813625ms till timeout)
2022-03-28 18:59:11 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:433] Wait for KafkaUser: my-user-1297536277-778093599 will have desired state: Ready
2022-03-28 18:59:11 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for KafkaUser: my-user-1297536277-778093599 will have desired state: Ready
2022-03-28 18:59:11 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1134913ms till timeout)
2022-03-28 18:59:11 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaBridge:http-bridge-scram-sha-cluster-name
2022-03-28 18:59:11 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (292857ms till timeout)
2022-03-28 18:59:11 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] KafkaUser: my-user-1297536277-778093599 will have desired state: Ready not ready, will try again in 1000 ms (179805ms till timeout)
2022-03-28 18:59:11 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:433] Wait for KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready
2022-03-28 18:59:11 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready
2022-03-28 18:59:11 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (294111ms till timeout)
2022-03-28 18:59:11 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1138333ms till timeout)
2022-03-28 18:59:11 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (479809ms till timeout)
2022-03-28 18:59:11 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (659748ms till timeout)
2022-03-28 18:59:11 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1137400ms till timeout)
2022-03-28 18:59:11 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1135073ms till timeout)
2022-03-28 18:59:11 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1138693ms till timeout)
2022-03-28 18:59:11 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (112933ms till timeout)
2022-03-28 18:59:12 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1136373ms till timeout)
2022-03-28 18:59:12 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1135520ms till timeout)
2022-03-28 18:59:12 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (466149ms till timeout)
2022-03-28 18:59:12 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (812469ms till timeout)
2022-03-28 18:59:12 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1133756ms till timeout)
2022-03-28 18:59:12 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (291700ms till timeout)
2022-03-28 18:59:12 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] KafkaUser: my-user-1297536277-778093599 will have desired state: Ready not ready, will try again in 1000 ms (178639ms till timeout)
2022-03-28 18:59:12 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (292954ms till timeout)
2022-03-28 18:59:12 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1137167ms till timeout)
2022-03-28 18:59:12 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (658584ms till timeout)
2022-03-28 18:59:12 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (478643ms till timeout)
2022-03-28 18:59:12 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1136235ms till timeout)
2022-03-28 18:59:13 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1133910ms till timeout)
2022-03-28 18:59:13 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1137516ms till timeout)
2022-03-28 18:59:13 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (111721ms till timeout)
2022-03-28 18:59:13 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1135190ms till timeout)
2022-03-28 18:59:13 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1134338ms till timeout)
2022-03-28 18:59:13 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (464967ms till timeout)
2022-03-28 18:59:13 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (811287ms till timeout)
2022-03-28 18:59:13 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1132575ms till timeout)
2022-03-28 18:59:13 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (290517ms till timeout)
2022-03-28 18:59:13 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:444] KafkaUser: my-user-1297536277-778093599 is in desired state: Ready
2022-03-28 18:59:13 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (291776ms till timeout)
2022-03-28 18:59:13 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1135995ms till timeout)
2022-03-28 18:59:14 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (477472ms till timeout)
2022-03-28 18:59:14 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (657411ms till timeout)
2022-03-28 18:59:14 [ForkJoinPool-1-worker-1] DEBUG [CompiledPath:93] Evaluating path: $['data']['ca.crt']
2022-03-28 18:59:14 [ForkJoinPool-1-worker-1] DEBUG [CompiledPath:93] Evaluating path: $['data']['user.crt']
2022-03-28 18:59:14 [ForkJoinPool-1-worker-1] DEBUG [CompiledPath:93] Evaluating path: $['data']['user.key']
2022-03-28 18:59:14 [ForkJoinPool-1-worker-1] DEBUG [CompiledPath:93] Evaluating path: $['metadata']['name']
2022-03-28 18:59:14 [ForkJoinPool-1-worker-1] DEBUG [CompiledPath:93] Evaluating path: $['metadata']['namespace']
2022-03-28 18:59:14 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1135062ms till timeout)
2022-03-28 18:59:14 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1132722ms till timeout)
2022-03-28 18:59:14 [ForkJoinPool-1-worker-1] DEBUG [CompiledPath:93] Evaluating path: $['metadata']['name']
2022-03-28 18:59:14 [ForkJoinPool-1-worker-1] DEBUG [CompiledPath:93] Evaluating path: $['metadata']['namespace']
2022-03-28 18:59:14 [ForkJoinPool-1-worker-1] DEBUG [CompiledPath:93] Evaluating path: $['spec']['authentication']['type']
2022-03-28 18:59:14 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1136338ms till timeout)
2022-03-28 18:59:14 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (110582ms till timeout)
2022-03-28 18:59:14 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1134017ms till timeout)
2022-03-28 18:59:14 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1133167ms till timeout)
2022-03-28 18:59:14 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (463794ms till timeout)
2022-03-28 18:59:14 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (810113ms till timeout)
2022-03-28 18:59:14 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1131399ms till timeout)
2022-03-28 18:59:14 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (289345ms till timeout)
2022-03-28 18:59:14 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for increase observation generation from 1 for user my-user-1297536277-778093599
2022-03-28 18:59:15 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (290598ms till timeout)
2022-03-28 18:59:15 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1134809ms till timeout)
2022-03-28 18:59:15 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] increase observation generation from 1 for user my-user-1297536277-778093599 not ready, will try again in 1000 ms (179833ms till timeout)
2022-03-28 18:59:15 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (476286ms till timeout)
2022-03-28 18:59:15 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (656225ms till timeout)
2022-03-28 18:59:15 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1133876ms till timeout)
2022-03-28 18:59:15 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1131552ms till timeout)
2022-03-28 18:59:15 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1135171ms till timeout)
2022-03-28 18:59:15 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (109412ms till timeout)
2022-03-28 18:59:15 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1132850ms till timeout)
2022-03-28 18:59:15 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1131993ms till timeout)
2022-03-28 18:59:15 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (462618ms till timeout)
2022-03-28 18:59:15 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (808905ms till timeout)
2022-03-28 18:59:16 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1130189ms till timeout)
2022-03-28 18:59:16 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (288216ms till timeout)
2022-03-28 18:59:16 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (289487ms till timeout)
2022-03-28 18:59:16 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] increase observation generation from 1 for user my-user-1297536277-778093599 not ready, will try again in 1000 ms (178712ms till timeout)
2022-03-28 18:59:16 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1133680ms till timeout)
2022-03-28 18:59:16 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (475170ms till timeout)
2022-03-28 18:59:16 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (655109ms till timeout)
2022-03-28 18:59:16 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1132761ms till timeout)
2022-03-28 18:59:16 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1130435ms till timeout)
2022-03-28 18:59:16 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1134055ms till timeout)
2022-03-28 18:59:16 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (108298ms till timeout)
2022-03-28 18:59:16 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1131732ms till timeout)
2022-03-28 18:59:16 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1130883ms till timeout)
2022-03-28 18:59:16 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (461513ms till timeout)
2022-03-28 18:59:17 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (807804ms till timeout)
2022-03-28 18:59:17 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1129092ms till timeout)
2022-03-28 18:59:17 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (287036ms till timeout)
2022-03-28 18:59:17 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (288297ms till timeout)
2022-03-28 18:59:17 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] increase observation generation from 1 for user my-user-1297536277-778093599 not ready, will try again in 1000 ms (177533ms till timeout)
2022-03-28 18:59:17 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1132508ms till timeout)
2022-03-28 18:59:17 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (653925ms till timeout)
2022-03-28 18:59:17 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (473984ms till timeout)
2022-03-28 18:59:17 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1131573ms till timeout)
2022-03-28 18:59:17 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1129250ms till timeout)
2022-03-28 18:59:17 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1132866ms till timeout)
2022-03-28 18:59:17 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (107103ms till timeout)
2022-03-28 18:59:17 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1130542ms till timeout)
2022-03-28 18:59:17 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1129692ms till timeout)
2022-03-28 18:59:18 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (460322ms till timeout)
2022-03-28 18:59:18 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (806641ms till timeout)
2022-03-28 18:59:18 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1127929ms till timeout)
2022-03-28 18:59:18 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (285771ms till timeout)
2022-03-28 18:59:18 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (287119ms till timeout)
2022-03-28 18:59:18 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1131342ms till timeout)
2022-03-28 18:59:18 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (472819ms till timeout)
2022-03-28 18:59:18 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (652759ms till timeout)
2022-03-28 18:59:18 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1130409ms till timeout)
2022-03-28 18:59:18 [ForkJoinPool-1-worker-1] INFO  [SecretUtils:46] Waiting for Secret my-user-1297536277-778093599
2022-03-28 18:59:18 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Expected secret my-user-1297536277-778093599 exists
2022-03-28 18:59:18 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1128087ms till timeout)
2022-03-28 18:59:18 [ForkJoinPool-1-worker-1] INFO  [SecretUtils:50] Secret my-user-1297536277-778093599 created
2022-03-28 18:59:18 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:433] Wait for KafkaUser: my-user-1297536277-778093599 will have desired state: Ready
2022-03-28 18:59:18 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for KafkaUser: my-user-1297536277-778093599 will have desired state: Ready
2022-03-28 18:59:18 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1131706ms till timeout)
2022-03-28 18:59:18 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (105942ms till timeout)
2022-03-28 18:59:19 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1129387ms till timeout)
2022-03-28 18:59:19 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:444] KafkaUser: my-user-1297536277-778093599 is in desired state: Ready
2022-03-28 18:59:19 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1128533ms till timeout)
2022-03-28 18:59:19 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (459164ms till timeout)
2022-03-28 18:59:19 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (805484ms till timeout)
2022-03-28 18:59:19 [ForkJoinPool-1-worker-1] DEBUG [CompiledPath:93] Evaluating path: $['data']['password']
2022-03-28 18:59:19 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1126772ms till timeout)
2022-03-28 18:59:19 [ForkJoinPool-1-worker-1] DEBUG [CompiledPath:93] Evaluating path: $['metadata']['name']
2022-03-28 18:59:19 [ForkJoinPool-1-worker-1] DEBUG [CompiledPath:93] Evaluating path: $['metadata']['namespace']
2022-03-28 18:59:19 [ForkJoinPool-1-worker-1] DEBUG [CompiledPath:93] Evaluating path: $['spec']['authentication']['type']
2022-03-28 18:59:19 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (285970ms till timeout)
2022-03-28 18:59:19 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (284593ms till timeout)
2022-03-28 18:59:19 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1130184ms till timeout)
2022-03-28 18:59:19 [ForkJoinPool-1-worker-1] INFO  [KafkaUserUtils:62] Waiting for KafkaUser deletion my-user-1297536277-778093599
2022-03-28 18:59:19 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for KafkaUser deletion my-user-1297536277-778093599
2022-03-28 18:59:19 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (651604ms till timeout)
2022-03-28 18:59:19 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (471664ms till timeout)
2022-03-28 18:59:19 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1129256ms till timeout)
2022-03-28 18:59:19 [ForkJoinPool-1-worker-1] INFO  [KafkaUserUtils:75] KafkaUser my-user-1297536277-778093599 deleted
2022-03-28 18:59:19 [ForkJoinPool-1-worker-1] DEBUG [AbstractST:674] ============================================================================
2022-03-28 18:59:19 [ForkJoinPool-1-worker-1] DEBUG [AbstractST:675] [operators.user.UserST - After Each] - Clean up after test
2022-03-28 18:59:19 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:344] ############################################################################
2022-03-28 18:59:19 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:348] Delete all resources for testUpdateUser
2022-03-28 18:59:19 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:241] Delete of KafkaUser my-user-1297536277-778093599 in namespace user-st
2022-03-28 18:59:20 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1126925ms till timeout)
2022-03-28 18:59:20 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-1297536277-778093599
2022-03-28 18:59:20 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1130544ms till timeout)
2022-03-28 18:59:20 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (104771ms till timeout)
2022-03-28 18:59:20 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1128228ms till timeout)
2022-03-28 18:59:20 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1127379ms till timeout)
2022-03-28 18:59:20 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:369] ############################################################################
2022-03-28 18:59:20 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:267] testUpdateUser - Notifies waiting test cases:[testPauseReconciliationInKafkaRebalanceAndTopic, testPauseReconciliationInKafkaAndKafkaConnectWithConnector, testCapacityFile, testConfigurationFileIsCreated, testConfigurationPerformanceOptions, testConfigurationReflection, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testDeployAndUnDeployCruiseControl, testTlsExternalUserWithQuotas, testCreatingUsersWithSecretPrefix, testThrottlingQuotasCreateTopic, testThrottlingQuotasCreateAlterPartitions, testKafkaAdminTopicOperations, testUserTemplate, testUpdateUser] to and randomly select one to start execution
2022-03-28 18:59:20 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:93] [operators.user.UserST] - Removing parallel test: testUpdateUser
2022-03-28 18:59:20 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:97] [operators.user.UserST] - Parallel test count: 12
2022-03-28 18:59:20 [ForkJoinPool-1-worker-1] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.user.UserST.testUpdateUser-FINISHED
2022-03-28 18:59:20 [ForkJoinPool-1-worker-1] INFO  [TestSeparator:30] ############################################################################
2022-03-28 18:59:20 [ForkJoinPool-1-worker-1] INFO  [TestSeparator:23] ############################################################################
2022-03-28 18:59:20 [ForkJoinPool-1-worker-1] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.user.UserST.testTlsExternalUser-STARTED
2022-03-28 18:59:20 [ForkJoinPool-1-worker-1] DEBUG [AbstractST:658] ============================================================================
2022-03-28 18:59:20 [ForkJoinPool-1-worker-1] DEBUG [AbstractST:659] [operators.user.UserST - Before Each] - Setup test case environment
2022-03-28 18:59:20 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:77] [operators.user.UserST] - Adding parallel test: testTlsExternalUser
2022-03-28 18:59:20 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:81] [operators.user.UserST] - Parallel test count: 13
2022-03-28 18:59:20 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:230] testTlsExternalUser test now can proceed its execution
2022-03-28 18:59:20 [ForkJoinPool-1-worker-1] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 18:59:20 [ForkJoinPool-1-worker-1] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaAdminTopicOperations=my-cluster-506fda27, testConfigurationFileIsCreated=my-cluster-17fc585b, testThrottlingQuotasCreateAlterPartitions=my-cluster-ec1b34f8, testCreatingUsersWithSecretPrefix=my-cluster-f743fdfd, testUpdateUser=my-cluster-d582eb8c, testConfigurationReflection=my-cluster-188382d8, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-959c6ed5, testTlsExternalUserWithQuotas=my-cluster-100f5461, testDeployAndUnDeployCruiseControl=my-cluster-5dfd606f, testThrottlingQuotasCreateTopic=my-cluster-e9955717, testCapacityFile=my-cluster-855b9105, testConfigurationPerformanceOptions=my-cluster-08515847, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-afe273c9, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-e6343cec, testTlsExternalUser=my-cluster-45cf8ce4, testUserTemplate=my-cluster-927d5a1b}
2022-03-28 18:59:20 [ForkJoinPool-1-worker-1] TRACE [AbstractST:607] USERS_NAME_MAP: {testKafkaAdminTopicOperations=my-user-1780746402-846362125, testConfigurationFileIsCreated=my-user-355815510-1316268151, testThrottlingQuotasCreateAlterPartitions=my-user-333986137-1152182125, testCreatingUsersWithSecretPrefix=my-user-607975993-1210920737, testUpdateUser=my-user-1297536277-778093599, testConfigurationReflection=my-user-1890969183-1754686912, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-2104248162-45187051, testTlsExternalUserWithQuotas=my-user-1909583883-1106194424, testDeployAndUnDeployCruiseControl=my-user-506268651-1309117986, testThrottlingQuotasCreateTopic=my-user-1876060546-1887905718, testCapacityFile=my-user-1460643829-103298436, testConfigurationPerformanceOptions=my-user-98362545-1476587236, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-user-8456079-650483351, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-2069267102-581538560, testTlsExternalUser=my-user-399414674-1203409092, testUserTemplate=my-user-989633374-940717398}
2022-03-28 18:59:20 [ForkJoinPool-1-worker-1] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaAdminTopicOperations=my-topic-481231182-953607496, testConfigurationFileIsCreated=my-topic-423850169-855699454, testThrottlingQuotasCreateAlterPartitions=my-topic-325891726-169328137, testCreatingUsersWithSecretPrefix=my-topic-1727421070-668192795, testUpdateUser=my-topic-1325563869-1324403074, testConfigurationReflection=my-topic-1394577332-1936226438, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-634125597-1746493046, testTlsExternalUserWithQuotas=my-topic-1986059791-167638004, testDeployAndUnDeployCruiseControl=my-topic-465377127-800134460, testThrottlingQuotasCreateTopic=my-topic-1246597695-228473879, testCapacityFile=my-topic-549312506-649708453, testConfigurationPerformanceOptions=my-topic-696588651-1198599652, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-topic-1687496771-142091227, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-293775067-432059493, testTlsExternalUser=my-topic-707627084-257659651, testUserTemplate=my-topic-1542379258-2002911578}
2022-03-28 18:59:20 [ForkJoinPool-1-worker-1] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaAdminTopicOperations=my-cluster-506fda27-kafka-clients, testConfigurationFileIsCreated=my-cluster-17fc585b-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-ec1b34f8-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-f743fdfd-kafka-clients, testUpdateUser=my-cluster-d582eb8c-kafka-clients, testConfigurationReflection=my-cluster-188382d8-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-959c6ed5-kafka-clients, testTlsExternalUserWithQuotas=my-cluster-100f5461-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-5dfd606f-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-e9955717-kafka-clients, testCapacityFile=my-cluster-855b9105-kafka-clients, testConfigurationPerformanceOptions=my-cluster-08515847-kafka-clients, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-afe273c9-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-e6343cec-kafka-clients, testTlsExternalUser=my-cluster-45cf8ce4-kafka-clients, testUserTemplate=my-cluster-927d5a1b-kafka-clients}
2022-03-28 18:59:20 [ForkJoinPool-1-worker-1] INFO  [TestSuiteNamespaceManager:163] Creating namespace:namespace-9 for test case:testTlsExternalUser
2022-03-28 18:59:20 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (458010ms till timeout)
2022-03-28 18:59:20 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (804330ms till timeout)
2022-03-28 18:59:20 [ForkJoinPool-1-worker-1] INFO  [KubeClusterResource:156] Creating Namespace: namespace-9
2022-03-28 18:59:20 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1125616ms till timeout)
2022-03-28 18:59:20 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Namespace namespace-9
2022-03-28 18:59:20 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-9 -o json
2022-03-28 18:59:20 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (284803ms till timeout)
2022-03-28 18:59:20 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (283442ms till timeout)
2022-03-28 18:59:20 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1129018ms till timeout)
2022-03-28 18:59:20 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (650438ms till timeout)
2022-03-28 18:59:20 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (470498ms till timeout)
2022-03-28 18:59:21 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1128090ms till timeout)
2022-03-28 18:59:21 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1125766ms till timeout)
2022-03-28 18:59:21 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-9 -o json
2022-03-28 18:59:21 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 18:59:21 [ForkJoinPool-1-worker-1] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c42,c34",
            "openshift.io/sa.scc.supplemental-groups": "1001790000/10000",
            "openshift.io/sa.scc.uid-range": "1001790000/10000"
        },
        "creationTimestamp": "2022-03-28T18:59:16Z",
        "labels": {
            "kubernetes.io/metadata.name": "namespace-9"
        },
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T18:59:16Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:labels": {
                            ".": {},
                            "f:kubernetes.io/metadata.name": {}
                        }
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T18:59:16Z"
            }
        ],
        "name": "namespace-9",
        "resourceVersion": "264442",
        "uid": "ac146d65-3d14-4b49-8fcc-5f70b1556b79"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 18:59:21 [ForkJoinPool-1-worker-1] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[http-bridge-scram-sha-st], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[namespace-8], io.strimzi.test.logs.CollectorElement@c4c0ea0=[namespace-1], io.strimzi.test.logs.CollectorElement@aec142ef=[namespace-5], io.strimzi.test.logs.CollectorElement@24b97ba9=[namespace-7], io.strimzi.test.logs.CollectorElement@f851b6c3=[namespace-4], io.strimzi.test.logs.CollectorElement@3881d5f2=[namespace-3], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@5c7379cb=[namespace-9], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[namespace-0], io.strimzi.test.logs.CollectorElement@f4405b3b=[namespace-2], io.strimzi.test.logs.CollectorElement@3e255cd9=[namespace-6]}
2022-03-28 18:59:21 [ForkJoinPool-1-worker-1] INFO  [KubeClusterResource:82] Client use Namespace: namespace-9
2022-03-28 18:59:21 [ForkJoinPool-1-worker-1] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-9, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 18:59:21 [ForkJoinPool-1-worker-1] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-9
2022-03-28 18:59:21 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:155] Create/Update Kafka my-cluster-45cf8ce4 in namespace namespace-9
2022-03-28 18:59:21 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:164] Using Namespace: namespace-9
2022-03-28 18:59:21 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1129387ms till timeout)
2022-03-28 18:59:21 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (103623ms till timeout)
2022-03-28 18:59:21 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1127064ms till timeout)
2022-03-28 18:59:21 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-45cf8ce4
2022-03-28 18:59:21 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1126210ms till timeout)
2022-03-28 18:59:21 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (456837ms till timeout)
2022-03-28 18:59:21 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:433] Wait for Kafka: my-cluster-45cf8ce4 will have desired state: Ready
2022-03-28 18:59:21 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Kafka: my-cluster-45cf8ce4 will have desired state: Ready
2022-03-28 18:59:21 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (803157ms till timeout)
2022-03-28 18:59:21 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (839811ms till timeout)
2022-03-28 18:59:21 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1124444ms till timeout)
2022-03-28 18:59:21 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (283696ms till timeout)
2022-03-28 18:59:22 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1127912ms till timeout)
2022-03-28 18:59:22 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (282237ms till timeout)
2022-03-28 18:59:22 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (469389ms till timeout)
2022-03-28 18:59:22 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (649328ms till timeout)
2022-03-28 18:59:22 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1126980ms till timeout)
2022-03-28 18:59:22 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1124655ms till timeout)
2022-03-28 18:59:22 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1128275ms till timeout)
2022-03-28 18:59:22 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (102502ms till timeout)
2022-03-28 18:59:22 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1125954ms till timeout)
2022-03-28 18:59:22 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1125106ms till timeout)
2022-03-28 18:59:22 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (455736ms till timeout)
2022-03-28 18:59:22 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (802056ms till timeout)
2022-03-28 18:59:22 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (838710ms till timeout)
2022-03-28 18:59:22 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1123343ms till timeout)
2022-03-28 18:59:23 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (282592ms till timeout)
2022-03-28 18:59:23 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1126804ms till timeout)
2022-03-28 18:59:23 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (281133ms till timeout)
2022-03-28 18:59:23 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (468281ms till timeout)
2022-03-28 18:59:23 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (648219ms till timeout)
2022-03-28 18:59:23 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1125872ms till timeout)
2022-03-28 18:59:23 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1123549ms till timeout)
2022-03-28 18:59:23 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1127169ms till timeout)
2022-03-28 18:59:23 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1124850ms till timeout)
2022-03-28 18:59:23 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (101314ms till timeout)
2022-03-28 18:59:23 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1124000ms till timeout)
2022-03-28 18:59:23 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (454630ms till timeout)
2022-03-28 18:59:23 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (800949ms till timeout)
2022-03-28 18:59:23 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (837604ms till timeout)
2022-03-28 18:59:23 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1122236ms till timeout)
2022-03-28 18:59:24 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (281491ms till timeout)
2022-03-28 18:59:24 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1125704ms till timeout)
2022-03-28 18:59:24 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (647121ms till timeout)
2022-03-28 18:59:24 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (467179ms till timeout)
2022-03-28 18:59:24 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (279934ms till timeout)
2022-03-28 18:59:24 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1124771ms till timeout)
2022-03-28 18:59:24 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1122447ms till timeout)
2022-03-28 18:59:24 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1126068ms till timeout)
2022-03-28 18:59:24 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1123748ms till timeout)
2022-03-28 18:59:24 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1122896ms till timeout)
2022-03-28 18:59:24 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (100116ms till timeout)
2022-03-28 18:59:24 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (453527ms till timeout)
2022-03-28 18:59:24 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (799846ms till timeout)
2022-03-28 18:59:25 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (836499ms till timeout)
2022-03-28 18:59:25 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1121133ms till timeout)
2022-03-28 18:59:25 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (280379ms till timeout)
2022-03-28 18:59:25 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1124590ms till timeout)
2022-03-28 18:59:25 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (466064ms till timeout)
2022-03-28 18:59:25 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (646002ms till timeout)
2022-03-28 18:59:25 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (278810ms till timeout)
2022-03-28 18:59:25 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1123657ms till timeout)
2022-03-28 18:59:25 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1121334ms till timeout)
2022-03-28 18:59:25 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1124953ms till timeout)
2022-03-28 18:59:25 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1122633ms till timeout)
2022-03-28 18:59:25 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1121783ms till timeout)
2022-03-28 18:59:25 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (99002ms till timeout)
2022-03-28 18:59:25 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (452413ms till timeout)
2022-03-28 18:59:26 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (798733ms till timeout)
2022-03-28 18:59:26 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1120021ms till timeout)
2022-03-28 18:59:26 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (835387ms till timeout)
2022-03-28 18:59:26 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (279278ms till timeout)
2022-03-28 18:59:26 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1123490ms till timeout)
2022-03-28 18:59:26 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (464966ms till timeout)
2022-03-28 18:59:26 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (644809ms till timeout)
2022-03-28 18:59:26 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1122555ms till timeout)
2022-03-28 18:59:26 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (277628ms till timeout)
2022-03-28 18:59:26 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1120232ms till timeout)
2022-03-28 18:59:26 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1123853ms till timeout)
2022-03-28 18:59:26 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1121533ms till timeout)
2022-03-28 18:59:27 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1120684ms till timeout)
2022-03-28 18:59:27 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (451314ms till timeout)
2022-03-28 18:59:27 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (97802ms till timeout)
2022-03-28 18:59:27 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (797633ms till timeout)
2022-03-28 18:59:27 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1118921ms till timeout)
2022-03-28 18:59:27 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (834287ms till timeout)
2022-03-28 18:59:27 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (278175ms till timeout)
2022-03-28 18:59:27 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1122389ms till timeout)
2022-03-28 18:59:27 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (463866ms till timeout)
2022-03-28 18:59:27 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (643711ms till timeout)
2022-03-28 18:59:27 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1119132ms till timeout)
2022-03-28 18:59:27 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1121360ms till timeout)
2022-03-28 18:59:27 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (276420ms till timeout)
2022-03-28 18:59:27 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1122751ms till timeout)
2022-03-28 18:59:28 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1120433ms till timeout)
2022-03-28 18:59:28 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1119583ms till timeout)
2022-03-28 18:59:28 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (450213ms till timeout)
2022-03-28 18:59:28 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (796530ms till timeout)
2022-03-28 18:59:28 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (96607ms till timeout)
2022-03-28 18:59:28 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (833169ms till timeout)
2022-03-28 18:59:28 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1117786ms till timeout)
2022-03-28 18:59:28 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (277061ms till timeout)
2022-03-28 18:59:28 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1121262ms till timeout)
2022-03-28 18:59:28 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (462738ms till timeout)
2022-03-28 18:59:28 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (642583ms till timeout)
2022-03-28 18:59:28 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1120233ms till timeout)
2022-03-28 18:59:28 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1118004ms till timeout)
2022-03-28 18:59:28 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (275273ms till timeout)
2022-03-28 18:59:29 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1121625ms till timeout)
2022-03-28 18:59:29 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1119305ms till timeout)
2022-03-28 18:59:29 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1118455ms till timeout)
2022-03-28 18:59:29 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (449086ms till timeout)
2022-03-28 18:59:29 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (795405ms till timeout)
2022-03-28 18:59:29 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (95477ms till timeout)
2022-03-28 18:59:29 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (832059ms till timeout)
2022-03-28 18:59:29 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1116598ms till timeout)
2022-03-28 18:59:29 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (275901ms till timeout)
2022-03-28 18:59:29 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1120114ms till timeout)
2022-03-28 18:59:29 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (461589ms till timeout)
2022-03-28 18:59:29 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (641434ms till timeout)
2022-03-28 18:59:30 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1119078ms till timeout)
2022-03-28 18:59:30 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1116850ms till timeout)
2022-03-28 18:59:30 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (274153ms till timeout)
2022-03-28 18:59:30 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1120476ms till timeout)
2022-03-28 18:59:30 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1118157ms till timeout)
2022-03-28 18:59:30 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1117291ms till timeout)
2022-03-28 18:59:30 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (447921ms till timeout)
2022-03-28 18:59:30 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (794234ms till timeout)
2022-03-28 18:59:30 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (94311ms till timeout)
2022-03-28 18:59:30 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (830880ms till timeout)
2022-03-28 18:59:30 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1115414ms till timeout)
2022-03-28 18:59:30 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (274709ms till timeout)
2022-03-28 18:59:30 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1118930ms till timeout)
2022-03-28 18:59:31 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (460407ms till timeout)
2022-03-28 18:59:31 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (640250ms till timeout)
2022-03-28 18:59:31 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1117901ms till timeout)
2022-03-28 18:59:31 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1115673ms till timeout)
2022-03-28 18:59:31 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (272971ms till timeout)
2022-03-28 18:59:31 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1119291ms till timeout)
2022-03-28 18:59:31 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1116972ms till timeout)
2022-03-28 18:59:31 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1116122ms till timeout)
2022-03-28 18:59:31 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (446751ms till timeout)
2022-03-28 18:59:31 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (793071ms till timeout)
2022-03-28 18:59:31 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (93150ms till timeout)
2022-03-28 18:59:31 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (829725ms till timeout)
2022-03-28 18:59:31 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1114263ms till timeout)
2022-03-28 18:59:32 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (273563ms till timeout)
2022-03-28 18:59:32 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1117778ms till timeout)
2022-03-28 18:59:32 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (459255ms till timeout)
2022-03-28 18:59:32 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (639099ms till timeout)
2022-03-28 18:59:32 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1116750ms till timeout)
2022-03-28 18:59:32 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1114521ms till timeout)
2022-03-28 18:59:32 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (271818ms till timeout)
2022-03-28 18:59:32 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1118142ms till timeout)
2022-03-28 18:59:32 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1115822ms till timeout)
2022-03-28 18:59:32 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1114973ms till timeout)
2022-03-28 18:59:32 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (445603ms till timeout)
2022-03-28 18:59:32 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (791923ms till timeout)
2022-03-28 18:59:32 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (92000ms till timeout)
2022-03-28 18:59:32 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (828576ms till timeout)
2022-03-28 18:59:33 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1113114ms till timeout)
2022-03-28 18:59:33 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (272411ms till timeout)
2022-03-28 18:59:33 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1116628ms till timeout)
2022-03-28 18:59:33 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (458104ms till timeout)
2022-03-28 18:59:33 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (637949ms till timeout)
2022-03-28 18:59:33 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1115599ms till timeout)
2022-03-28 18:59:33 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1113371ms till timeout)
2022-03-28 18:59:33 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (270646ms till timeout)
2022-03-28 18:59:33 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1116991ms till timeout)
2022-03-28 18:59:33 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1114671ms till timeout)
2022-03-28 18:59:33 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1113822ms till timeout)
2022-03-28 18:59:33 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (444450ms till timeout)
2022-03-28 18:59:34 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (790769ms till timeout)
2022-03-28 18:59:34 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (90838ms till timeout)
2022-03-28 18:59:34 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (827423ms till timeout)
2022-03-28 18:59:34 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1111961ms till timeout)
2022-03-28 18:59:34 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (271257ms till timeout)
2022-03-28 18:59:34 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1115475ms till timeout)
2022-03-28 18:59:34 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (456952ms till timeout)
2022-03-28 18:59:34 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (636796ms till timeout)
2022-03-28 18:59:34 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1114447ms till timeout)
2022-03-28 18:59:34 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1112218ms till timeout)
2022-03-28 18:59:34 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (269511ms till timeout)
2022-03-28 18:59:34 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1115839ms till timeout)
2022-03-28 18:59:34 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1113520ms till timeout)
2022-03-28 18:59:35 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1112671ms till timeout)
2022-03-28 18:59:35 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (443300ms till timeout)
2022-03-28 18:59:35 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (789619ms till timeout)
2022-03-28 18:59:35 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (89694ms till timeout)
2022-03-28 18:59:35 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (826272ms till timeout)
2022-03-28 18:59:35 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1110811ms till timeout)
2022-03-28 18:59:35 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (270115ms till timeout)
2022-03-28 18:59:35 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1114322ms till timeout)
2022-03-28 18:59:35 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (455797ms till timeout)
2022-03-28 18:59:35 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (635641ms till timeout)
2022-03-28 18:59:35 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1113291ms till timeout)
2022-03-28 18:59:35 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1111062ms till timeout)
2022-03-28 18:59:35 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (268357ms till timeout)
2022-03-28 18:59:35 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1114682ms till timeout)
2022-03-28 18:59:36 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1112364ms till timeout)
2022-03-28 18:59:36 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1111514ms till timeout)
2022-03-28 18:59:36 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (442145ms till timeout)
2022-03-28 18:59:36 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (788461ms till timeout)
2022-03-28 18:59:36 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (88540ms till timeout)
2022-03-28 18:59:36 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (825115ms till timeout)
2022-03-28 18:59:36 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1109654ms till timeout)
2022-03-28 18:59:36 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (268954ms till timeout)
2022-03-28 18:59:36 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1113166ms till timeout)
2022-03-28 18:59:36 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (454642ms till timeout)
2022-03-28 18:59:36 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (634486ms till timeout)
2022-03-28 18:59:37 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1109902ms till timeout)
2022-03-28 18:59:37 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1112130ms till timeout)
2022-03-28 18:59:37 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (267190ms till timeout)
2022-03-28 18:59:37 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1113518ms till timeout)
2022-03-28 18:59:37 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1111198ms till timeout)
2022-03-28 18:59:37 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1110349ms till timeout)
2022-03-28 18:59:37 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (440978ms till timeout)
2022-03-28 18:59:37 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (787298ms till timeout)
2022-03-28 18:59:37 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (87369ms till timeout)
2022-03-28 18:59:37 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (823952ms till timeout)
2022-03-28 18:59:37 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1108490ms till timeout)
2022-03-28 18:59:37 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (267774ms till timeout)
2022-03-28 18:59:37 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1112000ms till timeout)
2022-03-28 18:59:38 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (453476ms till timeout)
2022-03-28 18:59:38 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (633322ms till timeout)
2022-03-28 18:59:38 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1110972ms till timeout)
2022-03-28 18:59:38 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1108744ms till timeout)
2022-03-28 18:59:38 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (266035ms till timeout)
2022-03-28 18:59:38 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1112365ms till timeout)
2022-03-28 18:59:38 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1110045ms till timeout)
2022-03-28 18:59:38 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1109196ms till timeout)
2022-03-28 18:59:38 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (439826ms till timeout)
2022-03-28 18:59:38 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (786145ms till timeout)
2022-03-28 18:59:38 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (86225ms till timeout)
2022-03-28 18:59:38 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (822799ms till timeout)
2022-03-28 18:59:38 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1107337ms till timeout)
2022-03-28 18:59:38 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (266642ms till timeout)
2022-03-28 18:59:39 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1110855ms till timeout)
2022-03-28 18:59:39 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (452332ms till timeout)
2022-03-28 18:59:39 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (632176ms till timeout)
2022-03-28 18:59:39 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1107598ms till timeout)
2022-03-28 18:59:39 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1109826ms till timeout)
2022-03-28 18:59:39 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (264896ms till timeout)
2022-03-28 18:59:39 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1111217ms till timeout)
2022-03-28 18:59:39 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1108898ms till timeout)
2022-03-28 18:59:39 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1108049ms till timeout)
2022-03-28 18:59:39 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (438679ms till timeout)
2022-03-28 18:59:39 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (784998ms till timeout)
2022-03-28 18:59:39 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (85076ms till timeout)
2022-03-28 18:59:39 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (821652ms till timeout)
2022-03-28 18:59:40 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1106190ms till timeout)
2022-03-28 18:59:40 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (265487ms till timeout)
2022-03-28 18:59:40 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1109706ms till timeout)
2022-03-28 18:59:40 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (451183ms till timeout)
2022-03-28 18:59:40 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (631028ms till timeout)
2022-03-28 18:59:40 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1108679ms till timeout)
2022-03-28 18:59:40 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1106449ms till timeout)
2022-03-28 18:59:40 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (263741ms till timeout)
2022-03-28 18:59:40 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1110070ms till timeout)
2022-03-28 18:59:40 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1107750ms till timeout)
2022-03-28 18:59:40 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1106901ms till timeout)
2022-03-28 18:59:40 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:444] KafkaBridge: http-bridge-tls-cluster-name is in desired state: Ready
2022-03-28 18:59:40 [ForkJoinPool-1-worker-23] INFO  [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-03-28 18:59:40 [ForkJoinPool-1-worker-23] INFO  [TestSeparator:23] ############################################################################
2022-03-28 18:59:40 [ForkJoinPool-1-worker-23] INFO  [TestSeparator:24] io.strimzi.systemtest.bridge.HttpBridgeTlsST.testSendSimpleMessageTls-STARTED
2022-03-28 18:59:40 [ForkJoinPool-1-worker-23] DEBUG [AbstractST:658] ============================================================================
2022-03-28 18:59:40 [ForkJoinPool-1-worker-23] DEBUG [AbstractST:659] [bridge.HttpBridgeTlsST - Before Each] - Setup test case environment
2022-03-28 18:59:40 [ForkJoinPool-1-worker-23] DEBUG [SuiteThreadController:77] [bridge.HttpBridgeTlsST] - Adding parallel test: testSendSimpleMessageTls
2022-03-28 18:59:40 [ForkJoinPool-1-worker-23] DEBUG [SuiteThreadController:81] [bridge.HttpBridgeTlsST] - Parallel test count: 14
2022-03-28 18:59:40 [ForkJoinPool-1-worker-23] DEBUG [SuiteThreadController:230] testSendSimpleMessageTls test now can proceed its execution
2022-03-28 18:59:40 [ForkJoinPool-1-worker-23] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 18:59:40 [ForkJoinPool-1-worker-23] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaAdminTopicOperations=my-cluster-506fda27, testConfigurationFileIsCreated=my-cluster-17fc585b, testThrottlingQuotasCreateAlterPartitions=my-cluster-ec1b34f8, testSendSimpleMessageTls=my-cluster-89673e85, testCreatingUsersWithSecretPrefix=my-cluster-f743fdfd, testUpdateUser=my-cluster-d582eb8c, testConfigurationReflection=my-cluster-188382d8, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-959c6ed5, testTlsExternalUserWithQuotas=my-cluster-100f5461, testDeployAndUnDeployCruiseControl=my-cluster-5dfd606f, testThrottlingQuotasCreateTopic=my-cluster-e9955717, testCapacityFile=my-cluster-855b9105, testConfigurationPerformanceOptions=my-cluster-08515847, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-afe273c9, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-e6343cec, testTlsExternalUser=my-cluster-45cf8ce4, testUserTemplate=my-cluster-927d5a1b}
2022-03-28 18:59:40 [ForkJoinPool-1-worker-23] TRACE [AbstractST:607] USERS_NAME_MAP: {testKafkaAdminTopicOperations=my-user-1780746402-846362125, testConfigurationFileIsCreated=my-user-355815510-1316268151, testThrottlingQuotasCreateAlterPartitions=my-user-333986137-1152182125, testSendSimpleMessageTls=my-user-1887866274-1013125243, testCreatingUsersWithSecretPrefix=my-user-607975993-1210920737, testUpdateUser=my-user-1297536277-778093599, testConfigurationReflection=my-user-1890969183-1754686912, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-2104248162-45187051, testTlsExternalUserWithQuotas=my-user-1909583883-1106194424, testDeployAndUnDeployCruiseControl=my-user-506268651-1309117986, testThrottlingQuotasCreateTopic=my-user-1876060546-1887905718, testCapacityFile=my-user-1460643829-103298436, testConfigurationPerformanceOptions=my-user-98362545-1476587236, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-user-8456079-650483351, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-2069267102-581538560, testTlsExternalUser=my-user-399414674-1203409092, testUserTemplate=my-user-989633374-940717398}
2022-03-28 18:59:40 [ForkJoinPool-1-worker-23] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaAdminTopicOperations=my-topic-481231182-953607496, testConfigurationFileIsCreated=my-topic-423850169-855699454, testThrottlingQuotasCreateAlterPartitions=my-topic-325891726-169328137, testSendSimpleMessageTls=my-topic-874607937-87907051, testCreatingUsersWithSecretPrefix=my-topic-1727421070-668192795, testUpdateUser=my-topic-1325563869-1324403074, testConfigurationReflection=my-topic-1394577332-1936226438, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-634125597-1746493046, testTlsExternalUserWithQuotas=my-topic-1986059791-167638004, testDeployAndUnDeployCruiseControl=my-topic-465377127-800134460, testThrottlingQuotasCreateTopic=my-topic-1246597695-228473879, testCapacityFile=my-topic-549312506-649708453, testConfigurationPerformanceOptions=my-topic-696588651-1198599652, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-topic-1687496771-142091227, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-293775067-432059493, testTlsExternalUser=my-topic-707627084-257659651, testUserTemplate=my-topic-1542379258-2002911578}
2022-03-28 18:59:40 [ForkJoinPool-1-worker-23] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaAdminTopicOperations=my-cluster-506fda27-kafka-clients, testConfigurationFileIsCreated=my-cluster-17fc585b-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-ec1b34f8-kafka-clients, testSendSimpleMessageTls=my-cluster-89673e85-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-f743fdfd-kafka-clients, testUpdateUser=my-cluster-d582eb8c-kafka-clients, testConfigurationReflection=my-cluster-188382d8-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-959c6ed5-kafka-clients, testTlsExternalUserWithQuotas=my-cluster-100f5461-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-5dfd606f-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-e9955717-kafka-clients, testCapacityFile=my-cluster-855b9105-kafka-clients, testConfigurationPerformanceOptions=my-cluster-08515847-kafka-clients, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-afe273c9-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-e6343cec-kafka-clients, testTlsExternalUser=my-cluster-45cf8ce4-kafka-clients, testUserTemplate=my-cluster-927d5a1b-kafka-clients}
2022-03-28 18:59:40 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:155] Create/Update KafkaTopic my-topic-205794532-362905525 in namespace http-bridge-tls-st
2022-03-28 18:59:40 [ForkJoinPool-1-worker-23] WARN  [VersionUsageUtils:60] The client is using resource type 'kafkatopics' with unstable version 'v1beta2'
2022-03-28 18:59:40 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (783853ms till timeout)
2022-03-28 18:59:40 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (83932ms till timeout)
2022-03-28 18:59:41 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (820506ms till timeout)
2022-03-28 18:59:41 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-205794532-362905525
2022-03-28 18:59:41 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1105047ms till timeout)
2022-03-28 18:59:41 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:433] Wait for KafkaTopic: my-topic-205794532-362905525 will have desired state: Ready
2022-03-28 18:59:41 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for KafkaTopic: my-topic-205794532-362905525 will have desired state: Ready
2022-03-28 18:59:41 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (264350ms till timeout)
2022-03-28 18:59:41 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1108566ms till timeout)
2022-03-28 18:59:41 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] KafkaTopic: my-topic-205794532-362905525 will have desired state: Ready not ready, will try again in 1000 ms (179810ms till timeout)
2022-03-28 18:59:41 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (450042ms till timeout)
2022-03-28 18:59:41 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (629888ms till timeout)
2022-03-28 18:59:41 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1107539ms till timeout)
2022-03-28 18:59:41 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1105310ms till timeout)
2022-03-28 18:59:41 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (262600ms till timeout)
2022-03-28 18:59:41 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1108931ms till timeout)
2022-03-28 18:59:41 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1106612ms till timeout)
2022-03-28 18:59:41 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1105762ms till timeout)
2022-03-28 18:59:42 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (782757ms till timeout)
2022-03-28 18:59:42 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (82738ms till timeout)
2022-03-28 18:59:42 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1103946ms till timeout)
2022-03-28 18:59:42 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (819312ms till timeout)
2022-03-28 18:59:42 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (263250ms till timeout)
2022-03-28 18:59:42 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1107463ms till timeout)
2022-03-28 18:59:42 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (448940ms till timeout)
2022-03-28 18:59:42 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:444] KafkaTopic: my-topic-205794532-362905525 is in desired state: Ready
2022-03-28 18:59:42 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:155] Create/Update Job producer-1817671133 in namespace http-bridge-tls-st
2022-03-28 18:59:42 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (628785ms till timeout)
2022-03-28 18:59:42 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1106435ms till timeout)
2022-03-28 18:59:42 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1104207ms till timeout)
2022-03-28 18:59:42 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:producer-1817671133
2022-03-28 18:59:42 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1107829ms till timeout)
2022-03-28 18:59:42 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (261412ms till timeout)
2022-03-28 18:59:42 [ForkJoinPool-1-worker-23] INFO  [JobUtils:81] Waiting for job: producer-1817671133 will be in active state
2022-03-28 18:59:42 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 18:59:42 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1105509ms till timeout)
2022-03-28 18:59:43 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1104656ms till timeout)
2022-03-28 18:59:43 [ForkJoinPool-1-worker-23] INFO  [ClientUtils:76] Waiting for producer/consumer:producer-1817671133 to finished
2022-03-28 18:59:43 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for job finished
2022-03-28 18:59:43 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (781601ms till timeout)
2022-03-28 18:59:43 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job producer-1817671133 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T18:59:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 18:59:43 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (81583ms till timeout)
2022-03-28 18:59:43 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1102794ms till timeout)
2022-03-28 18:59:43 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (818160ms till timeout)
2022-03-28 18:59:43 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (219619ms till timeout)
2022-03-28 18:59:43 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (262098ms till timeout)
2022-03-28 18:59:43 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1106314ms till timeout)
2022-03-28 18:59:43 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (447791ms till timeout)
2022-03-28 18:59:43 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (627634ms till timeout)
2022-03-28 18:59:43 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1105285ms till timeout)
2022-03-28 18:59:43 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1103057ms till timeout)
2022-03-28 18:59:43 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1106677ms till timeout)
2022-03-28 18:59:43 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (260261ms till timeout)
2022-03-28 18:59:44 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1104356ms till timeout)
2022-03-28 18:59:44 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1103507ms till timeout)
2022-03-28 18:59:44 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (780502ms till timeout)
2022-03-28 18:59:44 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (80477ms till timeout)
2022-03-28 18:59:44 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (817049ms till timeout)
2022-03-28 18:59:44 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1101683ms till timeout)
2022-03-28 18:59:44 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job producer-1817671133 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T18:59:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 18:59:44 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (260985ms till timeout)
2022-03-28 18:59:44 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1105202ms till timeout)
2022-03-28 18:59:44 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (446680ms till timeout)
2022-03-28 18:59:44 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (218316ms till timeout)
2022-03-28 18:59:44 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (626524ms till timeout)
2022-03-28 18:59:45 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1101946ms till timeout)
2022-03-28 18:59:45 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1104174ms till timeout)
2022-03-28 18:59:45 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1105551ms till timeout)
2022-03-28 18:59:45 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (259127ms till timeout)
2022-03-28 18:59:45 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1103212ms till timeout)
2022-03-28 18:59:45 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1102363ms till timeout)
2022-03-28 18:59:45 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (779405ms till timeout)
2022-03-28 18:59:45 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (79374ms till timeout)
2022-03-28 18:59:45 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1100581ms till timeout)
2022-03-28 18:59:45 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (815947ms till timeout)
2022-03-28 18:59:45 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (259879ms till timeout)
2022-03-28 18:59:45 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1104094ms till timeout)
2022-03-28 18:59:45 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (445569ms till timeout)
2022-03-28 18:59:45 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job producer-1817671133 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T18:59:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 18:59:46 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (625414ms till timeout)
2022-03-28 18:59:46 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1100837ms till timeout)
2022-03-28 18:59:46 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (217017ms till timeout)
2022-03-28 18:59:46 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1103064ms till timeout)
2022-03-28 18:59:46 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1104324ms till timeout)
2022-03-28 18:59:46 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1102100ms till timeout)
2022-03-28 18:59:46 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (257906ms till timeout)
2022-03-28 18:59:46 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1101255ms till timeout)
2022-03-28 18:59:46 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (778300ms till timeout)
2022-03-28 18:59:46 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1099454ms till timeout)
2022-03-28 18:59:46 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (814820ms till timeout)
2022-03-28 18:59:46 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (78133ms till timeout)
2022-03-28 18:59:46 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (258759ms till timeout)
2022-03-28 18:59:46 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1102972ms till timeout)
2022-03-28 18:59:47 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (444448ms till timeout)
2022-03-28 18:59:47 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (624287ms till timeout)
2022-03-28 18:59:47 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1101938ms till timeout)
2022-03-28 18:59:47 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1099709ms till timeout)
2022-03-28 18:59:47 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job producer-1817671133 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T18:59:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 18:59:47 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (215792ms till timeout)
2022-03-28 18:59:47 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1101002ms till timeout)
2022-03-28 18:59:47 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1103226ms till timeout)
2022-03-28 18:59:47 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1100152ms till timeout)
2022-03-28 18:59:47 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (256679ms till timeout)
2022-03-28 18:59:47 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (777196ms till timeout)
2022-03-28 18:59:47 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1098352ms till timeout)
2022-03-28 18:59:47 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (813718ms till timeout)
2022-03-28 18:59:47 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (76945ms till timeout)
2022-03-28 18:59:47 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (257646ms till timeout)
2022-03-28 18:59:48 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1101865ms till timeout)
2022-03-28 18:59:48 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (443342ms till timeout)
2022-03-28 18:59:48 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (623181ms till timeout)
2022-03-28 18:59:48 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1100832ms till timeout)
2022-03-28 18:59:48 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1098604ms till timeout)
2022-03-28 18:59:48 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job producer-1817671133 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T18:59:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 18:59:48 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1099904ms till timeout)
2022-03-28 18:59:48 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (214498ms till timeout)
2022-03-28 18:59:48 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1102033ms till timeout)
2022-03-28 18:59:48 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1098960ms till timeout)
2022-03-28 18:59:48 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (255510ms till timeout)
2022-03-28 18:59:48 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (776005ms till timeout)
2022-03-28 18:59:49 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1097158ms till timeout)
2022-03-28 18:59:49 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (812524ms till timeout)
2022-03-28 18:59:49 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (75761ms till timeout)
2022-03-28 18:59:49 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (256463ms till timeout)
2022-03-28 18:59:49 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1100679ms till timeout)
2022-03-28 18:59:49 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (442156ms till timeout)
2022-03-28 18:59:49 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (622001ms till timeout)
2022-03-28 18:59:49 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1099652ms till timeout)
2022-03-28 18:59:49 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1097424ms till timeout)
2022-03-28 18:59:49 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1098808ms till timeout)
2022-03-28 18:59:49 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job producer-1817671133 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T18:59:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 18:59:49 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1100934ms till timeout)
2022-03-28 18:59:49 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1097862ms till timeout)
2022-03-28 18:59:49 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (213211ms till timeout)
2022-03-28 18:59:49 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (774907ms till timeout)
2022-03-28 18:59:49 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (254329ms till timeout)
2022-03-28 18:59:50 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1096062ms till timeout)
2022-03-28 18:59:50 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (811336ms till timeout)
2022-03-28 18:59:50 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (255256ms till timeout)
2022-03-28 18:59:50 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (74542ms till timeout)
2022-03-28 18:59:50 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (441043ms till timeout)
2022-03-28 18:59:50 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1099470ms till timeout)
2022-03-28 18:59:50 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (620887ms till timeout)
2022-03-28 18:59:50 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1096310ms till timeout)
2022-03-28 18:59:50 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1098538ms till timeout)
2022-03-28 18:59:50 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1097705ms till timeout)
2022-03-28 18:59:50 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1099834ms till timeout)
2022-03-28 18:59:50 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1096760ms till timeout)
2022-03-28 18:59:51 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (773805ms till timeout)
2022-03-28 18:59:51 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job producer-1817671133 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T18:59:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 18:59:51 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (253118ms till timeout)
2022-03-28 18:59:51 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (211908ms till timeout)
2022-03-28 18:59:51 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1094892ms till timeout)
2022-03-28 18:59:51 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (810161ms till timeout)
2022-03-28 18:59:51 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (254082ms till timeout)
2022-03-28 18:59:51 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (73373ms till timeout)
2022-03-28 18:59:51 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (439867ms till timeout)
2022-03-28 18:59:51 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1098294ms till timeout)
2022-03-28 18:59:51 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (619712ms till timeout)
2022-03-28 18:59:51 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1095134ms till timeout)
2022-03-28 18:59:51 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1097360ms till timeout)
2022-03-28 18:59:51 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1096529ms till timeout)
2022-03-28 18:59:52 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1098658ms till timeout)
2022-03-28 18:59:52 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1095585ms till timeout)
2022-03-28 18:59:52 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (772627ms till timeout)
2022-03-28 18:59:52 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (251948ms till timeout)
2022-03-28 18:59:52 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job producer-1817671133 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T18:59:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 18:59:52 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1093720ms till timeout)
2022-03-28 18:59:52 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (808990ms till timeout)
2022-03-28 18:59:52 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (210543ms till timeout)
2022-03-28 18:59:52 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (72210ms till timeout)
2022-03-28 18:59:52 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (252912ms till timeout)
2022-03-28 18:59:52 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1097138ms till timeout)
2022-03-28 18:59:52 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (438710ms till timeout)
2022-03-28 18:59:52 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (618553ms till timeout)
2022-03-28 18:59:52 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1096205ms till timeout)
2022-03-28 18:59:52 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1093977ms till timeout)
2022-03-28 18:59:53 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1095372ms till timeout)
2022-03-28 18:59:53 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1097501ms till timeout)
2022-03-28 18:59:53 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1094428ms till timeout)
2022-03-28 18:59:53 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (771474ms till timeout)
2022-03-28 18:59:53 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (250800ms till timeout)
2022-03-28 18:59:53 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1092622ms till timeout)
2022-03-28 18:59:53 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (807893ms till timeout)
2022-03-28 18:59:53 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job producer-1817671133 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T18:59:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 18:59:53 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (251718ms till timeout)
2022-03-28 18:59:53 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (71013ms till timeout)
2022-03-28 18:59:53 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (209152ms till timeout)
2022-03-28 18:59:53 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1095940ms till timeout)
2022-03-28 18:59:53 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:444] KafkaBridge: http-bridge-scram-sha-cluster-name is in desired state: Ready
2022-03-28 18:59:53 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (617451ms till timeout)
2022-03-28 18:59:53 [ForkJoinPool-1-worker-5] INFO  [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-03-28 18:59:53 [ForkJoinPool-1-worker-5] INFO  [TestSeparator:23] ############################################################################
2022-03-28 18:59:53 [ForkJoinPool-1-worker-5] INFO  [TestSeparator:24] io.strimzi.systemtest.bridge.HttpBridgeScramShaST.testReceiveSimpleMessageTlsScramSha-STARTED
2022-03-28 18:59:53 [ForkJoinPool-1-worker-5] DEBUG [AbstractST:658] ============================================================================
2022-03-28 18:59:53 [ForkJoinPool-1-worker-5] DEBUG [AbstractST:659] [bridge.HttpBridgeScramShaST - Before Each] - Setup test case environment
2022-03-28 18:59:53 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:77] [bridge.HttpBridgeScramShaST] - Adding parallel test: testReceiveSimpleMessageTlsScramSha
2022-03-28 18:59:53 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:81] [bridge.HttpBridgeScramShaST] - Parallel test count: 15
2022-03-28 18:59:53 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:230] testReceiveSimpleMessageTlsScramSha test now can proceed its execution
2022-03-28 18:59:53 [ForkJoinPool-1-worker-5] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 18:59:53 [ForkJoinPool-1-worker-5] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaAdminTopicOperations=my-cluster-506fda27, testReceiveSimpleMessageTlsScramSha=my-cluster-abc5d34b, testConfigurationFileIsCreated=my-cluster-17fc585b, testThrottlingQuotasCreateAlterPartitions=my-cluster-ec1b34f8, testSendSimpleMessageTls=my-cluster-89673e85, testCreatingUsersWithSecretPrefix=my-cluster-f743fdfd, testUpdateUser=my-cluster-d582eb8c, testConfigurationReflection=my-cluster-188382d8, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-959c6ed5, testTlsExternalUserWithQuotas=my-cluster-100f5461, testDeployAndUnDeployCruiseControl=my-cluster-5dfd606f, testThrottlingQuotasCreateTopic=my-cluster-e9955717, testCapacityFile=my-cluster-855b9105, testConfigurationPerformanceOptions=my-cluster-08515847, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-afe273c9, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-e6343cec, testTlsExternalUser=my-cluster-45cf8ce4, testUserTemplate=my-cluster-927d5a1b}
2022-03-28 18:59:53 [ForkJoinPool-1-worker-5] TRACE [AbstractST:607] USERS_NAME_MAP: {testKafkaAdminTopicOperations=my-user-1780746402-846362125, testReceiveSimpleMessageTlsScramSha=my-user-330562220-1155646423, testConfigurationFileIsCreated=my-user-355815510-1316268151, testThrottlingQuotasCreateAlterPartitions=my-user-333986137-1152182125, testSendSimpleMessageTls=my-user-1887866274-1013125243, testCreatingUsersWithSecretPrefix=my-user-607975993-1210920737, testUpdateUser=my-user-1297536277-778093599, testConfigurationReflection=my-user-1890969183-1754686912, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-2104248162-45187051, testTlsExternalUserWithQuotas=my-user-1909583883-1106194424, testDeployAndUnDeployCruiseControl=my-user-506268651-1309117986, testThrottlingQuotasCreateTopic=my-user-1876060546-1887905718, testCapacityFile=my-user-1460643829-103298436, testConfigurationPerformanceOptions=my-user-98362545-1476587236, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-user-8456079-650483351, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-2069267102-581538560, testTlsExternalUser=my-user-399414674-1203409092, testUserTemplate=my-user-989633374-940717398}
2022-03-28 18:59:53 [ForkJoinPool-1-worker-5] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaAdminTopicOperations=my-topic-481231182-953607496, testReceiveSimpleMessageTlsScramSha=my-topic-1027850608-734754781, testConfigurationFileIsCreated=my-topic-423850169-855699454, testThrottlingQuotasCreateAlterPartitions=my-topic-325891726-169328137, testSendSimpleMessageTls=my-topic-874607937-87907051, testCreatingUsersWithSecretPrefix=my-topic-1727421070-668192795, testUpdateUser=my-topic-1325563869-1324403074, testConfigurationReflection=my-topic-1394577332-1936226438, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-634125597-1746493046, testTlsExternalUserWithQuotas=my-topic-1986059791-167638004, testDeployAndUnDeployCruiseControl=my-topic-465377127-800134460, testThrottlingQuotasCreateTopic=my-topic-1246597695-228473879, testCapacityFile=my-topic-549312506-649708453, testConfigurationPerformanceOptions=my-topic-696588651-1198599652, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-topic-1687496771-142091227, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-293775067-432059493, testTlsExternalUser=my-topic-707627084-257659651, testUserTemplate=my-topic-1542379258-2002911578}
2022-03-28 18:59:53 [ForkJoinPool-1-worker-5] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaAdminTopicOperations=my-cluster-506fda27-kafka-clients, testReceiveSimpleMessageTlsScramSha=my-cluster-abc5d34b-kafka-clients, testConfigurationFileIsCreated=my-cluster-17fc585b-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-ec1b34f8-kafka-clients, testSendSimpleMessageTls=my-cluster-89673e85-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-f743fdfd-kafka-clients, testUpdateUser=my-cluster-d582eb8c-kafka-clients, testConfigurationReflection=my-cluster-188382d8-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-959c6ed5-kafka-clients, testTlsExternalUserWithQuotas=my-cluster-100f5461-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-5dfd606f-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-e9955717-kafka-clients, testCapacityFile=my-cluster-855b9105-kafka-clients, testConfigurationPerformanceOptions=my-cluster-08515847-kafka-clients, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-afe273c9-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-e6343cec-kafka-clients, testTlsExternalUser=my-cluster-45cf8ce4-kafka-clients, testUserTemplate=my-cluster-927d5a1b-kafka-clients}
2022-03-28 18:59:53 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:155] Create/Update KafkaTopic my-topic-1059523275-384667503 in namespace http-bridge-scram-sha-st
2022-03-28 18:59:54 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1095105ms till timeout)
2022-03-28 18:59:54 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1092877ms till timeout)
2022-03-28 18:59:54 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-e6343cec will have desired state: Ready not ready, will try again in 1000 ms (1094273ms till timeout)
2022-03-28 18:59:54 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-1059523275-384667503
2022-03-28 18:59:54 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1096404ms till timeout)
2022-03-28 18:59:54 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:433] Wait for KafkaTopic: my-topic-1059523275-384667503 will have desired state: Ready
2022-03-28 18:59:54 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for KafkaTopic: my-topic-1059523275-384667503 will have desired state: Ready
2022-03-28 18:59:54 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (770377ms till timeout)
2022-03-28 18:59:54 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1093236ms till timeout)
2022-03-28 18:59:54 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaTopic: my-topic-1059523275-384667503 will have desired state: Ready not ready, will try again in 1000 ms (179811ms till timeout)
2022-03-28 18:59:54 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (249601ms till timeout)
2022-03-28 18:59:54 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1091464ms till timeout)
2022-03-28 18:59:54 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (806734ms till timeout)
2022-03-28 18:59:54 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (250618ms till timeout)
2022-03-28 18:59:55 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (616341ms till timeout)
2022-03-28 18:59:55 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job producer-1817671133 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T18:59:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 18:59:55 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1094826ms till timeout)
2022-03-28 18:59:55 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (69811ms till timeout)
2022-03-28 18:59:55 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1093992ms till timeout)
2022-03-28 18:59:55 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1091764ms till timeout)
2022-03-28 18:59:55 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (207850ms till timeout)
2022-03-28 18:59:55 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:444] Kafka: my-cluster-e6343cec is in desired state: Ready
2022-03-28 18:59:55 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1095289ms till timeout)
2022-03-28 18:59:55 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (769260ms till timeout)
2022-03-28 18:59:55 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1092117ms till timeout)
2022-03-28 18:59:55 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:444] KafkaTopic: my-topic-1059523275-384667503 is in desired state: Ready
2022-03-28 18:59:55 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:155] Create/Update Job consumer-1028606357 in namespace http-bridge-scram-sha-st
2022-03-28 18:59:55 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (248483ms till timeout)
2022-03-28 18:59:55 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1090350ms till timeout)
2022-03-28 18:59:55 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:consumer-1028606357
2022-03-28 18:59:55 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (805626ms till timeout)
2022-03-28 18:59:56 [ForkJoinPool-1-worker-5] INFO  [JobUtils:81] Waiting for job: consumer-1028606357 will be in active state
2022-03-28 18:59:56 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 18:59:56 [ForkJoinPool-1-worker-3] INFO  [CruiseControlConfigurationST:157] Changing the broker capacity of the cruise control
2022-03-28 18:59:56 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (249469ms till timeout)
2022-03-28 18:59:56 [ForkJoinPool-1-worker-5] INFO  [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-03-28 18:59:56 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (615241ms till timeout)
2022-03-28 18:59:56 [ForkJoinPool-1-worker-3] INFO  [CruiseControlConfigurationST:168] Verifying that CC pod is rolling, because of change size of disk
2022-03-28 18:59:56 [ForkJoinPool-1-worker-3] INFO  [DeploymentUtils:136] Waiting for Deployment my-cluster-e6343cec-cruise-control rolling update
2022-03-28 18:59:56 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Deployment my-cluster-e6343cec-cruise-control rolling update in namespace:namespace-3
2022-03-28 18:59:56 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-e6343cec-cruise-control-77c657c774-9pqlz=4f9ddbe4-cd0e-41b5-b7b8-1cacc27bbc84}
2022-03-28 18:59:56 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1093682ms till timeout)
2022-03-28 18:59:56 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (68665ms till timeout)
2022-03-28 18:59:56 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1092844ms till timeout)
2022-03-28 18:59:56 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1090613ms till timeout)
2022-03-28 18:59:56 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:155] Create/Update Job producer-703025921 in namespace http-bridge-scram-sha-st
2022-03-28 18:59:56 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job producer-1817671133 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T18:59:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 18:59:56 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-e6343cec-cruise-control-77c657c774-9pqlz=4f9ddbe4-cd0e-41b5-b7b8-1cacc27bbc84}
2022-03-28 18:59:56 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-e6343cec-cruise-control-77c657c774-9pqlz=4f9ddbe4-cd0e-41b5-b7b8-1cacc27bbc84}
2022-03-28 18:59:56 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Deployment my-cluster-e6343cec-cruise-control rolling update in namespace:namespace-3 not ready, will try again in 5000 ms (599754ms till timeout)
2022-03-28 18:59:56 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1094137ms till timeout)
2022-03-28 18:59:56 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:producer-703025921
2022-03-28 18:59:56 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (206513ms till timeout)
2022-03-28 18:59:56 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1090972ms till timeout)
2022-03-28 18:59:56 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (768112ms till timeout)
2022-03-28 18:59:56 [ForkJoinPool-1-worker-5] INFO  [JobUtils:81] Waiting for job: producer-703025921 will be in active state
2022-03-28 18:59:56 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 18:59:56 [ForkJoinPool-1-worker-5] INFO  [ClientUtils:61] Waiting till producer producer-703025921 and consumer consumer-1028606357 finish
2022-03-28 18:59:56 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for clients finished
2022-03-28 18:59:56 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (247327ms till timeout)
2022-03-28 18:59:57 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1089191ms till timeout)
2022-03-28 18:59:57 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (219807ms till timeout)
2022-03-28 18:59:57 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (804462ms till timeout)
2022-03-28 18:59:57 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (248356ms till timeout)
2022-03-28 18:59:57 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1092568ms till timeout)
2022-03-28 18:59:57 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (614080ms till timeout)
2022-03-28 18:59:57 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (67550ms till timeout)
2022-03-28 18:59:57 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1091731ms till timeout)
2022-03-28 18:59:57 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1089503ms till timeout)
2022-03-28 18:59:57 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1092990ms till timeout)
2022-03-28 18:59:57 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job producer-1817671133 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T18:59:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 18:59:57 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (766966ms till timeout)
2022-03-28 18:59:57 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1089826ms till timeout)
2022-03-28 18:59:57 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (205174ms till timeout)
2022-03-28 18:59:58 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (246194ms till timeout)
2022-03-28 18:59:58 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (218672ms till timeout)
2022-03-28 18:59:58 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1088056ms till timeout)
2022-03-28 18:59:58 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (803327ms till timeout)
2022-03-28 18:59:58 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (247253ms till timeout)
2022-03-28 18:59:58 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1091466ms till timeout)
2022-03-28 18:59:58 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (612977ms till timeout)
2022-03-28 18:59:58 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1088400ms till timeout)
2022-03-28 18:59:58 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1090627ms till timeout)
2022-03-28 18:59:58 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (66356ms till timeout)
2022-03-28 18:59:58 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1091887ms till timeout)
2022-03-28 18:59:58 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (765869ms till timeout)
2022-03-28 18:59:59 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1088634ms till timeout)
2022-03-28 18:59:59 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job producer-1817671133 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T18:59:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 18:59:59 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (203981ms till timeout)
2022-03-28 18:59:59 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (245005ms till timeout)
2022-03-28 18:59:59 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (217486ms till timeout)
2022-03-28 18:59:59 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1086869ms till timeout)
2022-03-28 18:59:59 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (802141ms till timeout)
2022-03-28 18:59:59 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (246078ms till timeout)
2022-03-28 18:59:59 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (611804ms till timeout)
2022-03-28 18:59:59 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1090292ms till timeout)
2022-03-28 18:59:59 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1089455ms till timeout)
2022-03-28 18:59:59 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1087226ms till timeout)
2022-03-28 18:59:59 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (65179ms till timeout)
2022-03-28 18:59:59 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1090713ms till timeout)
2022-03-28 19:00:00 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (764772ms till timeout)
2022-03-28 19:00:00 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1087536ms till timeout)
2022-03-28 19:00:00 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job producer-1817671133 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T18:59:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:00 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (202789ms till timeout)
2022-03-28 19:00:00 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (243802ms till timeout)
2022-03-28 19:00:00 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (216288ms till timeout)
2022-03-28 19:00:00 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1085671ms till timeout)
2022-03-28 19:00:00 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (801037ms till timeout)
2022-03-28 19:00:00 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (244975ms till timeout)
2022-03-28 19:00:00 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (610700ms till timeout)
2022-03-28 19:00:00 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1089188ms till timeout)
2022-03-28 19:00:00 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1088350ms till timeout)
2022-03-28 19:00:00 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1086121ms till timeout)
2022-03-28 19:00:00 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (63971ms till timeout)
2022-03-28 19:00:01 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1089513ms till timeout)
2022-03-28 19:00:01 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (763584ms till timeout)
2022-03-28 19:00:01 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1086349ms till timeout)
2022-03-28 19:00:01 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job producer-1817671133 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T18:59:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:01 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-e6343cec-cruise-control-77c657c774-9pqlz=4f9ddbe4-cd0e-41b5-b7b8-1cacc27bbc84}
2022-03-28 19:00:01 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (201595ms till timeout)
2022-03-28 19:00:01 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (242616ms till timeout)
2022-03-28 19:00:01 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1084485ms till timeout)
2022-03-28 19:00:01 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (215100ms till timeout)
2022-03-28 19:00:01 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (799850ms till timeout)
2022-03-28 19:00:01 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-e6343cec-cruise-control-77c657c774-9pqlz=4f9ddbe4-cd0e-41b5-b7b8-1cacc27bbc84}
2022-03-28 19:00:01 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-e6343cec-cruise-control-77c657c774-9pqlz=4f9ddbe4-cd0e-41b5-b7b8-1cacc27bbc84}
2022-03-28 19:00:01 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Deployment my-cluster-e6343cec-cruise-control rolling update in namespace:namespace-3 not ready, will try again in 5000 ms (594376ms till timeout)
2022-03-28 19:00:01 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (243786ms till timeout)
2022-03-28 19:00:01 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1088004ms till timeout)
2022-03-28 19:00:01 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (609516ms till timeout)
2022-03-28 19:00:02 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1087166ms till timeout)
2022-03-28 19:00:02 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1084937ms till timeout)
2022-03-28 19:00:02 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (62799ms till timeout)
2022-03-28 19:00:02 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1088414ms till timeout)
2022-03-28 19:00:02 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (762480ms till timeout)
2022-03-28 19:00:02 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1085245ms till timeout)
2022-03-28 19:00:02 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job producer-1817671133 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T18:59:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:02 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (200400ms till timeout)
2022-03-28 19:00:02 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (213996ms till timeout)
2022-03-28 19:00:02 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1083380ms till timeout)
2022-03-28 19:00:02 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (798746ms till timeout)
2022-03-28 19:00:02 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (241413ms till timeout)
2022-03-28 19:00:02 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (242683ms till timeout)
2022-03-28 19:00:03 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (608407ms till timeout)
2022-03-28 19:00:03 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1086894ms till timeout)
2022-03-28 19:00:03 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1083827ms till timeout)
2022-03-28 19:00:03 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1086055ms till timeout)
2022-03-28 19:00:03 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (61689ms till timeout)
2022-03-28 19:00:03 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1087317ms till timeout)
2022-03-28 19:00:03 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (761381ms till timeout)
2022-03-28 19:00:03 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1084146ms till timeout)
2022-03-28 19:00:03 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job producer-1817671133 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T18:59:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:03 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (199205ms till timeout)
2022-03-28 19:00:04 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (212807ms till timeout)
2022-03-28 19:00:04 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1082190ms till timeout)
2022-03-28 19:00:04 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (797555ms till timeout)
2022-03-28 19:00:04 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (240228ms till timeout)
2022-03-28 19:00:04 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (241494ms till timeout)
2022-03-28 19:00:04 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (607216ms till timeout)
2022-03-28 19:00:04 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1085704ms till timeout)
2022-03-28 19:00:04 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1082639ms till timeout)
2022-03-28 19:00:04 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1084867ms till timeout)
2022-03-28 19:00:04 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (60499ms till timeout)
2022-03-28 19:00:04 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1086160ms till timeout)
2022-03-28 19:00:04 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (760225ms till timeout)
2022-03-28 19:00:04 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1082987ms till timeout)
2022-03-28 19:00:05 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job producer-1817671133 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T18:59:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:05 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (211707ms till timeout)
2022-03-28 19:00:05 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (197917ms till timeout)
2022-03-28 19:00:05 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1080992ms till timeout)
2022-03-28 19:00:05 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (796358ms till timeout)
2022-03-28 19:00:05 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (238946ms till timeout)
2022-03-28 19:00:05 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (240293ms till timeout)
2022-03-28 19:00:05 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (606027ms till timeout)
2022-03-28 19:00:05 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1084512ms till timeout)
2022-03-28 19:00:05 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1083679ms till timeout)
2022-03-28 19:00:05 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1081449ms till timeout)
2022-03-28 19:00:05 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (59310ms till timeout)
2022-03-28 19:00:05 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1084969ms till timeout)
2022-03-28 19:00:05 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (759036ms till timeout)
2022-03-28 19:00:05 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1081799ms till timeout)
2022-03-28 19:00:06 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (210609ms till timeout)
2022-03-28 19:00:06 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job producer-1817671133 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T18:59:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:06 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1079803ms till timeout)
2022-03-28 19:00:06 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (795169ms till timeout)
2022-03-28 19:00:06 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (604929ms till timeout)
2022-03-28 19:00:06 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (196627ms till timeout)
2022-03-28 19:00:06 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (239109ms till timeout)
2022-03-28 19:00:06 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (237658ms till timeout)
2022-03-28 19:00:06 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1080350ms till timeout)
2022-03-28 19:00:06 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1083320ms till timeout)
2022-03-28 19:00:06 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1082577ms till timeout)
2022-03-28 19:00:06 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (58210ms till timeout)
2022-03-28 19:00:06 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1083870ms till timeout)
2022-03-28 19:00:06 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-e6343cec-cruise-control-77c657c774-9pqlz=4f9ddbe4-cd0e-41b5-b7b8-1cacc27bbc84}
2022-03-28 19:00:06 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (757931ms till timeout)
2022-03-28 19:00:06 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1080693ms till timeout)
2022-03-28 19:00:07 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-e6343cec-cruise-control-6cd68d58b9-k7qb2=f6c34d2d-b608-4a7b-912f-33c5db736711, my-cluster-e6343cec-cruise-control-77c657c774-9pqlz=4f9ddbe4-cd0e-41b5-b7b8-1cacc27bbc84}
2022-03-28 19:00:07 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-e6343cec-cruise-control-77c657c774-9pqlz=4f9ddbe4-cd0e-41b5-b7b8-1cacc27bbc84}
2022-03-28 19:00:07 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Deployment my-cluster-e6343cec-cruise-control rolling update in namespace:namespace-3 not ready, will try again in 5000 ms (589104ms till timeout)
2022-03-28 19:00:07 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (209510ms till timeout)
2022-03-28 19:00:07 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1078704ms till timeout)
2022-03-28 19:00:07 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (793975ms till timeout)
2022-03-28 19:00:07 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job producer-1817671133 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T18:59:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:07 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (603734ms till timeout)
2022-03-28 19:00:07 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (237910ms till timeout)
2022-03-28 19:00:07 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (236552ms till timeout)
2022-03-28 19:00:07 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1081385ms till timeout)
2022-03-28 19:00:07 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1079156ms till timeout)
2022-03-28 19:00:07 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1082126ms till timeout)
2022-03-28 19:00:07 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (195243ms till timeout)
2022-03-28 19:00:07 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (57018ms till timeout)
2022-03-28 19:00:07 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1082681ms till timeout)
2022-03-28 19:00:08 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (756748ms till timeout)
2022-03-28 19:00:08 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1079512ms till timeout)
2022-03-28 19:00:08 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (208413ms till timeout)
2022-03-28 19:00:08 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1077605ms till timeout)
2022-03-28 19:00:08 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (792875ms till timeout)
2022-03-28 19:00:08 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (602636ms till timeout)
2022-03-28 19:00:08 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1080286ms till timeout)
2022-03-28 19:00:08 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1078056ms till timeout)
2022-03-28 19:00:08 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1081025ms till timeout)
2022-03-28 19:00:08 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (236717ms till timeout)
2022-03-28 19:00:08 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (235351ms till timeout)
2022-03-28 19:00:08 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job producer-1817671133 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T18:59:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:09 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1081581ms till timeout)
2022-03-28 19:00:09 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (55821ms till timeout)
2022-03-28 19:00:09 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (193950ms till timeout)
2022-03-28 19:00:09 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (755647ms till timeout)
2022-03-28 19:00:09 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1078412ms till timeout)
2022-03-28 19:00:09 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (207313ms till timeout)
2022-03-28 19:00:09 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1076507ms till timeout)
2022-03-28 19:00:09 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (791776ms till timeout)
2022-03-28 19:00:09 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (601537ms till timeout)
2022-03-28 19:00:09 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1079188ms till timeout)
2022-03-28 19:00:10 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1079834ms till timeout)
2022-03-28 19:00:10 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1076864ms till timeout)
2022-03-28 19:00:10 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (235520ms till timeout)
2022-03-28 19:00:10 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (234159ms till timeout)
2022-03-28 19:00:10 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1080482ms till timeout)
2022-03-28 19:00:10 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (54629ms till timeout)
2022-03-28 19:00:10 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1077310ms till timeout)
2022-03-28 19:00:10 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job producer-1817671133 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T18:59:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:10 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (754449ms till timeout)
2022-03-28 19:00:10 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (192657ms till timeout)
2022-03-28 19:00:10 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (206212ms till timeout)
2022-03-28 19:00:10 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1075408ms till timeout)
2022-03-28 19:00:10 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (790587ms till timeout)
2022-03-28 19:00:11 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (600349ms till timeout)
2022-03-28 19:00:11 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1078000ms till timeout)
2022-03-28 19:00:11 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1075675ms till timeout)
2022-03-28 19:00:11 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1078645ms till timeout)
2022-03-28 19:00:11 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (234335ms till timeout)
2022-03-28 19:00:11 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (232973ms till timeout)
2022-03-28 19:00:11 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-855b9105 will have desired state: Ready not ready, will try again in 1000 ms (1079294ms till timeout)
2022-03-28 19:00:11 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (53436ms till timeout)
2022-03-28 19:00:11 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (753257ms till timeout)
2022-03-28 19:00:11 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1076116ms till timeout)
2022-03-28 19:00:11 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job producer-1817671133 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T18:59:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:11 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (205067ms till timeout)
2022-03-28 19:00:11 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (191273ms till timeout)
2022-03-28 19:00:11 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1074257ms till timeout)
2022-03-28 19:00:12 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (789490ms till timeout)
2022-03-28 19:00:12 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-e6343cec-cruise-control-77c657c774-9pqlz=4f9ddbe4-cd0e-41b5-b7b8-1cacc27bbc84}
2022-03-28 19:00:12 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (599247ms till timeout)
2022-03-28 19:00:12 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1076898ms till timeout)
2022-03-28 19:00:12 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1077543ms till timeout)
2022-03-28 19:00:12 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1074572ms till timeout)
2022-03-28 19:00:12 [ForkJoinPool-1-worker-17] INFO  [ResourceManager:444] Kafka: my-cluster-855b9105 is in desired state: Ready
2022-03-28 19:00:12 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-e6343cec-cruise-control-6cd68d58b9-k7qb2=f6c34d2d-b608-4a7b-912f-33c5db736711, my-cluster-e6343cec-cruise-control-77c657c774-9pqlz=4f9ddbe4-cd0e-41b5-b7b8-1cacc27bbc84}
2022-03-28 19:00:12 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-e6343cec-cruise-control-77c657c774-9pqlz=4f9ddbe4-cd0e-41b5-b7b8-1cacc27bbc84}
2022-03-28 19:00:12 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Deployment my-cluster-e6343cec-cruise-control rolling update in namespace:namespace-3 not ready, will try again in 5000 ms (583710ms till timeout)
2022-03-28 19:00:12 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (233106ms till timeout)
2022-03-28 19:00:12 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (231714ms till timeout)
2022-03-28 19:00:12 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (52313ms till timeout)
2022-03-28 19:00:12 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (752151ms till timeout)
2022-03-28 19:00:12 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1075009ms till timeout)
2022-03-28 19:00:12 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-6 exec my-cluster-855b9105-cruise-control-5d65667776-l2m59 -- /bin/bash -c cat /tmp/capacity.json
2022-03-28 19:00:12 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (203922ms till timeout)
2022-03-28 19:00:12 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job producer-1817671133 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T18:59:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:13 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1073116ms till timeout)
2022-03-28 19:00:13 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (788387ms till timeout)
2022-03-28 19:00:13 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (189941ms till timeout)
2022-03-28 19:00:13 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (598146ms till timeout)
2022-03-28 19:00:13 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1075794ms till timeout)
2022-03-28 19:00:13 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1073471ms till timeout)
2022-03-28 19:00:13 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1076439ms till timeout)
2022-03-28 19:00:13 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (232006ms till timeout)
2022-03-28 19:00:13 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (230546ms till timeout)
2022-03-28 19:00:13 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (51200ms till timeout)
2022-03-28 19:00:13 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (751027ms till timeout)
2022-03-28 19:00:13 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1073886ms till timeout)
2022-03-28 19:00:13 [ForkJoinPool-1-worker-17] INFO  [Exec:417] Command: oc --namespace namespace-6 exec my-cluster-855b9105-cruise-control-5d65667776-l2m59 -- /bin/bash -c cat /tmp/capacity.json
2022-03-28 19:00:13 [ForkJoinPool-1-worker-17] INFO  [Exec:417] Return code: 0
2022-03-28 19:00:13 [ForkJoinPool-1-worker-55] INFO  [TestSeparator:23] ############################################################################
2022-03-28 19:00:13 [ForkJoinPool-1-worker-55] INFO  [TestSeparator:24] io.strimzi.systemtest.bridge.HttpBridgeScramShaST.testSendSimpleMessageTlsScramSha-STARTED
2022-03-28 19:00:13 [ForkJoinPool-1-worker-55] DEBUG [AbstractST:658] ============================================================================
2022-03-28 19:00:13 [ForkJoinPool-1-worker-55] DEBUG [AbstractST:659] [bridge.HttpBridgeScramShaST - Before Each] - Setup test case environment
2022-03-28 19:00:13 [ForkJoinPool-1-worker-55] DEBUG [SuiteThreadController:77] [bridge.HttpBridgeScramShaST] - Adding parallel test: testSendSimpleMessageTlsScramSha
2022-03-28 19:00:13 [ForkJoinPool-1-worker-55] DEBUG [SuiteThreadController:81] [bridge.HttpBridgeScramShaST] - Parallel test count: 16
2022-03-28 19:00:13 [ForkJoinPool-1-worker-55] DEBUG [SuiteThreadController:205] [testSendSimpleMessageTlsScramSha] moved to the WaitZone, because current thread exceed maximum of allowed test cases in parallel. (16/15)
2022-03-28 19:00:13 [ForkJoinPool-1-worker-55] TRACE [SuiteThreadController:210] testSendSimpleMessageTlsScramSha is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (16/15)
2022-03-28 19:00:14 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (202823ms till timeout)
2022-03-28 19:00:14 [ForkJoinPool-1-worker-17] INFO  [CruiseControlConfigurationST:80] We got only one configuration of broker-capacities
2022-03-28 19:00:14 [ForkJoinPool-1-worker-17] INFO  [CruiseControlConfigurationST:83] Verifying cruise control configuration.
2022-03-28 19:00:14 [ForkJoinPool-1-worker-17] INFO  [CruiseControlConfigurationST:92] Verifying default cruise control capacities
2022-03-28 19:00:14 [ForkJoinPool-1-worker-17] DEBUG [AbstractST:674] ============================================================================
2022-03-28 19:00:14 [ForkJoinPool-1-worker-17] DEBUG [AbstractST:675] [cruisecontrol.CruiseControlConfigurationST - After Each] - Clean up after test
2022-03-28 19:00:14 [ForkJoinPool-1-worker-17] INFO  [ResourceManager:344] ############################################################################
2022-03-28 19:00:14 [ForkJoinPool-1-worker-17] INFO  [ResourceManager:348] Delete all resources for testCapacityFile
2022-03-28 19:00:14 [ForkJoinPool-1-worker-17] INFO  [ResourceManager:241] Delete of Kafka my-cluster-855b9105 in namespace namespace-6
2022-03-28 19:00:14 [ForkJoinPool-1-worker-17] INFO  [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-6, for cruise control Kafka cluster my-cluster-855b9105
2022-03-28 19:00:14 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1072003ms till timeout)
2022-03-28 19:00:14 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job producer-1817671133 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T18:59:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:14 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (787272ms till timeout)
2022-03-28 19:00:14 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (597037ms till timeout)
2022-03-28 19:00:14 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1074679ms till timeout)
2022-03-28 19:00:14 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (188623ms till timeout)
2022-03-28 19:00:14 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1072365ms till timeout)
2022-03-28 19:00:14 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1075334ms till timeout)
2022-03-28 19:00:14 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (230838ms till timeout)
2022-03-28 19:00:14 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (50038ms till timeout)
2022-03-28 19:00:14 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (229379ms till timeout)
2022-03-28 19:00:14 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (749863ms till timeout)
2022-03-28 19:00:14 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1072722ms till timeout)
2022-03-28 19:00:15 [ForkJoinPool-1-worker-17] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-855b9105
2022-03-28 19:00:15 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (201653ms till timeout)
2022-03-28 19:00:15 [ForkJoinPool-1-worker-17] INFO  [ResourceManager:369] ############################################################################
2022-03-28 19:00:15 [ForkJoinPool-1-worker-17] INFO  [TestSuiteNamespaceManager:200] Deleting namespace:namespace-6 for test case:testCapacityFile
2022-03-28 19:00:15 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1070790ms till timeout)
2022-03-28 19:00:15 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (595931ms till timeout)
2022-03-28 19:00:15 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (786051ms till timeout)
2022-03-28 19:00:15 [ForkJoinPool-1-worker-17] DEBUG [TestUtils:125] Waiting for Namespace namespace-6 removal
2022-03-28 19:00:15 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 19:00:15 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1073579ms till timeout)
2022-03-28 19:00:15 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job producer-1817671133 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T18:59:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:15 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1074219ms till timeout)
2022-03-28 19:00:15 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1071247ms till timeout)
2022-03-28 19:00:15 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (187328ms till timeout)
2022-03-28 19:00:15 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (229710ms till timeout)
2022-03-28 19:00:16 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (228246ms till timeout)
2022-03-28 19:00:16 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (48895ms till timeout)
2022-03-28 19:00:16 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 19:00:16 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 19:00:16 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (479488ms till timeout)
2022-03-28 19:00:16 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1071587ms till timeout)
2022-03-28 19:00:16 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (748727ms till timeout)
2022-03-28 19:00:16 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (200556ms till timeout)
2022-03-28 19:00:16 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1069693ms till timeout)
2022-03-28 19:00:16 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:444] Kafka: my-cluster-afe273c9 is in desired state: Ready
2022-03-28 19:00:16 [ForkJoinPool-1-worker-25] INFO  [ReconciliationST:80] Adding pause annotation into Kafka resource and also scaling replicas to 4, new pod should not appear
2022-03-28 19:00:16 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1072468ms till timeout)
2022-03-28 19:00:16 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (784864ms till timeout)
2022-03-28 19:00:16 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1073112ms till timeout)
2022-03-28 19:00:16 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1070142ms till timeout)
2022-03-28 19:00:16 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job producer-1817671133 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T18:59:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:17 [ForkJoinPool-1-worker-25] INFO  [ReconciliationST:86] Kafka should contain status with ReconciliationPaused
2022-03-28 19:00:17 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 19:00:17 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (186037ms till timeout)
2022-03-28 19:00:17 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (228511ms till timeout)
2022-03-28 19:00:17 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:433] Wait for Kafka: my-cluster-afe273c9 will have desired state: ReconciliationPaused
2022-03-28 19:00:17 [ForkJoinPool-1-worker-25] DEBUG [TestUtils:125] Waiting for Kafka: my-cluster-afe273c9 will have desired state: ReconciliationPaused
2022-03-28 19:00:17 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (227051ms till timeout)
2022-03-28 19:00:17 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (47706ms till timeout)
2022-03-28 19:00:17 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (747537ms till timeout)
2022-03-28 19:00:17 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1070396ms till timeout)
2022-03-28 19:00:17 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (199440ms till timeout)
2022-03-28 19:00:17 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: ReconciliationPaused not ready, will try again in 1000 ms (839811ms till timeout)
2022-03-28 19:00:17 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-e6343cec-cruise-control-77c657c774-9pqlz=4f9ddbe4-cd0e-41b5-b7b8-1cacc27bbc84}
2022-03-28 19:00:17 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 19:00:17 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 19:00:17 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (477970ms till timeout)
2022-03-28 19:00:17 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1068530ms till timeout)
2022-03-28 19:00:17 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-e6343cec-cruise-control-6cd68d58b9-k7qb2=f6c34d2d-b608-4a7b-912f-33c5db736711, my-cluster-e6343cec-cruise-control-77c657c774-9pqlz=4f9ddbe4-cd0e-41b5-b7b8-1cacc27bbc84}
2022-03-28 19:00:17 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-e6343cec-cruise-control-77c657c774-9pqlz=4f9ddbe4-cd0e-41b5-b7b8-1cacc27bbc84}
2022-03-28 19:00:17 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Deployment my-cluster-e6343cec-cruise-control rolling update in namespace:namespace-3 not ready, will try again in 5000 ms (578420ms till timeout)
2022-03-28 19:00:17 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1071306ms till timeout)
2022-03-28 19:00:17 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (783704ms till timeout)
2022-03-28 19:00:17 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1071951ms till timeout)
2022-03-28 19:00:17 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1068981ms till timeout)
2022-03-28 19:00:18 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job producer-1817671133 in namespace http-bridge-tls-st, has status JobStatus(active=null, completedIndexes=null, completionTime=2022-03-28T19:00:12Z, conditions=[JobCondition(lastProbeTime=2022-03-28T19:00:12Z, lastTransitionTime=2022-03-28T19:00:12Z, message=null, reason=null, status=True, type=Complete, additionalProperties={})], failed=null, ready=null, startTime=2022-03-28T18:59:38Z, succeeded=1, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:18 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (227325ms till timeout)
2022-03-28 19:00:18 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (46518ms till timeout)
2022-03-28 19:00:18 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (225860ms till timeout)
2022-03-28 19:00:18 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (746336ms till timeout)
2022-03-28 19:00:18 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1069196ms till timeout)
2022-03-28 19:00:18 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 19:00:18 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (198237ms till timeout)
2022-03-28 19:00:18 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: ReconciliationPaused not ready, will try again in 1000 ms (838601ms till timeout)
2022-03-28 19:00:18 [ForkJoinPool-1-worker-23] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment producer-1817671133 deletion
2022-03-28 19:00:18 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for ReplicaSet producer-1817671133 to be deleted
2022-03-28 19:00:18 [ForkJoinPool-1-worker-23] DEBUG [JobUtils:40] Job producer-1817671133 was deleted
2022-03-28 19:00:18 [ForkJoinPool-1-worker-23] INFO  [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-03-28 19:00:18 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:155] Create/Update Job consumer-1378029767 in namespace http-bridge-tls-st
2022-03-28 19:00:18 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1067381ms till timeout)
2022-03-28 19:00:18 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:consumer-1378029767
2022-03-28 19:00:18 [ForkJoinPool-1-worker-55] DEBUG [SuiteThreadController:230] testSendSimpleMessageTlsScramSha test now can proceed its execution
2022-03-28 19:00:18 [ForkJoinPool-1-worker-55] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 19:00:18 [ForkJoinPool-1-worker-55] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaAdminTopicOperations=my-cluster-506fda27, testReceiveSimpleMessageTlsScramSha=my-cluster-abc5d34b, testConfigurationFileIsCreated=my-cluster-17fc585b, testThrottlingQuotasCreateAlterPartitions=my-cluster-ec1b34f8, testSendSimpleMessageTls=my-cluster-89673e85, testCreatingUsersWithSecretPrefix=my-cluster-f743fdfd, testUpdateUser=my-cluster-d582eb8c, testConfigurationReflection=my-cluster-188382d8, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-959c6ed5, testTlsExternalUserWithQuotas=my-cluster-100f5461, testDeployAndUnDeployCruiseControl=my-cluster-5dfd606f, testThrottlingQuotasCreateTopic=my-cluster-e9955717, testCapacityFile=my-cluster-855b9105, testConfigurationPerformanceOptions=my-cluster-08515847, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-afe273c9, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-e6343cec, testTlsExternalUser=my-cluster-45cf8ce4, testSendSimpleMessageTlsScramSha=my-cluster-561d4fae, testUserTemplate=my-cluster-927d5a1b}
2022-03-28 19:00:18 [ForkJoinPool-1-worker-55] TRACE [AbstractST:607] USERS_NAME_MAP: {testKafkaAdminTopicOperations=my-user-1780746402-846362125, testReceiveSimpleMessageTlsScramSha=my-user-330562220-1155646423, testConfigurationFileIsCreated=my-user-355815510-1316268151, testThrottlingQuotasCreateAlterPartitions=my-user-333986137-1152182125, testSendSimpleMessageTls=my-user-1887866274-1013125243, testCreatingUsersWithSecretPrefix=my-user-607975993-1210920737, testUpdateUser=my-user-1297536277-778093599, testConfigurationReflection=my-user-1890969183-1754686912, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-2104248162-45187051, testTlsExternalUserWithQuotas=my-user-1909583883-1106194424, testDeployAndUnDeployCruiseControl=my-user-506268651-1309117986, testThrottlingQuotasCreateTopic=my-user-1876060546-1887905718, testCapacityFile=my-user-1460643829-103298436, testConfigurationPerformanceOptions=my-user-98362545-1476587236, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-user-8456079-650483351, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-2069267102-581538560, testTlsExternalUser=my-user-399414674-1203409092, testSendSimpleMessageTlsScramSha=my-user-1601209876-518796379, testUserTemplate=my-user-989633374-940717398}
2022-03-28 19:00:18 [ForkJoinPool-1-worker-55] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaAdminTopicOperations=my-topic-481231182-953607496, testReceiveSimpleMessageTlsScramSha=my-topic-1027850608-734754781, testConfigurationFileIsCreated=my-topic-423850169-855699454, testThrottlingQuotasCreateAlterPartitions=my-topic-325891726-169328137, testSendSimpleMessageTls=my-topic-874607937-87907051, testCreatingUsersWithSecretPrefix=my-topic-1727421070-668192795, testUpdateUser=my-topic-1325563869-1324403074, testConfigurationReflection=my-topic-1394577332-1936226438, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-634125597-1746493046, testTlsExternalUserWithQuotas=my-topic-1986059791-167638004, testDeployAndUnDeployCruiseControl=my-topic-465377127-800134460, testThrottlingQuotasCreateTopic=my-topic-1246597695-228473879, testCapacityFile=my-topic-549312506-649708453, testConfigurationPerformanceOptions=my-topic-696588651-1198599652, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-topic-1687496771-142091227, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-293775067-432059493, testTlsExternalUser=my-topic-707627084-257659651, testSendSimpleMessageTlsScramSha=my-topic-1586843669-332477842, testUserTemplate=my-topic-1542379258-2002911578}
2022-03-28 19:00:18 [ForkJoinPool-1-worker-55] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaAdminTopicOperations=my-cluster-506fda27-kafka-clients, testReceiveSimpleMessageTlsScramSha=my-cluster-abc5d34b-kafka-clients, testConfigurationFileIsCreated=my-cluster-17fc585b-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-ec1b34f8-kafka-clients, testSendSimpleMessageTls=my-cluster-89673e85-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-f743fdfd-kafka-clients, testUpdateUser=my-cluster-d582eb8c-kafka-clients, testConfigurationReflection=my-cluster-188382d8-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-959c6ed5-kafka-clients, testTlsExternalUserWithQuotas=my-cluster-100f5461-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-5dfd606f-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-e9955717-kafka-clients, testCapacityFile=my-cluster-855b9105-kafka-clients, testConfigurationPerformanceOptions=my-cluster-08515847-kafka-clients, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-afe273c9-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-e6343cec-kafka-clients, testTlsExternalUser=my-cluster-45cf8ce4-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-561d4fae-kafka-clients, testUserTemplate=my-cluster-927d5a1b-kafka-clients}
2022-03-28 19:00:19 [ForkJoinPool-1-worker-55] INFO  [ResourceManager:155] Create/Update KafkaTopic my-topic-367534742-1633325021 in namespace http-bridge-scram-sha-st
2022-03-28 19:00:19 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (782565ms till timeout)
2022-03-28 19:00:19 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1070167ms till timeout)
2022-03-28 19:00:19 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 19:00:19 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 19:00:19 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (476476ms till timeout)
2022-03-28 19:00:19 [ForkJoinPool-1-worker-23] INFO  [JobUtils:81] Waiting for job: consumer-1378029767 will be in active state
2022-03-28 19:00:19 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 19:00:19 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1070812ms till timeout)
2022-03-28 19:00:19 [ForkJoinPool-1-worker-55] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-367534742-1633325021
2022-03-28 19:00:19 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1067833ms till timeout)
2022-03-28 19:00:19 [ForkJoinPool-1-worker-23] INFO  [ClientUtils:76] Waiting for producer/consumer:consumer-1378029767 to finished
2022-03-28 19:00:19 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for job finished
2022-03-28 19:00:19 [ForkJoinPool-1-worker-55] INFO  [ResourceManager:433] Wait for KafkaTopic: my-topic-367534742-1633325021 will have desired state: Ready
2022-03-28 19:00:19 [ForkJoinPool-1-worker-55] DEBUG [TestUtils:125] Waiting for KafkaTopic: my-topic-367534742-1633325021 will have desired state: Ready
2022-03-28 19:00:19 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job consumer-1378029767 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:14Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:19 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (226216ms till timeout)
2022-03-28 19:00:19 [ForkJoinPool-1-worker-55] TRACE [TestUtils:176] KafkaTopic: my-topic-367534742-1633325021 will have desired state: Ready not ready, will try again in 1000 ms (179810ms till timeout)
2022-03-28 19:00:19 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (219619ms till timeout)
2022-03-28 19:00:19 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (45321ms till timeout)
2022-03-28 19:00:19 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (224664ms till timeout)
2022-03-28 19:00:19 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (745148ms till timeout)
2022-03-28 19:00:19 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1068008ms till timeout)
2022-03-28 19:00:19 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (197051ms till timeout)
2022-03-28 19:00:19 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: ReconciliationPaused not ready, will try again in 1000 ms (837421ms till timeout)
2022-03-28 19:00:19 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1066283ms till timeout)
2022-03-28 19:00:20 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 19:00:20 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (781468ms till timeout)
2022-03-28 19:00:20 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1068975ms till timeout)
2022-03-28 19:00:20 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1066651ms till timeout)
2022-03-28 19:00:20 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1069621ms till timeout)
2022-03-28 19:00:20 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (225114ms till timeout)
2022-03-28 19:00:20 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 19:00:20 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 19:00:20 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (474993ms till timeout)
2022-03-28 19:00:20 [ForkJoinPool-1-worker-55] INFO  [ResourceManager:444] KafkaTopic: my-topic-367534742-1633325021 is in desired state: Ready
2022-03-28 19:00:20 [ForkJoinPool-1-worker-55] INFO  [ResourceManager:155] Create/Update Job producer-791126931 in namespace http-bridge-scram-sha-st
2022-03-28 19:00:20 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job consumer-1378029767 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:14Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:20 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (223548ms till timeout)
2022-03-28 19:00:20 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (44202ms till timeout)
2022-03-28 19:00:20 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (744030ms till timeout)
2022-03-28 19:00:20 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1066890ms till timeout)
2022-03-28 19:00:20 [ForkJoinPool-1-worker-55] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:producer-791126931
2022-03-28 19:00:20 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (195938ms till timeout)
2022-03-28 19:00:20 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (218311ms till timeout)
2022-03-28 19:00:20 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-afe273c9 will have desired state: ReconciliationPaused not ready, will try again in 1000 ms (836305ms till timeout)
2022-03-28 19:00:20 [ForkJoinPool-1-worker-55] INFO  [JobUtils:81] Waiting for job: producer-791126931 will be in active state
2022-03-28 19:00:20 [ForkJoinPool-1-worker-55] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 19:00:21 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1065130ms till timeout)
2022-03-28 19:00:21 [ForkJoinPool-1-worker-55] INFO  [ClientUtils:76] Waiting for producer/consumer:producer-791126931 to finished
2022-03-28 19:00:21 [ForkJoinPool-1-worker-55] DEBUG [TestUtils:125] Waiting for job finished
2022-03-28 19:00:21 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (780306ms till timeout)
2022-03-28 19:00:21 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1067812ms till timeout)
2022-03-28 19:00:21 [ForkJoinPool-1-worker-55] DEBUG [ClientUtils:79] Job producer-791126931 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:16Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:21 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1065486ms till timeout)
2022-03-28 19:00:21 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1068456ms till timeout)
2022-03-28 19:00:21 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 19:00:21 [ForkJoinPool-1-worker-55] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (219616ms till timeout)
2022-03-28 19:00:21 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (223958ms till timeout)
2022-03-28 19:00:21 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (222431ms till timeout)
2022-03-28 19:00:21 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1065771ms till timeout)
2022-03-28 19:00:21 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (742911ms till timeout)
2022-03-28 19:00:21 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (42988ms till timeout)
2022-03-28 19:00:22 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (194816ms till timeout)
2022-03-28 19:00:22 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job consumer-1378029767 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:14Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:22 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:444] Kafka: my-cluster-afe273c9 is in desired state: ReconciliationPaused
2022-03-28 19:00:22 [ForkJoinPool-1-worker-25] INFO  [PodUtils:209] Wait until Pod my-cluster-afe273c9-kafka will have stable 3 replicas
2022-03-28 19:00:22 [ForkJoinPool-1-worker-25] DEBUG [TestUtils:125] Waiting for  Podmy-cluster-afe273c9-kafka will have 3 replicas
2022-03-28 19:00:22 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 19:00:22 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 19:00:22 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (473496ms till timeout)
2022-03-28 19:00:22 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (217097ms till timeout)
2022-03-28 19:00:22 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1064007ms till timeout)
2022-03-28 19:00:22 [ForkJoinPool-1-worker-25] INFO  [PodUtils:225] Pod replicas gonna be stable in 19 polls
2022-03-28 19:00:22 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176]  Podmy-cluster-afe273c9-kafka will have 3 replicas not ready, will try again in 1000 ms (179809ms till timeout)
2022-03-28 19:00:22 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (779209ms till timeout)
2022-03-28 19:00:22 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1066714ms till timeout)
2022-03-28 19:00:22 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1067359ms till timeout)
2022-03-28 19:00:22 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1064388ms till timeout)
2022-03-28 19:00:22 [ForkJoinPool-1-worker-55] DEBUG [ClientUtils:79] Job producer-791126931 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:16Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:22 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (222858ms till timeout)
2022-03-28 19:00:22 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-e6343cec-cruise-control-77c657c774-9pqlz=4f9ddbe4-cd0e-41b5-b7b8-1cacc27bbc84}
2022-03-28 19:00:22 [ForkJoinPool-1-worker-55] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (218325ms till timeout)
2022-03-28 19:00:22 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (221299ms till timeout)
2022-03-28 19:00:23 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 19:00:23 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (741782ms till timeout)
2022-03-28 19:00:23 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1064642ms till timeout)
2022-03-28 19:00:23 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (41860ms till timeout)
2022-03-28 19:00:23 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (193686ms till timeout)
2022-03-28 19:00:23 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-e6343cec-cruise-control-6cd68d58b9-k7qb2=f6c34d2d-b608-4a7b-912f-33c5db736711, my-cluster-e6343cec-cruise-control-77c657c774-9pqlz=4f9ddbe4-cd0e-41b5-b7b8-1cacc27bbc84}
2022-03-28 19:00:23 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-e6343cec-cruise-control-77c657c774-9pqlz=4f9ddbe4-cd0e-41b5-b7b8-1cacc27bbc84}
2022-03-28 19:00:23 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Deployment my-cluster-e6343cec-cruise-control rolling update in namespace:namespace-3 not ready, will try again in 5000 ms (573056ms till timeout)
2022-03-28 19:00:23 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job consumer-1378029767 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:14Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:23 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1062880ms till timeout)
2022-03-28 19:00:23 [ForkJoinPool-1-worker-25] INFO  [PodUtils:225] Pod replicas gonna be stable in 18 polls
2022-03-28 19:00:23 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176]  Podmy-cluster-afe273c9-kafka will have 3 replicas not ready, will try again in 1000 ms (178688ms till timeout)
2022-03-28 19:00:23 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (215778ms till timeout)
2022-03-28 19:00:23 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (778056ms till timeout)
2022-03-28 19:00:23 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 19:00:23 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 19:00:23 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (472004ms till timeout)
2022-03-28 19:00:23 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1065563ms till timeout)
2022-03-28 19:00:23 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1066209ms till timeout)
2022-03-28 19:00:23 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1063239ms till timeout)
2022-03-28 19:00:23 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (221748ms till timeout)
2022-03-28 19:00:23 [ForkJoinPool-1-worker-55] DEBUG [ClientUtils:79] Job producer-791126931 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:16Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:24 [ForkJoinPool-1-worker-55] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (217129ms till timeout)
2022-03-28 19:00:24 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (220105ms till timeout)
2022-03-28 19:00:24 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1063448ms till timeout)
2022-03-28 19:00:24 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (740587ms till timeout)
2022-03-28 19:00:24 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (192583ms till timeout)
2022-03-28 19:00:24 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (40666ms till timeout)
2022-03-28 19:00:24 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1061783ms till timeout)
2022-03-28 19:00:24 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job consumer-1378029767 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:14Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:24 [ForkJoinPool-1-worker-25] INFO  [PodUtils:225] Pod replicas gonna be stable in 17 polls
2022-03-28 19:00:24 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176]  Podmy-cluster-afe273c9-kafka will have 3 replicas not ready, will try again in 1000 ms (177490ms till timeout)
2022-03-28 19:00:24 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 19:00:24 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (776955ms till timeout)
2022-03-28 19:00:24 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1064461ms till timeout)
2022-03-28 19:00:24 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (214485ms till timeout)
2022-03-28 19:00:24 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1062139ms till timeout)
2022-03-28 19:00:24 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1065109ms till timeout)
2022-03-28 19:00:24 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (220647ms till timeout)
2022-03-28 19:00:25 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 19:00:25 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 19:00:25 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (470470ms till timeout)
2022-03-28 19:00:25 [ForkJoinPool-1-worker-55] DEBUG [ClientUtils:79] Job producer-791126931 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:16Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:25 [ForkJoinPool-1-worker-55] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (215934ms till timeout)
2022-03-28 19:00:25 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (218913ms till timeout)
2022-03-28 19:00:25 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (191390ms till timeout)
2022-03-28 19:00:25 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1062251ms till timeout)
2022-03-28 19:00:25 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (739390ms till timeout)
2022-03-28 19:00:25 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (39470ms till timeout)
2022-03-28 19:00:25 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1060680ms till timeout)
2022-03-28 19:00:25 [ForkJoinPool-1-worker-25] INFO  [PodUtils:225] Pod replicas gonna be stable in 16 polls
2022-03-28 19:00:25 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176]  Podmy-cluster-afe273c9-kafka will have 3 replicas not ready, will try again in 1000 ms (176390ms till timeout)
2022-03-28 19:00:25 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (775854ms till timeout)
2022-03-28 19:00:25 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job consumer-1378029767 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:14Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:25 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1063361ms till timeout)
2022-03-28 19:00:25 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1064008ms till timeout)
2022-03-28 19:00:25 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1061038ms till timeout)
2022-03-28 19:00:26 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (213197ms till timeout)
2022-03-28 19:00:26 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 19:00:26 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (219509ms till timeout)
2022-03-28 19:00:26 [ForkJoinPool-1-worker-55] DEBUG [ClientUtils:79] Job producer-791126931 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:16Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:26 [ForkJoinPool-1-worker-55] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (214742ms till timeout)
2022-03-28 19:00:26 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 19:00:26 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 19:00:26 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (469016ms till timeout)
2022-03-28 19:00:26 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (217723ms till timeout)
2022-03-28 19:00:26 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (190200ms till timeout)
2022-03-28 19:00:26 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (738201ms till timeout)
2022-03-28 19:00:26 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1061061ms till timeout)
2022-03-28 19:00:26 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (38280ms till timeout)
2022-03-28 19:00:26 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1059489ms till timeout)
2022-03-28 19:00:26 [ForkJoinPool-1-worker-25] INFO  [PodUtils:225] Pod replicas gonna be stable in 15 polls
2022-03-28 19:00:26 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176]  Podmy-cluster-afe273c9-kafka will have 3 replicas not ready, will try again in 1000 ms (175201ms till timeout)
2022-03-28 19:00:26 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (774665ms till timeout)
2022-03-28 19:00:27 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1062171ms till timeout)
2022-03-28 19:00:27 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1062818ms till timeout)
2022-03-28 19:00:27 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1059848ms till timeout)
2022-03-28 19:00:27 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job consumer-1378029767 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:14Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:27 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (218317ms till timeout)
2022-03-28 19:00:27 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (211813ms till timeout)
2022-03-28 19:00:27 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 19:00:27 [ForkJoinPool-1-worker-55] DEBUG [ClientUtils:79] Job producer-791126931 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:16Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:27 [ForkJoinPool-1-worker-55] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (213549ms till timeout)
2022-03-28 19:00:27 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (216528ms till timeout)
2022-03-28 19:00:27 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (737007ms till timeout)
2022-03-28 19:00:27 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1058390ms till timeout)
2022-03-28 19:00:27 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (189005ms till timeout)
2022-03-28 19:00:27 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1059864ms till timeout)
2022-03-28 19:00:27 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (37085ms till timeout)
2022-03-28 19:00:27 [ForkJoinPool-1-worker-25] INFO  [PodUtils:225] Pod replicas gonna be stable in 14 polls
2022-03-28 19:00:27 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176]  Podmy-cluster-afe273c9-kafka will have 3 replicas not ready, will try again in 1000 ms (174101ms till timeout)
2022-03-28 19:00:28 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (773565ms till timeout)
2022-03-28 19:00:28 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 19:00:28 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 19:00:28 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (467508ms till timeout)
2022-03-28 19:00:28 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1061072ms till timeout)
2022-03-28 19:00:28 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-e6343cec-cruise-control-77c657c774-9pqlz=4f9ddbe4-cd0e-41b5-b7b8-1cacc27bbc84}
2022-03-28 19:00:28 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1061719ms till timeout)
2022-03-28 19:00:28 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1058749ms till timeout)
2022-03-28 19:00:28 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-e6343cec-cruise-control-6cd68d58b9-k7qb2=f6c34d2d-b608-4a7b-912f-33c5db736711, my-cluster-e6343cec-cruise-control-77c657c774-9pqlz=4f9ddbe4-cd0e-41b5-b7b8-1cacc27bbc84}
2022-03-28 19:00:28 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-e6343cec-cruise-control-77c657c774-9pqlz=4f9ddbe4-cd0e-41b5-b7b8-1cacc27bbc84}
2022-03-28 19:00:28 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Deployment my-cluster-e6343cec-cruise-control rolling update in namespace:namespace-3 not ready, will try again in 5000 ms (567803ms till timeout)
2022-03-28 19:00:28 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job consumer-1378029767 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:14Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:28 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (217120ms till timeout)
2022-03-28 19:00:28 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (210620ms till timeout)
2022-03-28 19:00:28 [ForkJoinPool-1-worker-55] DEBUG [ClientUtils:79] Job producer-791126931 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:16Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:28 [ForkJoinPool-1-worker-55] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (212356ms till timeout)
2022-03-28 19:00:28 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (215337ms till timeout)
2022-03-28 19:00:29 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (735815ms till timeout)
2022-03-28 19:00:29 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1057198ms till timeout)
2022-03-28 19:00:29 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (187813ms till timeout)
2022-03-28 19:00:29 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1058673ms till timeout)
2022-03-28 19:00:29 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (35893ms till timeout)
2022-03-28 19:00:29 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 19:00:29 [ForkJoinPool-1-worker-25] INFO  [PodUtils:225] Pod replicas gonna be stable in 13 polls
2022-03-28 19:00:29 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176]  Podmy-cluster-afe273c9-kafka will have 3 replicas not ready, will try again in 1000 ms (172908ms till timeout)
2022-03-28 19:00:29 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (772374ms till timeout)
2022-03-28 19:00:29 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1059880ms till timeout)
2022-03-28 19:00:29 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1060527ms till timeout)
2022-03-28 19:00:29 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1057557ms till timeout)
2022-03-28 19:00:29 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 19:00:29 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 19:00:29 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (465975ms till timeout)
2022-03-28 19:00:29 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (216021ms till timeout)
2022-03-28 19:00:29 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job consumer-1378029767 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:14Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:29 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (209419ms till timeout)
2022-03-28 19:00:29 [ForkJoinPool-1-worker-55] DEBUG [ClientUtils:79] Job producer-791126931 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:16Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:30 [ForkJoinPool-1-worker-55] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (211162ms till timeout)
2022-03-28 19:00:30 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (214143ms till timeout)
2022-03-28 19:00:30 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (186620ms till timeout)
2022-03-28 19:00:30 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (734620ms till timeout)
2022-03-28 19:00:30 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1056003ms till timeout)
2022-03-28 19:00:30 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1057479ms till timeout)
2022-03-28 19:00:30 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (34700ms till timeout)
2022-03-28 19:00:30 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (771275ms till timeout)
2022-03-28 19:00:30 [ForkJoinPool-1-worker-25] INFO  [PodUtils:225] Pod replicas gonna be stable in 12 polls
2022-03-28 19:00:30 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176]  Podmy-cluster-afe273c9-kafka will have 3 replicas not ready, will try again in 1000 ms (171715ms till timeout)
2022-03-28 19:00:30 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1058782ms till timeout)
2022-03-28 19:00:30 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1056458ms till timeout)
2022-03-28 19:00:30 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1059427ms till timeout)
2022-03-28 19:00:30 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 19:00:30 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (214889ms till timeout)
2022-03-28 19:00:30 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job consumer-1378029767 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:14Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:30 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (208221ms till timeout)
2022-03-28 19:00:31 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 19:00:31 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 19:00:31 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (464491ms till timeout)
2022-03-28 19:00:31 [ForkJoinPool-1-worker-55] DEBUG [ClientUtils:79] Job producer-791126931 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:16Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:31 [ForkJoinPool-1-worker-55] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (209969ms till timeout)
2022-03-28 19:00:31 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (212950ms till timeout)
2022-03-28 19:00:31 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1056289ms till timeout)
2022-03-28 19:00:31 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (185427ms till timeout)
2022-03-28 19:00:31 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1054809ms till timeout)
2022-03-28 19:00:31 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (733425ms till timeout)
2022-03-28 19:00:31 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (33506ms till timeout)
2022-03-28 19:00:31 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1057684ms till timeout)
2022-03-28 19:00:31 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (770077ms till timeout)
2022-03-28 19:00:31 [ForkJoinPool-1-worker-25] INFO  [PodUtils:225] Pod replicas gonna be stable in 11 polls
2022-03-28 19:00:31 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176]  Podmy-cluster-afe273c9-kafka will have 3 replicas not ready, will try again in 1000 ms (170518ms till timeout)
2022-03-28 19:00:31 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1055356ms till timeout)
2022-03-28 19:00:31 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1058326ms till timeout)
2022-03-28 19:00:31 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (213789ms till timeout)
2022-03-28 19:00:32 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 19:00:32 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job consumer-1378029767 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:14Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:32 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (207026ms till timeout)
2022-03-28 19:00:32 [ForkJoinPool-1-worker-55] DEBUG [ClientUtils:79] Job producer-791126931 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:16Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:32 [ForkJoinPool-1-worker-55] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (208777ms till timeout)
2022-03-28 19:00:32 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (211756ms till timeout)
2022-03-28 19:00:32 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 19:00:32 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 19:00:32 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (463021ms till timeout)
2022-03-28 19:00:32 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1053618ms till timeout)
2022-03-28 19:00:32 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (184230ms till timeout)
2022-03-28 19:00:32 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1055090ms till timeout)
2022-03-28 19:00:32 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (732230ms till timeout)
2022-03-28 19:00:32 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (32307ms till timeout)
2022-03-28 19:00:32 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (768889ms till timeout)
2022-03-28 19:00:32 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1056488ms till timeout)
2022-03-28 19:00:32 [ForkJoinPool-1-worker-25] INFO  [PodUtils:225] Pod replicas gonna be stable in 10 polls
2022-03-28 19:00:32 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176]  Podmy-cluster-afe273c9-kafka will have 3 replicas not ready, will try again in 1000 ms (169329ms till timeout)
2022-03-28 19:00:32 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1054167ms till timeout)
2022-03-28 19:00:32 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1057137ms till timeout)
2022-03-28 19:00:32 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (212688ms till timeout)
2022-03-28 19:00:33 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job consumer-1378029767 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:14Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:33 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (205832ms till timeout)
2022-03-28 19:00:33 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-e6343cec-cruise-control-77c657c774-9pqlz=4f9ddbe4-cd0e-41b5-b7b8-1cacc27bbc84}
2022-03-28 19:00:33 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 19:00:33 [ForkJoinPool-1-worker-55] DEBUG [ClientUtils:79] Job producer-791126931 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:16Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:33 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-e6343cec-cruise-control-6cd68d58b9-k7qb2=f6c34d2d-b608-4a7b-912f-33c5db736711, my-cluster-e6343cec-cruise-control-77c657c774-9pqlz=4f9ddbe4-cd0e-41b5-b7b8-1cacc27bbc84}
2022-03-28 19:00:33 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-e6343cec-cruise-control-77c657c774-9pqlz=4f9ddbe4-cd0e-41b5-b7b8-1cacc27bbc84}
2022-03-28 19:00:33 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Deployment my-cluster-e6343cec-cruise-control rolling update in namespace:namespace-3 not ready, will try again in 5000 ms (562513ms till timeout)
2022-03-28 19:00:33 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (210480ms till timeout)
2022-03-28 19:00:33 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (731043ms till timeout)
2022-03-28 19:00:33 [ForkJoinPool-1-worker-55] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (207391ms till timeout)
2022-03-28 19:00:33 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (183041ms till timeout)
2022-03-28 19:00:33 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1053902ms till timeout)
2022-03-28 19:00:33 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1052426ms till timeout)
2022-03-28 19:00:33 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (31122ms till timeout)
2022-03-28 19:00:33 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (767706ms till timeout)
2022-03-28 19:00:33 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1055308ms till timeout)
2022-03-28 19:00:33 [ForkJoinPool-1-worker-25] INFO  [PodUtils:225] Pod replicas gonna be stable in 9 polls
2022-03-28 19:00:33 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176]  Podmy-cluster-afe273c9-kafka will have 3 replicas not ready, will try again in 1000 ms (168144ms till timeout)
2022-03-28 19:00:33 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1055954ms till timeout)
2022-03-28 19:00:33 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1052984ms till timeout)
2022-03-28 19:00:34 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 19:00:34 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 19:00:34 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (461528ms till timeout)
2022-03-28 19:00:34 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (211548ms till timeout)
2022-03-28 19:00:34 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job consumer-1378029767 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:14Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:34 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (204638ms till timeout)
2022-03-28 19:00:34 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (209289ms till timeout)
2022-03-28 19:00:34 [ForkJoinPool-1-worker-55] DEBUG [ClientUtils:79] Job producer-791126931 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:16Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:34 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (729857ms till timeout)
2022-03-28 19:00:34 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1052717ms till timeout)
2022-03-28 19:00:34 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (181855ms till timeout)
2022-03-28 19:00:34 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1051239ms till timeout)
2022-03-28 19:00:34 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (766603ms till timeout)
2022-03-28 19:00:35 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 19:00:35 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (29854ms till timeout)
2022-03-28 19:00:35 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1054117ms till timeout)
2022-03-28 19:00:35 [ForkJoinPool-1-worker-25] INFO  [PodUtils:225] Pod replicas gonna be stable in 8 polls
2022-03-28 19:00:35 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176]  Podmy-cluster-afe273c9-kafka will have 3 replicas not ready, will try again in 1000 ms (166955ms till timeout)
2022-03-28 19:00:35 [ForkJoinPool-1-worker-55] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (206023ms till timeout)
2022-03-28 19:00:35 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1054767ms till timeout)
2022-03-28 19:00:35 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1051797ms till timeout)
2022-03-28 19:00:35 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (210362ms till timeout)
2022-03-28 19:00:35 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 19:00:35 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 19:00:35 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (460051ms till timeout)
2022-03-28 19:00:35 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job consumer-1378029767 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:14Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:35 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (203443ms till timeout)
2022-03-28 19:00:36 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (208097ms till timeout)
2022-03-28 19:00:36 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1051526ms till timeout)
2022-03-28 19:00:36 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1050049ms till timeout)
2022-03-28 19:00:36 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1053016ms till timeout)
2022-03-28 19:00:36 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (765414ms till timeout)
2022-03-28 19:00:36 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (180663ms till timeout)
2022-03-28 19:00:36 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (728579ms till timeout)
2022-03-28 19:00:36 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (28661ms till timeout)
2022-03-28 19:00:36 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1053668ms till timeout)
2022-03-28 19:00:36 [ForkJoinPool-1-worker-55] DEBUG [ClientUtils:79] Job producer-791126931 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:16Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:36 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1050696ms till timeout)
2022-03-28 19:00:36 [ForkJoinPool-1-worker-25] INFO  [PodUtils:225] Pod replicas gonna be stable in 7 polls
2022-03-28 19:00:36 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176]  Podmy-cluster-afe273c9-kafka will have 3 replicas not ready, will try again in 1000 ms (165765ms till timeout)
2022-03-28 19:00:36 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (209262ms till timeout)
2022-03-28 19:00:36 [ForkJoinPool-1-worker-55] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (204730ms till timeout)
2022-03-28 19:00:36 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 19:00:36 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job consumer-1378029767 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:14Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:36 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 19:00:36 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 19:00:36 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (458583ms till timeout)
2022-03-28 19:00:36 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (202247ms till timeout)
2022-03-28 19:00:37 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (206905ms till timeout)
2022-03-28 19:00:37 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (764221ms till timeout)
2022-03-28 19:00:37 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1048855ms till timeout)
2022-03-28 19:00:37 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1051823ms till timeout)
2022-03-28 19:00:37 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1050331ms till timeout)
2022-03-28 19:00:37 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (727471ms till timeout)
2022-03-28 19:00:37 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1049508ms till timeout)
2022-03-28 19:00:37 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1052477ms till timeout)
2022-03-28 19:00:37 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (27463ms till timeout)
2022-03-28 19:00:37 [ForkJoinPool-1-worker-25] INFO  [PodUtils:225] Pod replicas gonna be stable in 6 polls
2022-03-28 19:00:37 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176]  Podmy-cluster-afe273c9-kafka will have 3 replicas not ready, will try again in 1000 ms (164572ms till timeout)
2022-03-28 19:00:37 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (208080ms till timeout)
2022-03-28 19:00:37 [ForkJoinPool-1-worker-55] DEBUG [ClientUtils:79] Job producer-791126931 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:16Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:37 [ForkJoinPool-1-worker-5] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment producer-703025921 deletion
2022-03-28 19:00:37 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for ReplicaSet producer-703025921 to be deleted
2022-03-28 19:00:37 [ForkJoinPool-1-worker-55] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (203353ms till timeout)
2022-03-28 19:00:37 [ForkJoinPool-1-worker-5] DEBUG [JobUtils:40] Job producer-703025921 was deleted
2022-03-28 19:00:37 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 19:00:38 [ForkJoinPool-1-worker-5] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment consumer-1028606357 deletion
2022-03-28 19:00:38 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for ReplicaSet consumer-1028606357 to be deleted
2022-03-28 19:00:38 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job consumer-1378029767 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:14Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:38 [ForkJoinPool-1-worker-5] DEBUG [JobUtils:40] Job consumer-1028606357 was deleted
2022-03-28 19:00:38 [ForkJoinPool-1-worker-5] DEBUG [AbstractST:674] ============================================================================
2022-03-28 19:00:38 [ForkJoinPool-1-worker-5] DEBUG [AbstractST:675] [bridge.HttpBridgeScramShaST - After Each] - Clean up after test
2022-03-28 19:00:38 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:344] ############################################################################
2022-03-28 19:00:38 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:348] Delete all resources for testReceiveSimpleMessageTlsScramSha
2022-03-28 19:00:38 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:241] Delete of Job consumer-1028606357 in namespace http-bridge-scram-sha-st
2022-03-28 19:00:38 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (200901ms till timeout)
2022-03-28 19:00:38 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:consumer-1028606357
2022-03-28 19:00:38 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 19:00:38 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 19:00:38 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (457132ms till timeout)
2022-03-28 19:00:38 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1050687ms till timeout)
2022-03-28 19:00:38 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (763085ms till timeout)
2022-03-28 19:00:38 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1047719ms till timeout)
2022-03-28 19:00:38 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (726335ms till timeout)
2022-03-28 19:00:38 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1049195ms till timeout)
2022-03-28 19:00:38 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (205760ms till timeout)
2022-03-28 19:00:38 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:241] Delete of Job producer-703025921 in namespace http-bridge-scram-sha-st
2022-03-28 19:00:38 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1051331ms till timeout)
2022-03-28 19:00:38 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1048361ms till timeout)
2022-03-28 19:00:38 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (26319ms till timeout)
2022-03-28 19:00:38 [ForkJoinPool-1-worker-25] INFO  [PodUtils:225] Pod replicas gonna be stable in 5 polls
2022-03-28 19:00:38 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176]  Podmy-cluster-afe273c9-kafka will have 3 replicas not ready, will try again in 1000 ms (163428ms till timeout)
2022-03-28 19:00:38 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (206929ms till timeout)
2022-03-28 19:00:38 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-e6343cec-cruise-control-77c657c774-9pqlz=4f9ddbe4-cd0e-41b5-b7b8-1cacc27bbc84}
2022-03-28 19:00:38 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:producer-703025921
2022-03-28 19:00:38 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:241] Delete of KafkaTopic my-topic-1059523275-384667503 in namespace http-bridge-scram-sha-st
2022-03-28 19:00:38 [ForkJoinPool-1-worker-55] DEBUG [ClientUtils:79] Job producer-791126931 in namespace http-bridge-scram-sha-st, has status JobStatus(active=null, completedIndexes=null, completionTime=2022-03-28T19:00:34Z, conditions=[JobCondition(lastProbeTime=2022-03-28T19:00:34Z, lastTransitionTime=2022-03-28T19:00:34Z, message=null, reason=null, status=True, type=Complete, additionalProperties={})], failed=null, ready=null, startTime=2022-03-28T19:00:16Z, succeeded=1, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:39 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-e6343cec-cruise-control-6cd68d58b9-k7qb2=f6c34d2d-b608-4a7b-912f-33c5db736711, my-cluster-e6343cec-cruise-control-77c657c774-9pqlz=4f9ddbe4-cd0e-41b5-b7b8-1cacc27bbc84}
2022-03-28 19:00:39 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-e6343cec-cruise-control-77c657c774-9pqlz=4f9ddbe4-cd0e-41b5-b7b8-1cacc27bbc84}
2022-03-28 19:00:39 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Deployment my-cluster-e6343cec-cruise-control rolling update in namespace:namespace-3 not ready, will try again in 5000 ms (557134ms till timeout)
2022-03-28 19:00:39 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1059523275-384667503
2022-03-28 19:00:39 [ForkJoinPool-1-worker-55] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment producer-791126931 deletion
2022-03-28 19:00:39 [ForkJoinPool-1-worker-55] DEBUG [TestUtils:125] Waiting for ReplicaSet producer-791126931 to be deleted
2022-03-28 19:00:39 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:369] ############################################################################
2022-03-28 19:00:39 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:267] testReceiveSimpleMessageTlsScramSha - Notifies waiting test cases:[testPauseReconciliationInKafkaRebalanceAndTopic, testPauseReconciliationInKafkaAndKafkaConnectWithConnector, testCapacityFile, testConfigurationFileIsCreated, testConfigurationPerformanceOptions, testConfigurationReflection, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testDeployAndUnDeployCruiseControl, testTlsExternalUserWithQuotas, testCreatingUsersWithSecretPrefix, testThrottlingQuotasCreateTopic, testThrottlingQuotasCreateAlterPartitions, testKafkaAdminTopicOperations, testUserTemplate, testUpdateUser, testTlsExternalUser, testSendSimpleMessageTls, testReceiveSimpleMessageTlsScramSha] to and randomly select one to start execution
2022-03-28 19:00:39 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:93] [bridge.HttpBridgeScramShaST] - Removing parallel test: testReceiveSimpleMessageTlsScramSha
2022-03-28 19:00:39 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:97] [bridge.HttpBridgeScramShaST] - Parallel test count: 15
2022-03-28 19:00:39 [ForkJoinPool-1-worker-5] INFO  [TestSeparator:29] io.strimzi.systemtest.bridge.HttpBridgeScramShaST.testReceiveSimpleMessageTlsScramSha-FINISHED
2022-03-28 19:00:39 [ForkJoinPool-1-worker-5] INFO  [TestSeparator:30] ############################################################################
2022-03-28 19:00:39 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 19:00:39 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job consumer-1378029767 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:14Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:39 [ForkJoinPool-1-worker-55] DEBUG [JobUtils:40] Job producer-791126931 was deleted
2022-03-28 19:00:39 [ForkJoinPool-1-worker-55] INFO  [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-03-28 19:00:39 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (199667ms till timeout)
2022-03-28 19:00:39 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (761936ms till timeout)
2022-03-28 19:00:39 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1048047ms till timeout)
2022-03-28 19:00:39 [ForkJoinPool-1-worker-55] INFO  [ResourceManager:155] Create/Update Job consumer-1716979977 in namespace http-bridge-scram-sha-st
2022-03-28 19:00:39 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1046567ms till timeout)
2022-03-28 19:00:39 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1049535ms till timeout)
2022-03-28 19:00:39 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (725183ms till timeout)
2022-03-28 19:00:39 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (204597ms till timeout)
2022-03-28 19:00:39 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1050185ms till timeout)
2022-03-28 19:00:39 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1047214ms till timeout)
2022-03-28 19:00:39 [ForkJoinPool-1-worker-25] INFO  [PodUtils:225] Pod replicas gonna be stable in 4 polls
2022-03-28 19:00:39 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176]  Podmy-cluster-afe273c9-kafka will have 3 replicas not ready, will try again in 1000 ms (162281ms till timeout)
2022-03-28 19:00:39 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (25162ms till timeout)
2022-03-28 19:00:39 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (205780ms till timeout)
2022-03-28 19:00:39 [ForkJoinPool-1-worker-55] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:consumer-1716979977
2022-03-28 19:00:39 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 19:00:39 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 19:00:39 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (455673ms till timeout)
2022-03-28 19:00:39 [ForkJoinPool-1-worker-55] INFO  [JobUtils:81] Waiting for job: consumer-1716979977 will be in active state
2022-03-28 19:00:39 [ForkJoinPool-1-worker-55] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 19:00:40 [ForkJoinPool-1-worker-55] INFO  [ClientUtils:76] Waiting for producer/consumer:consumer-1716979977 to finished
2022-03-28 19:00:40 [ForkJoinPool-1-worker-55] DEBUG [TestUtils:125] Waiting for job finished
2022-03-28 19:00:40 [ForkJoinPool-1-worker-55] DEBUG [ClientUtils:79] Job consumer-1716979977 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:35Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:40 [ForkJoinPool-1-worker-55] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (219807ms till timeout)
2022-03-28 19:00:40 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job consumer-1378029767 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:14Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:40 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (198471ms till timeout)
2022-03-28 19:00:40 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1045382ms till timeout)
2022-03-28 19:00:40 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (723999ms till timeout)
2022-03-28 19:00:40 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1046858ms till timeout)
2022-03-28 19:00:40 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (760747ms till timeout)
2022-03-28 19:00:40 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1048349ms till timeout)
2022-03-28 19:00:40 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (203405ms till timeout)
2022-03-28 19:00:40 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 19:00:40 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1046026ms till timeout)
2022-03-28 19:00:40 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1048995ms till timeout)
2022-03-28 19:00:40 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (23970ms till timeout)
2022-03-28 19:00:40 [ForkJoinPool-1-worker-25] INFO  [PodUtils:225] Pod replicas gonna be stable in 3 polls
2022-03-28 19:00:40 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176]  Podmy-cluster-afe273c9-kafka will have 3 replicas not ready, will try again in 1000 ms (161079ms till timeout)
2022-03-28 19:00:41 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (204596ms till timeout)
2022-03-28 19:00:41 [ForkJoinPool-1-worker-55] DEBUG [ClientUtils:79] Job consumer-1716979977 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:35Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:41 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 19:00:41 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 19:00:41 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (454200ms till timeout)
2022-03-28 19:00:41 [ForkJoinPool-1-worker-55] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (218614ms till timeout)
2022-03-28 19:00:41 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job consumer-1378029767 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:14Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:41 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1044285ms till timeout)
2022-03-28 19:00:42 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (759555ms till timeout)
2022-03-28 19:00:42 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1047156ms till timeout)
2022-03-28 19:00:42 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1045664ms till timeout)
2022-03-28 19:00:42 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (197179ms till timeout)
2022-03-28 19:00:42 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (722803ms till timeout)
2022-03-28 19:00:42 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (202140ms till timeout)
2022-03-28 19:00:42 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1044833ms till timeout)
2022-03-28 19:00:42 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (22705ms till timeout)
2022-03-28 19:00:42 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1047715ms till timeout)
2022-03-28 19:00:42 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (203407ms till timeout)
2022-03-28 19:00:42 [ForkJoinPool-1-worker-25] INFO  [PodUtils:225] Pod replicas gonna be stable in 2 polls
2022-03-28 19:00:42 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176]  Podmy-cluster-afe273c9-kafka will have 3 replicas not ready, will try again in 1000 ms (159814ms till timeout)
2022-03-28 19:00:42 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 19:00:42 [ForkJoinPool-1-worker-55] DEBUG [ClientUtils:79] Job consumer-1716979977 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:35Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:42 [ForkJoinPool-1-worker-55] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (217421ms till timeout)
2022-03-28 19:00:42 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 19:00:42 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 19:00:42 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (452726ms till timeout)
2022-03-28 19:00:43 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1043188ms till timeout)
2022-03-28 19:00:43 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (758458ms till timeout)
2022-03-28 19:00:43 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1044474ms till timeout)
2022-03-28 19:00:43 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job consumer-1378029767 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:14Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:43 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (721612ms till timeout)
2022-03-28 19:00:43 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1045963ms till timeout)
2022-03-28 19:00:43 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (200949ms till timeout)
2022-03-28 19:00:43 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1046612ms till timeout)
2022-03-28 19:00:43 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1043641ms till timeout)
2022-03-28 19:00:43 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (21513ms till timeout)
2022-03-28 19:00:43 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (202216ms till timeout)
2022-03-28 19:00:43 [ForkJoinPool-1-worker-25] INFO  [PodUtils:225] Pod replicas gonna be stable in 1 polls
2022-03-28 19:00:43 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176]  Podmy-cluster-afe273c9-kafka will have 3 replicas not ready, will try again in 1000 ms (158622ms till timeout)
2022-03-28 19:00:43 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (195804ms till timeout)
2022-03-28 19:00:43 [ForkJoinPool-1-worker-55] DEBUG [ClientUtils:79] Job consumer-1716979977 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:35Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:43 [ForkJoinPool-1-worker-55] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (216227ms till timeout)
2022-03-28 19:00:43 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 19:00:44 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-e6343cec-cruise-control-77c657c774-9pqlz=4f9ddbe4-cd0e-41b5-b7b8-1cacc27bbc84}
2022-03-28 19:00:44 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1042090ms till timeout)
2022-03-28 19:00:44 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (757351ms till timeout)
2022-03-28 19:00:44 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1043376ms till timeout)
2022-03-28 19:00:44 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 19:00:44 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 19:00:44 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (451199ms till timeout)
2022-03-28 19:00:44 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (720420ms till timeout)
2022-03-28 19:00:44 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1044771ms till timeout)
2022-03-28 19:00:44 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1045511ms till timeout)
2022-03-28 19:00:44 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1042541ms till timeout)
2022-03-28 19:00:44 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-e6343cec-cruise-control-6cd68d58b9-k7qb2=f6c34d2d-b608-4a7b-912f-33c5db736711, my-cluster-e6343cec-cruise-control-77c657c774-9pqlz=4f9ddbe4-cd0e-41b5-b7b8-1cacc27bbc84}
2022-03-28 19:00:44 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-e6343cec-cruise-control-77c657c774-9pqlz=4f9ddbe4-cd0e-41b5-b7b8-1cacc27bbc84}
2022-03-28 19:00:44 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Deployment my-cluster-e6343cec-cruise-control rolling update in namespace:namespace-3 not ready, will try again in 5000 ms (551787ms till timeout)
2022-03-28 19:00:44 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (199754ms till timeout)
2022-03-28 19:00:44 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job consumer-1378029767 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:14Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:44 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (201021ms till timeout)
2022-03-28 19:00:44 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (20318ms till timeout)
2022-03-28 19:00:44 [ForkJoinPool-1-worker-25] INFO  [PodUtils:217] Pod replicas are stable for 20 polls intervals
2022-03-28 19:00:44 [ForkJoinPool-1-worker-25] INFO  [PodUtils:228] Pod my-cluster-afe273c9-kafka has 3 replicas
2022-03-28 19:00:44 [ForkJoinPool-1-worker-25] INFO  [ReconciliationST:90] Setting annotation to "false", Kafka should be scaled to 4
2022-03-28 19:00:44 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (194604ms till timeout)
2022-03-28 19:00:44 [ForkJoinPool-1-worker-25] INFO  [RollingUpdateUtils:127] Waiting for 4 Pod(s) of my-cluster-afe273c9-kafka to be ready
2022-03-28 19:00:44 [ForkJoinPool-1-worker-25] DEBUG [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready
2022-03-28 19:00:44 [ForkJoinPool-1-worker-55] DEBUG [ClientUtils:79] Job consumer-1716979977 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:35Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:44 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:00:44 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2399807ms till timeout)
2022-03-28 19:00:45 [ForkJoinPool-1-worker-55] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (214937ms till timeout)
2022-03-28 19:00:45 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1040991ms till timeout)
2022-03-28 19:00:45 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (756253ms till timeout)
2022-03-28 19:00:45 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 19:00:45 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1042270ms till timeout)
2022-03-28 19:00:45 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1041437ms till timeout)
2022-03-28 19:00:45 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1043665ms till timeout)
2022-03-28 19:00:45 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (719313ms till timeout)
2022-03-28 19:00:45 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1044406ms till timeout)
2022-03-28 19:00:45 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (198642ms till timeout)
2022-03-28 19:00:45 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job consumer-1378029767 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:14Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:45 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (199905ms till timeout)
2022-03-28 19:00:45 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (19201ms till timeout)
2022-03-28 19:00:45 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (193400ms till timeout)
2022-03-28 19:00:45 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 19:00:45 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 1
2022-03-28 19:00:45 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 19:00:45 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Error from server (NotFound): namespaces "namespace-6" not found
2022-03-28 19:00:45 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] ======STDERR END======
2022-03-28 19:00:45 [ForkJoinPool-1-worker-17] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[http-bridge-scram-sha-st], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[namespace-8], io.strimzi.test.logs.CollectorElement@c4c0ea0=[namespace-1], io.strimzi.test.logs.CollectorElement@aec142ef=[namespace-5], io.strimzi.test.logs.CollectorElement@24b97ba9=[namespace-7], io.strimzi.test.logs.CollectorElement@f851b6c3=[namespace-4], io.strimzi.test.logs.CollectorElement@3881d5f2=[namespace-3], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@5c7379cb=[namespace-9], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[namespace-0], io.strimzi.test.logs.CollectorElement@f4405b3b=[namespace-2], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 19:00:45 [ForkJoinPool-1-worker-17] DEBUG [SuiteThreadController:267] testCapacityFile - Notifies waiting test cases:[testPauseReconciliationInKafkaRebalanceAndTopic, testPauseReconciliationInKafkaAndKafkaConnectWithConnector, testCapacityFile, testConfigurationFileIsCreated, testConfigurationPerformanceOptions, testConfigurationReflection, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testDeployAndUnDeployCruiseControl, testTlsExternalUserWithQuotas, testCreatingUsersWithSecretPrefix, testThrottlingQuotasCreateTopic, testThrottlingQuotasCreateAlterPartitions, testKafkaAdminTopicOperations, testUserTemplate, testUpdateUser, testTlsExternalUser, testSendSimpleMessageTls, testReceiveSimpleMessageTlsScramSha] to and randomly select one to start execution
2022-03-28 19:00:45 [ForkJoinPool-1-worker-17] DEBUG [SuiteThreadController:93] [cruisecontrol.CruiseControlConfigurationST] - Removing parallel test: testCapacityFile
2022-03-28 19:00:45 [ForkJoinPool-1-worker-17] DEBUG [SuiteThreadController:97] [cruisecontrol.CruiseControlConfigurationST] - Parallel test count: 14
2022-03-28 19:00:45 [ForkJoinPool-1-worker-17] INFO  [TestSeparator:29] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testCapacityFile-FINISHED
2022-03-28 19:00:45 [ForkJoinPool-1-worker-17] INFO  [TestSeparator:30] ############################################################################
2022-03-28 19:00:45 [ForkJoinPool-1-worker-41] INFO  [TestSeparator:23] ############################################################################
2022-03-28 19:00:45 [ForkJoinPool-1-worker-41] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.topic.ThrottlingQuotaST.testThrottlingQuotasDeleteTopic-STARTED
2022-03-28 19:00:45 [ForkJoinPool-1-worker-41] DEBUG [AbstractST:658] ============================================================================
2022-03-28 19:00:45 [ForkJoinPool-1-worker-41] DEBUG [AbstractST:659] [operators.topic.ThrottlingQuotaST - Before Each] - Setup test case environment
2022-03-28 19:00:45 [ForkJoinPool-1-worker-41] DEBUG [SuiteThreadController:77] [operators.topic.ThrottlingQuotaST] - Adding parallel test: testThrottlingQuotasDeleteTopic
2022-03-28 19:00:45 [ForkJoinPool-1-worker-41] DEBUG [SuiteThreadController:81] [operators.topic.ThrottlingQuotaST] - Parallel test count: 15
2022-03-28 19:00:45 [ForkJoinPool-1-worker-41] DEBUG [SuiteThreadController:230] testThrottlingQuotasDeleteTopic test now can proceed its execution
2022-03-28 19:00:45 [ForkJoinPool-1-worker-41] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 19:00:45 [ForkJoinPool-1-worker-41] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaAdminTopicOperations=my-cluster-506fda27, testReceiveSimpleMessageTlsScramSha=my-cluster-abc5d34b, testConfigurationFileIsCreated=my-cluster-17fc585b, testThrottlingQuotasCreateAlterPartitions=my-cluster-ec1b34f8, testSendSimpleMessageTls=my-cluster-89673e85, testCreatingUsersWithSecretPrefix=my-cluster-f743fdfd, testUpdateUser=my-cluster-d582eb8c, testThrottlingQuotasDeleteTopic=my-cluster-836b31f8, testConfigurationReflection=my-cluster-188382d8, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-959c6ed5, testTlsExternalUserWithQuotas=my-cluster-100f5461, testDeployAndUnDeployCruiseControl=my-cluster-5dfd606f, testThrottlingQuotasCreateTopic=my-cluster-e9955717, testCapacityFile=my-cluster-855b9105, testConfigurationPerformanceOptions=my-cluster-08515847, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-afe273c9, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-e6343cec, testTlsExternalUser=my-cluster-45cf8ce4, testSendSimpleMessageTlsScramSha=my-cluster-561d4fae, testUserTemplate=my-cluster-927d5a1b}
2022-03-28 19:00:45 [ForkJoinPool-1-worker-41] TRACE [AbstractST:607] USERS_NAME_MAP: {testKafkaAdminTopicOperations=my-user-1780746402-846362125, testReceiveSimpleMessageTlsScramSha=my-user-330562220-1155646423, testConfigurationFileIsCreated=my-user-355815510-1316268151, testThrottlingQuotasCreateAlterPartitions=my-user-333986137-1152182125, testSendSimpleMessageTls=my-user-1887866274-1013125243, testCreatingUsersWithSecretPrefix=my-user-607975993-1210920737, testUpdateUser=my-user-1297536277-778093599, testThrottlingQuotasDeleteTopic=my-user-1567019138-1355600812, testConfigurationReflection=my-user-1890969183-1754686912, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-2104248162-45187051, testTlsExternalUserWithQuotas=my-user-1909583883-1106194424, testDeployAndUnDeployCruiseControl=my-user-506268651-1309117986, testThrottlingQuotasCreateTopic=my-user-1876060546-1887905718, testCapacityFile=my-user-1460643829-103298436, testConfigurationPerformanceOptions=my-user-98362545-1476587236, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-user-8456079-650483351, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-2069267102-581538560, testTlsExternalUser=my-user-399414674-1203409092, testSendSimpleMessageTlsScramSha=my-user-1601209876-518796379, testUserTemplate=my-user-989633374-940717398}
2022-03-28 19:00:45 [ForkJoinPool-1-worker-41] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaAdminTopicOperations=my-topic-481231182-953607496, testReceiveSimpleMessageTlsScramSha=my-topic-1027850608-734754781, testConfigurationFileIsCreated=my-topic-423850169-855699454, testThrottlingQuotasCreateAlterPartitions=my-topic-325891726-169328137, testSendSimpleMessageTls=my-topic-874607937-87907051, testCreatingUsersWithSecretPrefix=my-topic-1727421070-668192795, testUpdateUser=my-topic-1325563869-1324403074, testThrottlingQuotasDeleteTopic=my-topic-1674611395-2048879468, testConfigurationReflection=my-topic-1394577332-1936226438, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-634125597-1746493046, testTlsExternalUserWithQuotas=my-topic-1986059791-167638004, testDeployAndUnDeployCruiseControl=my-topic-465377127-800134460, testThrottlingQuotasCreateTopic=my-topic-1246597695-228473879, testCapacityFile=my-topic-549312506-649708453, testConfigurationPerformanceOptions=my-topic-696588651-1198599652, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-topic-1687496771-142091227, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-293775067-432059493, testTlsExternalUser=my-topic-707627084-257659651, testSendSimpleMessageTlsScramSha=my-topic-1586843669-332477842, testUserTemplate=my-topic-1542379258-2002911578}
2022-03-28 19:00:45 [ForkJoinPool-1-worker-41] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaAdminTopicOperations=my-cluster-506fda27-kafka-clients, testReceiveSimpleMessageTlsScramSha=my-cluster-abc5d34b-kafka-clients, testConfigurationFileIsCreated=my-cluster-17fc585b-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-ec1b34f8-kafka-clients, testSendSimpleMessageTls=my-cluster-89673e85-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-f743fdfd-kafka-clients, testUpdateUser=my-cluster-d582eb8c-kafka-clients, testThrottlingQuotasDeleteTopic=my-cluster-836b31f8-kafka-clients, testConfigurationReflection=my-cluster-188382d8-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-959c6ed5-kafka-clients, testTlsExternalUserWithQuotas=my-cluster-100f5461-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-5dfd606f-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-e9955717-kafka-clients, testCapacityFile=my-cluster-855b9105-kafka-clients, testConfigurationPerformanceOptions=my-cluster-08515847-kafka-clients, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-afe273c9-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-e6343cec-kafka-clients, testTlsExternalUser=my-cluster-45cf8ce4-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-561d4fae-kafka-clients, testUserTemplate=my-cluster-927d5a1b-kafka-clients}
2022-03-28 19:00:45 [ForkJoinPool-1-worker-41] INFO  [ResourceManager:155] Create/Update KafkaUser my-user-1567019138-1355600812 in namespace throttling-quota-st
2022-03-28 19:00:45 [ForkJoinPool-1-worker-41] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-1567019138-1355600812
2022-03-28 19:00:46 [ForkJoinPool-1-worker-41] INFO  [ResourceManager:433] Wait for KafkaUser: my-user-1567019138-1355600812 will have desired state: Ready
2022-03-28 19:00:46 [ForkJoinPool-1-worker-41] DEBUG [TestUtils:125] Waiting for KafkaUser: my-user-1567019138-1355600812 will have desired state: Ready
2022-03-28 19:00:46 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:00:46 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2398647ms till timeout)
2022-03-28 19:00:46 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] KafkaUser: my-user-1567019138-1355600812 will have desired state: Ready not ready, will try again in 1000 ms (179810ms till timeout)
2022-03-28 19:00:46 [ForkJoinPool-1-worker-55] DEBUG [ClientUtils:79] Job consumer-1716979977 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:35Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:46 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1039863ms till timeout)
2022-03-28 19:00:46 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (755134ms till timeout)
2022-03-28 19:00:46 [ForkJoinPool-1-worker-55] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (213586ms till timeout)
2022-03-28 19:00:46 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1041150ms till timeout)
2022-03-28 19:00:46 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1042546ms till timeout)
2022-03-28 19:00:46 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1043287ms till timeout)
2022-03-28 19:00:46 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (718194ms till timeout)
2022-03-28 19:00:46 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1040317ms till timeout)
2022-03-28 19:00:46 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (197515ms till timeout)
2022-03-28 19:00:46 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (198754ms till timeout)
2022-03-28 19:00:46 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (18011ms till timeout)
2022-03-28 19:00:46 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job consumer-1378029767 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:14Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:47 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (192178ms till timeout)
2022-03-28 19:00:47 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:00:47 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2397547ms till timeout)
2022-03-28 19:00:47 [ForkJoinPool-1-worker-41] INFO  [ResourceManager:444] KafkaUser: my-user-1567019138-1355600812 is in desired state: Ready
2022-03-28 19:00:47 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1038763ms till timeout)
2022-03-28 19:00:47 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (754032ms till timeout)
2022-03-28 19:00:47 [ForkJoinPool-1-worker-55] DEBUG [ClientUtils:79] Job consumer-1716979977 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:35Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:47 [ForkJoinPool-1-worker-41] INFO  [ThrottlingQuotaST:112] Executing 1/5 iteration.
2022-03-28 19:00:47 [ForkJoinPool-1-worker-41] INFO  [ResourceManager:155] Create/Update Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st
2022-03-28 19:00:47 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1040048ms till timeout)
2022-03-28 19:00:47 [ForkJoinPool-1-worker-55] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (212385ms till timeout)
2022-03-28 19:00:47 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1042181ms till timeout)
2022-03-28 19:00:47 [ForkJoinPool-1-worker-41] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:create-admin-my-cluster-836b31f8-kafka-clients
2022-03-28 19:00:47 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1041437ms till timeout)
2022-03-28 19:00:47 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1039208ms till timeout)
2022-03-28 19:00:47 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (717082ms till timeout)
2022-03-28 19:00:47 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (196387ms till timeout)
2022-03-28 19:00:47 [ForkJoinPool-1-worker-41] INFO  [JobUtils:81] Waiting for job: create-admin-my-cluster-836b31f8-kafka-clients will be in active state
2022-03-28 19:00:47 [ForkJoinPool-1-worker-41] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 19:00:47 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (197641ms till timeout)
2022-03-28 19:00:48 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (16854ms till timeout)
2022-03-28 19:00:48 [ForkJoinPool-1-worker-41] INFO  [ClientUtils:76] Waiting for producer/consumer:create-admin-my-cluster-836b31f8-kafka-clients to finished
2022-03-28 19:00:48 [ForkJoinPool-1-worker-41] DEBUG [TestUtils:125] Waiting for job finished
2022-03-28 19:00:48 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job consumer-1378029767 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:14Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:48 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:48 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (190863ms till timeout)
2022-03-28 19:00:48 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (219714ms till timeout)
2022-03-28 19:00:48 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:00:48 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2396365ms till timeout)
2022-03-28 19:00:48 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1037665ms till timeout)
2022-03-28 19:00:48 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (752924ms till timeout)
2022-03-28 19:00:48 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1038936ms till timeout)
2022-03-28 19:00:48 [ForkJoinPool-1-worker-55] DEBUG [ClientUtils:79] Job consumer-1716979977 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:35Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:48 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1038070ms till timeout)
2022-03-28 19:00:48 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (715945ms till timeout)
2022-03-28 19:00:48 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1040294ms till timeout)
2022-03-28 19:00:48 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1041035ms till timeout)
2022-03-28 19:00:49 [ForkJoinPool-1-worker-55] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (211027ms till timeout)
2022-03-28 19:00:49 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (195244ms till timeout)
2022-03-28 19:00:49 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (196510ms till timeout)
2022-03-28 19:00:49 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (15678ms till timeout)
2022-03-28 19:00:49 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-e6343cec-cruise-control-77c657c774-9pqlz=4f9ddbe4-cd0e-41b5-b7b8-1cacc27bbc84}
2022-03-28 19:00:49 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job consumer-1378029767 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:14Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:49 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:49 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (189575ms till timeout)
2022-03-28 19:00:49 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:00:49 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2395173ms till timeout)
2022-03-28 19:00:49 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (218427ms till timeout)
2022-03-28 19:00:49 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1036486ms till timeout)
2022-03-28 19:00:49 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-e6343cec-cruise-control-6cd68d58b9-k7qb2=f6c34d2d-b608-4a7b-912f-33c5db736711, my-cluster-e6343cec-cruise-control-77c657c774-9pqlz=4f9ddbe4-cd0e-41b5-b7b8-1cacc27bbc84}
2022-03-28 19:00:49 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-e6343cec-cruise-control-77c657c774-9pqlz=4f9ddbe4-cd0e-41b5-b7b8-1cacc27bbc84}
2022-03-28 19:00:49 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Deployment my-cluster-e6343cec-cruise-control rolling update in namespace:namespace-3 not ready, will try again in 5000 ms (546471ms till timeout)
2022-03-28 19:00:49 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (751758ms till timeout)
2022-03-28 19:00:49 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1037774ms till timeout)
2022-03-28 19:00:50 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (714818ms till timeout)
2022-03-28 19:00:50 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1036941ms till timeout)
2022-03-28 19:00:50 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1039169ms till timeout)
2022-03-28 19:00:50 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1039910ms till timeout)
2022-03-28 19:00:50 [ForkJoinPool-1-worker-55] DEBUG [ClientUtils:79] Job consumer-1716979977 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:35Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:50 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (194040ms till timeout)
2022-03-28 19:00:50 [ForkJoinPool-1-worker-55] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (209727ms till timeout)
2022-03-28 19:00:50 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (195307ms till timeout)
2022-03-28 19:00:50 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (14507ms till timeout)
2022-03-28 19:00:50 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job consumer-1378029767 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:14Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:50 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:50 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1035388ms till timeout)
2022-03-28 19:00:50 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:00:50 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2393981ms till timeout)
2022-03-28 19:00:50 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (750658ms till timeout)
2022-03-28 19:00:50 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (188285ms till timeout)
2022-03-28 19:00:51 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (217134ms till timeout)
2022-03-28 19:00:51 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1036669ms till timeout)
2022-03-28 19:00:51 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1038066ms till timeout)
2022-03-28 19:00:51 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (713715ms till timeout)
2022-03-28 19:00:51 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1038808ms till timeout)
2022-03-28 19:00:51 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1035837ms till timeout)
2022-03-28 19:00:51 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (192936ms till timeout)
2022-03-28 19:00:51 [ForkJoinPool-1-worker-55] DEBUG [ClientUtils:79] Job consumer-1716979977 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:35Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:51 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (194197ms till timeout)
2022-03-28 19:00:51 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (13394ms till timeout)
2022-03-28 19:00:51 [ForkJoinPool-1-worker-55] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (208419ms till timeout)
2022-03-28 19:00:51 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1034288ms till timeout)
2022-03-28 19:00:52 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job consumer-1378029767 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:14Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:52 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (749556ms till timeout)
2022-03-28 19:00:52 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:00:52 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2392783ms till timeout)
2022-03-28 19:00:52 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:52 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (186993ms till timeout)
2022-03-28 19:00:52 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1035477ms till timeout)
2022-03-28 19:00:52 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1036967ms till timeout)
2022-03-28 19:00:52 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1037708ms till timeout)
2022-03-28 19:00:52 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (712615ms till timeout)
2022-03-28 19:00:52 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1034738ms till timeout)
2022-03-28 19:00:52 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (215846ms till timeout)
2022-03-28 19:00:52 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (193076ms till timeout)
2022-03-28 19:00:52 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (191715ms till timeout)
2022-03-28 19:00:52 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (12280ms till timeout)
2022-03-28 19:00:52 [ForkJoinPool-1-worker-55] DEBUG [ClientUtils:79] Job consumer-1716979977 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:35Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:52 [ForkJoinPool-1-worker-55] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (207198ms till timeout)
2022-03-28 19:00:53 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1033191ms till timeout)
2022-03-28 19:00:53 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (748459ms till timeout)
2022-03-28 19:00:53 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:00:53 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2391592ms till timeout)
2022-03-28 19:00:53 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job consumer-1378029767 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:14Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:53 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1035868ms till timeout)
2022-03-28 19:00:53 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1034377ms till timeout)
2022-03-28 19:00:53 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:53 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (711423ms till timeout)
2022-03-28 19:00:53 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1036515ms till timeout)
2022-03-28 19:00:53 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1033545ms till timeout)
2022-03-28 19:00:53 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (185705ms till timeout)
2022-03-28 19:00:53 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (214558ms till timeout)
2022-03-28 19:00:53 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (191917ms till timeout)
2022-03-28 19:00:53 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (190559ms till timeout)
2022-03-28 19:00:53 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (11120ms till timeout)
2022-03-28 19:00:53 [ForkJoinPool-1-worker-55] DEBUG [ClientUtils:79] Job consumer-1716979977 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:35Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:54 [ForkJoinPool-1-worker-55] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (206007ms till timeout)
2022-03-28 19:00:54 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1032094ms till timeout)
2022-03-28 19:00:54 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (747361ms till timeout)
2022-03-28 19:00:54 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:00:54 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2390492ms till timeout)
2022-03-28 19:00:54 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1034770ms till timeout)
2022-03-28 19:00:54 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1035416ms till timeout)
2022-03-28 19:00:54 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1032446ms till timeout)
2022-03-28 19:00:54 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1033183ms till timeout)
2022-03-28 19:00:54 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (710323ms till timeout)
2022-03-28 19:00:54 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job consumer-1378029767 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:14Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:54 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:54 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-e6343cec-cruise-control-77c657c774-9pqlz=4f9ddbe4-cd0e-41b5-b7b8-1cacc27bbc84}
2022-03-28 19:00:54 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (184412ms till timeout)
2022-03-28 19:00:54 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (213264ms till timeout)
2022-03-28 19:00:54 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (190721ms till timeout)
2022-03-28 19:00:54 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (189358ms till timeout)
2022-03-28 19:00:54 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (9917ms till timeout)
2022-03-28 19:00:55 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-e6343cec-cruise-control-6cd68d58b9-k7qb2=f6c34d2d-b608-4a7b-912f-33c5db736711, my-cluster-e6343cec-cruise-control-77c657c774-9pqlz=4f9ddbe4-cd0e-41b5-b7b8-1cacc27bbc84}
2022-03-28 19:00:55 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-e6343cec-cruise-control-77c657c774-9pqlz=4f9ddbe4-cd0e-41b5-b7b8-1cacc27bbc84}
2022-03-28 19:00:55 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Deployment my-cluster-e6343cec-cruise-control rolling update in namespace:namespace-3 not ready, will try again in 5000 ms (541111ms till timeout)
2022-03-28 19:00:55 [ForkJoinPool-1-worker-55] DEBUG [ClientUtils:79] Job consumer-1716979977 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:35Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:55 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1030933ms till timeout)
2022-03-28 19:00:55 [ForkJoinPool-1-worker-55] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (204657ms till timeout)
2022-03-28 19:00:55 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (746204ms till timeout)
2022-03-28 19:00:55 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:00:55 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2389337ms till timeout)
2022-03-28 19:00:55 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1033616ms till timeout)
2022-03-28 19:00:55 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1032030ms till timeout)
2022-03-28 19:00:55 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1031292ms till timeout)
2022-03-28 19:00:55 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1034262ms till timeout)
2022-03-28 19:00:55 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (709168ms till timeout)
2022-03-28 19:00:55 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job consumer-1378029767 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:14Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:55 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:56 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (183124ms till timeout)
2022-03-28 19:00:56 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (189521ms till timeout)
2022-03-28 19:00:56 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (188147ms till timeout)
2022-03-28 19:00:56 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (211977ms till timeout)
2022-03-28 19:00:56 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (8730ms till timeout)
2022-03-28 19:00:56 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1029836ms till timeout)
2022-03-28 19:00:56 [ForkJoinPool-1-worker-55] DEBUG [ClientUtils:79] Job consumer-1716979977 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:35Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:56 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (745105ms till timeout)
2022-03-28 19:00:56 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:00:56 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2388239ms till timeout)
2022-03-28 19:00:56 [ForkJoinPool-1-worker-55] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (203368ms till timeout)
2022-03-28 19:00:56 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1032518ms till timeout)
2022-03-28 19:00:56 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1030931ms till timeout)
2022-03-28 19:00:56 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1030194ms till timeout)
2022-03-28 19:00:56 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1033164ms till timeout)
2022-03-28 19:00:56 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (707975ms till timeout)
2022-03-28 19:00:57 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job consumer-1378029767 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:14Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:57 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:57 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (186973ms till timeout)
2022-03-28 19:00:57 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (188329ms till timeout)
2022-03-28 19:00:57 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (181835ms till timeout)
2022-03-28 19:00:57 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (7532ms till timeout)
2022-03-28 19:00:57 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (210687ms till timeout)
2022-03-28 19:00:57 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1028652ms till timeout)
2022-03-28 19:00:57 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (743923ms till timeout)
2022-03-28 19:00:57 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:00:57 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2387056ms till timeout)
2022-03-28 19:00:57 [ForkJoinPool-1-worker-55] DEBUG [ClientUtils:79] Job consumer-1716979977 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:35Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:57 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1031327ms till timeout)
2022-03-28 19:00:57 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1029750ms till timeout)
2022-03-28 19:00:57 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1031983ms till timeout)
2022-03-28 19:00:57 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1029012ms till timeout)
2022-03-28 19:00:58 [ForkJoinPool-1-worker-55] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (201995ms till timeout)
2022-03-28 19:00:58 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (706794ms till timeout)
2022-03-28 19:00:58 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (185861ms till timeout)
2022-03-28 19:00:58 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job consumer-1378029767 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:14Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:58 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (6423ms till timeout)
2022-03-28 19:00:58 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (187125ms till timeout)
2022-03-28 19:00:58 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:58 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (180530ms till timeout)
2022-03-28 19:00:58 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1027537ms till timeout)
2022-03-28 19:00:58 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (209383ms till timeout)
2022-03-28 19:00:58 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (742807ms till timeout)
2022-03-28 19:00:58 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:00:58 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2385940ms till timeout)
2022-03-28 19:00:58 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1030220ms till timeout)
2022-03-28 19:00:59 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1027897ms till timeout)
2022-03-28 19:00:59 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1030866ms till timeout)
2022-03-28 19:00:59 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1028633ms till timeout)
2022-03-28 19:00:59 [ForkJoinPool-1-worker-55] DEBUG [ClientUtils:79] Job consumer-1716979977 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:35Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:59 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (705678ms till timeout)
2022-03-28 19:00:59 [ForkJoinPool-1-worker-55] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (200783ms till timeout)
2022-03-28 19:00:59 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (184758ms till timeout)
2022-03-28 19:00:59 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (186011ms till timeout)
2022-03-28 19:00:59 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (5308ms till timeout)
2022-03-28 19:00:59 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1026439ms till timeout)
2022-03-28 19:00:59 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:59 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (741709ms till timeout)
2022-03-28 19:00:59 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job consumer-1378029767 in namespace http-bridge-tls-st, has status JobStatus(active=null, completedIndexes=null, completionTime=2022-03-28T19:00:54Z, conditions=[JobCondition(lastProbeTime=2022-03-28T19:00:54Z, lastTransitionTime=2022-03-28T19:00:54Z, message=null, reason=null, status=True, type=Complete, additionalProperties={})], failed=null, ready=null, startTime=2022-03-28T19:00:14Z, succeeded=1, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:00:59 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:00:59 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2384841ms till timeout)
2022-03-28 19:01:00 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (208092ms till timeout)
2022-03-28 19:01:00 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1029119ms till timeout)
2022-03-28 19:01:00 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-e6343cec-cruise-control-77c657c774-9pqlz=4f9ddbe4-cd0e-41b5-b7b8-1cacc27bbc84}
2022-03-28 19:01:00 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1027535ms till timeout)
2022-03-28 19:01:00 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1029767ms till timeout)
2022-03-28 19:01:00 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1026798ms till timeout)
2022-03-28 19:01:00 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (704578ms till timeout)
2022-03-28 19:01:00 [ForkJoinPool-1-worker-23] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment consumer-1378029767 deletion
2022-03-28 19:01:00 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for ReplicaSet consumer-1378029767 to be deleted
2022-03-28 19:01:00 [ForkJoinPool-1-worker-55] DEBUG [ClientUtils:79] Job consumer-1716979977 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:35Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:01:00 [ForkJoinPool-1-worker-23] DEBUG [JobUtils:40] Job consumer-1378029767 was deleted
2022-03-28 19:01:00 [ForkJoinPool-1-worker-23] DEBUG [AbstractST:674] ============================================================================
2022-03-28 19:01:00 [ForkJoinPool-1-worker-23] DEBUG [AbstractST:675] [bridge.HttpBridgeTlsST - After Each] - Clean up after test
2022-03-28 19:01:00 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:344] ############################################################################
2022-03-28 19:01:00 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:348] Delete all resources for testSendSimpleMessageTls
2022-03-28 19:01:00 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:241] Delete of Job producer-1817671133 in namespace http-bridge-tls-st
2022-03-28 19:01:00 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-e6343cec-cruise-control-6cd68d58b9-k7qb2=f6c34d2d-b608-4a7b-912f-33c5db736711, my-cluster-e6343cec-cruise-control-77c657c774-9pqlz=4f9ddbe4-cd0e-41b5-b7b8-1cacc27bbc84}
2022-03-28 19:01:00 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-e6343cec-cruise-control-77c657c774-9pqlz=4f9ddbe4-cd0e-41b5-b7b8-1cacc27bbc84}
2022-03-28 19:01:00 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Deployment my-cluster-e6343cec-cruise-control rolling update in namespace:namespace-3 not ready, will try again in 5000 ms (535758ms till timeout)
2022-03-28 19:01:00 [ForkJoinPool-1-worker-55] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (199495ms till timeout)
2022-03-28 19:01:00 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:producer-1817671133
2022-03-28 19:01:00 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (183597ms till timeout)
2022-03-28 19:01:00 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (184877ms till timeout)
2022-03-28 19:01:00 [ForkJoinPool-1-worker-35] INFO  [PodUtils:189] Message All topics created found in create-admin-my-cluster-506fda27-kafka-clients-7d2b5 log
2022-03-28 19:01:00 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:241] Delete of Job consumer-1378029767 in namespace http-bridge-tls-st
2022-03-28 19:01:00 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1025286ms till timeout)
2022-03-28 19:01:00 [ForkJoinPool-1-worker-35] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment create-admin-my-cluster-506fda27-kafka-clients deletion
2022-03-28 19:01:00 [ForkJoinPool-1-worker-35] DEBUG [TestUtils:125] Waiting for ReplicaSet create-admin-my-cluster-506fda27-kafka-clients to be deleted
2022-03-28 19:01:01 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:consumer-1378029767
2022-03-28 19:01:01 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (740559ms till timeout)
2022-03-28 19:01:01 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] ReplicaSet create-admin-my-cluster-506fda27-kafka-clients to be deleted not ready, will try again in 5000 ms (179820ms till timeout)
2022-03-28 19:01:01 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:01:01 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2383693ms till timeout)
2022-03-28 19:01:01 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:01:01 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:241] Delete of KafkaTopic my-topic-205794532-362905525 in namespace http-bridge-tls-st
2022-03-28 19:01:01 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1027976ms till timeout)
2022-03-28 19:01:01 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1028623ms till timeout)
2022-03-28 19:01:01 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1026390ms till timeout)
2022-03-28 19:01:01 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1025653ms till timeout)
2022-03-28 19:01:01 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (206758ms till timeout)
2022-03-28 19:01:01 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (703434ms till timeout)
2022-03-28 19:01:01 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-205794532-362905525
2022-03-28 19:01:01 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:369] ############################################################################
2022-03-28 19:01:01 [ForkJoinPool-1-worker-23] DEBUG [SuiteThreadController:267] testSendSimpleMessageTls - Notifies waiting test cases:[testPauseReconciliationInKafkaRebalanceAndTopic, testPauseReconciliationInKafkaAndKafkaConnectWithConnector, testCapacityFile, testConfigurationFileIsCreated, testConfigurationPerformanceOptions, testConfigurationReflection, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testDeployAndUnDeployCruiseControl, testTlsExternalUserWithQuotas, testCreatingUsersWithSecretPrefix, testThrottlingQuotasCreateTopic, testThrottlingQuotasCreateAlterPartitions, testKafkaAdminTopicOperations, testUserTemplate, testUpdateUser, testTlsExternalUser, testSendSimpleMessageTls, testReceiveSimpleMessageTlsScramSha, testThrottlingQuotasDeleteTopic] to and randomly select one to start execution
2022-03-28 19:01:01 [ForkJoinPool-1-worker-23] DEBUG [SuiteThreadController:93] [bridge.HttpBridgeTlsST] - Removing parallel test: testSendSimpleMessageTls
2022-03-28 19:01:01 [ForkJoinPool-1-worker-23] DEBUG [SuiteThreadController:97] [bridge.HttpBridgeTlsST] - Parallel test count: 14
2022-03-28 19:01:01 [ForkJoinPool-1-worker-23] INFO  [TestSeparator:29] io.strimzi.systemtest.bridge.HttpBridgeTlsST.testSendSimpleMessageTls-FINISHED
2022-03-28 19:01:01 [ForkJoinPool-1-worker-23] INFO  [TestSeparator:30] ############################################################################
2022-03-28 19:01:01 [ForkJoinPool-1-worker-23] INFO  [TestSeparator:23] ############################################################################
2022-03-28 19:01:01 [ForkJoinPool-1-worker-23] INFO  [TestSeparator:24] io.strimzi.systemtest.bridge.HttpBridgeTlsST.testReceiveSimpleMessageTls-STARTED
2022-03-28 19:01:01 [ForkJoinPool-1-worker-23] DEBUG [AbstractST:658] ============================================================================
2022-03-28 19:01:01 [ForkJoinPool-1-worker-23] DEBUG [AbstractST:659] [bridge.HttpBridgeTlsST - Before Each] - Setup test case environment
2022-03-28 19:01:01 [ForkJoinPool-1-worker-23] DEBUG [SuiteThreadController:77] [bridge.HttpBridgeTlsST] - Adding parallel test: testReceiveSimpleMessageTls
2022-03-28 19:01:01 [ForkJoinPool-1-worker-23] DEBUG [SuiteThreadController:81] [bridge.HttpBridgeTlsST] - Parallel test count: 15
2022-03-28 19:01:01 [ForkJoinPool-1-worker-23] DEBUG [SuiteThreadController:230] testReceiveSimpleMessageTls test now can proceed its execution
2022-03-28 19:01:01 [ForkJoinPool-1-worker-23] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 19:01:01 [ForkJoinPool-1-worker-23] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaAdminTopicOperations=my-cluster-506fda27, testReceiveSimpleMessageTlsScramSha=my-cluster-abc5d34b, testConfigurationFileIsCreated=my-cluster-17fc585b, testThrottlingQuotasCreateAlterPartitions=my-cluster-ec1b34f8, testSendSimpleMessageTls=my-cluster-89673e85, testCreatingUsersWithSecretPrefix=my-cluster-f743fdfd, testUpdateUser=my-cluster-d582eb8c, testThrottlingQuotasDeleteTopic=my-cluster-836b31f8, testConfigurationReflection=my-cluster-188382d8, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-959c6ed5, testTlsExternalUserWithQuotas=my-cluster-100f5461, testDeployAndUnDeployCruiseControl=my-cluster-5dfd606f, testThrottlingQuotasCreateTopic=my-cluster-e9955717, testCapacityFile=my-cluster-855b9105, testConfigurationPerformanceOptions=my-cluster-08515847, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-afe273c9, testReceiveSimpleMessageTls=my-cluster-634e4d65, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-e6343cec, testTlsExternalUser=my-cluster-45cf8ce4, testSendSimpleMessageTlsScramSha=my-cluster-561d4fae, testUserTemplate=my-cluster-927d5a1b}
2022-03-28 19:01:01 [ForkJoinPool-1-worker-23] TRACE [AbstractST:607] USERS_NAME_MAP: {testKafkaAdminTopicOperations=my-user-1780746402-846362125, testReceiveSimpleMessageTlsScramSha=my-user-330562220-1155646423, testConfigurationFileIsCreated=my-user-355815510-1316268151, testThrottlingQuotasCreateAlterPartitions=my-user-333986137-1152182125, testSendSimpleMessageTls=my-user-1887866274-1013125243, testCreatingUsersWithSecretPrefix=my-user-607975993-1210920737, testUpdateUser=my-user-1297536277-778093599, testThrottlingQuotasDeleteTopic=my-user-1567019138-1355600812, testConfigurationReflection=my-user-1890969183-1754686912, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-2104248162-45187051, testTlsExternalUserWithQuotas=my-user-1909583883-1106194424, testDeployAndUnDeployCruiseControl=my-user-506268651-1309117986, testThrottlingQuotasCreateTopic=my-user-1876060546-1887905718, testCapacityFile=my-user-1460643829-103298436, testConfigurationPerformanceOptions=my-user-98362545-1476587236, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-user-8456079-650483351, testReceiveSimpleMessageTls=my-user-337002809-1497514615, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-2069267102-581538560, testTlsExternalUser=my-user-399414674-1203409092, testSendSimpleMessageTlsScramSha=my-user-1601209876-518796379, testUserTemplate=my-user-989633374-940717398}
2022-03-28 19:01:01 [ForkJoinPool-1-worker-23] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaAdminTopicOperations=my-topic-481231182-953607496, testReceiveSimpleMessageTlsScramSha=my-topic-1027850608-734754781, testConfigurationFileIsCreated=my-topic-423850169-855699454, testThrottlingQuotasCreateAlterPartitions=my-topic-325891726-169328137, testSendSimpleMessageTls=my-topic-874607937-87907051, testCreatingUsersWithSecretPrefix=my-topic-1727421070-668192795, testUpdateUser=my-topic-1325563869-1324403074, testThrottlingQuotasDeleteTopic=my-topic-1674611395-2048879468, testConfigurationReflection=my-topic-1394577332-1936226438, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-634125597-1746493046, testTlsExternalUserWithQuotas=my-topic-1986059791-167638004, testDeployAndUnDeployCruiseControl=my-topic-465377127-800134460, testThrottlingQuotasCreateTopic=my-topic-1246597695-228473879, testCapacityFile=my-topic-549312506-649708453, testConfigurationPerformanceOptions=my-topic-696588651-1198599652, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-topic-1687496771-142091227, testReceiveSimpleMessageTls=my-topic-829184226-1519275114, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-293775067-432059493, testTlsExternalUser=my-topic-707627084-257659651, testSendSimpleMessageTlsScramSha=my-topic-1586843669-332477842, testUserTemplate=my-topic-1542379258-2002911578}
2022-03-28 19:01:01 [ForkJoinPool-1-worker-23] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaAdminTopicOperations=my-cluster-506fda27-kafka-clients, testReceiveSimpleMessageTlsScramSha=my-cluster-abc5d34b-kafka-clients, testConfigurationFileIsCreated=my-cluster-17fc585b-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-ec1b34f8-kafka-clients, testSendSimpleMessageTls=my-cluster-89673e85-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-f743fdfd-kafka-clients, testUpdateUser=my-cluster-d582eb8c-kafka-clients, testThrottlingQuotasDeleteTopic=my-cluster-836b31f8-kafka-clients, testConfigurationReflection=my-cluster-188382d8-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-959c6ed5-kafka-clients, testTlsExternalUserWithQuotas=my-cluster-100f5461-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-5dfd606f-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-e9955717-kafka-clients, testCapacityFile=my-cluster-855b9105-kafka-clients, testConfigurationPerformanceOptions=my-cluster-08515847-kafka-clients, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-afe273c9-kafka-clients, testReceiveSimpleMessageTls=my-cluster-634e4d65-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-e6343cec-kafka-clients, testTlsExternalUser=my-cluster-45cf8ce4-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-561d4fae-kafka-clients, testUserTemplate=my-cluster-927d5a1b-kafka-clients}
2022-03-28 19:01:01 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:155] Create/Update KafkaTopic my-topic-1031306309-1345743071 in namespace http-bridge-tls-st
2022-03-28 19:01:01 [ForkJoinPool-1-worker-55] DEBUG [ClientUtils:79] Job consumer-1716979977 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:35Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:01:01 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-1031306309-1345743071
2022-03-28 19:01:01 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (182441ms till timeout)
2022-03-28 19:01:01 [ForkJoinPool-1-worker-55] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (198125ms till timeout)
2022-03-28 19:01:01 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (183693ms till timeout)
2022-03-28 19:01:01 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:433] Wait for KafkaTopic: my-topic-1031306309-1345743071 will have desired state: Ready
2022-03-28 19:01:01 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for KafkaTopic: my-topic-1031306309-1345743071 will have desired state: Ready
2022-03-28 19:01:02 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1024116ms till timeout)
2022-03-28 19:01:02 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] KafkaTopic: my-topic-1031306309-1345743071 will have desired state: Ready not ready, will try again in 1000 ms (179810ms till timeout)
2022-03-28 19:01:02 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (739387ms till timeout)
2022-03-28 19:01:02 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:01:02 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2382519ms till timeout)
2022-03-28 19:01:02 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1026798ms till timeout)
2022-03-28 19:01:02 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1027445ms till timeout)
2022-03-28 19:01:02 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1024469ms till timeout)
2022-03-28 19:01:02 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1025206ms till timeout)
2022-03-28 19:01:02 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:01:02 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (702256ms till timeout)
2022-03-28 19:01:02 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (205352ms till timeout)
2022-03-28 19:01:03 [ForkJoinPool-1-worker-55] DEBUG [ClientUtils:79] Job consumer-1716979977 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:35Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:01:03 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (182584ms till timeout)
2022-03-28 19:01:03 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (181223ms till timeout)
2022-03-28 19:01:03 [ForkJoinPool-1-worker-55] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (196903ms till timeout)
2022-03-28 19:01:03 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1022990ms till timeout)
2022-03-28 19:01:03 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:444] KafkaTopic: my-topic-1031306309-1345743071 is in desired state: Ready
2022-03-28 19:01:03 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:155] Create/Update Job consumer-142659915 in namespace http-bridge-tls-st
2022-03-28 19:01:03 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (738260ms till timeout)
2022-03-28 19:01:03 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:01:03 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2381393ms till timeout)
2022-03-28 19:01:03 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1025671ms till timeout)
2022-03-28 19:01:03 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:consumer-142659915
2022-03-28 19:01:03 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1026320ms till timeout)
2022-03-28 19:01:03 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1024087ms till timeout)
2022-03-28 19:01:03 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1023349ms till timeout)
2022-03-28 19:01:03 [ForkJoinPool-1-worker-23] INFO  [JobUtils:81] Waiting for job: consumer-142659915 will be in active state
2022-03-28 19:01:03 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 19:01:03 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (701132ms till timeout)
2022-03-28 19:01:03 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:01:03 [ForkJoinPool-1-worker-23] INFO  [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-03-28 19:01:03 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:155] Create/Update Job producer-136568709 in namespace http-bridge-tls-st
2022-03-28 19:01:04 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (204130ms till timeout)
2022-03-28 19:01:04 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:producer-136568709
2022-03-28 19:01:04 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (181399ms till timeout)
2022-03-28 19:01:04 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (180036ms till timeout)
2022-03-28 19:01:04 [ForkJoinPool-1-worker-23] INFO  [JobUtils:81] Waiting for job: producer-136568709 will be in active state
2022-03-28 19:01:04 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 19:01:04 [ForkJoinPool-1-worker-55] DEBUG [ClientUtils:79] Job consumer-1716979977 in namespace http-bridge-scram-sha-st, has status JobStatus(active=null, completedIndexes=null, completionTime=2022-03-28T19:00:59Z, conditions=[JobCondition(lastProbeTime=2022-03-28T19:00:59Z, lastTransitionTime=2022-03-28T19:00:59Z, message=null, reason=null, status=True, type=Complete, additionalProperties={})], failed=null, ready=null, startTime=2022-03-28T19:00:35Z, succeeded=1, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:01:04 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1021810ms till timeout)
2022-03-28 19:01:04 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (737080ms till timeout)
2022-03-28 19:01:04 [ForkJoinPool-1-worker-23] INFO  [ClientUtils:61] Waiting till producer producer-136568709 and consumer consumer-142659915 finish
2022-03-28 19:01:04 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for clients finished
2022-03-28 19:01:04 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:01:04 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2380204ms till timeout)
2022-03-28 19:01:04 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1024481ms till timeout)
2022-03-28 19:01:04 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (219801ms till timeout)
2022-03-28 19:01:04 [ForkJoinPool-1-worker-55] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment consumer-1716979977 deletion
2022-03-28 19:01:04 [ForkJoinPool-1-worker-55] DEBUG [TestUtils:125] Waiting for ReplicaSet consumer-1716979977 to be deleted
2022-03-28 19:01:04 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1025131ms till timeout)
2022-03-28 19:01:04 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1022161ms till timeout)
2022-03-28 19:01:04 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1022897ms till timeout)
2022-03-28 19:01:04 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (699939ms till timeout)
2022-03-28 19:01:04 [ForkJoinPool-1-worker-55] DEBUG [JobUtils:40] Job consumer-1716979977 was deleted
2022-03-28 19:01:04 [ForkJoinPool-1-worker-55] DEBUG [AbstractST:674] ============================================================================
2022-03-28 19:01:04 [ForkJoinPool-1-worker-55] DEBUG [AbstractST:675] [bridge.HttpBridgeScramShaST - After Each] - Clean up after test
2022-03-28 19:01:04 [ForkJoinPool-1-worker-55] INFO  [ResourceManager:344] ############################################################################
2022-03-28 19:01:04 [ForkJoinPool-1-worker-55] INFO  [ResourceManager:348] Delete all resources for testSendSimpleMessageTlsScramSha
2022-03-28 19:01:04 [ForkJoinPool-1-worker-55] INFO  [ResourceManager:241] Delete of Job producer-791126931 in namespace http-bridge-scram-sha-st
2022-03-28 19:01:05 [ForkJoinPool-1-worker-55] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:producer-791126931
2022-03-28 19:01:05 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:01:05 [ForkJoinPool-1-worker-55] INFO  [ResourceManager:241] Delete of Job consumer-1716979977 in namespace http-bridge-scram-sha-st
2022-03-28 19:01:05 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (202743ms till timeout)
2022-03-28 19:01:05 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (180203ms till timeout)
2022-03-28 19:01:05 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (178840ms till timeout)
2022-03-28 19:01:05 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-e6343cec-cruise-control-77c657c774-9pqlz=4f9ddbe4-cd0e-41b5-b7b8-1cacc27bbc84}
2022-03-28 19:01:05 [ForkJoinPool-1-worker-55] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:consumer-1716979977
2022-03-28 19:01:05 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1020622ms till timeout)
2022-03-28 19:01:05 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (735893ms till timeout)
2022-03-28 19:01:05 [ForkJoinPool-1-worker-55] INFO  [ResourceManager:241] Delete of KafkaTopic my-topic-367534742-1633325021 in namespace http-bridge-scram-sha-st
2022-03-28 19:01:05 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-e6343cec-cruise-control-6cd68d58b9-k7qb2=f6c34d2d-b608-4a7b-912f-33c5db736711, my-cluster-e6343cec-cruise-control-77c657c774-9pqlz=4f9ddbe4-cd0e-41b5-b7b8-1cacc27bbc84}
2022-03-28 19:01:05 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:01:05 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-e6343cec-cruise-control-77c657c774-9pqlz=4f9ddbe4-cd0e-41b5-b7b8-1cacc27bbc84}
2022-03-28 19:01:05 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2379024ms till timeout)
2022-03-28 19:01:05 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Deployment my-cluster-e6343cec-cruise-control rolling update in namespace:namespace-3 not ready, will try again in 5000 ms (530417ms till timeout)
2022-03-28 19:01:05 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (218624ms till timeout)
2022-03-28 19:01:05 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-08515847 will have desired state: Ready not ready, will try again in 1000 ms (1023304ms till timeout)
2022-03-28 19:01:05 [ForkJoinPool-1-worker-55] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-367534742-1633325021
2022-03-28 19:01:05 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1021720ms till timeout)
2022-03-28 19:01:05 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1020983ms till timeout)
2022-03-28 19:01:05 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1023952ms till timeout)
2022-03-28 19:01:06 [ForkJoinPool-1-worker-55] INFO  [ResourceManager:369] ############################################################################
2022-03-28 19:01:06 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (698764ms till timeout)
2022-03-28 19:01:06 [ForkJoinPool-1-worker-55] DEBUG [SuiteThreadController:267] testSendSimpleMessageTlsScramSha - Notifies waiting test cases:[testPauseReconciliationInKafkaRebalanceAndTopic, testPauseReconciliationInKafkaAndKafkaConnectWithConnector, testCapacityFile, testConfigurationFileIsCreated, testConfigurationPerformanceOptions, testConfigurationReflection, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testDeployAndUnDeployCruiseControl, testTlsExternalUserWithQuotas, testCreatingUsersWithSecretPrefix, testThrottlingQuotasCreateTopic, testThrottlingQuotasCreateAlterPartitions, testKafkaAdminTopicOperations, testUserTemplate, testUpdateUser, testTlsExternalUser, testSendSimpleMessageTls, testReceiveSimpleMessageTlsScramSha, testThrottlingQuotasDeleteTopic, testReceiveSimpleMessageTls] to and randomly select one to start execution
2022-03-28 19:01:06 [ForkJoinPool-1-worker-55] DEBUG [SuiteThreadController:93] [bridge.HttpBridgeScramShaST] - Removing parallel test: testSendSimpleMessageTlsScramSha
2022-03-28 19:01:06 [ForkJoinPool-1-worker-55] DEBUG [SuiteThreadController:97] [bridge.HttpBridgeScramShaST] - Parallel test count: 14
2022-03-28 19:01:06 [ForkJoinPool-1-worker-55] INFO  [TestSeparator:29] io.strimzi.systemtest.bridge.HttpBridgeScramShaST.testSendSimpleMessageTlsScramSha-FINISHED
2022-03-28 19:01:06 [ForkJoinPool-1-worker-55] INFO  [TestSeparator:30] ############################################################################
2022-03-28 19:01:06 [ForkJoinPool-1-worker-55] INFO  [TestSeparator:23] ############################################################################
2022-03-28 19:01:06 [ForkJoinPool-1-worker-55] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.topic.TopicST.testTopicModificationOfReplicationFactor-STARTED
2022-03-28 19:01:06 [ForkJoinPool-1-worker-55] DEBUG [AbstractST:658] ============================================================================
2022-03-28 19:01:06 [ForkJoinPool-1-worker-55] DEBUG [AbstractST:659] [operators.topic.TopicST - Before Each] - Setup test case environment
2022-03-28 19:01:06 [ForkJoinPool-1-worker-55] DEBUG [SuiteThreadController:77] [operators.topic.TopicST] - Adding parallel test: testTopicModificationOfReplicationFactor
2022-03-28 19:01:06 [ForkJoinPool-1-worker-55] DEBUG [SuiteThreadController:81] [operators.topic.TopicST] - Parallel test count: 15
2022-03-28 19:01:06 [ForkJoinPool-1-worker-55] DEBUG [SuiteThreadController:230] testTopicModificationOfReplicationFactor test now can proceed its execution
2022-03-28 19:01:06 [ForkJoinPool-1-worker-55] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 19:01:06 [ForkJoinPool-1-worker-55] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testTopicModificationOfReplicationFactor=my-cluster-15f5f09f, testKafkaAdminTopicOperations=my-cluster-506fda27, testReceiveSimpleMessageTlsScramSha=my-cluster-abc5d34b, testConfigurationFileIsCreated=my-cluster-17fc585b, testThrottlingQuotasCreateAlterPartitions=my-cluster-ec1b34f8, testSendSimpleMessageTls=my-cluster-89673e85, testCreatingUsersWithSecretPrefix=my-cluster-f743fdfd, testUpdateUser=my-cluster-d582eb8c, testThrottlingQuotasDeleteTopic=my-cluster-836b31f8, testConfigurationReflection=my-cluster-188382d8, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-959c6ed5, testTlsExternalUserWithQuotas=my-cluster-100f5461, testDeployAndUnDeployCruiseControl=my-cluster-5dfd606f, testThrottlingQuotasCreateTopic=my-cluster-e9955717, testCapacityFile=my-cluster-855b9105, testConfigurationPerformanceOptions=my-cluster-08515847, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-afe273c9, testReceiveSimpleMessageTls=my-cluster-634e4d65, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-e6343cec, testTlsExternalUser=my-cluster-45cf8ce4, testSendSimpleMessageTlsScramSha=my-cluster-561d4fae, testUserTemplate=my-cluster-927d5a1b}
2022-03-28 19:01:06 [ForkJoinPool-1-worker-55] TRACE [AbstractST:607] USERS_NAME_MAP: {testTopicModificationOfReplicationFactor=my-user-259690203-415132624, testKafkaAdminTopicOperations=my-user-1780746402-846362125, testReceiveSimpleMessageTlsScramSha=my-user-330562220-1155646423, testConfigurationFileIsCreated=my-user-355815510-1316268151, testThrottlingQuotasCreateAlterPartitions=my-user-333986137-1152182125, testSendSimpleMessageTls=my-user-1887866274-1013125243, testCreatingUsersWithSecretPrefix=my-user-607975993-1210920737, testUpdateUser=my-user-1297536277-778093599, testThrottlingQuotasDeleteTopic=my-user-1567019138-1355600812, testConfigurationReflection=my-user-1890969183-1754686912, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-2104248162-45187051, testTlsExternalUserWithQuotas=my-user-1909583883-1106194424, testDeployAndUnDeployCruiseControl=my-user-506268651-1309117986, testThrottlingQuotasCreateTopic=my-user-1876060546-1887905718, testCapacityFile=my-user-1460643829-103298436, testConfigurationPerformanceOptions=my-user-98362545-1476587236, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-user-8456079-650483351, testReceiveSimpleMessageTls=my-user-337002809-1497514615, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-2069267102-581538560, testTlsExternalUser=my-user-399414674-1203409092, testSendSimpleMessageTlsScramSha=my-user-1601209876-518796379, testUserTemplate=my-user-989633374-940717398}
2022-03-28 19:01:06 [ForkJoinPool-1-worker-55] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testTopicModificationOfReplicationFactor=my-topic-1480509436-578706559, testKafkaAdminTopicOperations=my-topic-481231182-953607496, testReceiveSimpleMessageTlsScramSha=my-topic-1027850608-734754781, testConfigurationFileIsCreated=my-topic-423850169-855699454, testThrottlingQuotasCreateAlterPartitions=my-topic-325891726-169328137, testSendSimpleMessageTls=my-topic-874607937-87907051, testCreatingUsersWithSecretPrefix=my-topic-1727421070-668192795, testUpdateUser=my-topic-1325563869-1324403074, testThrottlingQuotasDeleteTopic=my-topic-1674611395-2048879468, testConfigurationReflection=my-topic-1394577332-1936226438, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-634125597-1746493046, testTlsExternalUserWithQuotas=my-topic-1986059791-167638004, testDeployAndUnDeployCruiseControl=my-topic-465377127-800134460, testThrottlingQuotasCreateTopic=my-topic-1246597695-228473879, testCapacityFile=my-topic-549312506-649708453, testConfigurationPerformanceOptions=my-topic-696588651-1198599652, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-topic-1687496771-142091227, testReceiveSimpleMessageTls=my-topic-829184226-1519275114, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-293775067-432059493, testTlsExternalUser=my-topic-707627084-257659651, testSendSimpleMessageTlsScramSha=my-topic-1586843669-332477842, testUserTemplate=my-topic-1542379258-2002911578}
2022-03-28 19:01:06 [ForkJoinPool-1-worker-55] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testTopicModificationOfReplicationFactor=my-cluster-15f5f09f-kafka-clients, testKafkaAdminTopicOperations=my-cluster-506fda27-kafka-clients, testReceiveSimpleMessageTlsScramSha=my-cluster-abc5d34b-kafka-clients, testConfigurationFileIsCreated=my-cluster-17fc585b-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-ec1b34f8-kafka-clients, testSendSimpleMessageTls=my-cluster-89673e85-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-f743fdfd-kafka-clients, testUpdateUser=my-cluster-d582eb8c-kafka-clients, testThrottlingQuotasDeleteTopic=my-cluster-836b31f8-kafka-clients, testConfigurationReflection=my-cluster-188382d8-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-959c6ed5-kafka-clients, testTlsExternalUserWithQuotas=my-cluster-100f5461-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-5dfd606f-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-e9955717-kafka-clients, testCapacityFile=my-cluster-855b9105-kafka-clients, testConfigurationPerformanceOptions=my-cluster-08515847-kafka-clients, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-afe273c9-kafka-clients, testReceiveSimpleMessageTls=my-cluster-634e4d65-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-e6343cec-kafka-clients, testTlsExternalUser=my-cluster-45cf8ce4-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-561d4fae-kafka-clients, testUserTemplate=my-cluster-927d5a1b-kafka-clients}
2022-03-28 19:01:06 [ForkJoinPool-1-worker-5] DEBUG [AbstractST:689] ============================================================================
2022-03-28 19:01:06 [ForkJoinPool-1-worker-5] DEBUG [AbstractST:690] [bridge.HttpBridgeScramShaST - After All] - Clean up after test suite
2022-03-28 19:01:06 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:344] ############################################################################
2022-03-28 19:01:06 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:348] Delete all resources for HttpBridgeScramShaST
2022-03-28 19:01:06 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:241] Delete of Deployment http-bridge-scram-sha-st-shared-kafka-clients in namespace http-bridge-scram-sha-st
2022-03-28 19:01:06 [ForkJoinPool-1-worker-55] INFO  [ResourceManager:155] Create/Update KafkaTopic my-topic-1480509436-578706559 in namespace topic-st
2022-03-28 19:01:06 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:http-bridge-scram-sha-st-shared-kafka-clients
2022-03-28 19:01:06 [ForkJoinPool-1-worker-35] DEBUG [JobUtils:40] Job create-admin-my-cluster-506fda27-kafka-clients was deleted
2022-03-28 19:01:06 [ForkJoinPool-1-worker-55] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-1480509436-578706559
2022-03-28 19:01:06 [ForkJoinPool-1-worker-55] INFO  [ResourceManager:433] Wait for KafkaTopic: my-topic-1480509436-578706559 will have desired state: Ready
2022-03-28 19:01:06 [ForkJoinPool-1-worker-55] DEBUG [TestUtils:125] Waiting for KafkaTopic: my-topic-1480509436-578706559 will have desired state: Ready
2022-03-28 19:01:06 [ForkJoinPool-1-worker-35] INFO  [ResourceManager:155] Create/Update Job list-admin-my-cluster-506fda27-kafka-clients in namespace throttling-quota-st
2022-03-28 19:01:06 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:01:06 [ForkJoinPool-1-worker-55] TRACE [TestUtils:176] KafkaTopic: my-topic-1480509436-578706559 will have desired state: Ready not ready, will try again in 1000 ms (179901ms till timeout)
2022-03-28 19:01:06 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (179064ms till timeout)
2022-03-28 19:01:06 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (177702ms till timeout)
2022-03-28 19:01:06 [ForkJoinPool-1-worker-35] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:list-admin-my-cluster-506fda27-kafka-clients
2022-03-28 19:01:06 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1019480ms till timeout)
2022-03-28 19:01:06 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (201420ms till timeout)
2022-03-28 19:01:06 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:http-bridge-scram-sha-st-shared-kafka-clients not ready, will try again in 10000 ms (479442ms till timeout)
2022-03-28 19:01:06 [ForkJoinPool-1-worker-35] INFO  [JobUtils:81] Waiting for job: list-admin-my-cluster-506fda27-kafka-clients will be in active state
2022-03-28 19:01:06 [ForkJoinPool-1-worker-35] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 19:01:06 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (734753ms till timeout)
2022-03-28 19:01:06 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:01:06 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2377887ms till timeout)
2022-03-28 19:01:07 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:444] Kafka: my-cluster-08515847 is in desired state: Ready
2022-03-28 19:01:07 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (217476ms till timeout)
2022-03-28 19:01:07 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1022804ms till timeout)
2022-03-28 19:01:07 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1019833ms till timeout)
2022-03-28 19:01:07 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1020570ms till timeout)
2022-03-28 19:01:07 [ForkJoinPool-1-worker-35] INFO  [PodUtils:186] Waiting for message will be in the log
2022-03-28 19:01:07 [ForkJoinPool-1-worker-35] DEBUG [TestUtils:125] Waiting for Waiting for message will be in the log
2022-03-28 19:01:07 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (697614ms till timeout)
2022-03-28 19:01:07 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (119900ms till timeout)
2022-03-28 19:01:07 [ForkJoinPool-1-worker-11] INFO  [CruiseControlConfigurationST:271] Changing cruise control performance tuning options
2022-03-28 19:01:07 [ForkJoinPool-1-worker-55] INFO  [ResourceManager:444] KafkaTopic: my-topic-1480509436-578706559 is in desired state: Ready
2022-03-28 19:01:07 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (177890ms till timeout)
2022-03-28 19:01:07 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (176526ms till timeout)
2022-03-28 19:01:07 [ForkJoinPool-1-worker-11] INFO  [CruiseControlConfigurationST:277] Verifying that CC pod is rolling, after changing options
2022-03-28 19:01:07 [ForkJoinPool-1-worker-11] INFO  [DeploymentUtils:136] Waiting for Deployment my-cluster-08515847-cruise-control rolling update
2022-03-28 19:01:07 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Deployment my-cluster-08515847-cruise-control rolling update in namespace:namespace-4
2022-03-28 19:01:07 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929}
2022-03-28 19:01:07 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:01:07 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1018323ms till timeout)
2022-03-28 19:01:07 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (733594ms till timeout)
2022-03-28 19:01:07 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (200165ms till timeout)
2022-03-28 19:01:08 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:01:08 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2376726ms till timeout)
2022-03-28 19:01:08 [ForkJoinPool-1-worker-55] INFO  [ResourceManager:433] Wait for KafkaTopic: my-topic-1480509436-578706559 will have desired state: NotReady
2022-03-28 19:01:08 [ForkJoinPool-1-worker-55] DEBUG [TestUtils:125] Waiting for KafkaTopic: my-topic-1480509436-578706559 will have desired state: NotReady
2022-03-28 19:01:08 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (216329ms till timeout)
2022-03-28 19:01:08 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929}
2022-03-28 19:01:08 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929}
2022-03-28 19:01:08 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Deployment my-cluster-08515847-cruise-control rolling update in namespace:namespace-4 not ready, will try again in 5000 ms (599630ms till timeout)
2022-03-28 19:01:08 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1018687ms till timeout)
2022-03-28 19:01:08 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1019424ms till timeout)
2022-03-28 19:01:08 [ForkJoinPool-1-worker-55] TRACE [TestUtils:176] KafkaTopic: my-topic-1480509436-578706559 will have desired state: NotReady not ready, will try again in 1000 ms (179816ms till timeout)
2022-03-28 19:01:08 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1021656ms till timeout)
2022-03-28 19:01:08 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (696468ms till timeout)
2022-03-28 19:01:08 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (118753ms till timeout)
2022-03-28 19:01:08 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (176786ms till timeout)
2022-03-28 19:01:08 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (175322ms till timeout)
2022-03-28 19:01:09 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1017186ms till timeout)
2022-03-28 19:01:09 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:01:09 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (732454ms till timeout)
2022-03-28 19:01:09 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:01:09 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2375587ms till timeout)
2022-03-28 19:01:09 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (198825ms till timeout)
2022-03-28 19:01:09 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (215172ms till timeout)
2022-03-28 19:01:09 [ForkJoinPool-1-worker-55] INFO  [ResourceManager:444] KafkaTopic: my-topic-1480509436-578706559 is in desired state: NotReady
2022-03-28 19:01:09 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1017519ms till timeout)
2022-03-28 19:01:09 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1018255ms till timeout)
2022-03-28 19:01:09 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1020487ms till timeout)
2022-03-28 19:01:09 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (695302ms till timeout)
2022-03-28 19:01:09 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (117589ms till timeout)
2022-03-28 19:01:09 [ForkJoinPool-1-worker-55] TRACE [Exec:248] Running command - oc --namespace topic-st delete kafkatopic my-topic-1480509436-578706559
2022-03-28 19:01:09 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (175683ms till timeout)
2022-03-28 19:01:10 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (174207ms till timeout)
2022-03-28 19:01:10 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1016076ms till timeout)
2022-03-28 19:01:10 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (731346ms till timeout)
2022-03-28 19:01:10 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:01:10 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2374478ms till timeout)
2022-03-28 19:01:10 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:01:10 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (213981ms till timeout)
2022-03-28 19:01:10 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (197537ms till timeout)
2022-03-28 19:01:10 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1019307ms till timeout)
2022-03-28 19:01:10 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1016335ms till timeout)
2022-03-28 19:01:10 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1017071ms till timeout)
2022-03-28 19:01:10 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (694119ms till timeout)
2022-03-28 19:01:10 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-e6343cec-cruise-control-77c657c774-9pqlz=4f9ddbe4-cd0e-41b5-b7b8-1cacc27bbc84}
2022-03-28 19:01:10 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (116373ms till timeout)
2022-03-28 19:01:11 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-e6343cec-cruise-control-6cd68d58b9-k7qb2=f6c34d2d-b608-4a7b-912f-33c5db736711, my-cluster-e6343cec-cruise-control-77c657c774-9pqlz=4f9ddbe4-cd0e-41b5-b7b8-1cacc27bbc84}
2022-03-28 19:01:11 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-e6343cec-cruise-control-77c657c774-9pqlz=4f9ddbe4-cd0e-41b5-b7b8-1cacc27bbc84}
2022-03-28 19:01:11 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Deployment my-cluster-e6343cec-cruise-control rolling update in namespace:namespace-3 not ready, will try again in 5000 ms (525193ms till timeout)
2022-03-28 19:01:11 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (174488ms till timeout)
2022-03-28 19:01:11 [ForkJoinPool-1-worker-55] DEBUG [Exec:419] Command: oc --namespace topic-st delete kafkatopic my-topic-1480509436-578706559
2022-03-28 19:01:11 [ForkJoinPool-1-worker-55] DEBUG [Exec:419] Return code: 0
2022-03-28 19:01:11 [ForkJoinPool-1-worker-55] INFO  [KafkaTopicUtils:104] Waiting for KafkaTopic my-topic-1480509436-578706559 deletion
2022-03-28 19:01:11 [ForkJoinPool-1-worker-55] DEBUG [TestUtils:125] Waiting for KafkaTopic deletion my-topic-1480509436-578706559
2022-03-28 19:01:11 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (173003ms till timeout)
2022-03-28 19:01:11 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1014897ms till timeout)
2022-03-28 19:01:11 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (730168ms till timeout)
2022-03-28 19:01:11 [ForkJoinPool-1-worker-55] DEBUG [AbstractST:674] ============================================================================
2022-03-28 19:01:11 [ForkJoinPool-1-worker-55] DEBUG [AbstractST:675] [operators.topic.TopicST - After Each] - Clean up after test
2022-03-28 19:01:11 [ForkJoinPool-1-worker-55] INFO  [ResourceManager:344] ############################################################################
2022-03-28 19:01:11 [ForkJoinPool-1-worker-55] INFO  [ResourceManager:348] Delete all resources for testTopicModificationOfReplicationFactor
2022-03-28 19:01:11 [ForkJoinPool-1-worker-55] INFO  [ResourceManager:241] Delete of KafkaTopic my-topic-1480509436-578706559 in namespace topic-st
2022-03-28 19:01:11 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:01:11 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2373298ms till timeout)
2022-03-28 19:01:11 [ForkJoinPool-1-worker-55] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1480509436-578706559
2022-03-28 19:01:11 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (212806ms till timeout)
2022-03-28 19:01:11 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1015900ms till timeout)
2022-03-28 19:01:11 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1015163ms till timeout)
2022-03-28 19:01:11 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:01:11 [ForkJoinPool-1-worker-55] INFO  [ResourceManager:369] ############################################################################
2022-03-28 19:01:11 [ForkJoinPool-1-worker-55] DEBUG [SuiteThreadController:267] testTopicModificationOfReplicationFactor - Notifies waiting test cases:[testPauseReconciliationInKafkaRebalanceAndTopic, testPauseReconciliationInKafkaAndKafkaConnectWithConnector, testCapacityFile, testConfigurationFileIsCreated, testConfigurationPerformanceOptions, testConfigurationReflection, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testDeployAndUnDeployCruiseControl, testTlsExternalUserWithQuotas, testCreatingUsersWithSecretPrefix, testThrottlingQuotasCreateTopic, testThrottlingQuotasCreateAlterPartitions, testKafkaAdminTopicOperations, testUserTemplate, testUpdateUser, testTlsExternalUser, testSendSimpleMessageTls, testReceiveSimpleMessageTlsScramSha, testThrottlingQuotasDeleteTopic, testReceiveSimpleMessageTls, testTopicModificationOfReplicationFactor] to and randomly select one to start execution
2022-03-28 19:01:11 [ForkJoinPool-1-worker-55] DEBUG [SuiteThreadController:93] [operators.topic.TopicST] - Removing parallel test: testTopicModificationOfReplicationFactor
2022-03-28 19:01:11 [ForkJoinPool-1-worker-55] DEBUG [SuiteThreadController:97] [operators.topic.TopicST] - Parallel test count: 14
2022-03-28 19:01:11 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1018131ms till timeout)
2022-03-28 19:01:11 [ForkJoinPool-1-worker-55] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.topic.TopicST.testTopicModificationOfReplicationFactor-FINISHED
2022-03-28 19:01:11 [ForkJoinPool-1-worker-55] INFO  [TestSeparator:30] ############################################################################
2022-03-28 19:01:11 [ForkJoinPool-1-worker-55] INFO  [TestSeparator:23] ############################################################################
2022-03-28 19:01:11 [ForkJoinPool-1-worker-55] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.topic.TopicST.testSendingMessagesToNonExistingTopic-STARTED
2022-03-28 19:01:11 [ForkJoinPool-1-worker-55] DEBUG [AbstractST:658] ============================================================================
2022-03-28 19:01:11 [ForkJoinPool-1-worker-55] DEBUG [AbstractST:659] [operators.topic.TopicST - Before Each] - Setup test case environment
2022-03-28 19:01:11 [ForkJoinPool-1-worker-55] DEBUG [SuiteThreadController:77] [operators.topic.TopicST] - Adding parallel test: testSendingMessagesToNonExistingTopic
2022-03-28 19:01:11 [ForkJoinPool-1-worker-55] DEBUG [SuiteThreadController:81] [operators.topic.TopicST] - Parallel test count: 15
2022-03-28 19:01:11 [ForkJoinPool-1-worker-55] DEBUG [SuiteThreadController:230] testSendingMessagesToNonExistingTopic test now can proceed its execution
2022-03-28 19:01:11 [ForkJoinPool-1-worker-55] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 19:01:11 [ForkJoinPool-1-worker-55] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testTopicModificationOfReplicationFactor=my-cluster-15f5f09f, testKafkaAdminTopicOperations=my-cluster-506fda27, testReceiveSimpleMessageTlsScramSha=my-cluster-abc5d34b, testConfigurationFileIsCreated=my-cluster-17fc585b, testThrottlingQuotasCreateAlterPartitions=my-cluster-ec1b34f8, testSendSimpleMessageTls=my-cluster-89673e85, testCreatingUsersWithSecretPrefix=my-cluster-f743fdfd, testUpdateUser=my-cluster-d582eb8c, testThrottlingQuotasDeleteTopic=my-cluster-836b31f8, testConfigurationReflection=my-cluster-188382d8, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-959c6ed5, testTlsExternalUserWithQuotas=my-cluster-100f5461, testDeployAndUnDeployCruiseControl=my-cluster-5dfd606f, testThrottlingQuotasCreateTopic=my-cluster-e9955717, testSendingMessagesToNonExistingTopic=my-cluster-a3c1c5ed, testCapacityFile=my-cluster-855b9105, testConfigurationPerformanceOptions=my-cluster-08515847, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-afe273c9, testReceiveSimpleMessageTls=my-cluster-634e4d65, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-e6343cec, testTlsExternalUser=my-cluster-45cf8ce4, testSendSimpleMessageTlsScramSha=my-cluster-561d4fae, testUserTemplate=my-cluster-927d5a1b}
2022-03-28 19:01:11 [ForkJoinPool-1-worker-55] TRACE [AbstractST:607] USERS_NAME_MAP: {testTopicModificationOfReplicationFactor=my-user-259690203-415132624, testKafkaAdminTopicOperations=my-user-1780746402-846362125, testReceiveSimpleMessageTlsScramSha=my-user-330562220-1155646423, testConfigurationFileIsCreated=my-user-355815510-1316268151, testThrottlingQuotasCreateAlterPartitions=my-user-333986137-1152182125, testSendSimpleMessageTls=my-user-1887866274-1013125243, testCreatingUsersWithSecretPrefix=my-user-607975993-1210920737, testUpdateUser=my-user-1297536277-778093599, testThrottlingQuotasDeleteTopic=my-user-1567019138-1355600812, testConfigurationReflection=my-user-1890969183-1754686912, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-2104248162-45187051, testTlsExternalUserWithQuotas=my-user-1909583883-1106194424, testDeployAndUnDeployCruiseControl=my-user-506268651-1309117986, testThrottlingQuotasCreateTopic=my-user-1876060546-1887905718, testSendingMessagesToNonExistingTopic=my-user-134768902-582007171, testCapacityFile=my-user-1460643829-103298436, testConfigurationPerformanceOptions=my-user-98362545-1476587236, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-user-8456079-650483351, testReceiveSimpleMessageTls=my-user-337002809-1497514615, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-2069267102-581538560, testTlsExternalUser=my-user-399414674-1203409092, testSendSimpleMessageTlsScramSha=my-user-1601209876-518796379, testUserTemplate=my-user-989633374-940717398}
2022-03-28 19:01:11 [ForkJoinPool-1-worker-55] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testTopicModificationOfReplicationFactor=my-topic-1480509436-578706559, testKafkaAdminTopicOperations=my-topic-481231182-953607496, testReceiveSimpleMessageTlsScramSha=my-topic-1027850608-734754781, testConfigurationFileIsCreated=my-topic-423850169-855699454, testThrottlingQuotasCreateAlterPartitions=my-topic-325891726-169328137, testSendSimpleMessageTls=my-topic-874607937-87907051, testCreatingUsersWithSecretPrefix=my-topic-1727421070-668192795, testUpdateUser=my-topic-1325563869-1324403074, testThrottlingQuotasDeleteTopic=my-topic-1674611395-2048879468, testConfigurationReflection=my-topic-1394577332-1936226438, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-634125597-1746493046, testTlsExternalUserWithQuotas=my-topic-1986059791-167638004, testDeployAndUnDeployCruiseControl=my-topic-465377127-800134460, testThrottlingQuotasCreateTopic=my-topic-1246597695-228473879, testSendingMessagesToNonExistingTopic=my-topic-1692964628-1348460540, testCapacityFile=my-topic-549312506-649708453, testConfigurationPerformanceOptions=my-topic-696588651-1198599652, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-topic-1687496771-142091227, testReceiveSimpleMessageTls=my-topic-829184226-1519275114, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-293775067-432059493, testTlsExternalUser=my-topic-707627084-257659651, testSendSimpleMessageTlsScramSha=my-topic-1586843669-332477842, testUserTemplate=my-topic-1542379258-2002911578}
2022-03-28 19:01:11 [ForkJoinPool-1-worker-55] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testTopicModificationOfReplicationFactor=my-cluster-15f5f09f-kafka-clients, testKafkaAdminTopicOperations=my-cluster-506fda27-kafka-clients, testReceiveSimpleMessageTlsScramSha=my-cluster-abc5d34b-kafka-clients, testConfigurationFileIsCreated=my-cluster-17fc585b-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-ec1b34f8-kafka-clients, testSendSimpleMessageTls=my-cluster-89673e85-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-f743fdfd-kafka-clients, testUpdateUser=my-cluster-d582eb8c-kafka-clients, testThrottlingQuotasDeleteTopic=my-cluster-836b31f8-kafka-clients, testConfigurationReflection=my-cluster-188382d8-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-959c6ed5-kafka-clients, testTlsExternalUserWithQuotas=my-cluster-100f5461-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-5dfd606f-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-e9955717-kafka-clients, testSendingMessagesToNonExistingTopic=my-cluster-a3c1c5ed-kafka-clients, testCapacityFile=my-cluster-855b9105-kafka-clients, testConfigurationPerformanceOptions=my-cluster-08515847-kafka-clients, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-afe273c9-kafka-clients, testReceiveSimpleMessageTls=my-cluster-634e4d65-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-e6343cec-kafka-clients, testTlsExternalUser=my-cluster-45cf8ce4-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-561d4fae-kafka-clients, testUserTemplate=my-cluster-927d5a1b-kafka-clients}
2022-03-28 19:01:11 [ForkJoinPool-1-worker-55] INFO  [ResourceManager:155] Create/Update Deployment topic-cluster-name-kafka-clients in namespace topic-st
2022-03-28 19:01:11 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (692945ms till timeout)
2022-03-28 19:01:11 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (196173ms till timeout)
2022-03-28 19:01:11 [ForkJoinPool-1-worker-55] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:topic-cluster-name-kafka-clients
2022-03-28 19:01:11 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (115223ms till timeout)
2022-03-28 19:01:12 [ForkJoinPool-1-worker-55] INFO  [DeploymentUtils:161] Wait for Deployment: topic-cluster-name-kafka-clients will be ready
2022-03-28 19:01:12 [ForkJoinPool-1-worker-55] DEBUG [TestUtils:125] Waiting for Wait for Deployment: topic-cluster-name-kafka-clients will be ready
2022-03-28 19:01:12 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (173334ms till timeout)
2022-03-28 19:01:12 [ForkJoinPool-1-worker-55] TRACE [TestUtils:176] Wait for Deployment: topic-cluster-name-kafka-clients will be ready not ready, will try again in 1000 ms (479807ms till timeout)
2022-03-28 19:01:12 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (171876ms till timeout)
2022-03-28 19:01:12 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1013744ms till timeout)
2022-03-28 19:01:12 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (729014ms till timeout)
2022-03-28 19:01:12 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:01:12 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2372145ms till timeout)
2022-03-28 19:01:12 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (211710ms till timeout)
2022-03-28 19:01:12 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1014064ms till timeout)
2022-03-28 19:01:12 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1014702ms till timeout)
2022-03-28 19:01:12 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1016935ms till timeout)
2022-03-28 19:01:12 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (691841ms till timeout)
2022-03-28 19:01:13 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:01:13 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929}
2022-03-28 19:01:13 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (114028ms till timeout)
2022-03-28 19:01:13 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (194877ms till timeout)
2022-03-28 19:01:13 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929, my-cluster-08515847-cruise-control-dcd7c895b-4pkw7=37920413-f627-4a97-919c-4ff0c818c13a}
2022-03-28 19:01:13 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929}
2022-03-28 19:01:13 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Deployment my-cluster-08515847-cruise-control rolling update in namespace:namespace-4 not ready, will try again in 5000 ms (594413ms till timeout)
2022-03-28 19:01:13 [ForkJoinPool-1-worker-55] TRACE [TestUtils:176] Wait for Deployment: topic-cluster-name-kafka-clients will be ready not ready, will try again in 1000 ms (478687ms till timeout)
2022-03-28 19:01:13 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (172096ms till timeout)
2022-03-28 19:01:13 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (170726ms till timeout)
2022-03-28 19:01:13 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1012535ms till timeout)
2022-03-28 19:01:13 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (727858ms till timeout)
2022-03-28 19:01:13 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:01:13 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2370988ms till timeout)
2022-03-28 19:01:13 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (210580ms till timeout)
2022-03-28 19:01:14 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1012936ms till timeout)
2022-03-28 19:01:14 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1013578ms till timeout)
2022-03-28 19:01:14 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1015811ms till timeout)
2022-03-28 19:01:14 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (690718ms till timeout)
2022-03-28 19:01:14 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (112888ms till timeout)
2022-03-28 19:01:14 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:01:14 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (193667ms till timeout)
2022-03-28 19:01:14 [ForkJoinPool-1-worker-55] TRACE [TestUtils:176] Wait for Deployment: topic-cluster-name-kafka-clients will be ready not ready, will try again in 1000 ms (477591ms till timeout)
2022-03-28 19:01:14 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (170919ms till timeout)
2022-03-28 19:01:14 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (169561ms till timeout)
2022-03-28 19:01:14 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1011428ms till timeout)
2022-03-28 19:01:14 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (726698ms till timeout)
2022-03-28 19:01:14 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:01:14 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2369821ms till timeout)
2022-03-28 19:01:15 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (209418ms till timeout)
2022-03-28 19:01:15 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1011756ms till timeout)
2022-03-28 19:01:15 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1014631ms till timeout)
2022-03-28 19:01:15 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (689537ms till timeout)
2022-03-28 19:01:15 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1012396ms till timeout)
2022-03-28 19:01:15 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (111681ms till timeout)
2022-03-28 19:01:15 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:01:15 [ForkJoinPool-1-worker-55] TRACE [TestUtils:176] Wait for Deployment: topic-cluster-name-kafka-clients will be ready not ready, will try again in 1000 ms (476474ms till timeout)
2022-03-28 19:01:15 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (192351ms till timeout)
2022-03-28 19:01:15 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (169804ms till timeout)
2022-03-28 19:01:15 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (168442ms till timeout)
2022-03-28 19:01:15 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1010316ms till timeout)
2022-03-28 19:01:15 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (725585ms till timeout)
2022-03-28 19:01:16 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-e6343cec-cruise-control-77c657c774-9pqlz=4f9ddbe4-cd0e-41b5-b7b8-1cacc27bbc84}
2022-03-28 19:01:16 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:01:16 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2368715ms till timeout)
2022-03-28 19:01:16 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (208313ms till timeout)
2022-03-28 19:01:16 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-e6343cec-cruise-control-6cd68d58b9-k7qb2=f6c34d2d-b608-4a7b-912f-33c5db736711, my-cluster-e6343cec-cruise-control-77c657c774-9pqlz=4f9ddbe4-cd0e-41b5-b7b8-1cacc27bbc84}
2022-03-28 19:01:16 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-e6343cec-cruise-control-77c657c774-9pqlz=4f9ddbe4-cd0e-41b5-b7b8-1cacc27bbc84}
2022-03-28 19:01:16 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Deployment my-cluster-e6343cec-cruise-control rolling update in namespace:namespace-3 not ready, will try again in 5000 ms (519913ms till timeout)
2022-03-28 19:01:16 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1010572ms till timeout)
2022-03-28 19:01:16 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (688353ms till timeout)
2022-03-28 19:01:16 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1013445ms till timeout)
2022-03-28 19:01:16 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1011212ms till timeout)
2022-03-28 19:01:16 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (110507ms till timeout)
2022-03-28 19:01:16 [ForkJoinPool-1-worker-55] TRACE [TestUtils:176] Wait for Deployment: topic-cluster-name-kafka-clients will be ready not ready, will try again in 1000 ms (475376ms till timeout)
2022-03-28 19:01:16 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:01:17 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1009201ms till timeout)
2022-03-28 19:01:17 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (168595ms till timeout)
2022-03-28 19:01:17 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (167231ms till timeout)
2022-03-28 19:01:17 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (191042ms till timeout)
2022-03-28 19:01:17 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (724467ms till timeout)
2022-03-28 19:01:17 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:01:17 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2367593ms till timeout)
2022-03-28 19:01:17 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (207192ms till timeout)
2022-03-28 19:01:17 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:http-bridge-scram-sha-st-shared-kafka-clients not ready, will try again in 10000 ms (468868ms till timeout)
2022-03-28 19:01:17 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1009420ms till timeout)
2022-03-28 19:01:17 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1012298ms till timeout)
2022-03-28 19:01:17 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (687205ms till timeout)
2022-03-28 19:01:17 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1010065ms till timeout)
2022-03-28 19:01:17 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (109351ms till timeout)
2022-03-28 19:01:17 [ForkJoinPool-1-worker-55] INFO  [DeploymentUtils:168] Deployment: topic-cluster-name-kafka-clients is ready
2022-03-28 19:01:18 [ForkJoinPool-1-worker-55] INFO  [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-03-28 19:01:18 [ForkJoinPool-1-worker-55] INFO  [TopicST:320] Checking if my-topic-1692964628-1348460540 is on topic list
2022-03-28 19:01:18 [ForkJoinPool-1-worker-55] INFO  [TopicST:456] Checking topic my-topic-1692964628-1348460540 in Kafka
2022-03-28 19:01:18 [ForkJoinPool-1-worker-55] TRACE [Exec:248] Running command - oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-03-28 19:01:18 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1008072ms till timeout)
2022-03-28 19:01:18 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (167459ms till timeout)
2022-03-28 19:01:18 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (166041ms till timeout)
2022-03-28 19:01:18 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:01:18 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (723340ms till timeout)
2022-03-28 19:01:18 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:01:18 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2366474ms till timeout)
2022-03-28 19:01:18 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929}
2022-03-28 19:01:18 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (206072ms till timeout)
2022-03-28 19:01:18 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (189725ms till timeout)
2022-03-28 19:01:18 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929, my-cluster-08515847-cruise-control-dcd7c895b-4pkw7=37920413-f627-4a97-919c-4ff0c818c13a}
2022-03-28 19:01:18 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929}
2022-03-28 19:01:18 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Deployment my-cluster-08515847-cruise-control rolling update in namespace:namespace-4 not ready, will try again in 5000 ms (589183ms till timeout)
2022-03-28 19:01:18 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-188382d8 will have desired state: Ready not ready, will try again in 1000 ms (1008236ms till timeout)
2022-03-28 19:01:18 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (686014ms till timeout)
2022-03-28 19:01:18 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1008874ms till timeout)
2022-03-28 19:01:18 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1011107ms till timeout)
2022-03-28 19:01:18 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (108246ms till timeout)
2022-03-28 19:01:19 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1006972ms till timeout)
2022-03-28 19:01:19 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (166272ms till timeout)
2022-03-28 19:01:19 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (164914ms till timeout)
2022-03-28 19:01:19 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (722144ms till timeout)
2022-03-28 19:01:19 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:01:19 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2365371ms till timeout)
2022-03-28 19:01:19 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (204970ms till timeout)
2022-03-28 19:01:19 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:01:19 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (188527ms till timeout)
2022-03-28 19:01:19 [ForkJoinPool-1-worker-21] INFO  [ResourceManager:444] Kafka: my-cluster-188382d8 is in desired state: Ready
2022-03-28 19:01:19 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1007773ms till timeout)
2022-03-28 19:01:19 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (684911ms till timeout)
2022-03-28 19:01:20 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1009910ms till timeout)
2022-03-28 19:01:20 [ForkJoinPool-1-worker-21] TRACE [Exec:248] Running command - oc --namespace namespace-1 exec my-cluster-188382d8-cruise-control-b47d4fc97-s8rvs -- /bin/bash -c cat /tmp/cruisecontrol.properties
2022-03-28 19:01:20 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (107100ms till timeout)
2022-03-28 19:01:20 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1005875ms till timeout)
2022-03-28 19:01:20 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (165171ms till timeout)
2022-03-28 19:01:20 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (721039ms till timeout)
2022-03-28 19:01:20 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:01:20 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2364266ms till timeout)
2022-03-28 19:01:20 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (163709ms till timeout)
2022-03-28 19:01:20 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (203865ms till timeout)
2022-03-28 19:01:20 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:01:20 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (187327ms till timeout)
2022-03-28 19:01:21 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1006667ms till timeout)
2022-03-28 19:01:21 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1008804ms till timeout)
2022-03-28 19:01:21 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (683711ms till timeout)
2022-03-28 19:01:21 [ForkJoinPool-1-worker-35] INFO  [PodUtils:189] Message quota-topic-test-simple-99 found in list-admin-my-cluster-506fda27-kafka-clients-5qm9p log
2022-03-28 19:01:21 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-e6343cec-cruise-control-77c657c774-9pqlz=4f9ddbe4-cd0e-41b5-b7b8-1cacc27bbc84}
2022-03-28 19:01:21 [ForkJoinPool-1-worker-21] INFO  [Exec:417] Command: oc --namespace namespace-1 exec my-cluster-188382d8-cruise-control-b47d4fc97-s8rvs -- /bin/bash -c cat /tmp/cruisecontrol.properties
2022-03-28 19:01:21 [ForkJoinPool-1-worker-21] INFO  [Exec:417] Return code: 0
2022-03-28 19:01:21 [ForkJoinPool-1-worker-21] INFO  [CruiseControlConfigurationST:221] Verifying that all configuration in the cruise control container matching the cruise control file /tmp/cruisecontrol.properties properties
2022-03-28 19:01:21 [ForkJoinPool-1-worker-21] DEBUG [AbstractST:674] ============================================================================
2022-03-28 19:01:21 [ForkJoinPool-1-worker-21] DEBUG [AbstractST:675] [cruisecontrol.CruiseControlConfigurationST - After Each] - Clean up after test
2022-03-28 19:01:21 [ForkJoinPool-1-worker-21] INFO  [ResourceManager:344] ############################################################################
2022-03-28 19:01:21 [ForkJoinPool-1-worker-21] INFO  [ResourceManager:348] Delete all resources for testConfigurationReflection
2022-03-28 19:01:21 [ForkJoinPool-1-worker-21] INFO  [ResourceManager:241] Delete of Kafka my-cluster-188382d8 in namespace namespace-1
2022-03-28 19:01:21 [ForkJoinPool-1-worker-21] INFO  [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-1, for cruise control Kafka cluster my-cluster-188382d8
2022-03-28 19:01:21 [ForkJoinPool-1-worker-35] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment list-admin-my-cluster-506fda27-kafka-clients deletion
2022-03-28 19:01:21 [ForkJoinPool-1-worker-35] DEBUG [TestUtils:125] Waiting for ReplicaSet list-admin-my-cluster-506fda27-kafka-clients to be deleted
2022-03-28 19:01:21 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1004702ms till timeout)
2022-03-28 19:01:21 [ForkJoinPool-1-worker-35] DEBUG [JobUtils:40] Job list-admin-my-cluster-506fda27-kafka-clients was deleted
2022-03-28 19:01:21 [ForkJoinPool-1-worker-35] INFO  [ResourceManager:155] Create/Update Job delete-admin-my-cluster-506fda27-kafka-clients in namespace throttling-quota-st
2022-03-28 19:01:21 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-e6343cec-cruise-control-6cd68d58b9-k7qb2=f6c34d2d-b608-4a7b-912f-33c5db736711}
2022-03-28 19:01:21 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:119] All pods seem to have rolled
2022-03-28 19:01:21 [ForkJoinPool-1-worker-3] INFO  [DeploymentUtils:161] Wait for Deployment: my-cluster-e6343cec-cruise-control will be ready
2022-03-28 19:01:21 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Wait for Deployment: my-cluster-e6343cec-cruise-control will be ready
2022-03-28 19:01:21 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (164003ms till timeout)
2022-03-28 19:01:21 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (719876ms till timeout)
2022-03-28 19:01:21 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:01:21 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2363102ms till timeout)
2022-03-28 19:01:21 [ForkJoinPool-1-worker-35] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:delete-admin-my-cluster-506fda27-kafka-clients
2022-03-28 19:01:21 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (162540ms till timeout)
2022-03-28 19:01:21 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (202704ms till timeout)
2022-03-28 19:01:21 [ForkJoinPool-1-worker-3] INFO  [DeploymentUtils:168] Deployment: my-cluster-e6343cec-cruise-control is ready
2022-03-28 19:01:21 [ForkJoinPool-1-worker-35] INFO  [JobUtils:81] Waiting for job: delete-admin-my-cluster-506fda27-kafka-clients will be in active state
2022-03-28 19:01:21 [ForkJoinPool-1-worker-35] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 19:01:21 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:01:21 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-e6343cec, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-e6343cec-cruise-control}, additionalProperties={})to be ready
2022-03-28 19:01:22 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (185984ms till timeout)
2022-03-28 19:01:22 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1005519ms till timeout)
2022-03-28 19:01:22 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6343cec-cruise-control-6cd68d58b9-k7qb2 not ready: cruise-control)
2022-03-28 19:01:22 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6343cec-cruise-control-6cd68d58b9-k7qb2 not ready: tls-sidecar)
2022-03-28 19:01:22 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:106] Pods my-cluster-e6343cec-cruise-control-6cd68d58b9-k7qb2 are ready
2022-03-28 19:01:22 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-e6343cec, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-e6343cec-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599813ms till timeout)
2022-03-28 19:01:22 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1007659ms till timeout)
2022-03-28 19:01:22 [ForkJoinPool-1-worker-35] INFO  [PodUtils:186] Waiting for message will be in the log
2022-03-28 19:01:22 [ForkJoinPool-1-worker-35] DEBUG [TestUtils:125] Waiting for Waiting for message will be in the log
2022-03-28 19:01:22 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (682559ms till timeout)
2022-03-28 19:01:22 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (119816ms till timeout)
2022-03-28 19:01:22 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1003571ms till timeout)
2022-03-28 19:01:22 [ForkJoinPool-1-worker-21] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-188382d8
2022-03-28 19:01:22 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (162872ms till timeout)
2022-03-28 19:01:22 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (718731ms till timeout)
2022-03-28 19:01:22 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:01:22 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2361958ms till timeout)
2022-03-28 19:01:22 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (161399ms till timeout)
2022-03-28 19:01:22 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (201552ms till timeout)
2022-03-28 19:01:22 [ForkJoinPool-1-worker-21] INFO  [ResourceManager:369] ############################################################################
2022-03-28 19:01:22 [ForkJoinPool-1-worker-21] INFO  [TestSuiteNamespaceManager:200] Deleting namespace:namespace-1 for test case:testConfigurationReflection
2022-03-28 19:01:23 [ForkJoinPool-1-worker-21] DEBUG [TestUtils:125] Waiting for Namespace namespace-1 removal
2022-03-28 19:01:23 [ForkJoinPool-1-worker-21] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-1 -o yaml
2022-03-28 19:01:23 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:01:23 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1004386ms till timeout)
2022-03-28 19:01:23 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6343cec-cruise-control-6cd68d58b9-k7qb2 not ready: cruise-control)
2022-03-28 19:01:23 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6343cec-cruise-control-6cd68d58b9-k7qb2 not ready: tls-sidecar)
2022-03-28 19:01:23 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:106] Pods my-cluster-e6343cec-cruise-control-6cd68d58b9-k7qb2 are ready
2022-03-28 19:01:23 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-e6343cec, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-e6343cec-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598679ms till timeout)
2022-03-28 19:01:23 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (681442ms till timeout)
2022-03-28 19:01:23 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1006533ms till timeout)
2022-03-28 19:01:23 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (184669ms till timeout)
2022-03-28 19:01:23 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-1 -o yaml
2022-03-28 19:01:23 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Return code: 0
2022-03-28 19:01:23 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (479526ms till timeout)
2022-03-28 19:01:23 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (118682ms till timeout)
2022-03-28 19:01:23 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929}
2022-03-28 19:01:23 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1002397ms till timeout)
2022-03-28 19:01:23 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929, my-cluster-08515847-cruise-control-dcd7c895b-4pkw7=37920413-f627-4a97-919c-4ff0c818c13a}
2022-03-28 19:01:23 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929}
2022-03-28 19:01:23 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Deployment my-cluster-08515847-cruise-control rolling update in namespace:namespace-4 not ready, will try again in 5000 ms (583882ms till timeout)
2022-03-28 19:01:23 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (161688ms till timeout)
2022-03-28 19:01:24 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (717563ms till timeout)
2022-03-28 19:01:24 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:01:24 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2360789ms till timeout)
2022-03-28 19:01:24 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (160237ms till timeout)
2022-03-28 19:01:24 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (200386ms till timeout)
2022-03-28 19:01:24 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1003288ms till timeout)
2022-03-28 19:01:24 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1005404ms till timeout)
2022-03-28 19:01:24 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6343cec-cruise-control-6cd68d58b9-k7qb2 not ready: cruise-control)
2022-03-28 19:01:24 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6343cec-cruise-control-6cd68d58b9-k7qb2 not ready: tls-sidecar)
2022-03-28 19:01:24 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:106] Pods my-cluster-e6343cec-cruise-control-6cd68d58b9-k7qb2 are ready
2022-03-28 19:01:24 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-e6343cec, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-e6343cec-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597464ms till timeout)
2022-03-28 19:01:24 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (680308ms till timeout)
2022-03-28 19:01:24 [ForkJoinPool-1-worker-21] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-1 -o yaml
2022-03-28 19:01:24 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:01:24 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (117562ms till timeout)
2022-03-28 19:01:24 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (183339ms till timeout)
2022-03-28 19:01:24 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1001301ms till timeout)
2022-03-28 19:01:25 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-1 -o yaml
2022-03-28 19:01:25 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Return code: 0
2022-03-28 19:01:25 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (478037ms till timeout)
2022-03-28 19:01:25 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (160574ms till timeout)
2022-03-28 19:01:25 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (716438ms till timeout)
2022-03-28 19:01:25 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:01:25 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2359665ms till timeout)
2022-03-28 19:01:25 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (159073ms till timeout)
2022-03-28 19:01:25 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (199250ms till timeout)
2022-03-28 19:01:25 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-5dfd606f will have desired state: Ready not ready, will try again in 1000 ms (1002191ms till timeout)
2022-03-28 19:01:25 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1004306ms till timeout)
2022-03-28 19:01:25 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (679118ms till timeout)
2022-03-28 19:01:25 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6343cec-cruise-control-6cd68d58b9-k7qb2 not ready: cruise-control)
2022-03-28 19:01:25 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6343cec-cruise-control-6cd68d58b9-k7qb2 not ready: tls-sidecar)
2022-03-28 19:01:25 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:106] Pods my-cluster-e6343cec-cruise-control-6cd68d58b9-k7qb2 are ready
2022-03-28 19:01:25 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-e6343cec, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-e6343cec-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596270ms till timeout)
2022-03-28 19:01:25 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (116440ms till timeout)
2022-03-28 19:01:25 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:01:26 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (1000192ms till timeout)
2022-03-28 19:01:26 [ForkJoinPool-1-worker-21] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-1 -o yaml
2022-03-28 19:01:26 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (182035ms till timeout)
2022-03-28 19:01:26 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (159353ms till timeout)
2022-03-28 19:01:26 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (715233ms till timeout)
2022-03-28 19:01:26 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:01:26 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2358459ms till timeout)
2022-03-28 19:01:26 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (157902ms till timeout)
2022-03-28 19:01:26 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (198062ms till timeout)
2022-03-28 19:01:26 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-1 -o yaml
2022-03-28 19:01:26 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Return code: 0
2022-03-28 19:01:26 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (476574ms till timeout)
2022-03-28 19:01:26 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:444] Kafka: my-cluster-5dfd606f is in desired state: Ready
2022-03-28 19:01:26 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1003128ms till timeout)
2022-03-28 19:01:26 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (677939ms till timeout)
2022-03-28 19:01:26 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6343cec-cruise-control-6cd68d58b9-k7qb2 not ready: cruise-control)
2022-03-28 19:01:26 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6343cec-cruise-control-6cd68d58b9-k7qb2 not ready: tls-sidecar)
2022-03-28 19:01:26 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:106] Pods my-cluster-e6343cec-cruise-control-6cd68d58b9-k7qb2 are ready
2022-03-28 19:01:26 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-e6343cec, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-e6343cec-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595093ms till timeout)
2022-03-28 19:01:26 [ForkJoinPool-1-worker-15] INFO  [CruiseControlConfigurationST:111] Removing Cruise Control to the classic Kafka.
2022-03-28 19:01:26 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (115280ms till timeout)
2022-03-28 19:01:27 [ForkJoinPool-1-worker-15] INFO  [RollingUpdateUtils:73] Waiting for component with name: my-cluster-5dfd606f-kafka rolling update
2022-03-28 19:01:27 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:74] Waiting for rolling update of component matching LabelSelector: LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-5dfd606f-kafka, strimzi.io/cluster=my-cluster-5dfd606f, strimzi.io/kind=Kafka}, additionalProperties={})
2022-03-28 19:01:27 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for component with name my-cluster-5dfd606f-kafka rolling update
2022-03-28 19:01:27 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-5dfd606f-kafka-0=a766ef9e-2663-4c21-ad83-6b7d9083122c, my-cluster-5dfd606f-kafka-1=57938984-c88e-45c5-b973-9f953a8e782c, my-cluster-5dfd606f-kafka-2=044285fe-7f53-4c1c-84e1-c51aec79b2ce}
2022-03-28 19:01:27 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (998987ms till timeout)
2022-03-28 19:01:27 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:01:27 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-5dfd606f-kafka-0=a766ef9e-2663-4c21-ad83-6b7d9083122c, my-cluster-5dfd606f-kafka-1=57938984-c88e-45c5-b973-9f953a8e782c, my-cluster-5dfd606f-kafka-2=044285fe-7f53-4c1c-84e1-c51aec79b2ce}
2022-03-28 19:01:27 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-5dfd606f-kafka-0=a766ef9e-2663-4c21-ad83-6b7d9083122c, my-cluster-5dfd606f-kafka-1=57938984-c88e-45c5-b973-9f953a8e782c, my-cluster-5dfd606f-kafka-2=044285fe-7f53-4c1c-84e1-c51aec79b2ce}
2022-03-28 19:01:27 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:50] At least my-cluster-5dfd606f-kafka-1 hasn't rolled
2022-03-28 19:01:27 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] component with name my-cluster-5dfd606f-kafka rolling update not ready, will try again in 5000 ms (1799784ms till timeout)
2022-03-28 19:01:27 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (158210ms till timeout)
2022-03-28 19:01:27 [ForkJoinPool-1-worker-21] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-1 -o yaml
2022-03-28 19:01:27 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (180654ms till timeout)
2022-03-28 19:01:27 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (714077ms till timeout)
2022-03-28 19:01:27 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:01:27 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2357305ms till timeout)
2022-03-28 19:01:27 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (156750ms till timeout)
2022-03-28 19:01:27 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (196904ms till timeout)
2022-03-28 19:01:27 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:http-bridge-scram-sha-st-shared-kafka-clients not ready, will try again in 10000 ms (458383ms till timeout)
2022-03-28 19:01:27 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1002031ms till timeout)
2022-03-28 19:01:27 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-1 -o yaml
2022-03-28 19:01:27 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Return code: 0
2022-03-28 19:01:27 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (475094ms till timeout)
2022-03-28 19:01:27 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (676843ms till timeout)
2022-03-28 19:01:28 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6343cec-cruise-control-6cd68d58b9-k7qb2 not ready: cruise-control)
2022-03-28 19:01:28 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6343cec-cruise-control-6cd68d58b9-k7qb2 not ready: tls-sidecar)
2022-03-28 19:01:28 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:106] Pods my-cluster-e6343cec-cruise-control-6cd68d58b9-k7qb2 are ready
2022-03-28 19:01:28 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-e6343cec, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-e6343cec-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593900ms till timeout)
2022-03-28 19:01:28 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (114080ms till timeout)
2022-03-28 19:01:28 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (997797ms till timeout)
2022-03-28 19:01:28 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (157103ms till timeout)
2022-03-28 19:01:28 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:01:28 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:01:28 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2356197ms till timeout)
2022-03-28 19:01:28 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (712968ms till timeout)
2022-03-28 19:01:28 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (155627ms till timeout)
2022-03-28 19:01:28 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (195796ms till timeout)
2022-03-28 19:01:28 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (179355ms till timeout)
2022-03-28 19:01:28 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929}
2022-03-28 19:01:28 [ForkJoinPool-1-worker-21] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-1 -o yaml
2022-03-28 19:01:28 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (1000933ms till timeout)
2022-03-28 19:01:29 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (675744ms till timeout)
2022-03-28 19:01:29 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6343cec-cruise-control-6cd68d58b9-k7qb2 not ready: cruise-control)
2022-03-28 19:01:29 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6343cec-cruise-control-6cd68d58b9-k7qb2 not ready: tls-sidecar)
2022-03-28 19:01:29 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:106] Pods my-cluster-e6343cec-cruise-control-6cd68d58b9-k7qb2 are ready
2022-03-28 19:01:29 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-e6343cec, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-e6343cec-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592801ms till timeout)
2022-03-28 19:01:29 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929, my-cluster-08515847-cruise-control-dcd7c895b-4pkw7=37920413-f627-4a97-919c-4ff0c818c13a}
2022-03-28 19:01:29 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929}
2022-03-28 19:01:29 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Deployment my-cluster-08515847-cruise-control rolling update in namespace:namespace-4 not ready, will try again in 5000 ms (578520ms till timeout)
2022-03-28 19:01:29 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (112888ms till timeout)
2022-03-28 19:01:29 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-1 -o yaml
2022-03-28 19:01:29 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Return code: 0
2022-03-28 19:01:29 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (473553ms till timeout)
2022-03-28 19:01:29 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (996699ms till timeout)
2022-03-28 19:01:29 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (155998ms till timeout)
2022-03-28 19:01:29 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (711867ms till timeout)
2022-03-28 19:01:29 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:01:29 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2355090ms till timeout)
2022-03-28 19:01:29 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (194691ms till timeout)
2022-03-28 19:01:29 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (154433ms till timeout)
2022-03-28 19:01:29 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:01:29 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (178154ms till timeout)
2022-03-28 19:01:30 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (999829ms till timeout)
2022-03-28 19:01:30 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (674640ms till timeout)
2022-03-28 19:01:30 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6343cec-cruise-control-6cd68d58b9-k7qb2 not ready: cruise-control)
2022-03-28 19:01:30 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6343cec-cruise-control-6cd68d58b9-k7qb2 not ready: tls-sidecar)
2022-03-28 19:01:30 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:106] Pods my-cluster-e6343cec-cruise-control-6cd68d58b9-k7qb2 are ready
2022-03-28 19:01:30 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-e6343cec, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-e6343cec-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591698ms till timeout)
2022-03-28 19:01:30 [ForkJoinPool-1-worker-21] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-1 -o yaml
2022-03-28 19:01:30 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (111753ms till timeout)
2022-03-28 19:01:30 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (995602ms till timeout)
2022-03-28 19:01:30 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (154881ms till timeout)
2022-03-28 19:01:30 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (710755ms till timeout)
2022-03-28 19:01:30 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:01:30 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2353981ms till timeout)
2022-03-28 19:01:30 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-1 -o yaml
2022-03-28 19:01:30 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Return code: 0
2022-03-28 19:01:30 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (472115ms till timeout)
2022-03-28 19:01:31 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (153227ms till timeout)
2022-03-28 19:01:31 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:01:31 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (998716ms till timeout)
2022-03-28 19:01:31 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (673526ms till timeout)
2022-03-28 19:01:31 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (176849ms till timeout)
2022-03-28 19:01:31 [ForkJoinPool-1-worker-23] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment producer-136568709 deletion
2022-03-28 19:01:31 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for ReplicaSet producer-136568709 to be deleted
2022-03-28 19:01:31 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6343cec-cruise-control-6cd68d58b9-k7qb2 not ready: cruise-control)
2022-03-28 19:01:31 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6343cec-cruise-control-6cd68d58b9-k7qb2 not ready: tls-sidecar)
2022-03-28 19:01:31 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:106] Pods my-cluster-e6343cec-cruise-control-6cd68d58b9-k7qb2 are ready
2022-03-28 19:01:31 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-e6343cec, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-e6343cec-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590588ms till timeout)
2022-03-28 19:01:31 [ForkJoinPool-1-worker-23] DEBUG [JobUtils:40] Job producer-136568709 was deleted
2022-03-28 19:01:31 [ForkJoinPool-1-worker-23] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment consumer-142659915 deletion
2022-03-28 19:01:31 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for ReplicaSet consumer-142659915 to be deleted
2022-03-28 19:01:31 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (110574ms till timeout)
2022-03-28 19:01:31 [ForkJoinPool-1-worker-23] DEBUG [JobUtils:40] Job consumer-142659915 was deleted
2022-03-28 19:01:31 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (994435ms till timeout)
2022-03-28 19:01:31 [ForkJoinPool-1-worker-23] DEBUG [AbstractST:674] ============================================================================
2022-03-28 19:01:31 [ForkJoinPool-1-worker-23] DEBUG [AbstractST:675] [bridge.HttpBridgeTlsST - After Each] - Clean up after test
2022-03-28 19:01:31 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:344] ############################################################################
2022-03-28 19:01:31 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:348] Delete all resources for testReceiveSimpleMessageTls
2022-03-28 19:01:31 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:241] Delete of Job consumer-142659915 in namespace http-bridge-tls-st
2022-03-28 19:01:31 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (153739ms till timeout)
2022-03-28 19:01:31 [ForkJoinPool-1-worker-21] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-1 -o yaml
2022-03-28 19:01:31 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:consumer-142659915
2022-03-28 19:01:31 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (709608ms till timeout)
2022-03-28 19:01:31 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:01:31 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2352836ms till timeout)
2022-03-28 19:01:32 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:241] Delete of Job producer-136568709 in namespace http-bridge-tls-st
2022-03-28 19:01:32 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (152082ms till timeout)
2022-03-28 19:01:32 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:producer-136568709
2022-03-28 19:01:32 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-5dfd606f-kafka-0=a766ef9e-2663-4c21-ad83-6b7d9083122c, my-cluster-5dfd606f-kafka-1=57938984-c88e-45c5-b973-9f953a8e782c, my-cluster-5dfd606f-kafka-2=044285fe-7f53-4c1c-84e1-c51aec79b2ce}
2022-03-28 19:01:32 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (997567ms till timeout)
2022-03-28 19:01:32 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-1 -o yaml
2022-03-28 19:01:32 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Return code: 0
2022-03-28 19:01:32 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (470689ms till timeout)
2022-03-28 19:01:32 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:241] Delete of KafkaTopic my-topic-1031306309-1345743071 in namespace http-bridge-tls-st
2022-03-28 19:01:32 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-f743fdfd will have desired state: Ready not ready, will try again in 1000 ms (672379ms till timeout)
2022-03-28 19:01:32 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:01:32 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-5dfd606f-kafka-0=a766ef9e-2663-4c21-ad83-6b7d9083122c, my-cluster-5dfd606f-kafka-1=57938984-c88e-45c5-b973-9f953a8e782c, my-cluster-5dfd606f-kafka-2=044285fe-7f53-4c1c-84e1-c51aec79b2ce}
2022-03-28 19:01:32 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-5dfd606f-kafka-0=a766ef9e-2663-4c21-ad83-6b7d9083122c, my-cluster-5dfd606f-kafka-1=57938984-c88e-45c5-b973-9f953a8e782c, my-cluster-5dfd606f-kafka-2=044285fe-7f53-4c1c-84e1-c51aec79b2ce}
2022-03-28 19:01:32 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:50] At least my-cluster-5dfd606f-kafka-1 hasn't rolled
2022-03-28 19:01:32 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] component with name my-cluster-5dfd606f-kafka rolling update not ready, will try again in 5000 ms (1794643ms till timeout)
2022-03-28 19:01:32 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6343cec-cruise-control-6cd68d58b9-k7qb2 not ready: cruise-control)
2022-03-28 19:01:32 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6343cec-cruise-control-6cd68d58b9-k7qb2 not ready: tls-sidecar)
2022-03-28 19:01:32 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:106] Pods my-cluster-e6343cec-cruise-control-6cd68d58b9-k7qb2 are ready
2022-03-28 19:01:32 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-e6343cec, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-e6343cec-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (589438ms till timeout)
2022-03-28 19:01:32 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (175512ms till timeout)
2022-03-28 19:01:32 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1031306309-1345743071
2022-03-28 19:01:32 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:369] ############################################################################
2022-03-28 19:01:32 [ForkJoinPool-1-worker-23] DEBUG [SuiteThreadController:267] testReceiveSimpleMessageTls - Notifies waiting test cases:[testPauseReconciliationInKafkaRebalanceAndTopic, testPauseReconciliationInKafkaAndKafkaConnectWithConnector, testCapacityFile, testConfigurationFileIsCreated, testConfigurationPerformanceOptions, testConfigurationReflection, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testDeployAndUnDeployCruiseControl, testTlsExternalUserWithQuotas, testCreatingUsersWithSecretPrefix, testThrottlingQuotasCreateTopic, testThrottlingQuotasCreateAlterPartitions, testKafkaAdminTopicOperations, testUserTemplate, testUpdateUser, testTlsExternalUser, testSendSimpleMessageTls, testReceiveSimpleMessageTlsScramSha, testThrottlingQuotasDeleteTopic, testReceiveSimpleMessageTls, testTopicModificationOfReplicationFactor, testSendingMessagesToNonExistingTopic] to and randomly select one to start execution
2022-03-28 19:01:32 [ForkJoinPool-1-worker-23] DEBUG [SuiteThreadController:93] [bridge.HttpBridgeTlsST] - Removing parallel test: testReceiveSimpleMessageTls
2022-03-28 19:01:32 [ForkJoinPool-1-worker-23] DEBUG [SuiteThreadController:97] [bridge.HttpBridgeTlsST] - Parallel test count: 14
2022-03-28 19:01:32 [ForkJoinPool-1-worker-23] INFO  [TestSeparator:29] io.strimzi.systemtest.bridge.HttpBridgeTlsST.testReceiveSimpleMessageTls-FINISHED
2022-03-28 19:01:32 [ForkJoinPool-1-worker-23] INFO  [TestSeparator:30] ############################################################################
2022-03-28 19:01:32 [ForkJoinPool-1-worker-23] DEBUG [AbstractST:689] ============================================================================
2022-03-28 19:01:32 [ForkJoinPool-1-worker-23] DEBUG [AbstractST:690] [bridge.HttpBridgeTlsST - After All] - Clean up after test suite
2022-03-28 19:01:32 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:344] ############################################################################
2022-03-28 19:01:32 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:348] Delete all resources for HttpBridgeTlsST
2022-03-28 19:01:32 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:241] Delete of Deployment http-bridge-tls-st-kafka-clients in namespace http-bridge-tls-st
2022-03-28 19:01:32 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (109420ms till timeout)
2022-03-28 19:01:32 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (993277ms till timeout)
2022-03-28 19:01:32 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:http-bridge-tls-st-kafka-clients
2022-03-28 19:01:33 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (152576ms till timeout)
2022-03-28 19:01:33 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (708447ms till timeout)
2022-03-28 19:01:33 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:01:33 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2351673ms till timeout)
2022-03-28 19:01:33 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (150926ms till timeout)
2022-03-28 19:01:33 [ForkJoinPool-1-worker-21] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-1 -o yaml
2022-03-28 19:01:33 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:http-bridge-tls-st-kafka-clients not ready, will try again in 10000 ms (479517ms till timeout)
2022-03-28 19:01:33 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (996407ms till timeout)
2022-03-28 19:01:33 [ForkJoinPool-1-worker-31] INFO  [ResourceManager:444] Kafka: my-cluster-f743fdfd is in desired state: Ready
2022-03-28 19:01:33 [ForkJoinPool-1-worker-31] INFO  [ResourceManager:155] Create/Update KafkaTopic my-topic-1727421070-668192795 in namespace namespace-9
2022-03-28 19:01:33 [ForkJoinPool-1-worker-31] INFO  [ResourceManager:164] Using Namespace: namespace-8
2022-03-28 19:01:33 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6343cec-cruise-control-6cd68d58b9-k7qb2 not ready: cruise-control)
2022-03-28 19:01:33 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6343cec-cruise-control-6cd68d58b9-k7qb2 not ready: tls-sidecar)
2022-03-28 19:01:33 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:106] Pods my-cluster-e6343cec-cruise-control-6cd68d58b9-k7qb2 are ready
2022-03-28 19:01:33 [ForkJoinPool-1-worker-3] INFO  [DeploymentUtils:141] Deployment my-cluster-e6343cec-cruise-control rolling update finished
2022-03-28 19:01:33 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:01:33 [ForkJoinPool-1-worker-31] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-1727421070-668192795
2022-03-28 19:01:33 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-1 -o yaml
2022-03-28 19:01:33 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Return code: 0
2022-03-28 19:01:33 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (469222ms till timeout)
2022-03-28 19:01:33 [ForkJoinPool-1-worker-31] INFO  [ResourceManager:433] Wait for KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready
2022-03-28 19:01:33 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (174156ms till timeout)
2022-03-28 19:01:33 [ForkJoinPool-1-worker-31] DEBUG [TestUtils:125] Waiting for KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready
2022-03-28 19:01:33 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (108269ms till timeout)
2022-03-28 19:01:34 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (992120ms till timeout)
2022-03-28 19:01:34 [ForkJoinPool-1-worker-3] INFO  [CruiseControlConfigurationST:171] Verifying that Kafka pods did not roll
2022-03-28 19:01:34 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Waiting for stability of rolling update will be not triggered
2022-03-28 19:01:34 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:01:34 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (179811ms till timeout)
2022-03-28 19:01:34 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (151425ms till timeout)
2022-03-28 19:01:34 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (707296ms till timeout)
2022-03-28 19:01:34 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:01:34 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2350524ms till timeout)
2022-03-28 19:01:34 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:01:34 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:01:34 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-e6343cec-kafka-2 hasn't rolled
2022-03-28 19:01:34 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929}
2022-03-28 19:01:34 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02, my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a} pods didn't roll. Remaining seconds for stability: 50
2022-03-28 19:01:34 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (299810ms till timeout)
2022-03-28 19:01:34 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (149773ms till timeout)
2022-03-28 19:01:34 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929, my-cluster-08515847-cruise-control-dcd7c895b-4pkw7=37920413-f627-4a97-919c-4ff0c818c13a}
2022-03-28 19:01:34 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929}
2022-03-28 19:01:34 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Deployment my-cluster-08515847-cruise-control rolling update in namespace:namespace-4 not ready, will try again in 5000 ms (573227ms till timeout)
2022-03-28 19:01:34 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (995252ms till timeout)
2022-03-28 19:01:34 [ForkJoinPool-1-worker-21] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-1 -o yaml
2022-03-28 19:01:35 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:01:35 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (991016ms till timeout)
2022-03-28 19:01:35 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (107064ms till timeout)
2022-03-28 19:01:35 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:01:35 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (178705ms till timeout)
2022-03-28 19:01:35 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (172860ms till timeout)
2022-03-28 19:01:35 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (150319ms till timeout)
2022-03-28 19:01:35 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-1 -o yaml
2022-03-28 19:01:35 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Return code: 0
2022-03-28 19:01:35 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (467764ms till timeout)
2022-03-28 19:01:35 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (706191ms till timeout)
2022-03-28 19:01:35 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:01:35 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:01:35 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:01:35 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2349418ms till timeout)
2022-03-28 19:01:35 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-e6343cec-kafka-2 hasn't rolled
2022-03-28 19:01:35 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02, my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a} pods didn't roll. Remaining seconds for stability: 49
2022-03-28 19:01:35 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (298704ms till timeout)
2022-03-28 19:01:35 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (148667ms till timeout)
2022-03-28 19:01:35 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (994154ms till timeout)
2022-03-28 19:01:36 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (989918ms till timeout)
2022-03-28 19:01:36 [ForkJoinPool-1-worker-21] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-1 -o yaml
2022-03-28 19:01:36 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:01:36 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (105876ms till timeout)
2022-03-28 19:01:36 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:01:36 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (177508ms till timeout)
2022-03-28 19:01:36 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:01:36 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2348318ms till timeout)
2022-03-28 19:01:36 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:01:36 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:01:36 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-e6343cec-kafka-2 hasn't rolled
2022-03-28 19:01:36 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02, my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a} pods didn't roll. Remaining seconds for stability: 48
2022-03-28 19:01:36 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (297605ms till timeout)
2022-03-28 19:01:36 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (704999ms till timeout)
2022-03-28 19:01:36 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (149035ms till timeout)
2022-03-28 19:01:36 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (171567ms till timeout)
2022-03-28 19:01:36 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (147558ms till timeout)
2022-03-28 19:01:36 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-1 -o yaml
2022-03-28 19:01:36 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Return code: 0
2022-03-28 19:01:36 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (466256ms till timeout)
2022-03-28 19:01:36 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (993057ms till timeout)
2022-03-28 19:01:37 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (988821ms till timeout)
2022-03-28 19:01:37 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-5dfd606f-kafka-0=a766ef9e-2663-4c21-ad83-6b7d9083122c, my-cluster-5dfd606f-kafka-1=57938984-c88e-45c5-b973-9f953a8e782c, my-cluster-5dfd606f-kafka-2=044285fe-7f53-4c1c-84e1-c51aec79b2ce}
2022-03-28 19:01:37 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:01:37 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (104776ms till timeout)
2022-03-28 19:01:37 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (176410ms till timeout)
2022-03-28 19:01:37 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:01:37 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:01:37 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-e6343cec-kafka-2 hasn't rolled
2022-03-28 19:01:37 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02, my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a} pods didn't roll. Remaining seconds for stability: 47
2022-03-28 19:01:37 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (296504ms till timeout)
2022-03-28 19:01:37 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-5dfd606f-kafka-0=a766ef9e-2663-4c21-ad83-6b7d9083122c, my-cluster-5dfd606f-kafka-1=57938984-c88e-45c5-b973-9f953a8e782c, my-cluster-5dfd606f-kafka-2=044285fe-7f53-4c1c-84e1-c51aec79b2ce}
2022-03-28 19:01:37 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-5dfd606f-kafka-0=a766ef9e-2663-4c21-ad83-6b7d9083122c, my-cluster-5dfd606f-kafka-1=57938984-c88e-45c5-b973-9f953a8e782c, my-cluster-5dfd606f-kafka-2=044285fe-7f53-4c1c-84e1-c51aec79b2ce}
2022-03-28 19:01:37 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:50] At least my-cluster-5dfd606f-kafka-1 hasn't rolled
2022-03-28 19:01:37 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] component with name my-cluster-5dfd606f-kafka rolling update not ready, will try again in 5000 ms (1789505ms till timeout)
2022-03-28 19:01:37 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:01:37 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2347125ms till timeout)
2022-03-28 19:01:37 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:01:37 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (703893ms till timeout)
2022-03-28 19:01:37 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (147923ms till timeout)
2022-03-28 19:01:37 [ForkJoinPool-1-worker-21] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-1 -o yaml
2022-03-28 19:01:37 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (170244ms till timeout)
2022-03-28 19:01:37 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (146338ms till timeout)
2022-03-28 19:01:37 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (991920ms till timeout)
2022-03-28 19:01:38 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:241] Delete of KafkaBridge http-bridge-scram-sha-cluster-name in namespace http-bridge-scram-sha-st
2022-03-28 19:01:38 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaBridge:http-bridge-scram-sha-cluster-name
2022-03-28 19:01:38 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:241] Delete of KafkaUser my-user-410252673-1542305576 in namespace http-bridge-scram-sha-st
2022-03-28 19:01:38 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (987712ms till timeout)
2022-03-28 19:01:38 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:01:38 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-410252673-1542305576
2022-03-28 19:01:38 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (103625ms till timeout)
2022-03-28 19:01:38 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (175306ms till timeout)
2022-03-28 19:01:38 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:01:38 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:01:38 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-e6343cec-kafka-2 hasn't rolled
2022-03-28 19:01:38 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02, my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a} pods didn't roll. Remaining seconds for stability: 46
2022-03-28 19:01:38 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (295385ms till timeout)
2022-03-28 19:01:38 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:241] Delete of Kafka http-bridge-scram-sha-cluster-name in namespace http-bridge-scram-sha-st
2022-03-28 19:01:38 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (702789ms till timeout)
2022-03-28 19:01:38 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:01:38 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2346017ms till timeout)
2022-03-28 19:01:38 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (146731ms till timeout)
2022-03-28 19:01:38 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:http-bridge-scram-sha-cluster-name
2022-03-28 19:01:39 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:01:39 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (145149ms till timeout)
2022-03-28 19:01:39 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (990732ms till timeout)
2022-03-28 19:01:39 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:369] ############################################################################
2022-03-28 19:01:39 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (168850ms till timeout)
2022-03-28 19:01:39 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Namespace http-bridge-scram-sha-st removal
2022-03-28 19:01:39 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 19:01:39 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929}
2022-03-28 19:01:39 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (986614ms till timeout)
2022-03-28 19:01:39 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:01:39 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (102479ms till timeout)
2022-03-28 19:01:39 [ForkJoinPool-1-worker-55] INFO  [Exec:417] Command: oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-03-28 19:01:39 [ForkJoinPool-1-worker-55] INFO  [Exec:417] Return code: 0
2022-03-28 19:01:39 [ForkJoinPool-1-worker-55] INFO  [TopicST:323] Topic with name my-topic-1692964628-1348460540 is not created yet
2022-03-28 19:01:39 [ForkJoinPool-1-worker-55] INFO  [TopicST:325] Trying to send messages to non-existing topic my-topic-1692964628-1348460540
2022-03-28 19:01:39 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (174110ms till timeout)
2022-03-28 19:01:39 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929, my-cluster-08515847-cruise-control-dcd7c895b-4pkw7=37920413-f627-4a97-919c-4ff0c818c13a}
2022-03-28 19:01:39 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:01:39 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929}
2022-03-28 19:01:39 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:01:39 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Deployment my-cluster-08515847-cruise-control rolling update in namespace:namespace-4 not ready, will try again in 5000 ms (567915ms till timeout)
2022-03-28 19:01:39 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-e6343cec-kafka-2 hasn't rolled
2022-03-28 19:01:39 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02, my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a} pods didn't roll. Remaining seconds for stability: 45
2022-03-28 19:01:39 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (294205ms till timeout)
2022-03-28 19:01:39 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 19:01:39 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 19:01:39 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace http-bridge-scram-sha-st removal not ready, will try again in 1000 ms (479452ms till timeout)
2022-03-28 19:01:39 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (701594ms till timeout)
2022-03-28 19:01:39 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:01:39 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2344821ms till timeout)
2022-03-28 19:01:39 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (145627ms till timeout)
2022-03-28 19:01:39 [ForkJoinPool-1-worker-55] DEBUG [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@7a2380e8, which are set.
2022-03-28 19:01:40 [ForkJoinPool-1-worker-55] INFO  [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@25c4b1ac, messages=[], arguments=[--bootstrap-server, topic-cluster-name-kafka-bootstrap.topic-st.svc:9092, --topic, my-topic-1692964628-1348460540, --max-messages, 100], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='topic-cluster-name-kafka-clients-7c47b4459f-l2fwj', podNamespace='topic-st', bootstrapServer='topic-cluster-name-kafka-bootstrap.topic-st.svc:9092', topicName='my-topic-1692964628-1348460540', maxMessages=100, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@7a2380e8}
2022-03-28 19:01:40 [ForkJoinPool-1-worker-55] INFO  [InternalKafkaClient:94] Producing 100 messages to topic-cluster-name-kafka-bootstrap.topic-st.svc:9092:my-topic-1692964628-1348460540 from pod topic-cluster-name-kafka-clients-7c47b4459f-l2fwj
2022-03-28 19:01:40 [ForkJoinPool-1-worker-55] INFO  [VerifiableClient:192] Client command: oc exec topic-cluster-name-kafka-clients-7c47b4459f-l2fwj -n topic-st -- /opt/kafka/producer.sh --bootstrap-server topic-cluster-name-kafka-bootstrap.topic-st.svc:9092 --topic my-topic-1692964628-1348460540 --max-messages 100
2022-03-28 19:01:40 [ForkJoinPool-1-worker-55] TRACE [Exec:248] Running command - oc exec topic-cluster-name-kafka-clients-7c47b4459f-l2fwj -n topic-st -- /opt/kafka/producer.sh --bootstrap-server topic-cluster-name-kafka-bootstrap.topic-st.svc:9092 --topic my-topic-1692964628-1348460540 --max-messages 100
2022-03-28 19:01:40 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (144035ms till timeout)
2022-03-28 19:01:40 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (989619ms till timeout)
2022-03-28 19:01:40 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:01:40 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (167651ms till timeout)
2022-03-28 19:01:40 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (985509ms till timeout)
2022-03-28 19:01:40 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:01:40 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 19:01:41 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (101237ms till timeout)
2022-03-28 19:01:41 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (172914ms till timeout)
2022-03-28 19:01:41 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (700495ms till timeout)
2022-03-28 19:01:41 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:01:41 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:01:41 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-e6343cec-kafka-2 hasn't rolled
2022-03-28 19:01:41 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02, my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a} pods didn't roll. Remaining seconds for stability: 44
2022-03-28 19:01:41 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (292990ms till timeout)
2022-03-28 19:01:41 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:01:41 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2343625ms till timeout)
2022-03-28 19:01:41 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (144397ms till timeout)
2022-03-28 19:01:41 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (142929ms till timeout)
2022-03-28 19:01:41 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (988502ms till timeout)
2022-03-28 19:01:41 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 19:01:41 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 19:01:41 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace http-bridge-scram-sha-st removal not ready, will try again in 1000 ms (477931ms till timeout)
2022-03-28 19:01:41 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:01:41 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (166457ms till timeout)
2022-03-28 19:01:41 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (984394ms till timeout)
2022-03-28 19:01:42 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:01:42 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (100130ms till timeout)
2022-03-28 19:01:42 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (699342ms till timeout)
2022-03-28 19:01:42 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (171761ms till timeout)
2022-03-28 19:01:42 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:01:42 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:01:42 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-e6343cec-kafka-2 hasn't rolled
2022-03-28 19:01:42 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02, my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a} pods didn't roll. Remaining seconds for stability: 43
2022-03-28 19:01:42 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (291855ms till timeout)
2022-03-28 19:01:42 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:01:42 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2342474ms till timeout)
2022-03-28 19:01:42 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (143281ms till timeout)
2022-03-28 19:01:42 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (141817ms till timeout)
2022-03-28 19:01:42 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 19:01:42 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (987392ms till timeout)
2022-03-28 19:01:42 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-5dfd606f-kafka-0=a766ef9e-2663-4c21-ad83-6b7d9083122c, my-cluster-5dfd606f-kafka-1=57938984-c88e-45c5-b973-9f953a8e782c, my-cluster-5dfd606f-kafka-2=044285fe-7f53-4c1c-84e1-c51aec79b2ce}
2022-03-28 19:01:42 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-5dfd606f-kafka-0=d3be017c-9e19-4ee7-b0fc-58e8694c30b9, my-cluster-5dfd606f-kafka-1=57938984-c88e-45c5-b973-9f953a8e782c, my-cluster-5dfd606f-kafka-2=044285fe-7f53-4c1c-84e1-c51aec79b2ce}
2022-03-28 19:01:42 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-5dfd606f-kafka-0=d3be017c-9e19-4ee7-b0fc-58e8694c30b9, my-cluster-5dfd606f-kafka-1=57938984-c88e-45c5-b973-9f953a8e782c, my-cluster-5dfd606f-kafka-2=044285fe-7f53-4c1c-84e1-c51aec79b2ce}
2022-03-28 19:01:42 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:50] At least my-cluster-5dfd606f-kafka-1 hasn't rolled
2022-03-28 19:01:42 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] component with name my-cluster-5dfd606f-kafka rolling update not ready, will try again in 5000 ms (1784406ms till timeout)
2022-03-28 19:01:42 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:01:42 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (165262ms till timeout)
2022-03-28 19:01:42 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (983223ms till timeout)
2022-03-28 19:01:43 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:01:43 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (99027ms till timeout)
2022-03-28 19:01:43 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (170665ms till timeout)
2022-03-28 19:01:43 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (698245ms till timeout)
2022-03-28 19:01:43 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-1 -o yaml
2022-03-28 19:01:43 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Return code: 0
2022-03-28 19:01:43 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (459669ms till timeout)
2022-03-28 19:01:43 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:01:43 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:01:43 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-e6343cec-kafka-2 hasn't rolled
2022-03-28 19:01:43 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02, my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a} pods didn't roll. Remaining seconds for stability: 42
2022-03-28 19:01:43 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (290662ms till timeout)
2022-03-28 19:01:43 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:01:43 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2341279ms till timeout)
2022-03-28 19:01:43 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (142086ms till timeout)
2022-03-28 19:01:43 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (140630ms till timeout)
2022-03-28 19:01:43 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (986200ms till timeout)
2022-03-28 19:01:43 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:http-bridge-tls-st-kafka-clients not ready, will try again in 10000 ms (468989ms till timeout)
2022-03-28 19:01:44 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:01:44 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (982076ms till timeout)
2022-03-28 19:01:44 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (163921ms till timeout)
2022-03-28 19:01:44 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (97927ms till timeout)
2022-03-28 19:01:44 [ForkJoinPool-1-worker-21] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-1 -o yaml
2022-03-28 19:01:44 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:01:44 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (169560ms till timeout)
2022-03-28 19:01:44 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (697139ms till timeout)
2022-03-28 19:01:44 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:01:44 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:01:44 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-e6343cec-kafka-2 hasn't rolled
2022-03-28 19:01:44 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02, my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a} pods didn't roll. Remaining seconds for stability: 41
2022-03-28 19:01:44 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (289561ms till timeout)
2022-03-28 19:01:44 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:01:44 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2340177ms till timeout)
2022-03-28 19:01:44 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (140983ms till timeout)
2022-03-28 19:01:44 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (139525ms till timeout)
2022-03-28 19:01:44 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (985099ms till timeout)
2022-03-28 19:01:44 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929}
2022-03-28 19:01:44 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-1 -o yaml
2022-03-28 19:01:44 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Return code: 0
2022-03-28 19:01:44 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (458110ms till timeout)
2022-03-28 19:01:45 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929, my-cluster-08515847-cruise-control-dcd7c895b-4pkw7=37920413-f627-4a97-919c-4ff0c818c13a}
2022-03-28 19:01:45 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929}
2022-03-28 19:01:45 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Deployment my-cluster-08515847-cruise-control rolling update in namespace:namespace-4 not ready, will try again in 5000 ms (562719ms till timeout)
2022-03-28 19:01:45 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (980979ms till timeout)
2022-03-28 19:01:45 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:01:45 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (162724ms till timeout)
2022-03-28 19:01:45 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (96743ms till timeout)
2022-03-28 19:01:45 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:01:45 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (695958ms till timeout)
2022-03-28 19:01:45 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (168376ms till timeout)
2022-03-28 19:01:45 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:01:45 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:01:45 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-e6343cec-kafka-2 hasn't rolled
2022-03-28 19:01:45 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02, my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a} pods didn't roll. Remaining seconds for stability: 40
2022-03-28 19:01:45 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (288377ms till timeout)
2022-03-28 19:01:45 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:01:45 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2338994ms till timeout)
2022-03-28 19:01:45 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (139800ms till timeout)
2022-03-28 19:01:45 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (138344ms till timeout)
2022-03-28 19:01:45 [ForkJoinPool-1-worker-21] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-1 -o yaml
2022-03-28 19:01:46 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (983916ms till timeout)
2022-03-28 19:01:46 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (979879ms till timeout)
2022-03-28 19:01:46 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-1 -o yaml
2022-03-28 19:01:46 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Return code: 0
2022-03-28 19:01:46 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (456551ms till timeout)
2022-03-28 19:01:46 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:01:46 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (161530ms till timeout)
2022-03-28 19:01:46 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:01:46 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (694860ms till timeout)
2022-03-28 19:01:46 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (95545ms till timeout)
2022-03-28 19:01:46 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (167182ms till timeout)
2022-03-28 19:01:46 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:01:46 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:01:46 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-e6343cec-kafka-2 hasn't rolled
2022-03-28 19:01:46 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02, my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a} pods didn't roll. Remaining seconds for stability: 39
2022-03-28 19:01:46 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (287182ms till timeout)
2022-03-28 19:01:47 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:01:47 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2337798ms till timeout)
2022-03-28 19:01:47 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (138592ms till timeout)
2022-03-28 19:01:47 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (137143ms till timeout)
2022-03-28 19:01:47 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (982711ms till timeout)
2022-03-28 19:01:47 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (978782ms till timeout)
2022-03-28 19:01:47 [ForkJoinPool-1-worker-21] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-1 -o yaml
2022-03-28 19:01:47 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-5dfd606f-kafka-0=a766ef9e-2663-4c21-ad83-6b7d9083122c, my-cluster-5dfd606f-kafka-1=57938984-c88e-45c5-b973-9f953a8e782c, my-cluster-5dfd606f-kafka-2=044285fe-7f53-4c1c-84e1-c51aec79b2ce}
2022-03-28 19:01:47 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:01:47 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-5dfd606f-kafka-0=d3be017c-9e19-4ee7-b0fc-58e8694c30b9, my-cluster-5dfd606f-kafka-1=57938984-c88e-45c5-b973-9f953a8e782c, my-cluster-5dfd606f-kafka-2=044285fe-7f53-4c1c-84e1-c51aec79b2ce}
2022-03-28 19:01:47 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-5dfd606f-kafka-0=d3be017c-9e19-4ee7-b0fc-58e8694c30b9, my-cluster-5dfd606f-kafka-1=57938984-c88e-45c5-b973-9f953a8e782c, my-cluster-5dfd606f-kafka-2=044285fe-7f53-4c1c-84e1-c51aec79b2ce}
2022-03-28 19:01:47 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:50] At least my-cluster-5dfd606f-kafka-1 hasn't rolled
2022-03-28 19:01:47 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] component with name my-cluster-5dfd606f-kafka rolling update not ready, will try again in 5000 ms (1779279ms till timeout)
2022-03-28 19:01:47 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (166086ms till timeout)
2022-03-28 19:01:47 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (693667ms till timeout)
2022-03-28 19:01:47 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:01:47 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (160240ms till timeout)
2022-03-28 19:01:47 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (94351ms till timeout)
2022-03-28 19:01:48 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:01:48 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:01:48 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-e6343cec-kafka-2 hasn't rolled
2022-03-28 19:01:48 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02, my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a} pods didn't roll. Remaining seconds for stability: 38
2022-03-28 19:01:48 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (286084ms till timeout)
2022-03-28 19:01:48 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-1 -o yaml
2022-03-28 19:01:48 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Return code: 0
2022-03-28 19:01:48 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (454974ms till timeout)
2022-03-28 19:01:48 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:01:48 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2336699ms till timeout)
2022-03-28 19:01:48 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (137403ms till timeout)
2022-03-28 19:01:48 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (135946ms till timeout)
2022-03-28 19:01:48 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (981520ms till timeout)
2022-03-28 19:01:48 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 19:01:48 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 19:01:48 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace http-bridge-scram-sha-st removal not ready, will try again in 1000 ms (470922ms till timeout)
2022-03-28 19:01:48 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (977685ms till timeout)
2022-03-28 19:01:49 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (164990ms till timeout)
2022-03-28 19:01:49 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:01:49 [ForkJoinPool-1-worker-21] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-1 -o yaml
2022-03-28 19:01:49 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:01:49 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (692475ms till timeout)
2022-03-28 19:01:49 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (93159ms till timeout)
2022-03-28 19:01:49 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:01:49 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:01:49 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-e6343cec-kafka-2 hasn't rolled
2022-03-28 19:01:49 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02, my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a} pods didn't roll. Remaining seconds for stability: 37
2022-03-28 19:01:49 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (284889ms till timeout)
2022-03-28 19:01:49 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (158854ms till timeout)
2022-03-28 19:01:49 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:01:49 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2335507ms till timeout)
2022-03-28 19:01:49 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (136215ms till timeout)
2022-03-28 19:01:49 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 19:01:49 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (134758ms till timeout)
2022-03-28 19:01:49 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (980332ms till timeout)
2022-03-28 19:01:49 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-1 -o yaml
2022-03-28 19:01:49 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Return code: 0
2022-03-28 19:01:49 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (453430ms till timeout)
2022-03-28 19:01:49 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (976527ms till timeout)
2022-03-28 19:01:50 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929}
2022-03-28 19:01:50 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (163893ms till timeout)
2022-03-28 19:01:50 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:01:50 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (691283ms till timeout)
2022-03-28 19:01:50 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (91971ms till timeout)
2022-03-28 19:01:50 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929, my-cluster-08515847-cruise-control-dcd7c895b-4pkw7=37920413-f627-4a97-919c-4ff0c818c13a}
2022-03-28 19:01:50 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:01:50 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929}
2022-03-28 19:01:50 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:01:50 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Deployment my-cluster-08515847-cruise-control rolling update in namespace:namespace-4 not ready, will try again in 5000 ms (557411ms till timeout)
2022-03-28 19:01:50 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-e6343cec-kafka-2 hasn't rolled
2022-03-28 19:01:50 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02, my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a} pods didn't roll. Remaining seconds for stability: 36
2022-03-28 19:01:50 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (283701ms till timeout)
2022-03-28 19:01:50 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:01:50 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:01:50 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2334319ms till timeout)
2022-03-28 19:01:50 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (135032ms till timeout)
2022-03-28 19:01:50 [ForkJoinPool-1-worker-21] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-1 -o yaml
2022-03-28 19:01:50 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (157474ms till timeout)
2022-03-28 19:01:50 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (133573ms till timeout)
2022-03-28 19:01:50 [ForkJoinPool-1-worker-55] INFO  [InternalKafkaClient:97] Producer finished correctly: true
2022-03-28 19:01:50 [ForkJoinPool-1-worker-55] INFO  [InternalKafkaClient:101] Producer produced 100 messages
2022-03-28 19:01:50 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (979149ms till timeout)
2022-03-28 19:01:50 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (975344ms till timeout)
2022-03-28 19:01:50 [ForkJoinPool-1-worker-55] DEBUG [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@6be36051, which are set.
2022-03-28 19:01:50 [ForkJoinPool-1-worker-55] INFO  [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@332b86ab, messages=[], arguments=[--bootstrap-server, topic-cluster-name-kafka-bootstrap.topic-st.svc:9092, --topic, my-topic-1692964628-1348460540, --group-instance-id, instance912896638, --group-id, my-consumer-group-887477604, --max-messages, 100], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='topic-cluster-name-kafka-clients-7c47b4459f-l2fwj', podNamespace='topic-st', bootstrapServer='topic-cluster-name-kafka-bootstrap.topic-st.svc:9092', topicName='my-topic-1692964628-1348460540', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-887477604', consumerInstanceId='instance912896638', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@6be36051}
2022-03-28 19:01:50 [ForkJoinPool-1-worker-55] INFO  [InternalKafkaClient:157] Consuming 100 messages from topic-cluster-name-kafka-bootstrap.topic-st.svc:9092#my-topic-1692964628-1348460540 from pod topic-cluster-name-kafka-clients-7c47b4459f-l2fwj
2022-03-28 19:01:50 [ForkJoinPool-1-worker-55] INFO  [VerifiableClient:192] Client command: oc exec topic-cluster-name-kafka-clients-7c47b4459f-l2fwj -n topic-st -- /opt/kafka/consumer.sh --bootstrap-server topic-cluster-name-kafka-bootstrap.topic-st.svc:9092 --topic my-topic-1692964628-1348460540 --group-instance-id instance912896638 --group-id my-consumer-group-887477604 --max-messages 100
2022-03-28 19:01:50 [ForkJoinPool-1-worker-55] TRACE [Exec:248] Running command - oc exec topic-cluster-name-kafka-clients-7c47b4459f-l2fwj -n topic-st -- /opt/kafka/consumer.sh --bootstrap-server topic-cluster-name-kafka-bootstrap.topic-st.svc:9092 --topic my-topic-1692964628-1348460540 --group-instance-id instance912896638 --group-id my-consumer-group-887477604 --max-messages 100
2022-03-28 19:01:51 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (162796ms till timeout)
2022-03-28 19:01:51 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-1 -o yaml
2022-03-28 19:01:51 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Return code: 0
2022-03-28 19:01:51 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (451840ms till timeout)
2022-03-28 19:01:51 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:01:51 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (690184ms till timeout)
2022-03-28 19:01:51 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (90775ms till timeout)
2022-03-28 19:01:51 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:01:51 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:01:51 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-e6343cec-kafka-2 hasn't rolled
2022-03-28 19:01:51 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02, my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a} pods didn't roll. Remaining seconds for stability: 35
2022-03-28 19:01:51 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (282504ms till timeout)
2022-03-28 19:01:51 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:01:51 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2333216ms till timeout)
2022-03-28 19:01:51 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (133927ms till timeout)
2022-03-28 19:01:51 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:01:51 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (978047ms till timeout)
2022-03-28 19:01:51 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (132377ms till timeout)
2022-03-28 19:01:51 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (156180ms till timeout)
2022-03-28 19:01:51 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (974240ms till timeout)
2022-03-28 19:01:52 [ForkJoinPool-1-worker-21] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-1 -o yaml
2022-03-28 19:01:52 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (161700ms till timeout)
2022-03-28 19:01:52 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (689088ms till timeout)
2022-03-28 19:01:52 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:01:52 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (89675ms till timeout)
2022-03-28 19:01:52 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:01:52 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:01:52 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-e6343cec-kafka-2 hasn't rolled
2022-03-28 19:01:52 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02, my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a} pods didn't roll. Remaining seconds for stability: 34
2022-03-28 19:01:52 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (281402ms till timeout)
2022-03-28 19:01:52 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:01:52 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2332111ms till timeout)
2022-03-28 19:01:52 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (132827ms till timeout)
2022-03-28 19:01:52 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-5dfd606f-kafka-0=a766ef9e-2663-4c21-ad83-6b7d9083122c, my-cluster-5dfd606f-kafka-1=57938984-c88e-45c5-b973-9f953a8e782c, my-cluster-5dfd606f-kafka-2=044285fe-7f53-4c1c-84e1-c51aec79b2ce}
2022-03-28 19:01:52 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-5dfd606f-kafka-0=d3be017c-9e19-4ee7-b0fc-58e8694c30b9, my-cluster-5dfd606f-kafka-1=57938984-c88e-45c5-b973-9f953a8e782c, my-cluster-5dfd606f-kafka-2=044285fe-7f53-4c1c-84e1-c51aec79b2ce}
2022-03-28 19:01:52 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-5dfd606f-kafka-0=d3be017c-9e19-4ee7-b0fc-58e8694c30b9, my-cluster-5dfd606f-kafka-1=57938984-c88e-45c5-b973-9f953a8e782c, my-cluster-5dfd606f-kafka-2=044285fe-7f53-4c1c-84e1-c51aec79b2ce}
2022-03-28 19:01:52 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:50] At least my-cluster-5dfd606f-kafka-1 hasn't rolled
2022-03-28 19:01:52 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] component with name my-cluster-5dfd606f-kafka rolling update not ready, will try again in 5000 ms (1774179ms till timeout)
2022-03-28 19:01:53 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (976910ms till timeout)
2022-03-28 19:01:53 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (131240ms till timeout)
2022-03-28 19:01:53 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:01:53 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (973103ms till timeout)
2022-03-28 19:01:53 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (154949ms till timeout)
2022-03-28 19:01:53 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (160603ms till timeout)
2022-03-28 19:01:53 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (687991ms till timeout)
2022-03-28 19:01:53 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:01:53 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (88573ms till timeout)
2022-03-28 19:01:53 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:01:53 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:01:53 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-e6343cec-kafka-2 hasn't rolled
2022-03-28 19:01:53 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02, my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a} pods didn't roll. Remaining seconds for stability: 33
2022-03-28 19:01:53 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (280302ms till timeout)
2022-03-28 19:01:53 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:01:53 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2330919ms till timeout)
2022-03-28 19:01:53 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (131726ms till timeout)
2022-03-28 19:01:54 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (975778ms till timeout)
2022-03-28 19:01:54 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (130107ms till timeout)
2022-03-28 19:01:54 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (971972ms till timeout)
2022-03-28 19:01:54 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:01:54 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:http-bridge-tls-st-kafka-clients not ready, will try again in 10000 ms (458510ms till timeout)
2022-03-28 19:01:54 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (153631ms till timeout)
2022-03-28 19:01:54 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (159476ms till timeout)
2022-03-28 19:01:54 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (686894ms till timeout)
2022-03-28 19:01:54 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:01:54 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (87469ms till timeout)
2022-03-28 19:01:54 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:01:54 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:01:54 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-e6343cec-kafka-2 hasn't rolled
2022-03-28 19:01:54 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02, my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a} pods didn't roll. Remaining seconds for stability: 32
2022-03-28 19:01:54 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (279198ms till timeout)
2022-03-28 19:01:54 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:01:54 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2329815ms till timeout)
2022-03-28 19:01:54 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (130622ms till timeout)
2022-03-28 19:01:55 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (974609ms till timeout)
2022-03-28 19:01:55 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (128939ms till timeout)
2022-03-28 19:01:55 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929}
2022-03-28 19:01:55 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (970808ms till timeout)
2022-03-28 19:01:55 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 19:01:55 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 19:01:55 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace http-bridge-scram-sha-st removal not ready, will try again in 1000 ms (463984ms till timeout)
2022-03-28 19:01:55 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929, my-cluster-08515847-cruise-control-dcd7c895b-4pkw7=37920413-f627-4a97-919c-4ff0c818c13a}
2022-03-28 19:01:55 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929}
2022-03-28 19:01:55 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Deployment my-cluster-08515847-cruise-control rolling update in namespace:namespace-4 not ready, will try again in 5000 ms (552206ms till timeout)
2022-03-28 19:01:55 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (158307ms till timeout)
2022-03-28 19:01:55 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:01:55 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (685793ms till timeout)
2022-03-28 19:01:55 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (152271ms till timeout)
2022-03-28 19:01:55 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:01:55 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (86290ms till timeout)
2022-03-28 19:01:56 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:01:56 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:01:56 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-e6343cec-kafka-2 hasn't rolled
2022-03-28 19:01:56 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02, my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a} pods didn't roll. Remaining seconds for stability: 31
2022-03-28 19:01:56 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (278016ms till timeout)
2022-03-28 19:01:56 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:01:56 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2328633ms till timeout)
2022-03-28 19:01:56 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (129440ms till timeout)
2022-03-28 19:01:56 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 19:01:56 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (973422ms till timeout)
2022-03-28 19:01:56 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (127755ms till timeout)
2022-03-28 19:01:56 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (969620ms till timeout)
2022-03-28 19:01:56 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (157210ms till timeout)
2022-03-28 19:01:56 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (684696ms till timeout)
2022-03-28 19:01:56 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:01:57 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (151080ms till timeout)
2022-03-28 19:01:57 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:01:57 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (85099ms till timeout)
2022-03-28 19:01:57 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:01:57 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:01:57 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-e6343cec-kafka-2 hasn't rolled
2022-03-28 19:01:57 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02, my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a} pods didn't roll. Remaining seconds for stability: 30
2022-03-28 19:01:57 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (276823ms till timeout)
2022-03-28 19:01:57 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:01:57 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2327443ms till timeout)
2022-03-28 19:01:57 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (128251ms till timeout)
2022-03-28 19:01:57 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (972325ms till timeout)
2022-03-28 19:01:57 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (968518ms till timeout)
2022-03-28 19:01:57 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (126555ms till timeout)
2022-03-28 19:01:57 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (156114ms till timeout)
2022-03-28 19:01:57 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-5dfd606f-kafka-0=a766ef9e-2663-4c21-ad83-6b7d9083122c, my-cluster-5dfd606f-kafka-1=57938984-c88e-45c5-b973-9f953a8e782c, my-cluster-5dfd606f-kafka-2=044285fe-7f53-4c1c-84e1-c51aec79b2ce}
2022-03-28 19:01:57 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (683598ms till timeout)
2022-03-28 19:01:58 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-5dfd606f-kafka-0=d3be017c-9e19-4ee7-b0fc-58e8694c30b9, my-cluster-5dfd606f-kafka-1=57938984-c88e-45c5-b973-9f953a8e782c, my-cluster-5dfd606f-kafka-2=044285fe-7f53-4c1c-84e1-c51aec79b2ce}
2022-03-28 19:01:58 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-5dfd606f-kafka-0=d3be017c-9e19-4ee7-b0fc-58e8694c30b9, my-cluster-5dfd606f-kafka-1=57938984-c88e-45c5-b973-9f953a8e782c, my-cluster-5dfd606f-kafka-2=044285fe-7f53-4c1c-84e1-c51aec79b2ce}
2022-03-28 19:01:58 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:50] At least my-cluster-5dfd606f-kafka-1 hasn't rolled
2022-03-28 19:01:58 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] component with name my-cluster-5dfd606f-kafka rolling update not ready, will try again in 5000 ms (1769017ms till timeout)
2022-03-28 19:01:58 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:01:58 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:01:58 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (83993ms till timeout)
2022-03-28 19:01:58 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (149784ms till timeout)
2022-03-28 19:01:58 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:01:58 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:01:58 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-e6343cec-kafka-2 hasn't rolled
2022-03-28 19:01:58 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02, my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a} pods didn't roll. Remaining seconds for stability: 29
2022-03-28 19:01:58 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (275722ms till timeout)
2022-03-28 19:01:58 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:01:58 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2326342ms till timeout)
2022-03-28 19:01:58 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (127149ms till timeout)
2022-03-28 19:01:58 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-1 -o yaml
2022-03-28 19:01:58 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Return code: 0
2022-03-28 19:01:58 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (444585ms till timeout)
2022-03-28 19:01:58 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (971228ms till timeout)
2022-03-28 19:01:58 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (967422ms till timeout)
2022-03-28 19:01:58 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (125365ms till timeout)
2022-03-28 19:01:58 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (155014ms till timeout)
2022-03-28 19:01:59 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (682500ms till timeout)
2022-03-28 19:01:59 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:01:59 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (82891ms till timeout)
2022-03-28 19:01:59 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:01:59 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:01:59 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:01:59 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-e6343cec-kafka-2 hasn't rolled
2022-03-28 19:01:59 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02, my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a} pods didn't roll. Remaining seconds for stability: 28
2022-03-28 19:01:59 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (274619ms till timeout)
2022-03-28 19:01:59 [ForkJoinPool-1-worker-21] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-1 -o yaml
2022-03-28 19:01:59 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:01:59 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2325237ms till timeout)
2022-03-28 19:01:59 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (126045ms till timeout)
2022-03-28 19:01:59 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (148489ms till timeout)
2022-03-28 19:01:59 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (970131ms till timeout)
2022-03-28 19:01:59 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (966326ms till timeout)
2022-03-28 19:01:59 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (124263ms till timeout)
2022-03-28 19:02:00 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-1 -o yaml
2022-03-28 19:02:00 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Return code: 0
2022-03-28 19:02:00 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (443058ms till timeout)
2022-03-28 19:02:00 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (153905ms till timeout)
2022-03-28 19:02:00 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (681391ms till timeout)
2022-03-28 19:02:00 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:00 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (81791ms till timeout)
2022-03-28 19:02:00 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:00 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:00 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-e6343cec-kafka-2 hasn't rolled
2022-03-28 19:02:00 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02, my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a} pods didn't roll. Remaining seconds for stability: 27
2022-03-28 19:02:00 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (273520ms till timeout)
2022-03-28 19:02:00 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929}
2022-03-28 19:02:00 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:02:00 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2324135ms till timeout)
2022-03-28 19:02:00 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (124941ms till timeout)
2022-03-28 19:02:00 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:02:00 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (147290ms till timeout)
2022-03-28 19:02:00 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (968963ms till timeout)
2022-03-28 19:02:00 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929, my-cluster-08515847-cruise-control-dcd7c895b-4pkw7=37920413-f627-4a97-919c-4ff0c818c13a}
2022-03-28 19:02:00 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929}
2022-03-28 19:02:00 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Deployment my-cluster-08515847-cruise-control rolling update in namespace:namespace-4 not ready, will try again in 5000 ms (546844ms till timeout)
2022-03-28 19:02:01 [ForkJoinPool-1-worker-21] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-1 -o yaml
2022-03-28 19:02:01 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (965159ms till timeout)
2022-03-28 19:02:01 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (123102ms till timeout)
2022-03-28 19:02:01 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (152752ms till timeout)
2022-03-28 19:02:01 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (680238ms till timeout)
2022-03-28 19:02:01 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:01 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (80690ms till timeout)
2022-03-28 19:02:01 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:01 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:01 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-e6343cec-kafka-2 hasn't rolled
2022-03-28 19:02:01 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02, my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a} pods didn't roll. Remaining seconds for stability: 26
2022-03-28 19:02:01 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (272418ms till timeout)
2022-03-28 19:02:01 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:02:01 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2323035ms till timeout)
2022-03-28 19:02:01 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (123746ms till timeout)
2022-03-28 19:02:01 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 19:02:01 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 19:02:01 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace http-bridge-scram-sha-st removal not ready, will try again in 1000 ms (457474ms till timeout)
2022-03-28 19:02:01 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:02:02 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (967865ms till timeout)
2022-03-28 19:02:02 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (146000ms till timeout)
2022-03-28 19:02:02 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (964059ms till timeout)
2022-03-28 19:02:02 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (121992ms till timeout)
2022-03-28 19:02:02 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (151646ms till timeout)
2022-03-28 19:02:02 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (679132ms till timeout)
2022-03-28 19:02:02 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:02 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (79590ms till timeout)
2022-03-28 19:02:02 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:02 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:02 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-e6343cec-kafka-2 hasn't rolled
2022-03-28 19:02:02 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02, my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a} pods didn't roll. Remaining seconds for stability: 25
2022-03-28 19:02:02 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (271319ms till timeout)
2022-03-28 19:02:02 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:02:02 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2321936ms till timeout)
2022-03-28 19:02:02 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 19:02:02 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (122646ms till timeout)
2022-03-28 19:02:03 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-5dfd606f-kafka-0=a766ef9e-2663-4c21-ad83-6b7d9083122c, my-cluster-5dfd606f-kafka-1=57938984-c88e-45c5-b973-9f953a8e782c, my-cluster-5dfd606f-kafka-2=044285fe-7f53-4c1c-84e1-c51aec79b2ce}
2022-03-28 19:02:03 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (966768ms till timeout)
2022-03-28 19:02:03 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:02:03 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-5dfd606f-kafka-0=d3be017c-9e19-4ee7-b0fc-58e8694c30b9, my-cluster-5dfd606f-kafka-1=57938984-c88e-45c5-b973-9f953a8e782c, my-cluster-5dfd606f-kafka-2=044285fe-7f53-4c1c-84e1-c51aec79b2ce}
2022-03-28 19:02:03 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-5dfd606f-kafka-0=d3be017c-9e19-4ee7-b0fc-58e8694c30b9, my-cluster-5dfd606f-kafka-1=57938984-c88e-45c5-b973-9f953a8e782c, my-cluster-5dfd606f-kafka-2=044285fe-7f53-4c1c-84e1-c51aec79b2ce}
2022-03-28 19:02:03 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:50] At least my-cluster-5dfd606f-kafka-1 hasn't rolled
2022-03-28 19:02:03 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] component with name my-cluster-5dfd606f-kafka rolling update not ready, will try again in 5000 ms (1763846ms till timeout)
2022-03-28 19:02:03 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (962868ms till timeout)
2022-03-28 19:02:03 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (144713ms till timeout)
2022-03-28 19:02:03 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (120805ms till timeout)
2022-03-28 19:02:03 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (150463ms till timeout)
2022-03-28 19:02:03 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (677948ms till timeout)
2022-03-28 19:02:03 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:03 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (78487ms till timeout)
2022-03-28 19:02:03 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:03 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:03 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-e6343cec-kafka-2 hasn't rolled
2022-03-28 19:02:03 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02, my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a} pods didn't roll. Remaining seconds for stability: 24
2022-03-28 19:02:03 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (270217ms till timeout)
2022-03-28 19:02:03 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:02:03 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2320833ms till timeout)
2022-03-28 19:02:04 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (121452ms till timeout)
2022-03-28 19:02:04 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (965672ms till timeout)
2022-03-28 19:02:04 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (961764ms till timeout)
2022-03-28 19:02:04 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:02:04 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:241] Delete of KafkaBridge http-bridge-tls-cluster-name in namespace http-bridge-tls-st
2022-03-28 19:02:04 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (149358ms till timeout)
2022-03-28 19:02:04 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (119614ms till timeout)
2022-03-28 19:02:04 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (143416ms till timeout)
2022-03-28 19:02:04 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (676841ms till timeout)
2022-03-28 19:02:04 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaBridge:http-bridge-tls-cluster-name
2022-03-28 19:02:04 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:241] Delete of KafkaUser my-user-1679479847-1226530320 in namespace http-bridge-tls-st
2022-03-28 19:02:04 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:04 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (77332ms till timeout)
2022-03-28 19:02:05 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:05 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:05 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-e6343cec-kafka-2 hasn't rolled
2022-03-28 19:02:05 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02, my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a} pods didn't roll. Remaining seconds for stability: 23
2022-03-28 19:02:05 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (269061ms till timeout)
2022-03-28 19:02:05 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-1679479847-1226530320
2022-03-28 19:02:05 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:02:05 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2319680ms till timeout)
2022-03-28 19:02:05 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:241] Delete of Kafka http-bridge-tls-cluster-name in namespace http-bridge-tls-st
2022-03-28 19:02:05 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (120296ms till timeout)
2022-03-28 19:02:05 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (964508ms till timeout)
2022-03-28 19:02:05 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:http-bridge-tls-cluster-name
2022-03-28 19:02:05 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:http-bridge-tls-cluster-name not ready, will try again in 10000 ms (839890ms till timeout)
2022-03-28 19:02:05 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (960576ms till timeout)
2022-03-28 19:02:05 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (148246ms till timeout)
2022-03-28 19:02:05 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:02:05 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (675724ms till timeout)
2022-03-28 19:02:05 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (118379ms till timeout)
2022-03-28 19:02:05 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (142206ms till timeout)
2022-03-28 19:02:05 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929}
2022-03-28 19:02:06 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:06 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (76224ms till timeout)
2022-03-28 19:02:06 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:06 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:06 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-e6343cec-kafka-2 hasn't rolled
2022-03-28 19:02:06 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02, my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a} pods didn't roll. Remaining seconds for stability: 22
2022-03-28 19:02:06 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (267953ms till timeout)
2022-03-28 19:02:06 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:02:06 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2318570ms till timeout)
2022-03-28 19:02:06 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929, my-cluster-08515847-cruise-control-dcd7c895b-4pkw7=37920413-f627-4a97-919c-4ff0c818c13a}
2022-03-28 19:02:06 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929}
2022-03-28 19:02:06 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Deployment my-cluster-08515847-cruise-control rolling update in namespace:namespace-4 not ready, will try again in 5000 ms (541471ms till timeout)
2022-03-28 19:02:06 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (119184ms till timeout)
2022-03-28 19:02:06 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-9 get Namespace namespace-1 -o yaml
2022-03-28 19:02:06 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Return code: 1
2022-03-28 19:02:06 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 19:02:06 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Error from server (NotFound): namespaces "namespace-1" not found
2022-03-28 19:02:06 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] ======STDERR END======
2022-03-28 19:02:06 [ForkJoinPool-1-worker-21] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[http-bridge-scram-sha-st], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[namespace-8], io.strimzi.test.logs.CollectorElement@c4c0ea0=[], io.strimzi.test.logs.CollectorElement@aec142ef=[namespace-5], io.strimzi.test.logs.CollectorElement@24b97ba9=[namespace-7], io.strimzi.test.logs.CollectorElement@f851b6c3=[namespace-4], io.strimzi.test.logs.CollectorElement@3881d5f2=[namespace-3], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@5c7379cb=[namespace-9], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[namespace-0], io.strimzi.test.logs.CollectorElement@f4405b3b=[namespace-2], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 19:02:06 [ForkJoinPool-1-worker-21] DEBUG [SuiteThreadController:267] testConfigurationReflection - Notifies waiting test cases:[testPauseReconciliationInKafkaRebalanceAndTopic, testPauseReconciliationInKafkaAndKafkaConnectWithConnector, testCapacityFile, testConfigurationFileIsCreated, testConfigurationPerformanceOptions, testConfigurationReflection, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testDeployAndUnDeployCruiseControl, testTlsExternalUserWithQuotas, testCreatingUsersWithSecretPrefix, testThrottlingQuotasCreateTopic, testThrottlingQuotasCreateAlterPartitions, testKafkaAdminTopicOperations, testUserTemplate, testUpdateUser, testTlsExternalUser, testSendSimpleMessageTls, testReceiveSimpleMessageTlsScramSha, testThrottlingQuotasDeleteTopic, testReceiveSimpleMessageTls, testTopicModificationOfReplicationFactor, testSendingMessagesToNonExistingTopic] to and randomly select one to start execution
2022-03-28 19:02:06 [ForkJoinPool-1-worker-21] DEBUG [SuiteThreadController:93] [cruisecontrol.CruiseControlConfigurationST] - Removing parallel test: testConfigurationReflection
2022-03-28 19:02:06 [ForkJoinPool-1-worker-21] DEBUG [SuiteThreadController:97] [cruisecontrol.CruiseControlConfigurationST] - Parallel test count: 13
2022-03-28 19:02:06 [ForkJoinPool-1-worker-21] INFO  [TestSeparator:29] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testConfigurationReflection-FINISHED
2022-03-28 19:02:06 [ForkJoinPool-1-worker-21] INFO  [TestSeparator:30] ############################################################################
2022-03-28 19:02:06 [ForkJoinPool-1-worker-21] INFO  [TestSeparator:23] ############################################################################
2022-03-28 19:02:06 [ForkJoinPool-1-worker-21] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.topic.TopicST.testMoreReplicasThanAvailableBrokers-STARTED
2022-03-28 19:02:06 [ForkJoinPool-1-worker-21] DEBUG [AbstractST:658] ============================================================================
2022-03-28 19:02:06 [ForkJoinPool-1-worker-21] DEBUG [AbstractST:659] [operators.topic.TopicST - Before Each] - Setup test case environment
2022-03-28 19:02:06 [ForkJoinPool-1-worker-21] DEBUG [SuiteThreadController:77] [operators.topic.TopicST] - Adding parallel test: testMoreReplicasThanAvailableBrokers
2022-03-28 19:02:06 [ForkJoinPool-1-worker-21] DEBUG [SuiteThreadController:81] [operators.topic.TopicST] - Parallel test count: 14
2022-03-28 19:02:06 [ForkJoinPool-1-worker-21] DEBUG [SuiteThreadController:230] testMoreReplicasThanAvailableBrokers test now can proceed its execution
2022-03-28 19:02:06 [ForkJoinPool-1-worker-21] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 19:02:06 [ForkJoinPool-1-worker-21] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testTopicModificationOfReplicationFactor=my-cluster-15f5f09f, testKafkaAdminTopicOperations=my-cluster-506fda27, testReceiveSimpleMessageTlsScramSha=my-cluster-abc5d34b, testConfigurationFileIsCreated=my-cluster-17fc585b, testThrottlingQuotasCreateAlterPartitions=my-cluster-ec1b34f8, testSendSimpleMessageTls=my-cluster-89673e85, testCreatingUsersWithSecretPrefix=my-cluster-f743fdfd, testUpdateUser=my-cluster-d582eb8c, testThrottlingQuotasDeleteTopic=my-cluster-836b31f8, testConfigurationReflection=my-cluster-188382d8, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-959c6ed5, testTlsExternalUserWithQuotas=my-cluster-100f5461, testDeployAndUnDeployCruiseControl=my-cluster-5dfd606f, testThrottlingQuotasCreateTopic=my-cluster-e9955717, testSendingMessagesToNonExistingTopic=my-cluster-a3c1c5ed, testCapacityFile=my-cluster-855b9105, testConfigurationPerformanceOptions=my-cluster-08515847, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-afe273c9, testReceiveSimpleMessageTls=my-cluster-634e4d65, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-e6343cec, testTlsExternalUser=my-cluster-45cf8ce4, testSendSimpleMessageTlsScramSha=my-cluster-561d4fae, testUserTemplate=my-cluster-927d5a1b, testMoreReplicasThanAvailableBrokers=my-cluster-22050d08}
2022-03-28 19:02:06 [ForkJoinPool-1-worker-21] TRACE [AbstractST:607] USERS_NAME_MAP: {testTopicModificationOfReplicationFactor=my-user-259690203-415132624, testKafkaAdminTopicOperations=my-user-1780746402-846362125, testReceiveSimpleMessageTlsScramSha=my-user-330562220-1155646423, testConfigurationFileIsCreated=my-user-355815510-1316268151, testThrottlingQuotasCreateAlterPartitions=my-user-333986137-1152182125, testSendSimpleMessageTls=my-user-1887866274-1013125243, testCreatingUsersWithSecretPrefix=my-user-607975993-1210920737, testUpdateUser=my-user-1297536277-778093599, testThrottlingQuotasDeleteTopic=my-user-1567019138-1355600812, testConfigurationReflection=my-user-1890969183-1754686912, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-2104248162-45187051, testTlsExternalUserWithQuotas=my-user-1909583883-1106194424, testDeployAndUnDeployCruiseControl=my-user-506268651-1309117986, testThrottlingQuotasCreateTopic=my-user-1876060546-1887905718, testSendingMessagesToNonExistingTopic=my-user-134768902-582007171, testCapacityFile=my-user-1460643829-103298436, testConfigurationPerformanceOptions=my-user-98362545-1476587236, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-user-8456079-650483351, testReceiveSimpleMessageTls=my-user-337002809-1497514615, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-2069267102-581538560, testTlsExternalUser=my-user-399414674-1203409092, testSendSimpleMessageTlsScramSha=my-user-1601209876-518796379, testUserTemplate=my-user-989633374-940717398, testMoreReplicasThanAvailableBrokers=my-user-713898339-1095128639}
2022-03-28 19:02:06 [ForkJoinPool-1-worker-21] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testTopicModificationOfReplicationFactor=my-topic-1480509436-578706559, testKafkaAdminTopicOperations=my-topic-481231182-953607496, testReceiveSimpleMessageTlsScramSha=my-topic-1027850608-734754781, testConfigurationFileIsCreated=my-topic-423850169-855699454, testThrottlingQuotasCreateAlterPartitions=my-topic-325891726-169328137, testSendSimpleMessageTls=my-topic-874607937-87907051, testCreatingUsersWithSecretPrefix=my-topic-1727421070-668192795, testUpdateUser=my-topic-1325563869-1324403074, testThrottlingQuotasDeleteTopic=my-topic-1674611395-2048879468, testConfigurationReflection=my-topic-1394577332-1936226438, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-634125597-1746493046, testTlsExternalUserWithQuotas=my-topic-1986059791-167638004, testDeployAndUnDeployCruiseControl=my-topic-465377127-800134460, testThrottlingQuotasCreateTopic=my-topic-1246597695-228473879, testSendingMessagesToNonExistingTopic=my-topic-1692964628-1348460540, testCapacityFile=my-topic-549312506-649708453, testConfigurationPerformanceOptions=my-topic-696588651-1198599652, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-topic-1687496771-142091227, testReceiveSimpleMessageTls=my-topic-829184226-1519275114, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-293775067-432059493, testTlsExternalUser=my-topic-707627084-257659651, testSendSimpleMessageTlsScramSha=my-topic-1586843669-332477842, testUserTemplate=my-topic-1542379258-2002911578, testMoreReplicasThanAvailableBrokers=my-topic-840747042-63255423}
2022-03-28 19:02:06 [ForkJoinPool-1-worker-21] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testTopicModificationOfReplicationFactor=my-cluster-15f5f09f-kafka-clients, testKafkaAdminTopicOperations=my-cluster-506fda27-kafka-clients, testReceiveSimpleMessageTlsScramSha=my-cluster-abc5d34b-kafka-clients, testConfigurationFileIsCreated=my-cluster-17fc585b-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-ec1b34f8-kafka-clients, testSendSimpleMessageTls=my-cluster-89673e85-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-f743fdfd-kafka-clients, testUpdateUser=my-cluster-d582eb8c-kafka-clients, testThrottlingQuotasDeleteTopic=my-cluster-836b31f8-kafka-clients, testConfigurationReflection=my-cluster-188382d8-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-959c6ed5-kafka-clients, testTlsExternalUserWithQuotas=my-cluster-100f5461-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-5dfd606f-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-e9955717-kafka-clients, testSendingMessagesToNonExistingTopic=my-cluster-a3c1c5ed-kafka-clients, testCapacityFile=my-cluster-855b9105-kafka-clients, testConfigurationPerformanceOptions=my-cluster-08515847-kafka-clients, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-afe273c9-kafka-clients, testReceiveSimpleMessageTls=my-cluster-634e4d65-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-e6343cec-kafka-clients, testTlsExternalUser=my-cluster-45cf8ce4-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-561d4fae-kafka-clients, testUserTemplate=my-cluster-927d5a1b-kafka-clients, testMoreReplicasThanAvailableBrokers=my-cluster-22050d08-kafka-clients}
2022-03-28 19:02:06 [ForkJoinPool-1-worker-21] INFO  [ResourceManager:155] Create/Update KafkaTopic my-topic-840747042-63255423 in namespace topic-st
2022-03-28 19:02:06 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (963396ms till timeout)
2022-03-28 19:02:06 [ForkJoinPool-1-worker-21] INFO  [TopicST:461] Checking in KafkaTopic CR that topic my-topic-840747042-63255423 exists
2022-03-28 19:02:06 [ForkJoinPool-1-worker-21] INFO  [TopicST:456] Checking topic my-topic-840747042-63255423 in Kafka
2022-03-28 19:02:06 [ForkJoinPool-1-worker-21] TRACE [Exec:248] Running command - oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-03-28 19:02:06 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (959480ms till timeout)
2022-03-28 19:02:06 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (147149ms till timeout)
2022-03-28 19:02:06 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (674627ms till timeout)
2022-03-28 19:02:07 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:02:07 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (117205ms till timeout)
2022-03-28 19:02:07 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:07 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (75123ms till timeout)
2022-03-28 19:02:07 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (140912ms till timeout)
2022-03-28 19:02:07 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:07 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:07 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-e6343cec-kafka-2 hasn't rolled
2022-03-28 19:02:07 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02, my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a} pods didn't roll. Remaining seconds for stability: 21
2022-03-28 19:02:07 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (266852ms till timeout)
2022-03-28 19:02:07 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:02:07 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2317470ms till timeout)
2022-03-28 19:02:07 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (118080ms till timeout)
2022-03-28 19:02:07 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (962293ms till timeout)
2022-03-28 19:02:07 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (958382ms till timeout)
2022-03-28 19:02:07 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (146053ms till timeout)
2022-03-28 19:02:08 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-45cf8ce4 will have desired state: Ready not ready, will try again in 1000 ms (673531ms till timeout)
2022-03-28 19:02:08 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (116105ms till timeout)
2022-03-28 19:02:08 [ForkJoinPool-1-worker-55] INFO  [InternalKafkaClient:160] Consumer finished correctly: true
2022-03-28 19:02:08 [ForkJoinPool-1-worker-55] INFO  [InternalKafkaClient:163] Consumer consumed 100 messages
2022-03-28 19:02:08 [ForkJoinPool-1-worker-55] INFO  [TopicST:341] Checking if my-topic-1692964628-1348460540 is on topic list
2022-03-28 19:02:08 [ForkJoinPool-1-worker-55] INFO  [TopicST:456] Checking topic my-topic-1692964628-1348460540 in Kafka
2022-03-28 19:02:08 [ForkJoinPool-1-worker-55] TRACE [Exec:248] Running command - oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-03-28 19:02:08 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:08 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (74020ms till timeout)
2022-03-28 19:02:08 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-5dfd606f-kafka-0=a766ef9e-2663-4c21-ad83-6b7d9083122c, my-cluster-5dfd606f-kafka-1=57938984-c88e-45c5-b973-9f953a8e782c, my-cluster-5dfd606f-kafka-2=044285fe-7f53-4c1c-84e1-c51aec79b2ce}
2022-03-28 19:02:08 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:02:08 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:08 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:08 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-e6343cec-kafka-2 hasn't rolled
2022-03-28 19:02:08 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02, my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a} pods didn't roll. Remaining seconds for stability: 20
2022-03-28 19:02:08 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (265714ms till timeout)
2022-03-28 19:02:08 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:02:08 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2316357ms till timeout)
2022-03-28 19:02:08 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-5dfd606f-kafka-0=d3be017c-9e19-4ee7-b0fc-58e8694c30b9, my-cluster-5dfd606f-kafka-1=57938984-c88e-45c5-b973-9f953a8e782c, my-cluster-5dfd606f-kafka-2=044285fe-7f53-4c1c-84e1-c51aec79b2ce}
2022-03-28 19:02:08 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-5dfd606f-kafka-0=d3be017c-9e19-4ee7-b0fc-58e8694c30b9, my-cluster-5dfd606f-kafka-1=57938984-c88e-45c5-b973-9f953a8e782c, my-cluster-5dfd606f-kafka-2=044285fe-7f53-4c1c-84e1-c51aec79b2ce}
2022-03-28 19:02:08 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:50] At least my-cluster-5dfd606f-kafka-1 hasn't rolled
2022-03-28 19:02:08 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] component with name my-cluster-5dfd606f-kafka rolling update not ready, will try again in 5000 ms (1758647ms till timeout)
2022-03-28 19:02:08 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 19:02:08 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 19:02:08 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace http-bridge-scram-sha-st removal not ready, will try again in 1000 ms (450883ms till timeout)
2022-03-28 19:02:08 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (139601ms till timeout)
2022-03-28 19:02:08 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (116954ms till timeout)
2022-03-28 19:02:08 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (961165ms till timeout)
2022-03-28 19:02:08 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (957286ms till timeout)
2022-03-28 19:02:09 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (144955ms till timeout)
2022-03-28 19:02:09 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:444] Kafka: my-cluster-45cf8ce4 is in desired state: Ready
2022-03-28 19:02:09 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:155] Create/Update KafkaUser my-user-399414674-1203409092 in namespace namespace-9
2022-03-28 19:02:09 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:164] Using Namespace: namespace-9
2022-03-28 19:02:09 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (115003ms till timeout)
2022-03-28 19:02:09 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (72912ms till timeout)
2022-03-28 19:02:09 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-399414674-1203409092
2022-03-28 19:02:09 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:09 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:433] Wait for KafkaUser: my-user-399414674-1203409092 will have desired state: Ready
2022-03-28 19:02:09 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for KafkaUser: my-user-399414674-1203409092 will have desired state: Ready
2022-03-28 19:02:09 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 19:02:09 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:02:09 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2315256ms till timeout)
2022-03-28 19:02:09 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:09 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:09 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-e6343cec-kafka-2 hasn't rolled
2022-03-28 19:02:09 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02, my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a} pods didn't roll. Remaining seconds for stability: 19
2022-03-28 19:02:09 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (264541ms till timeout)
2022-03-28 19:02:09 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] KafkaUser: my-user-399414674-1203409092 will have desired state: Ready not ready, will try again in 1000 ms (179811ms till timeout)
2022-03-28 19:02:09 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:02:09 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (115778ms till timeout)
2022-03-28 19:02:09 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (138221ms till timeout)
2022-03-28 19:02:09 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (959991ms till timeout)
2022-03-28 19:02:09 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-9 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 19:02:09 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 1
2022-03-28 19:02:09 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 19:02:09 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Error from server (NotFound): namespaces "http-bridge-scram-sha-st" not found
2022-03-28 19:02:09 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] ======STDERR END======
2022-03-28 19:02:09 [ForkJoinPool-1-worker-5] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[namespace-8], io.strimzi.test.logs.CollectorElement@c4c0ea0=[], io.strimzi.test.logs.CollectorElement@aec142ef=[namespace-5], io.strimzi.test.logs.CollectorElement@24b97ba9=[namespace-7], io.strimzi.test.logs.CollectorElement@f851b6c3=[namespace-4], io.strimzi.test.logs.CollectorElement@3881d5f2=[namespace-3], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@5c7379cb=[namespace-9], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[namespace-0], io.strimzi.test.logs.CollectorElement@f4405b3b=[namespace-2], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 19:02:09 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:254] HttpBridgeScramShaST - Notifies waiting test suites:[TopicST, ThrottlingQuotaST, UserST, HttpBridgeTlsST, HttpBridgeScramShaST, ReconciliationST, CruiseControlConfigurationST] to and randomly select one to start execution
2022-03-28 19:02:09 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:85] [bridge.HttpBridgeScramShaST] - Removing parallel suite: HttpBridgeScramShaST
2022-03-28 19:02:09 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:89] [bridge.HttpBridgeScramShaST] - Parallel suites count: 6
[INFO] Tests run: 10, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 425.491 s - in io.strimzi.systemtest.bridge.HttpBridgeScramShaST
2022-03-28 19:02:09 [ForkJoinPool-1-worker-5] INFO  [TestSeparator:23] ############################################################################
2022-03-28 19:02:09 [ForkJoinPool-1-worker-5] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.topic.TopicST.testCreateTopicViaKafka-STARTED
2022-03-28 19:02:09 [ForkJoinPool-1-worker-5] DEBUG [AbstractST:658] ============================================================================
2022-03-28 19:02:09 [ForkJoinPool-1-worker-5] DEBUG [AbstractST:659] [operators.topic.TopicST - Before Each] - Setup test case environment
2022-03-28 19:02:09 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:77] [operators.topic.TopicST] - Adding parallel test: testCreateTopicViaKafka
2022-03-28 19:02:09 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:81] [operators.topic.TopicST] - Parallel test count: 15
2022-03-28 19:02:09 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:230] testCreateTopicViaKafka test now can proceed its execution
2022-03-28 19:02:09 [ForkJoinPool-1-worker-5] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 19:02:09 [ForkJoinPool-1-worker-5] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-abc5d34b, testThrottlingQuotasCreateAlterPartitions=my-cluster-ec1b34f8, testSendSimpleMessageTls=my-cluster-89673e85, testConfigurationReflection=my-cluster-188382d8, testDeployAndUnDeployCruiseControl=my-cluster-5dfd606f, testCapacityFile=my-cluster-855b9105, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-afe273c9, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-e6343cec, testTlsExternalUser=my-cluster-45cf8ce4, testSendSimpleMessageTlsScramSha=my-cluster-561d4fae, testTopicModificationOfReplicationFactor=my-cluster-15f5f09f, testKafkaAdminTopicOperations=my-cluster-506fda27, testConfigurationFileIsCreated=my-cluster-17fc585b, testCreateTopicViaKafka=my-cluster-75679697, testCreatingUsersWithSecretPrefix=my-cluster-f743fdfd, testUpdateUser=my-cluster-d582eb8c, testThrottlingQuotasDeleteTopic=my-cluster-836b31f8, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-959c6ed5, testTlsExternalUserWithQuotas=my-cluster-100f5461, testThrottlingQuotasCreateTopic=my-cluster-e9955717, testSendingMessagesToNonExistingTopic=my-cluster-a3c1c5ed, testConfigurationPerformanceOptions=my-cluster-08515847, testReceiveSimpleMessageTls=my-cluster-634e4d65, testUserTemplate=my-cluster-927d5a1b, testMoreReplicasThanAvailableBrokers=my-cluster-22050d08}
2022-03-28 19:02:09 [ForkJoinPool-1-worker-5] TRACE [AbstractST:607] USERS_NAME_MAP: {testReceiveSimpleMessageTlsScramSha=my-user-330562220-1155646423, testThrottlingQuotasCreateAlterPartitions=my-user-333986137-1152182125, testSendSimpleMessageTls=my-user-1887866274-1013125243, testConfigurationReflection=my-user-1890969183-1754686912, testDeployAndUnDeployCruiseControl=my-user-506268651-1309117986, testCapacityFile=my-user-1460643829-103298436, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-user-8456079-650483351, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-2069267102-581538560, testTlsExternalUser=my-user-399414674-1203409092, testSendSimpleMessageTlsScramSha=my-user-1601209876-518796379, testTopicModificationOfReplicationFactor=my-user-259690203-415132624, testKafkaAdminTopicOperations=my-user-1780746402-846362125, testConfigurationFileIsCreated=my-user-355815510-1316268151, testCreateTopicViaKafka=my-user-541976280-277947468, testCreatingUsersWithSecretPrefix=my-user-607975993-1210920737, testUpdateUser=my-user-1297536277-778093599, testThrottlingQuotasDeleteTopic=my-user-1567019138-1355600812, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-2104248162-45187051, testTlsExternalUserWithQuotas=my-user-1909583883-1106194424, testThrottlingQuotasCreateTopic=my-user-1876060546-1887905718, testSendingMessagesToNonExistingTopic=my-user-134768902-582007171, testConfigurationPerformanceOptions=my-user-98362545-1476587236, testReceiveSimpleMessageTls=my-user-337002809-1497514615, testUserTemplate=my-user-989633374-940717398, testMoreReplicasThanAvailableBrokers=my-user-713898339-1095128639}
2022-03-28 19:02:09 [ForkJoinPool-1-worker-5] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-topic-1027850608-734754781, testThrottlingQuotasCreateAlterPartitions=my-topic-325891726-169328137, testSendSimpleMessageTls=my-topic-874607937-87907051, testConfigurationReflection=my-topic-1394577332-1936226438, testDeployAndUnDeployCruiseControl=my-topic-465377127-800134460, testCapacityFile=my-topic-549312506-649708453, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-topic-1687496771-142091227, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-293775067-432059493, testTlsExternalUser=my-topic-707627084-257659651, testSendSimpleMessageTlsScramSha=my-topic-1586843669-332477842, testTopicModificationOfReplicationFactor=my-topic-1480509436-578706559, testKafkaAdminTopicOperations=my-topic-481231182-953607496, testConfigurationFileIsCreated=my-topic-423850169-855699454, testCreateTopicViaKafka=my-topic-1973235938-1730719386, testCreatingUsersWithSecretPrefix=my-topic-1727421070-668192795, testUpdateUser=my-topic-1325563869-1324403074, testThrottlingQuotasDeleteTopic=my-topic-1674611395-2048879468, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-634125597-1746493046, testTlsExternalUserWithQuotas=my-topic-1986059791-167638004, testThrottlingQuotasCreateTopic=my-topic-1246597695-228473879, testSendingMessagesToNonExistingTopic=my-topic-1692964628-1348460540, testConfigurationPerformanceOptions=my-topic-696588651-1198599652, testReceiveSimpleMessageTls=my-topic-829184226-1519275114, testUserTemplate=my-topic-1542379258-2002911578, testMoreReplicasThanAvailableBrokers=my-topic-840747042-63255423}
2022-03-28 19:02:09 [ForkJoinPool-1-worker-5] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-abc5d34b-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-ec1b34f8-kafka-clients, testSendSimpleMessageTls=my-cluster-89673e85-kafka-clients, testConfigurationReflection=my-cluster-188382d8-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-5dfd606f-kafka-clients, testCapacityFile=my-cluster-855b9105-kafka-clients, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-afe273c9-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-e6343cec-kafka-clients, testTlsExternalUser=my-cluster-45cf8ce4-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-561d4fae-kafka-clients, testTopicModificationOfReplicationFactor=my-cluster-15f5f09f-kafka-clients, testKafkaAdminTopicOperations=my-cluster-506fda27-kafka-clients, testConfigurationFileIsCreated=my-cluster-17fc585b-kafka-clients, testCreateTopicViaKafka=my-cluster-75679697-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-f743fdfd-kafka-clients, testUpdateUser=my-cluster-d582eb8c-kafka-clients, testThrottlingQuotasDeleteTopic=my-cluster-836b31f8-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-959c6ed5-kafka-clients, testTlsExternalUserWithQuotas=my-cluster-100f5461-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-e9955717-kafka-clients, testSendingMessagesToNonExistingTopic=my-cluster-a3c1c5ed-kafka-clients, testConfigurationPerformanceOptions=my-cluster-08515847-kafka-clients, testReceiveSimpleMessageTls=my-cluster-634e4d65-kafka-clients, testUserTemplate=my-cluster-927d5a1b-kafka-clients, testMoreReplicasThanAvailableBrokers=my-cluster-22050d08-kafka-clients}
2022-03-28 19:02:09 [ForkJoinPool-1-worker-5] DEBUG [TopicST:113] Creating topic my-topic-1973235938-1730719386 with 3 replicas and 3 partitions
2022-03-28 19:02:09 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --bootstrap-server localhost:9092 --create  --topic my-topic-1973235938-1730719386 --replication-factor 3 --partitions 3
2022-03-28 19:02:10 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (956183ms till timeout)
2022-03-28 19:02:10 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (143859ms till timeout)
2022-03-28 19:02:10 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (113896ms till timeout)
2022-03-28 19:02:10 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (71812ms till timeout)
2022-03-28 19:02:10 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:10 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:02:10 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2314157ms till timeout)
2022-03-28 19:02:10 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] KafkaUser: my-user-399414674-1203409092 will have desired state: Ready not ready, will try again in 1000 ms (178712ms till timeout)
2022-03-28 19:02:10 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:10 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:10 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-e6343cec-kafka-2 hasn't rolled
2022-03-28 19:02:10 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02, my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a} pods didn't roll. Remaining seconds for stability: 18
2022-03-28 19:02:10 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (263349ms till timeout)
2022-03-28 19:02:10 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (114630ms till timeout)
2022-03-28 19:02:11 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:02:11 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (958843ms till timeout)
2022-03-28 19:02:11 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (955039ms till timeout)
2022-03-28 19:02:11 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (136883ms till timeout)
2022-03-28 19:02:11 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (142728ms till timeout)
2022-03-28 19:02:11 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929}
2022-03-28 19:02:11 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (112728ms till timeout)
2022-03-28 19:02:11 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929, my-cluster-08515847-cruise-control-dcd7c895b-4pkw7=37920413-f627-4a97-919c-4ff0c818c13a}
2022-03-28 19:02:11 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929}
2022-03-28 19:02:11 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Deployment my-cluster-08515847-cruise-control rolling update in namespace:namespace-4 not ready, will try again in 5000 ms (536181ms till timeout)
2022-03-28 19:02:11 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (70646ms till timeout)
2022-03-28 19:02:11 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:11 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:02:11 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2313059ms till timeout)
2022-03-28 19:02:11 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:444] KafkaUser: my-user-399414674-1203409092 is in desired state: Ready
2022-03-28 19:02:11 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:11 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:11 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-e6343cec-kafka-2 hasn't rolled
2022-03-28 19:02:11 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02, my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a} pods didn't roll. Remaining seconds for stability: 17
2022-03-28 19:02:11 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (262155ms till timeout)
2022-03-28 19:02:12 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:433] Wait for KafkaUser: my-user-399414674-1203409092 will have desired state: Ready
2022-03-28 19:02:12 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for KafkaUser: my-user-399414674-1203409092 will have desired state: Ready
2022-03-28 19:02:12 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (113483ms till timeout)
2022-03-28 19:02:12 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (957696ms till timeout)
2022-03-28 19:02:12 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:444] KafkaUser: my-user-399414674-1203409092 is in desired state: Ready
2022-03-28 19:02:12 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (953890ms till timeout)
2022-03-28 19:02:12 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:02:12 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (141581ms till timeout)
2022-03-28 19:02:12 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (135640ms till timeout)
2022-03-28 19:02:12 [ForkJoinPool-1-worker-1] DEBUG [AbstractST:674] ============================================================================
2022-03-28 19:02:12 [ForkJoinPool-1-worker-1] DEBUG [AbstractST:675] [operators.user.UserST - After Each] - Clean up after test
2022-03-28 19:02:12 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:344] ############################################################################
2022-03-28 19:02:12 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:348] Delete all resources for testTlsExternalUser
2022-03-28 19:02:12 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:241] Delete of KafkaUser my-user-399414674-1203409092 in namespace namespace-9
2022-03-28 19:02:12 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (111547ms till timeout)
2022-03-28 19:02:12 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (69464ms till timeout)
2022-03-28 19:02:12 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-399414674-1203409092
2022-03-28 19:02:12 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:02:12 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2311909ms till timeout)
2022-03-28 19:02:12 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:12 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:241] Delete of Kafka my-cluster-45cf8ce4 in namespace namespace-9
2022-03-28 19:02:13 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:13 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:13 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-e6343cec-kafka-2 hasn't rolled
2022-03-28 19:02:13 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02, my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a} pods didn't roll. Remaining seconds for stability: 16
2022-03-28 19:02:13 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (261004ms till timeout)
2022-03-28 19:02:13 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-45cf8ce4
2022-03-28 19:02:13 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (112310ms till timeout)
2022-03-28 19:02:13 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:369] ############################################################################
2022-03-28 19:02:13 [ForkJoinPool-1-worker-1] INFO  [TestSuiteNamespaceManager:200] Deleting namespace:namespace-9 for test case:testTlsExternalUser
2022-03-28 19:02:13 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (956518ms till timeout)
2022-03-28 19:02:13 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-5dfd606f-kafka-0=a766ef9e-2663-4c21-ad83-6b7d9083122c, my-cluster-5dfd606f-kafka-1=57938984-c88e-45c5-b973-9f953a8e782c, my-cluster-5dfd606f-kafka-2=044285fe-7f53-4c1c-84e1-c51aec79b2ce}
2022-03-28 19:02:13 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (952711ms till timeout)
2022-03-28 19:02:13 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (140395ms till timeout)
2022-03-28 19:02:13 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-5dfd606f-kafka-0=d3be017c-9e19-4ee7-b0fc-58e8694c30b9, my-cluster-5dfd606f-kafka-1=57938984-c88e-45c5-b973-9f953a8e782c, my-cluster-5dfd606f-kafka-2=044285fe-7f53-4c1c-84e1-c51aec79b2ce}
2022-03-28 19:02:13 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-5dfd606f-kafka-0=d3be017c-9e19-4ee7-b0fc-58e8694c30b9, my-cluster-5dfd606f-kafka-1=57938984-c88e-45c5-b973-9f953a8e782c, my-cluster-5dfd606f-kafka-2=044285fe-7f53-4c1c-84e1-c51aec79b2ce}
2022-03-28 19:02:13 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:50] At least my-cluster-5dfd606f-kafka-1 hasn't rolled
2022-03-28 19:02:13 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] component with name my-cluster-5dfd606f-kafka rolling update not ready, will try again in 5000 ms (1753486ms till timeout)
2022-03-28 19:02:13 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Namespace namespace-9 removal
2022-03-28 19:02:13 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 19:02:13 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:02:13 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (134353ms till timeout)
2022-03-28 19:02:13 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (110354ms till timeout)
2022-03-28 19:02:13 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:02:13 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2310808ms till timeout)
2022-03-28 19:02:13 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (68268ms till timeout)
2022-03-28 19:02:14 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 19:02:14 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 19:02:14 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Namespace namespace-9 removal not ready, will try again in 1000 ms (479543ms till timeout)
2022-03-28 19:02:14 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:14 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:14 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:14 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-e6343cec-kafka-2 hasn't rolled
2022-03-28 19:02:14 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02, my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a} pods didn't roll. Remaining seconds for stability: 15
2022-03-28 19:02:14 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (259905ms till timeout)
2022-03-28 19:02:14 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (111202ms till timeout)
2022-03-28 19:02:14 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (955421ms till timeout)
2022-03-28 19:02:14 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (951615ms till timeout)
2022-03-28 19:02:14 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (139297ms till timeout)
2022-03-28 19:02:14 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:02:14 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (133161ms till timeout)
2022-03-28 19:02:15 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 19:02:15 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (109162ms till timeout)
2022-03-28 19:02:15 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:02:15 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2309621ms till timeout)
2022-03-28 19:02:15 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:15 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (67075ms till timeout)
2022-03-28 19:02:15 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:15 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:15 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-e6343cec-kafka-2 hasn't rolled
2022-03-28 19:02:15 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02, my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a} pods didn't roll. Remaining seconds for stability: 14
2022-03-28 19:02:15 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (258807ms till timeout)
2022-03-28 19:02:15 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 19:02:15 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 19:02:15 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Namespace namespace-9 removal not ready, will try again in 1000 ms (478101ms till timeout)
2022-03-28 19:02:15 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (110102ms till timeout)
2022-03-28 19:02:15 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (954316ms till timeout)
2022-03-28 19:02:15 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (950511ms till timeout)
2022-03-28 19:02:15 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:369] ############################################################################
2022-03-28 19:02:15 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (138200ms till timeout)
2022-03-28 19:02:15 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for Namespace http-bridge-tls-st removal
2022-03-28 19:02:15 [ForkJoinPool-1-worker-23] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace http-bridge-tls-st -o yaml
2022-03-28 19:02:16 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:02:16 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (131967ms till timeout)
2022-03-28 19:02:16 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:16 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (107966ms till timeout)
2022-03-28 19:02:16 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:02:16 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2308423ms till timeout)
2022-03-28 19:02:16 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (65883ms till timeout)
2022-03-28 19:02:16 [ForkJoinPool-1-worker-23] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace http-bridge-tls-st -o yaml
2022-03-28 19:02:16 [ForkJoinPool-1-worker-23] DEBUG [Exec:419] Return code: 0
2022-03-28 19:02:16 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (479476ms till timeout)
2022-03-28 19:02:16 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:16 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:16 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-e6343cec-kafka-2 hasn't rolled
2022-03-28 19:02:16 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02, my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a} pods didn't roll. Remaining seconds for stability: 13
2022-03-28 19:02:16 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (257615ms till timeout)
2022-03-28 19:02:16 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 19:02:16 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (109001ms till timeout)
2022-03-28 19:02:16 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929}
2022-03-28 19:02:16 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (953214ms till timeout)
2022-03-28 19:02:16 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (949408ms till timeout)
2022-03-28 19:02:16 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (137099ms till timeout)
2022-03-28 19:02:16 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 19:02:16 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 19:02:16 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Namespace namespace-9 removal not ready, will try again in 1000 ms (476643ms till timeout)
2022-03-28 19:02:16 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929, my-cluster-08515847-cruise-control-dcd7c895b-4pkw7=37920413-f627-4a97-919c-4ff0c818c13a}
2022-03-28 19:02:16 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929}
2022-03-28 19:02:16 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Deployment my-cluster-08515847-cruise-control rolling update in namespace:namespace-4 not ready, will try again in 5000 ms (530807ms till timeout)
2022-03-28 19:02:17 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:02:17 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (130774ms till timeout)
2022-03-28 19:02:17 [ForkJoinPool-1-worker-23] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace http-bridge-tls-st -o yaml
2022-03-28 19:02:17 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (106778ms till timeout)
2022-03-28 19:02:17 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:17 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:02:17 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2307234ms till timeout)
2022-03-28 19:02:17 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (64695ms till timeout)
2022-03-28 19:02:17 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:17 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:17 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-e6343cec-kafka-2 hasn't rolled
2022-03-28 19:02:17 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02, my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a} pods didn't roll. Remaining seconds for stability: 12
2022-03-28 19:02:17 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (256424ms till timeout)
2022-03-28 19:02:17 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (107850ms till timeout)
2022-03-28 19:02:17 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (952062ms till timeout)
2022-03-28 19:02:17 [ForkJoinPool-1-worker-23] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace http-bridge-tls-st -o yaml
2022-03-28 19:02:17 [ForkJoinPool-1-worker-23] DEBUG [Exec:419] Return code: 0
2022-03-28 19:02:17 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (478004ms till timeout)
2022-03-28 19:02:17 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (948256ms till timeout)
2022-03-28 19:02:17 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 19:02:18 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (135944ms till timeout)
2022-03-28 19:02:18 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 19:02:18 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 19:02:18 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Namespace namespace-9 removal not ready, will try again in 1000 ms (475186ms till timeout)
2022-03-28 19:02:18 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:02:18 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (129581ms till timeout)
2022-03-28 19:02:18 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-5dfd606f-kafka-0=a766ef9e-2663-4c21-ad83-6b7d9083122c, my-cluster-5dfd606f-kafka-1=57938984-c88e-45c5-b973-9f953a8e782c, my-cluster-5dfd606f-kafka-2=044285fe-7f53-4c1c-84e1-c51aec79b2ce}
2022-03-28 19:02:18 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:18 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (105485ms till timeout)
2022-03-28 19:02:18 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-5dfd606f-kafka-0=d3be017c-9e19-4ee7-b0fc-58e8694c30b9, my-cluster-5dfd606f-kafka-1=57938984-c88e-45c5-b973-9f953a8e782c, my-cluster-5dfd606f-kafka-2=044285fe-7f53-4c1c-84e1-c51aec79b2ce}
2022-03-28 19:02:18 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-5dfd606f-kafka-0=d3be017c-9e19-4ee7-b0fc-58e8694c30b9, my-cluster-5dfd606f-kafka-1=57938984-c88e-45c5-b973-9f953a8e782c, my-cluster-5dfd606f-kafka-2=044285fe-7f53-4c1c-84e1-c51aec79b2ce}
2022-03-28 19:02:18 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:50] At least my-cluster-5dfd606f-kafka-1 hasn't rolled
2022-03-28 19:02:18 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] component with name my-cluster-5dfd606f-kafka rolling update not ready, will try again in 5000 ms (1748320ms till timeout)
2022-03-28 19:02:18 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:18 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:18 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-e6343cec-kafka-2 hasn't rolled
2022-03-28 19:02:18 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02, my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a} pods didn't roll. Remaining seconds for stability: 11
2022-03-28 19:02:18 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (255315ms till timeout)
2022-03-28 19:02:18 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (63490ms till timeout)
2022-03-28 19:02:18 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:02:18 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2305939ms till timeout)
2022-03-28 19:02:18 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (106744ms till timeout)
2022-03-28 19:02:18 [ForkJoinPool-1-worker-23] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace http-bridge-tls-st -o yaml
2022-03-28 19:02:18 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (950964ms till timeout)
2022-03-28 19:02:19 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (947155ms till timeout)
2022-03-28 19:02:19 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (134845ms till timeout)
2022-03-28 19:02:19 [ForkJoinPool-1-worker-23] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace http-bridge-tls-st -o yaml
2022-03-28 19:02:19 [ForkJoinPool-1-worker-23] DEBUG [Exec:419] Return code: 0
2022-03-28 19:02:19 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (476503ms till timeout)
2022-03-28 19:02:19 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 19:02:19 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:02:19 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (128389ms till timeout)
2022-03-28 19:02:19 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:19 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 19:02:19 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 19:02:19 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Namespace namespace-9 removal not ready, will try again in 1000 ms (473738ms till timeout)
2022-03-28 19:02:19 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (104292ms till timeout)
2022-03-28 19:02:19 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:02:19 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2304836ms till timeout)
2022-03-28 19:02:19 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:19 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:19 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-e6343cec-kafka-2 hasn't rolled
2022-03-28 19:02:19 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02, my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a} pods didn't roll. Remaining seconds for stability: 10
2022-03-28 19:02:19 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (254122ms till timeout)
2022-03-28 19:02:19 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (105642ms till timeout)
2022-03-28 19:02:20 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (62213ms till timeout)
2022-03-28 19:02:20 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (949862ms till timeout)
2022-03-28 19:02:20 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (946058ms till timeout)
2022-03-28 19:02:20 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (133748ms till timeout)
2022-03-28 19:02:20 [ForkJoinPool-1-worker-23] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace http-bridge-tls-st -o yaml
2022-03-28 19:02:20 [ForkJoinPool-1-worker-23] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace http-bridge-tls-st -o yaml
2022-03-28 19:02:20 [ForkJoinPool-1-worker-23] DEBUG [Exec:419] Return code: 0
2022-03-28 19:02:20 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (475035ms till timeout)
2022-03-28 19:02:20 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:02:20 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 19:02:20 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (127192ms till timeout)
2022-03-28 19:02:20 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:21 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (103101ms till timeout)
2022-03-28 19:02:21 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (948762ms till timeout)
2022-03-28 19:02:21 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:02:21 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2303559ms till timeout)
2022-03-28 19:02:21 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:21 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:21 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-e6343cec-kafka-2 hasn't rolled
2022-03-28 19:02:21 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02, my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a} pods didn't roll. Remaining seconds for stability: 9
2022-03-28 19:02:21 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (252845ms till timeout)
2022-03-28 19:02:21 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (61020ms till timeout)
2022-03-28 19:02:21 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (104364ms till timeout)
2022-03-28 19:02:21 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (944961ms till timeout)
2022-03-28 19:02:21 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 19:02:21 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 19:02:21 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Namespace namespace-9 removal not ready, will try again in 1000 ms (472275ms till timeout)
2022-03-28 19:02:21 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (132651ms till timeout)
2022-03-28 19:02:21 [ForkJoinPool-1-worker-23] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace http-bridge-tls-st -o yaml
2022-03-28 19:02:21 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929}
2022-03-28 19:02:22 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:02:22 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (125905ms till timeout)
2022-03-28 19:02:22 [ForkJoinPool-1-worker-21] INFO  [Exec:417] Command: oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-03-28 19:02:22 [ForkJoinPool-1-worker-21] INFO  [Exec:417] Return code: 0
2022-03-28 19:02:22 [ForkJoinPool-1-worker-21] INFO  [ResourceManager:433] Wait for KafkaTopic: my-topic-840747042-63255423 will have desired state: NotReady
2022-03-28 19:02:22 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:22 [ForkJoinPool-1-worker-21] DEBUG [TestUtils:125] Waiting for KafkaTopic: my-topic-840747042-63255423 will have desired state: NotReady
2022-03-28 19:02:22 [ForkJoinPool-1-worker-23] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace http-bridge-tls-st -o yaml
2022-03-28 19:02:22 [ForkJoinPool-1-worker-23] DEBUG [Exec:419] Return code: 0
2022-03-28 19:02:22 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (473582ms till timeout)
2022-03-28 19:02:22 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 19:02:22 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (947579ms till timeout)
2022-03-28 19:02:22 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (101822ms till timeout)
2022-03-28 19:02:22 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929, my-cluster-08515847-cruise-control-dcd7c895b-4pkw7=37920413-f627-4a97-919c-4ff0c818c13a}
2022-03-28 19:02:22 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929}
2022-03-28 19:02:22 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Deployment my-cluster-08515847-cruise-control rolling update in namespace:namespace-4 not ready, will try again in 5000 ms (525367ms till timeout)
2022-03-28 19:02:22 [ForkJoinPool-1-worker-21] INFO  [ResourceManager:444] KafkaTopic: my-topic-840747042-63255423 is in desired state: NotReady
2022-03-28 19:02:22 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (103088ms till timeout)
2022-03-28 19:02:22 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (59742ms till timeout)
2022-03-28 19:02:22 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (943685ms till timeout)
2022-03-28 19:02:22 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:02:22 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2302279ms till timeout)
2022-03-28 19:02:22 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:22 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:22 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-e6343cec-kafka-2 hasn't rolled
2022-03-28 19:02:22 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02, my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a} pods didn't roll. Remaining seconds for stability: 8
2022-03-28 19:02:22 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (251566ms till timeout)
2022-03-28 19:02:22 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (131467ms till timeout)
2022-03-28 19:02:22 [ForkJoinPool-1-worker-21] INFO  [TopicST:90] Delete topic my-topic-840747042-63255423
2022-03-28 19:02:22 [ForkJoinPool-1-worker-21] TRACE [Exec:248] Running command - oc --namespace topic-st delete kafkatopic my-topic-840747042-63255423
2022-03-28 19:02:22 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 19:02:22 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 19:02:22 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Namespace namespace-9 removal not ready, will try again in 1000 ms (470770ms till timeout)
2022-03-28 19:02:23 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Command: oc --namespace topic-st delete kafkatopic my-topic-840747042-63255423
2022-03-28 19:02:23 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Return code: 0
2022-03-28 19:02:23 [ForkJoinPool-1-worker-21] INFO  [KafkaTopicUtils:104] Waiting for KafkaTopic my-topic-840747042-63255423 deletion
2022-03-28 19:02:23 [ForkJoinPool-1-worker-21] DEBUG [TestUtils:125] Waiting for KafkaTopic deletion my-topic-840747042-63255423
2022-03-28 19:02:23 [ForkJoinPool-1-worker-21] INFO  [ResourceManager:155] Create/Update KafkaTopic topic-example-new in namespace topic-st
2022-03-28 19:02:23 [ForkJoinPool-1-worker-23] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace http-bridge-tls-st -o yaml
2022-03-28 19:02:23 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:02:23 [ForkJoinPool-1-worker-55] INFO  [Exec:417] Command: oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-03-28 19:02:23 [ForkJoinPool-1-worker-55] INFO  [Exec:417] Return code: 0
2022-03-28 19:02:23 [ForkJoinPool-1-worker-55] INFO  [KafkaTopicUtils:78] Waiting for KafkaTopic my-topic-1692964628-1348460540 creation 
2022-03-28 19:02:23 [ForkJoinPool-1-worker-55] DEBUG [TestUtils:125] Waiting for KafkaTopic creation my-topic-1692964628-1348460540
2022-03-28 19:02:23 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (946460ms till timeout)
2022-03-28 19:02:23 [ForkJoinPool-1-worker-21] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:topic-example-new
2022-03-28 19:02:23 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:23 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (124597ms till timeout)
2022-03-28 19:02:23 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (100605ms till timeout)
2022-03-28 19:02:23 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (130345ms till timeout)
2022-03-28 19:02:23 [ForkJoinPool-1-worker-21] INFO  [ResourceManager:433] Wait for KafkaTopic: topic-example-new will have desired state: Ready
2022-03-28 19:02:23 [ForkJoinPool-1-worker-21] DEBUG [TestUtils:125] Waiting for KafkaTopic: topic-example-new will have desired state: Ready
2022-03-28 19:02:23 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:02:23 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2301067ms till timeout)
2022-03-28 19:02:23 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (942470ms till timeout)
2022-03-28 19:02:23 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:23 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:23 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-e6343cec-kafka-2 hasn't rolled
2022-03-28 19:02:23 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02, my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a} pods didn't roll. Remaining seconds for stability: 7
2022-03-28 19:02:23 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (250351ms till timeout)
2022-03-28 19:02:23 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (58523ms till timeout)
2022-03-28 19:02:23 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (101868ms till timeout)
2022-03-28 19:02:23 [ForkJoinPool-1-worker-55] INFO  [TopicST:353] Topic successfully created
2022-03-28 19:02:23 [ForkJoinPool-1-worker-55] DEBUG [AbstractST:674] ============================================================================
2022-03-28 19:02:23 [ForkJoinPool-1-worker-55] DEBUG [AbstractST:675] [operators.topic.TopicST - After Each] - Clean up after test
2022-03-28 19:02:23 [ForkJoinPool-1-worker-55] INFO  [ResourceManager:344] ############################################################################
2022-03-28 19:02:23 [ForkJoinPool-1-worker-55] INFO  [ResourceManager:348] Delete all resources for testSendingMessagesToNonExistingTopic
2022-03-28 19:02:23 [ForkJoinPool-1-worker-55] INFO  [ResourceManager:241] Delete of Deployment topic-cluster-name-kafka-clients in namespace topic-st
2022-03-28 19:02:23 [ForkJoinPool-1-worker-23] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-9 get Namespace http-bridge-tls-st -o yaml
2022-03-28 19:02:23 [ForkJoinPool-1-worker-23] DEBUG [Exec:419] Return code: 1
2022-03-28 19:02:23 [ForkJoinPool-1-worker-23] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 19:02:23 [ForkJoinPool-1-worker-23] DEBUG [Exec:419] Error from server (NotFound): namespaces "http-bridge-tls-st" not found
2022-03-28 19:02:23 [ForkJoinPool-1-worker-23] DEBUG [Exec:419] ======STDERR END======
2022-03-28 19:02:23 [ForkJoinPool-1-worker-23] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[namespace-8], io.strimzi.test.logs.CollectorElement@c4c0ea0=[], io.strimzi.test.logs.CollectorElement@aec142ef=[namespace-5], io.strimzi.test.logs.CollectorElement@24b97ba9=[namespace-7], io.strimzi.test.logs.CollectorElement@f851b6c3=[namespace-4], io.strimzi.test.logs.CollectorElement@3881d5f2=[namespace-3], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@5c7379cb=[namespace-9], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[namespace-0], io.strimzi.test.logs.CollectorElement@f4405b3b=[namespace-2], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 19:02:23 [ForkJoinPool-1-worker-23] DEBUG [SuiteThreadController:254] HttpBridgeTlsST - Notifies waiting test suites:[TopicST, ThrottlingQuotaST, UserST, HttpBridgeTlsST, HttpBridgeScramShaST, ReconciliationST, CruiseControlConfigurationST] to and randomly select one to start execution
2022-03-28 19:02:23 [ForkJoinPool-1-worker-23] DEBUG [SuiteThreadController:85] [bridge.HttpBridgeTlsST] - Removing parallel suite: HttpBridgeTlsST
2022-03-28 19:02:23 [ForkJoinPool-1-worker-23] DEBUG [SuiteThreadController:89] [bridge.HttpBridgeTlsST] - Parallel suites count: 5
[INFO] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 439.295 s - in io.strimzi.systemtest.bridge.HttpBridgeTlsST
2022-03-28 19:02:23 [ForkJoinPool-1-worker-23] INFO  [TestSeparator:23] ############################################################################
2022-03-28 19:02:23 [ForkJoinPool-1-worker-23] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.topic.TopicST.testCreateTopicAfterUnsupportedOperation-STARTED
2022-03-28 19:02:23 [ForkJoinPool-1-worker-23] DEBUG [AbstractST:658] ============================================================================
2022-03-28 19:02:23 [ForkJoinPool-1-worker-23] DEBUG [AbstractST:659] [operators.topic.TopicST - Before Each] - Setup test case environment
2022-03-28 19:02:23 [ForkJoinPool-1-worker-23] DEBUG [SuiteThreadController:77] [operators.topic.TopicST] - Adding parallel test: testCreateTopicAfterUnsupportedOperation
2022-03-28 19:02:23 [ForkJoinPool-1-worker-23] DEBUG [SuiteThreadController:81] [operators.topic.TopicST] - Parallel test count: 16
2022-03-28 19:02:23 [ForkJoinPool-1-worker-23] DEBUG [SuiteThreadController:205] [testCreateTopicAfterUnsupportedOperation] moved to the WaitZone, because current thread exceed maximum of allowed test cases in parallel. (16/15)
2022-03-28 19:02:23 [ForkJoinPool-1-worker-23] TRACE [SuiteThreadController:210] testCreateTopicAfterUnsupportedOperation is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (16/15)
2022-03-28 19:02:23 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-5dfd606f-kafka-0=a766ef9e-2663-4c21-ad83-6b7d9083122c, my-cluster-5dfd606f-kafka-1=57938984-c88e-45c5-b973-9f953a8e782c, my-cluster-5dfd606f-kafka-2=044285fe-7f53-4c1c-84e1-c51aec79b2ce}
2022-03-28 19:02:23 [ForkJoinPool-1-worker-21] INFO  [ResourceManager:444] KafkaTopic: topic-example-new is in desired state: Ready
2022-03-28 19:02:23 [ForkJoinPool-1-worker-21] INFO  [TopicST:456] Checking topic topic-example-new in Kafka
2022-03-28 19:02:23 [ForkJoinPool-1-worker-21] TRACE [Exec:248] Running command - oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-03-28 19:02:23 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 19:02:23 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-5dfd606f-kafka-0=d3be017c-9e19-4ee7-b0fc-58e8694c30b9, my-cluster-5dfd606f-kafka-1=57938984-c88e-45c5-b973-9f953a8e782c, my-cluster-5dfd606f-kafka-2=044285fe-7f53-4c1c-84e1-c51aec79b2ce}
2022-03-28 19:02:23 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-5dfd606f-kafka-0=d3be017c-9e19-4ee7-b0fc-58e8694c30b9, my-cluster-5dfd606f-kafka-1=57938984-c88e-45c5-b973-9f953a8e782c, my-cluster-5dfd606f-kafka-2=044285fe-7f53-4c1c-84e1-c51aec79b2ce}
2022-03-28 19:02:23 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:50] At least my-cluster-5dfd606f-kafka-1 hasn't rolled
2022-03-28 19:02:23 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] component with name my-cluster-5dfd606f-kafka rolling update not ready, will try again in 5000 ms (1743164ms till timeout)
2022-03-28 19:02:23 [ForkJoinPool-1-worker-55] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:topic-cluster-name-kafka-clients
2022-03-28 19:02:24 [ForkJoinPool-1-worker-55] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:topic-cluster-name-kafka-clients not ready, will try again in 10000 ms (479710ms till timeout)
2022-03-28 19:02:24 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 19:02:24 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 19:02:24 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Namespace namespace-9 removal not ready, will try again in 1000 ms (469302ms till timeout)
2022-03-28 19:02:24 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (945363ms till timeout)
2022-03-28 19:02:24 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:02:24 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:24 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (129246ms till timeout)
2022-03-28 19:02:24 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (99411ms till timeout)
2022-03-28 19:02:24 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (941362ms till timeout)
2022-03-28 19:02:24 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (123301ms till timeout)
2022-03-28 19:02:24 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:24 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:24 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-e6343cec-kafka-2 hasn't rolled
2022-03-28 19:02:24 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02, my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a} pods didn't roll. Remaining seconds for stability: 6
2022-03-28 19:02:24 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (249241ms till timeout)
2022-03-28 19:02:24 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:02:24 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2299954ms till timeout)
2022-03-28 19:02:24 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (57415ms till timeout)
2022-03-28 19:02:24 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (100759ms till timeout)
2022-03-28 19:02:25 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 19:02:25 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (944263ms till timeout)
2022-03-28 19:02:25 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 19:02:25 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 19:02:25 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Namespace namespace-9 removal not ready, will try again in 1000 ms (467864ms till timeout)
2022-03-28 19:02:25 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (128150ms till timeout)
2022-03-28 19:02:25 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:26 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (98221ms till timeout)
2022-03-28 19:02:26 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:00:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:02:26 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (940141ms till timeout)
2022-03-28 19:02:26 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:02:26 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2298735ms till timeout)
2022-03-28 19:02:26 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (99487ms till timeout)
2022-03-28 19:02:26 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:26 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:26 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-e6343cec-kafka-2 hasn't rolled
2022-03-28 19:02:26 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02, my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a} pods didn't roll. Remaining seconds for stability: 5
2022-03-28 19:02:26 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (247964ms till timeout)
2022-03-28 19:02:26 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (56139ms till timeout)
2022-03-28 19:02:26 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (122017ms till timeout)
2022-03-28 19:02:26 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 19:02:26 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (943165ms till timeout)
2022-03-28 19:02:26 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (127053ms till timeout)
2022-03-28 19:02:27 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:27 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 19:02:27 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 19:02:27 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Namespace namespace-9 removal not ready, will try again in 1000 ms (466416ms till timeout)
2022-03-28 19:02:27 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (97028ms till timeout)
2022-03-28 19:02:27 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (938977ms till timeout)
2022-03-28 19:02:27 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:02:27 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2297571ms till timeout)
2022-03-28 19:02:27 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (98295ms till timeout)
2022-03-28 19:02:27 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=null, completedIndexes=null, completionTime=2022-03-28T19:02:21Z, conditions=[JobCondition(lastProbeTime=2022-03-28T19:02:21Z, lastTransitionTime=2022-03-28T19:02:21Z, message=null, reason=null, status=True, type=Complete, additionalProperties={})], failed=null, ready=null, startTime=2022-03-28T19:00:43Z, succeeded=1, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:02:27 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:27 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:27 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-e6343cec-kafka-2 hasn't rolled
2022-03-28 19:02:27 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02, my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a} pods didn't roll. Remaining seconds for stability: 4
2022-03-28 19:02:27 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (246772ms till timeout)
2022-03-28 19:02:27 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (54943ms till timeout)
2022-03-28 19:02:27 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929}
2022-03-28 19:02:27 [ForkJoinPool-1-worker-41] INFO  [PodUtils:186] Waiting for message will be in the log
2022-03-28 19:02:27 [ForkJoinPool-1-worker-41] DEBUG [TestUtils:125] Waiting for Waiting for message will be in the log
2022-03-28 19:02:27 [ForkJoinPool-1-worker-41] INFO  [PodUtils:189] Message All topics created found in create-admin-my-cluster-836b31f8-kafka-clients-xrl9c log
2022-03-28 19:02:27 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929, my-cluster-08515847-cruise-control-dcd7c895b-4pkw7=37920413-f627-4a97-919c-4ff0c818c13a}
2022-03-28 19:02:27 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929}
2022-03-28 19:02:27 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Deployment my-cluster-08515847-cruise-control rolling update in namespace:namespace-4 not ready, will try again in 5000 ms (519999ms till timeout)
2022-03-28 19:02:27 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (942024ms till timeout)
2022-03-28 19:02:27 [ForkJoinPool-1-worker-41] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment create-admin-my-cluster-836b31f8-kafka-clients deletion
2022-03-28 19:02:27 [ForkJoinPool-1-worker-41] DEBUG [TestUtils:125] Waiting for ReplicaSet create-admin-my-cluster-836b31f8-kafka-clients to be deleted
2022-03-28 19:02:27 [ForkJoinPool-1-worker-41] DEBUG [JobUtils:40] Job create-admin-my-cluster-836b31f8-kafka-clients was deleted
2022-03-28 19:02:27 [ForkJoinPool-1-worker-41] INFO  [ThrottlingQuotaST:112] Executing 2/5 iteration.
2022-03-28 19:02:27 [ForkJoinPool-1-worker-41] INFO  [ResourceManager:155] Create/Update Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st
2022-03-28 19:02:28 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (125904ms till timeout)
2022-03-28 19:02:28 [ForkJoinPool-1-worker-41] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:create-admin-my-cluster-836b31f8-kafka-clients
2022-03-28 19:02:28 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 19:02:28 [ForkJoinPool-1-worker-41] INFO  [JobUtils:81] Waiting for job: create-admin-my-cluster-836b31f8-kafka-clients will be in active state
2022-03-28 19:02:28 [ForkJoinPool-1-worker-41] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 19:02:28 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:28 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (937826ms till timeout)
2022-03-28 19:02:28 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:02:28 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2296420ms till timeout)
2022-03-28 19:02:28 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (95868ms till timeout)
2022-03-28 19:02:28 [ForkJoinPool-1-worker-41] INFO  [ClientUtils:76] Waiting for producer/consumer:create-admin-my-cluster-836b31f8-kafka-clients to finished
2022-03-28 19:02:28 [ForkJoinPool-1-worker-41] DEBUG [TestUtils:125] Waiting for job finished
2022-03-28 19:02:28 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:28 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:28 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-e6343cec-kafka-2 hasn't rolled
2022-03-28 19:02:28 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02, my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a} pods didn't roll. Remaining seconds for stability: 3
2022-03-28 19:02:28 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (245610ms till timeout)
2022-03-28 19:02:28 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (97128ms till timeout)
2022-03-28 19:02:28 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (53782ms till timeout)
2022-03-28 19:02:28 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:02:28 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 19:02:28 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 19:02:28 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Namespace namespace-9 removal not ready, will try again in 1000 ms (464976ms till timeout)
2022-03-28 19:02:28 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (219806ms till timeout)
2022-03-28 19:02:28 [ForkJoinPool-1-worker-23] DEBUG [SuiteThreadController:230] testCreateTopicAfterUnsupportedOperation test now can proceed its execution
2022-03-28 19:02:28 [ForkJoinPool-1-worker-23] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 19:02:28 [ForkJoinPool-1-worker-23] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-abc5d34b, testThrottlingQuotasCreateAlterPartitions=my-cluster-ec1b34f8, testSendSimpleMessageTls=my-cluster-89673e85, testConfigurationReflection=my-cluster-188382d8, testDeployAndUnDeployCruiseControl=my-cluster-5dfd606f, testCapacityFile=my-cluster-855b9105, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-afe273c9, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-e6343cec, testTlsExternalUser=my-cluster-45cf8ce4, testSendSimpleMessageTlsScramSha=my-cluster-561d4fae, testCreateTopicAfterUnsupportedOperation=my-cluster-e8a83fb0, testTopicModificationOfReplicationFactor=my-cluster-15f5f09f, testKafkaAdminTopicOperations=my-cluster-506fda27, testConfigurationFileIsCreated=my-cluster-17fc585b, testCreateTopicViaKafka=my-cluster-75679697, testCreatingUsersWithSecretPrefix=my-cluster-f743fdfd, testUpdateUser=my-cluster-d582eb8c, testThrottlingQuotasDeleteTopic=my-cluster-836b31f8, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-959c6ed5, testTlsExternalUserWithQuotas=my-cluster-100f5461, testThrottlingQuotasCreateTopic=my-cluster-e9955717, testSendingMessagesToNonExistingTopic=my-cluster-a3c1c5ed, testConfigurationPerformanceOptions=my-cluster-08515847, testReceiveSimpleMessageTls=my-cluster-634e4d65, testUserTemplate=my-cluster-927d5a1b, testMoreReplicasThanAvailableBrokers=my-cluster-22050d08}
2022-03-28 19:02:28 [ForkJoinPool-1-worker-23] TRACE [AbstractST:607] USERS_NAME_MAP: {testReceiveSimpleMessageTlsScramSha=my-user-330562220-1155646423, testThrottlingQuotasCreateAlterPartitions=my-user-333986137-1152182125, testSendSimpleMessageTls=my-user-1887866274-1013125243, testConfigurationReflection=my-user-1890969183-1754686912, testDeployAndUnDeployCruiseControl=my-user-506268651-1309117986, testCapacityFile=my-user-1460643829-103298436, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-user-8456079-650483351, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-2069267102-581538560, testTlsExternalUser=my-user-399414674-1203409092, testSendSimpleMessageTlsScramSha=my-user-1601209876-518796379, testCreateTopicAfterUnsupportedOperation=my-user-1589831352-1271671319, testTopicModificationOfReplicationFactor=my-user-259690203-415132624, testKafkaAdminTopicOperations=my-user-1780746402-846362125, testConfigurationFileIsCreated=my-user-355815510-1316268151, testCreateTopicViaKafka=my-user-541976280-277947468, testCreatingUsersWithSecretPrefix=my-user-607975993-1210920737, testUpdateUser=my-user-1297536277-778093599, testThrottlingQuotasDeleteTopic=my-user-1567019138-1355600812, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-2104248162-45187051, testTlsExternalUserWithQuotas=my-user-1909583883-1106194424, testThrottlingQuotasCreateTopic=my-user-1876060546-1887905718, testSendingMessagesToNonExistingTopic=my-user-134768902-582007171, testConfigurationPerformanceOptions=my-user-98362545-1476587236, testReceiveSimpleMessageTls=my-user-337002809-1497514615, testUserTemplate=my-user-989633374-940717398, testMoreReplicasThanAvailableBrokers=my-user-713898339-1095128639}
2022-03-28 19:02:28 [ForkJoinPool-1-worker-23] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-topic-1027850608-734754781, testThrottlingQuotasCreateAlterPartitions=my-topic-325891726-169328137, testSendSimpleMessageTls=my-topic-874607937-87907051, testConfigurationReflection=my-topic-1394577332-1936226438, testDeployAndUnDeployCruiseControl=my-topic-465377127-800134460, testCapacityFile=my-topic-549312506-649708453, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-topic-1687496771-142091227, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-293775067-432059493, testTlsExternalUser=my-topic-707627084-257659651, testSendSimpleMessageTlsScramSha=my-topic-1586843669-332477842, testCreateTopicAfterUnsupportedOperation=my-topic-1619871970-423361410, testTopicModificationOfReplicationFactor=my-topic-1480509436-578706559, testKafkaAdminTopicOperations=my-topic-481231182-953607496, testConfigurationFileIsCreated=my-topic-423850169-855699454, testCreateTopicViaKafka=my-topic-1973235938-1730719386, testCreatingUsersWithSecretPrefix=my-topic-1727421070-668192795, testUpdateUser=my-topic-1325563869-1324403074, testThrottlingQuotasDeleteTopic=my-topic-1674611395-2048879468, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-634125597-1746493046, testTlsExternalUserWithQuotas=my-topic-1986059791-167638004, testThrottlingQuotasCreateTopic=my-topic-1246597695-228473879, testSendingMessagesToNonExistingTopic=my-topic-1692964628-1348460540, testConfigurationPerformanceOptions=my-topic-696588651-1198599652, testReceiveSimpleMessageTls=my-topic-829184226-1519275114, testUserTemplate=my-topic-1542379258-2002911578, testMoreReplicasThanAvailableBrokers=my-topic-840747042-63255423}
2022-03-28 19:02:28 [ForkJoinPool-1-worker-23] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-abc5d34b-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-ec1b34f8-kafka-clients, testSendSimpleMessageTls=my-cluster-89673e85-kafka-clients, testConfigurationReflection=my-cluster-188382d8-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-5dfd606f-kafka-clients, testCapacityFile=my-cluster-855b9105-kafka-clients, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-afe273c9-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-e6343cec-kafka-clients, testTlsExternalUser=my-cluster-45cf8ce4-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-561d4fae-kafka-clients, testCreateTopicAfterUnsupportedOperation=my-cluster-e8a83fb0-kafka-clients, testTopicModificationOfReplicationFactor=my-cluster-15f5f09f-kafka-clients, testKafkaAdminTopicOperations=my-cluster-506fda27-kafka-clients, testConfigurationFileIsCreated=my-cluster-17fc585b-kafka-clients, testCreateTopicViaKafka=my-cluster-75679697-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-f743fdfd-kafka-clients, testUpdateUser=my-cluster-d582eb8c-kafka-clients, testThrottlingQuotasDeleteTopic=my-cluster-836b31f8-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-959c6ed5-kafka-clients, testTlsExternalUserWithQuotas=my-cluster-100f5461-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-e9955717-kafka-clients, testSendingMessagesToNonExistingTopic=my-cluster-a3c1c5ed-kafka-clients, testConfigurationPerformanceOptions=my-cluster-08515847-kafka-clients, testReceiveSimpleMessageTls=my-cluster-634e4d65-kafka-clients, testUserTemplate=my-cluster-927d5a1b-kafka-clients, testMoreReplicasThanAvailableBrokers=my-cluster-22050d08-kafka-clients}
2022-03-28 19:02:28 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:155] Create/Update KafkaTopic topic-with-replication-to-change in namespace topic-st
2022-03-28 19:02:28 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:topic-with-replication-to-change
2022-03-28 19:02:28 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-5dfd606f-kafka-0=a766ef9e-2663-4c21-ad83-6b7d9083122c, my-cluster-5dfd606f-kafka-1=57938984-c88e-45c5-b973-9f953a8e782c, my-cluster-5dfd606f-kafka-2=044285fe-7f53-4c1c-84e1-c51aec79b2ce}
2022-03-28 19:02:28 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:433] Wait for KafkaTopic: topic-with-replication-to-change will have desired state: Ready
2022-03-28 19:02:28 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for KafkaTopic: topic-with-replication-to-change will have desired state: Ready
2022-03-28 19:02:29 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (940856ms till timeout)
2022-03-28 19:02:29 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-5dfd606f-kafka-0=d3be017c-9e19-4ee7-b0fc-58e8694c30b9, my-cluster-5dfd606f-kafka-1=57938984-c88e-45c5-b973-9f953a8e782c, my-cluster-5dfd606f-kafka-2=044285fe-7f53-4c1c-84e1-c51aec79b2ce}
2022-03-28 19:02:29 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-5dfd606f-kafka-0=d3be017c-9e19-4ee7-b0fc-58e8694c30b9, my-cluster-5dfd606f-kafka-1=57938984-c88e-45c5-b973-9f953a8e782c, my-cluster-5dfd606f-kafka-2=044285fe-7f53-4c1c-84e1-c51aec79b2ce}
2022-03-28 19:02:29 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:50] At least my-cluster-5dfd606f-kafka-1 hasn't rolled
2022-03-28 19:02:29 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] component with name my-cluster-5dfd606f-kafka rolling update not ready, will try again in 5000 ms (1738030ms till timeout)
2022-03-28 19:02:29 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:444] KafkaTopic: topic-with-replication-to-change is in desired state: Ready
2022-03-28 19:02:29 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (124742ms till timeout)
2022-03-28 19:02:29 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:433] Wait for KafkaTopic: topic-with-replication-to-change will have desired state: NotReady
2022-03-28 19:02:29 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for KafkaTopic: topic-with-replication-to-change will have desired state: NotReady
2022-03-28 19:02:29 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:29 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (936672ms till timeout)
2022-03-28 19:02:29 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:02:29 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2295266ms till timeout)
2022-03-28 19:02:29 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (94713ms till timeout)
2022-03-28 19:02:29 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] KafkaTopic: topic-with-replication-to-change will have desired state: NotReady not ready, will try again in 1000 ms (179816ms till timeout)
2022-03-28 19:02:29 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 19:02:29 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:29 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:29 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-e6343cec-kafka-2 hasn't rolled
2022-03-28 19:02:29 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02, my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a} pods didn't roll. Remaining seconds for stability: 2
2022-03-28 19:02:29 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (244455ms till timeout)
2022-03-28 19:02:29 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (95976ms till timeout)
2022-03-28 19:02:29 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (52630ms till timeout)
2022-03-28 19:02:29 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:02:29 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (218614ms till timeout)
2022-03-28 19:02:30 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 19:02:30 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 19:02:30 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Namespace namespace-9 removal not ready, will try again in 1000 ms (463535ms till timeout)
2022-03-28 19:02:30 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (939759ms till timeout)
2022-03-28 19:02:30 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (123646ms till timeout)
2022-03-28 19:02:30 [ForkJoinPool-1-worker-5] INFO  [Exec:417] Command: oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --bootstrap-server localhost:9092 --create  --topic my-topic-1973235938-1730719386 --replication-factor 3 --partitions 3
2022-03-28 19:02:30 [ForkJoinPool-1-worker-5] INFO  [Exec:417] Return code: 0
2022-03-28 19:02:30 [ForkJoinPool-1-worker-5] INFO  [KafkaTopicUtils:78] Waiting for KafkaTopic my-topic-1973235938-1730719386 creation 
2022-03-28 19:02:30 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for KafkaTopic creation my-topic-1973235938-1730719386
2022-03-28 19:02:30 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (935575ms till timeout)
2022-03-28 19:02:30 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:30 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:444] KafkaTopic: topic-with-replication-to-change is in desired state: NotReady
2022-03-28 19:02:30 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:02:30 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2294072ms till timeout)
2022-03-28 19:02:30 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (93432ms till timeout)
2022-03-28 19:02:30 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:30 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:30 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-e6343cec-kafka-2 hasn't rolled
2022-03-28 19:02:30 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02, my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a} pods didn't roll. Remaining seconds for stability: 1
2022-03-28 19:02:30 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (243264ms till timeout)
2022-03-28 19:02:30 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (51412ms till timeout)
2022-03-28 19:02:30 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (94757ms till timeout)
2022-03-28 19:02:30 [ForkJoinPool-1-worker-5] INFO  [TopicST:482] Checking in KafkaTopic CR that topic my-topic-1973235938-1730719386 was created with expected settings
2022-03-28 19:02:30 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-03-28 19:02:31 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:02:31 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 19:02:31 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:155] Create/Update KafkaTopic another-topic in namespace topic-st
2022-03-28 19:02:31 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (217277ms till timeout)
2022-03-28 19:02:31 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (938623ms till timeout)
2022-03-28 19:02:31 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:another-topic
2022-03-28 19:02:31 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:433] Wait for KafkaTopic: another-topic will have desired state: Ready
2022-03-28 19:02:31 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for KafkaTopic: another-topic will have desired state: Ready
2022-03-28 19:02:31 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (122505ms till timeout)
2022-03-28 19:02:31 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 19:02:31 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 19:02:31 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Namespace namespace-9 removal not ready, will try again in 1000 ms (462083ms till timeout)
2022-03-28 19:02:31 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:444] KafkaTopic: another-topic is in desired state: Ready
2022-03-28 19:02:31 [ForkJoinPool-1-worker-23] INFO  [TopicST:456] Checking topic topic-with-replication-to-change in Kafka
2022-03-28 19:02:31 [ForkJoinPool-1-worker-23] TRACE [Exec:248] Running command - oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-03-28 19:02:31 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (934476ms till timeout)
2022-03-28 19:02:31 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:31 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:02:31 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2292974ms till timeout)
2022-03-28 19:02:31 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (92323ms till timeout)
2022-03-28 19:02:32 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:32 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a, my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02}
2022-03-28 19:02:32 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-e6343cec-kafka-2 hasn't rolled
2022-03-28 19:02:32 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-e6343cec-kafka-2=7abb394d-84f4-45fb-9a0b-9cdd192f5c02, my-cluster-e6343cec-kafka-0=d5327597-c737-4a87-80df-06931c63d7f8, my-cluster-e6343cec-kafka-1=b6f3d031-c9a3-4ea6-bcb5-b52a609dcb3a} pods didn't roll. Remaining seconds for stability: 0
2022-03-28 19:02:32 [ForkJoinPool-1-worker-3] INFO  [CruiseControlConfigurationST:174] Verifying new configuration in the Kafka CR
2022-03-28 19:02:32 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (50239ms till timeout)
2022-03-28 19:02:32 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (93585ms till timeout)
2022-03-28 19:02:32 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Verify that kafka contains cruise control topics with related configuration.
2022-03-28 19:02:32 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Verify that kafka contains cruise control topics with related configuration.
2022-03-28 19:02:32 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:02:32 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (937504ms till timeout)
2022-03-28 19:02:32 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (215968ms till timeout)
2022-03-28 19:02:32 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 19:02:32 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (121388ms till timeout)
2022-03-28 19:02:32 [ForkJoinPool-1-worker-3] DEBUG [AbstractST:674] ============================================================================
2022-03-28 19:02:32 [ForkJoinPool-1-worker-3] DEBUG [AbstractST:675] [cruisecontrol.CruiseControlConfigurationST - After Each] - Clean up after test
2022-03-28 19:02:32 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:344] ############################################################################
2022-03-28 19:02:32 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:348] Delete all resources for testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods
2022-03-28 19:02:32 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:241] Delete of Kafka my-cluster-e6343cec in namespace namespace-3
2022-03-28 19:02:32 [ForkJoinPool-1-worker-3] INFO  [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-3, for cruise control Kafka cluster my-cluster-e6343cec
2022-03-28 19:02:32 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929}
2022-03-28 19:02:32 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (933314ms till timeout)
2022-03-28 19:02:32 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:02:32 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2291811ms till timeout)
2022-03-28 19:02:33 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 19:02:33 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 19:02:33 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Namespace namespace-9 removal not ready, will try again in 1000 ms (460602ms till timeout)
2022-03-28 19:02:33 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (91166ms till timeout)
2022-03-28 19:02:33 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929, my-cluster-08515847-cruise-control-dcd7c895b-4pkw7=37920413-f627-4a97-919c-4ff0c818c13a}
2022-03-28 19:02:33 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929}
2022-03-28 19:02:33 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Deployment my-cluster-08515847-cruise-control rolling update in namespace:namespace-4 not ready, will try again in 5000 ms (514616ms till timeout)
2022-03-28 19:02:33 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (49082ms till timeout)
2022-03-28 19:02:33 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (92428ms till timeout)
2022-03-28 19:02:33 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (936344ms till timeout)
2022-03-28 19:02:33 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:02:33 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-e6343cec
2022-03-28 19:02:33 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (120225ms till timeout)
2022-03-28 19:02:34 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 19:02:34 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (214469ms till timeout)
2022-03-28 19:02:34 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-5dfd606f-kafka-0=a766ef9e-2663-4c21-ad83-6b7d9083122c, my-cluster-5dfd606f-kafka-1=57938984-c88e-45c5-b973-9f953a8e782c, my-cluster-5dfd606f-kafka-2=044285fe-7f53-4c1c-84e1-c51aec79b2ce}
2022-03-28 19:02:34 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster-e6343cec not ready, will try again in 10000 ms (839568ms till timeout)
2022-03-28 19:02:34 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (932037ms till timeout)
2022-03-28 19:02:34 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:02:34 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2290590ms till timeout)
2022-03-28 19:02:34 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-5dfd606f-kafka-0=d3be017c-9e19-4ee7-b0fc-58e8694c30b9, my-cluster-5dfd606f-kafka-1=57938984-c88e-45c5-b973-9f953a8e782c, my-cluster-5dfd606f-kafka-2=044285fe-7f53-4c1c-84e1-c51aec79b2ce}
2022-03-28 19:02:34 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-5dfd606f-kafka-0=d3be017c-9e19-4ee7-b0fc-58e8694c30b9, my-cluster-5dfd606f-kafka-1=57938984-c88e-45c5-b973-9f953a8e782c, my-cluster-5dfd606f-kafka-2=044285fe-7f53-4c1c-84e1-c51aec79b2ce}
2022-03-28 19:02:34 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:50] At least my-cluster-5dfd606f-kafka-1 hasn't rolled
2022-03-28 19:02:34 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] component with name my-cluster-5dfd606f-kafka rolling update not ready, will try again in 5000 ms (1732795ms till timeout)
2022-03-28 19:02:34 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (47858ms till timeout)
2022-03-28 19:02:34 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (91155ms till timeout)
2022-03-28 19:02:34 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (89796ms till timeout)
2022-03-28 19:02:34 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 19:02:34 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 19:02:34 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Namespace namespace-9 removal not ready, will try again in 1000 ms (459119ms till timeout)
2022-03-28 19:02:34 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (935223ms till timeout)
2022-03-28 19:02:34 [ForkJoinPool-1-worker-55] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:topic-cluster-name-kafka-clients not ready, will try again in 10000 ms (469137ms till timeout)
2022-03-28 19:02:34 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (119110ms till timeout)
2022-03-28 19:02:35 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:02:35 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (213276ms till timeout)
2022-03-28 19:02:35 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (930912ms till timeout)
2022-03-28 19:02:35 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:02:35 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2289410ms till timeout)
2022-03-28 19:02:35 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 19:02:35 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (46757ms till timeout)
2022-03-28 19:02:35 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (90003ms till timeout)
2022-03-28 19:02:35 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (88643ms till timeout)
2022-03-28 19:02:35 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (934126ms till timeout)
2022-03-28 19:02:35 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 19:02:35 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 19:02:35 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Namespace namespace-9 removal not ready, will try again in 1000 ms (457684ms till timeout)
2022-03-28 19:02:35 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (118012ms till timeout)
2022-03-28 19:02:36 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:02:36 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (929815ms till timeout)
2022-03-28 19:02:36 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (211990ms till timeout)
2022-03-28 19:02:36 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:02:36 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2288220ms till timeout)
2022-03-28 19:02:36 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (45583ms till timeout)
2022-03-28 19:02:36 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (88832ms till timeout)
2022-03-28 19:02:36 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (87384ms till timeout)
2022-03-28 19:02:36 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (933024ms till timeout)
2022-03-28 19:02:36 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 19:02:37 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (116916ms till timeout)
2022-03-28 19:02:37 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 19:02:37 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 19:02:37 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Namespace namespace-9 removal not ready, will try again in 1000 ms (456249ms till timeout)
2022-03-28 19:02:37 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (928713ms till timeout)
2022-03-28 19:02:37 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:02:37 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:02:37 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2287104ms till timeout)
2022-03-28 19:02:37 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (210680ms till timeout)
2022-03-28 19:02:37 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (44464ms till timeout)
2022-03-28 19:02:37 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (87715ms till timeout)
2022-03-28 19:02:37 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (931925ms till timeout)
2022-03-28 19:02:38 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (86247ms till timeout)
2022-03-28 19:02:38 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (115820ms till timeout)
2022-03-28 19:02:38 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929}
2022-03-28 19:02:38 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 19:02:38 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929, my-cluster-08515847-cruise-control-dcd7c895b-4pkw7=37920413-f627-4a97-919c-4ff0c818c13a}
2022-03-28 19:02:38 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929}
2022-03-28 19:02:38 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Deployment my-cluster-08515847-cruise-control rolling update in namespace:namespace-4 not ready, will try again in 5000 ms (509422ms till timeout)
2022-03-28 19:02:38 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (927616ms till timeout)
2022-03-28 19:02:38 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 19:02:38 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 19:02:38 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Namespace namespace-9 removal not ready, will try again in 1000 ms (454821ms till timeout)
2022-03-28 19:02:38 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:02:38 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2286006ms till timeout)
2022-03-28 19:02:38 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:02:38 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (43270ms till timeout)
2022-03-28 19:02:39 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (209387ms till timeout)
2022-03-28 19:02:39 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (930828ms till timeout)
2022-03-28 19:02:39 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (86519ms till timeout)
2022-03-28 19:02:39 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (85062ms till timeout)
2022-03-28 19:02:39 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (114711ms till timeout)
2022-03-28 19:02:39 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-5dfd606f-kafka-0=a766ef9e-2663-4c21-ad83-6b7d9083122c, my-cluster-5dfd606f-kafka-1=57938984-c88e-45c5-b973-9f953a8e782c, my-cluster-5dfd606f-kafka-2=044285fe-7f53-4c1c-84e1-c51aec79b2ce}
2022-03-28 19:02:39 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-5dfd606f-kafka-0=d3be017c-9e19-4ee7-b0fc-58e8694c30b9, my-cluster-5dfd606f-kafka-1=57938984-c88e-45c5-b973-9f953a8e782c, my-cluster-5dfd606f-kafka-2=044285fe-7f53-4c1c-84e1-c51aec79b2ce}
2022-03-28 19:02:39 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-5dfd606f-kafka-0=d3be017c-9e19-4ee7-b0fc-58e8694c30b9, my-cluster-5dfd606f-kafka-1=57938984-c88e-45c5-b973-9f953a8e782c, my-cluster-5dfd606f-kafka-2=044285fe-7f53-4c1c-84e1-c51aec79b2ce}
2022-03-28 19:02:39 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:50] At least my-cluster-5dfd606f-kafka-1 hasn't rolled
2022-03-28 19:02:39 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] component with name my-cluster-5dfd606f-kafka rolling update not ready, will try again in 5000 ms (1727688ms till timeout)
2022-03-28 19:02:39 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (926515ms till timeout)
2022-03-28 19:02:39 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 19:02:39 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:02:39 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2284907ms till timeout)
2022-03-28 19:02:40 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (42170ms till timeout)
2022-03-28 19:02:40 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:02:40 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (929728ms till timeout)
2022-03-28 19:02:40 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (85418ms till timeout)
2022-03-28 19:02:40 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 19:02:40 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 19:02:40 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Namespace namespace-9 removal not ready, will try again in 1000 ms (453380ms till timeout)
2022-03-28 19:02:40 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (208193ms till timeout)
2022-03-28 19:02:40 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (113613ms till timeout)
2022-03-28 19:02:40 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (83869ms till timeout)
2022-03-28 19:02:40 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (925417ms till timeout)
2022-03-28 19:02:40 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:02:40 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2283808ms till timeout)
2022-03-28 19:02:41 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (41070ms till timeout)
2022-03-28 19:02:41 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 19:02:41 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (928629ms till timeout)
2022-03-28 19:02:41 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:02:41 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (84222ms till timeout)
2022-03-28 19:02:41 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (112514ms till timeout)
2022-03-28 19:02:41 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (206902ms till timeout)
2022-03-28 19:02:41 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (82587ms till timeout)
2022-03-28 19:02:41 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 19:02:41 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 19:02:41 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Namespace namespace-9 removal not ready, will try again in 1000 ms (451938ms till timeout)
2022-03-28 19:02:41 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (924321ms till timeout)
2022-03-28 19:02:42 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:02:42 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2282707ms till timeout)
2022-03-28 19:02:42 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (39970ms till timeout)
2022-03-28 19:02:42 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (927529ms till timeout)
2022-03-28 19:02:42 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (83112ms till timeout)
2022-03-28 19:02:42 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (111400ms till timeout)
2022-03-28 19:02:42 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 19:02:42 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:02:42 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (81463ms till timeout)
2022-03-28 19:02:42 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (205594ms till timeout)
2022-03-28 19:02:42 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (923224ms till timeout)
2022-03-28 19:02:43 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 19:02:43 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 19:02:43 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Namespace namespace-9 removal not ready, will try again in 1000 ms (450502ms till timeout)
2022-03-28 19:02:43 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:02:43 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2281608ms till timeout)
2022-03-28 19:02:43 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929}
2022-03-28 19:02:43 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (38869ms till timeout)
2022-03-28 19:02:43 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (926427ms till timeout)
2022-03-28 19:02:43 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929, my-cluster-08515847-cruise-control-dcd7c895b-4pkw7=37920413-f627-4a97-919c-4ff0c818c13a}
2022-03-28 19:02:43 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929}
2022-03-28 19:02:43 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Deployment my-cluster-08515847-cruise-control rolling update in namespace:namespace-4 not ready, will try again in 5000 ms (504207ms till timeout)
2022-03-28 19:02:43 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (81922ms till timeout)
2022-03-28 19:02:43 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (110200ms till timeout)
2022-03-28 19:02:43 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (80360ms till timeout)
2022-03-28 19:02:43 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:02:44 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (922125ms till timeout)
2022-03-28 19:02:44 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 19:02:44 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (204299ms till timeout)
2022-03-28 19:02:44 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:369] ############################################################################
2022-03-28 19:02:44 [ForkJoinPool-1-worker-3] INFO  [TestSuiteNamespaceManager:200] Deleting namespace:namespace-3 for test case:testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods
2022-03-28 19:02:44 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:02:44 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2280434ms till timeout)
2022-03-28 19:02:44 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-5dfd606f-kafka-0=a766ef9e-2663-4c21-ad83-6b7d9083122c, my-cluster-5dfd606f-kafka-1=57938984-c88e-45c5-b973-9f953a8e782c, my-cluster-5dfd606f-kafka-2=044285fe-7f53-4c1c-84e1-c51aec79b2ce}
2022-03-28 19:02:44 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Namespace namespace-3 removal
2022-03-28 19:02:44 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 19:02:44 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 19:02:44 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 19:02:44 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Namespace namespace-9 removal not ready, will try again in 1000 ms (449061ms till timeout)
2022-03-28 19:02:44 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-5dfd606f-kafka-0=d3be017c-9e19-4ee7-b0fc-58e8694c30b9, my-cluster-5dfd606f-kafka-1=1187ed10-515a-43c0-be8e-d43991c9d25d, my-cluster-5dfd606f-kafka-2=044285fe-7f53-4c1c-84e1-c51aec79b2ce}
2022-03-28 19:02:44 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-5dfd606f-kafka-0=d3be017c-9e19-4ee7-b0fc-58e8694c30b9, my-cluster-5dfd606f-kafka-1=1187ed10-515a-43c0-be8e-d43991c9d25d, my-cluster-5dfd606f-kafka-2=044285fe-7f53-4c1c-84e1-c51aec79b2ce}
2022-03-28 19:02:44 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:50] At least my-cluster-5dfd606f-kafka-2 hasn't rolled
2022-03-28 19:02:44 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] component with name my-cluster-5dfd606f-kafka rolling update not ready, will try again in 5000 ms (1722534ms till timeout)
2022-03-28 19:02:44 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (37704ms till timeout)
2022-03-28 19:02:44 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (925264ms till timeout)
2022-03-28 19:02:44 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (80821ms till timeout)
2022-03-28 19:02:44 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 19:02:44 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 19:02:44 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (479558ms till timeout)
2022-03-28 19:02:44 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (109008ms till timeout)
2022-03-28 19:02:45 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (79160ms till timeout)
2022-03-28 19:02:45 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (921023ms till timeout)
2022-03-28 19:02:45 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:02:45 [ForkJoinPool-1-worker-55] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:topic-cluster-name-kafka-clients not ready, will try again in 10000 ms (458647ms till timeout)
2022-03-28 19:02:45 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (203100ms till timeout)
2022-03-28 19:02:45 [ForkJoinPool-1-worker-21] INFO  [Exec:417] Command: oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-03-28 19:02:45 [ForkJoinPool-1-worker-21] INFO  [Exec:417] Return code: 0
2022-03-28 19:02:45 [ForkJoinPool-1-worker-21] INFO  [TopicST:461] Checking in KafkaTopic CR that topic topic-example-new exists
2022-03-28 19:02:45 [ForkJoinPool-1-worker-21] DEBUG [AbstractST:674] ============================================================================
2022-03-28 19:02:45 [ForkJoinPool-1-worker-21] DEBUG [AbstractST:675] [operators.topic.TopicST - After Each] - Clean up after test
2022-03-28 19:02:45 [ForkJoinPool-1-worker-21] INFO  [ResourceManager:344] ############################################################################
2022-03-28 19:02:45 [ForkJoinPool-1-worker-21] INFO  [ResourceManager:348] Delete all resources for testMoreReplicasThanAvailableBrokers
2022-03-28 19:02:45 [ForkJoinPool-1-worker-21] INFO  [ResourceManager:241] Delete of KafkaTopic topic-example-new in namespace topic-st
2022-03-28 19:02:45 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:02:45 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2279325ms till timeout)
2022-03-28 19:02:45 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 19:02:45 [ForkJoinPool-1-worker-21] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:topic-example-new
2022-03-28 19:02:45 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (36596ms till timeout)
2022-03-28 19:02:45 [ForkJoinPool-1-worker-21] INFO  [ResourceManager:241] Delete of KafkaTopic my-topic-840747042-63255423 in namespace topic-st
2022-03-28 19:02:45 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (924156ms till timeout)
2022-03-28 19:02:45 [ForkJoinPool-1-worker-21] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-840747042-63255423
2022-03-28 19:02:45 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 19:02:45 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (79649ms till timeout)
2022-03-28 19:02:46 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 19:02:46 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 19:02:46 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Namespace namespace-9 removal not ready, will try again in 1000 ms (447595ms till timeout)
2022-03-28 19:02:46 [ForkJoinPool-1-worker-21] INFO  [ResourceManager:369] ############################################################################
2022-03-28 19:02:46 [ForkJoinPool-1-worker-21] DEBUG [SuiteThreadController:267] testMoreReplicasThanAvailableBrokers - Notifies waiting test cases:[testPauseReconciliationInKafkaRebalanceAndTopic, testPauseReconciliationInKafkaAndKafkaConnectWithConnector, testCapacityFile, testConfigurationFileIsCreated, testConfigurationPerformanceOptions, testConfigurationReflection, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testDeployAndUnDeployCruiseControl, testTlsExternalUserWithQuotas, testCreatingUsersWithSecretPrefix, testThrottlingQuotasCreateTopic, testThrottlingQuotasCreateAlterPartitions, testKafkaAdminTopicOperations, testUserTemplate, testUpdateUser, testTlsExternalUser, testSendSimpleMessageTls, testReceiveSimpleMessageTlsScramSha, testThrottlingQuotasDeleteTopic, testReceiveSimpleMessageTls, testTopicModificationOfReplicationFactor, testSendingMessagesToNonExistingTopic, testMoreReplicasThanAvailableBrokers, testCreateTopicViaKafka] to and randomly select one to start execution
2022-03-28 19:02:46 [ForkJoinPool-1-worker-21] DEBUG [SuiteThreadController:93] [operators.topic.TopicST] - Removing parallel test: testMoreReplicasThanAvailableBrokers
2022-03-28 19:02:46 [ForkJoinPool-1-worker-21] DEBUG [SuiteThreadController:97] [operators.topic.TopicST] - Parallel test count: 15
2022-03-28 19:02:46 [ForkJoinPool-1-worker-21] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.topic.TopicST.testMoreReplicasThanAvailableBrokers-FINISHED
2022-03-28 19:02:46 [ForkJoinPool-1-worker-21] INFO  [TestSeparator:30] ############################################################################
2022-03-28 19:02:46 [ForkJoinPool-1-worker-21] INFO  [TestSeparator:23] ############################################################################
2022-03-28 19:02:46 [ForkJoinPool-1-worker-21] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.user.UserST.testUserWithNameMoreThan64Chars-STARTED
2022-03-28 19:02:46 [ForkJoinPool-1-worker-21] DEBUG [AbstractST:658] ============================================================================
2022-03-28 19:02:46 [ForkJoinPool-1-worker-21] DEBUG [AbstractST:659] [operators.user.UserST - Before Each] - Setup test case environment
2022-03-28 19:02:46 [ForkJoinPool-1-worker-21] DEBUG [SuiteThreadController:77] [operators.user.UserST] - Adding parallel test: testUserWithNameMoreThan64Chars
2022-03-28 19:02:46 [ForkJoinPool-1-worker-21] DEBUG [SuiteThreadController:81] [operators.user.UserST] - Parallel test count: 16
2022-03-28 19:02:46 [ForkJoinPool-1-worker-21] DEBUG [SuiteThreadController:205] [testUserWithNameMoreThan64Chars] moved to the WaitZone, because current thread exceed maximum of allowed test cases in parallel. (16/15)
2022-03-28 19:02:46 [ForkJoinPool-1-worker-21] TRACE [SuiteThreadController:210] testUserWithNameMoreThan64Chars is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (16/15)
2022-03-28 19:02:46 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (107842ms till timeout)
2022-03-28 19:02:46 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (77996ms till timeout)
2022-03-28 19:02:46 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (919856ms till timeout)
2022-03-28 19:02:46 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 19:02:46 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 19:02:46 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (478082ms till timeout)
2022-03-28 19:02:46 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:02:46 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (201908ms till timeout)
2022-03-28 19:02:46 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:02:46 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2278137ms till timeout)
2022-03-28 19:02:46 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (35494ms till timeout)
2022-03-28 19:02:46 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (923054ms till timeout)
2022-03-28 19:02:47 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 19:02:47 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (78545ms till timeout)
2022-03-28 19:02:47 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (106745ms till timeout)
2022-03-28 19:02:47 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (76892ms till timeout)
2022-03-28 19:02:47 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 19:02:47 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (918753ms till timeout)
2022-03-28 19:02:47 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 19:02:47 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 19:02:47 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Namespace namespace-9 removal not ready, will try again in 1000 ms (446148ms till timeout)
2022-03-28 19:02:47 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:02:47 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (200712ms till timeout)
2022-03-28 19:02:47 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:02:47 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2276942ms till timeout)
2022-03-28 19:02:47 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 19:02:47 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 19:02:47 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (476567ms till timeout)
2022-03-28 19:02:47 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (34303ms till timeout)
2022-03-28 19:02:48 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (921863ms till timeout)
2022-03-28 19:02:48 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (77445ms till timeout)
2022-03-28 19:02:48 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (105648ms till timeout)
2022-03-28 19:02:48 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 19:02:48 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (75785ms till timeout)
2022-03-28 19:02:48 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (917652ms till timeout)
2022-03-28 19:02:48 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929}
2022-03-28 19:02:48 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929, my-cluster-08515847-cruise-control-dcd7c895b-4pkw7=37920413-f627-4a97-919c-4ff0c818c13a}
2022-03-28 19:02:48 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929}
2022-03-28 19:02:48 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Deployment my-cluster-08515847-cruise-control rolling update in namespace:namespace-4 not ready, will try again in 5000 ms (499009ms till timeout)
2022-03-28 19:02:48 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:02:48 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 19:02:48 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:02:48 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2275824ms till timeout)
2022-03-28 19:02:49 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 19:02:49 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 19:02:49 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Namespace namespace-9 removal not ready, will try again in 1000 ms (444590ms till timeout)
2022-03-28 19:02:49 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (199404ms till timeout)
2022-03-28 19:02:49 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (33189ms till timeout)
2022-03-28 19:02:49 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (920750ms till timeout)
2022-03-28 19:02:49 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (76345ms till timeout)
2022-03-28 19:02:49 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 19:02:49 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 19:02:49 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (475105ms till timeout)
2022-03-28 19:02:49 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (104550ms till timeout)
2022-03-28 19:02:49 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-5dfd606f-kafka-0=a766ef9e-2663-4c21-ad83-6b7d9083122c, my-cluster-5dfd606f-kafka-1=57938984-c88e-45c5-b973-9f953a8e782c, my-cluster-5dfd606f-kafka-2=044285fe-7f53-4c1c-84e1-c51aec79b2ce}
2022-03-28 19:02:49 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (74680ms till timeout)
2022-03-28 19:02:49 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (916538ms till timeout)
2022-03-28 19:02:49 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-5dfd606f-kafka-0=d3be017c-9e19-4ee7-b0fc-58e8694c30b9, my-cluster-5dfd606f-kafka-1=1187ed10-515a-43c0-be8e-d43991c9d25d, my-cluster-5dfd606f-kafka-2=044285fe-7f53-4c1c-84e1-c51aec79b2ce}
2022-03-28 19:02:49 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-5dfd606f-kafka-0=d3be017c-9e19-4ee7-b0fc-58e8694c30b9, my-cluster-5dfd606f-kafka-1=1187ed10-515a-43c0-be8e-d43991c9d25d, my-cluster-5dfd606f-kafka-2=044285fe-7f53-4c1c-84e1-c51aec79b2ce}
2022-03-28 19:02:49 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:50] At least my-cluster-5dfd606f-kafka-2 hasn't rolled
2022-03-28 19:02:49 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] component with name my-cluster-5dfd606f-kafka rolling update not ready, will try again in 5000 ms (1717420ms till timeout)
2022-03-28 19:02:50 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 19:02:50 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:02:50 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2274722ms till timeout)
2022-03-28 19:02:50 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:02:50 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (32082ms till timeout)
2022-03-28 19:02:50 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (919648ms till timeout)
2022-03-28 19:02:50 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 19:02:50 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (198104ms till timeout)
2022-03-28 19:02:50 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (75235ms till timeout)
2022-03-28 19:02:50 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 19:02:50 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 19:02:50 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Namespace namespace-9 removal not ready, will try again in 1000 ms (443098ms till timeout)
2022-03-28 19:02:50 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (103452ms till timeout)
2022-03-28 19:02:50 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (73576ms till timeout)
2022-03-28 19:02:50 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (915437ms till timeout)
2022-03-28 19:02:50 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 19:02:50 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 19:02:50 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (473641ms till timeout)
2022-03-28 19:02:51 [ForkJoinPool-1-worker-21] DEBUG [SuiteThreadController:230] testUserWithNameMoreThan64Chars test now can proceed its execution
2022-03-28 19:02:51 [ForkJoinPool-1-worker-21] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 19:02:51 [ForkJoinPool-1-worker-21] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-abc5d34b, testThrottlingQuotasCreateAlterPartitions=my-cluster-ec1b34f8, testSendSimpleMessageTls=my-cluster-89673e85, testConfigurationReflection=my-cluster-188382d8, testUserWithNameMoreThan64Chars=my-cluster-2d5338bf, testDeployAndUnDeployCruiseControl=my-cluster-5dfd606f, testCapacityFile=my-cluster-855b9105, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-afe273c9, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-e6343cec, testTlsExternalUser=my-cluster-45cf8ce4, testSendSimpleMessageTlsScramSha=my-cluster-561d4fae, testCreateTopicAfterUnsupportedOperation=my-cluster-e8a83fb0, testTopicModificationOfReplicationFactor=my-cluster-15f5f09f, testKafkaAdminTopicOperations=my-cluster-506fda27, testConfigurationFileIsCreated=my-cluster-17fc585b, testCreateTopicViaKafka=my-cluster-75679697, testCreatingUsersWithSecretPrefix=my-cluster-f743fdfd, testUpdateUser=my-cluster-d582eb8c, testThrottlingQuotasDeleteTopic=my-cluster-836b31f8, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-959c6ed5, testTlsExternalUserWithQuotas=my-cluster-100f5461, testThrottlingQuotasCreateTopic=my-cluster-e9955717, testSendingMessagesToNonExistingTopic=my-cluster-a3c1c5ed, testConfigurationPerformanceOptions=my-cluster-08515847, testReceiveSimpleMessageTls=my-cluster-634e4d65, testUserTemplate=my-cluster-927d5a1b, testMoreReplicasThanAvailableBrokers=my-cluster-22050d08}
2022-03-28 19:02:51 [ForkJoinPool-1-worker-21] TRACE [AbstractST:607] USERS_NAME_MAP: {testReceiveSimpleMessageTlsScramSha=my-user-330562220-1155646423, testThrottlingQuotasCreateAlterPartitions=my-user-333986137-1152182125, testSendSimpleMessageTls=my-user-1887866274-1013125243, testConfigurationReflection=my-user-1890969183-1754686912, testUserWithNameMoreThan64Chars=my-user-1930755541-784294250, testDeployAndUnDeployCruiseControl=my-user-506268651-1309117986, testCapacityFile=my-user-1460643829-103298436, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-user-8456079-650483351, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-2069267102-581538560, testTlsExternalUser=my-user-399414674-1203409092, testSendSimpleMessageTlsScramSha=my-user-1601209876-518796379, testCreateTopicAfterUnsupportedOperation=my-user-1589831352-1271671319, testTopicModificationOfReplicationFactor=my-user-259690203-415132624, testKafkaAdminTopicOperations=my-user-1780746402-846362125, testConfigurationFileIsCreated=my-user-355815510-1316268151, testCreateTopicViaKafka=my-user-541976280-277947468, testCreatingUsersWithSecretPrefix=my-user-607975993-1210920737, testUpdateUser=my-user-1297536277-778093599, testThrottlingQuotasDeleteTopic=my-user-1567019138-1355600812, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-2104248162-45187051, testTlsExternalUserWithQuotas=my-user-1909583883-1106194424, testThrottlingQuotasCreateTopic=my-user-1876060546-1887905718, testSendingMessagesToNonExistingTopic=my-user-134768902-582007171, testConfigurationPerformanceOptions=my-user-98362545-1476587236, testReceiveSimpleMessageTls=my-user-337002809-1497514615, testUserTemplate=my-user-989633374-940717398, testMoreReplicasThanAvailableBrokers=my-user-713898339-1095128639}
2022-03-28 19:02:51 [ForkJoinPool-1-worker-21] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-topic-1027850608-734754781, testThrottlingQuotasCreateAlterPartitions=my-topic-325891726-169328137, testSendSimpleMessageTls=my-topic-874607937-87907051, testConfigurationReflection=my-topic-1394577332-1936226438, testUserWithNameMoreThan64Chars=my-topic-1542907219-2136330275, testDeployAndUnDeployCruiseControl=my-topic-465377127-800134460, testCapacityFile=my-topic-549312506-649708453, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-topic-1687496771-142091227, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-293775067-432059493, testTlsExternalUser=my-topic-707627084-257659651, testSendSimpleMessageTlsScramSha=my-topic-1586843669-332477842, testCreateTopicAfterUnsupportedOperation=my-topic-1619871970-423361410, testTopicModificationOfReplicationFactor=my-topic-1480509436-578706559, testKafkaAdminTopicOperations=my-topic-481231182-953607496, testConfigurationFileIsCreated=my-topic-423850169-855699454, testCreateTopicViaKafka=my-topic-1973235938-1730719386, testCreatingUsersWithSecretPrefix=my-topic-1727421070-668192795, testUpdateUser=my-topic-1325563869-1324403074, testThrottlingQuotasDeleteTopic=my-topic-1674611395-2048879468, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-634125597-1746493046, testTlsExternalUserWithQuotas=my-topic-1986059791-167638004, testThrottlingQuotasCreateTopic=my-topic-1246597695-228473879, testSendingMessagesToNonExistingTopic=my-topic-1692964628-1348460540, testConfigurationPerformanceOptions=my-topic-696588651-1198599652, testReceiveSimpleMessageTls=my-topic-829184226-1519275114, testUserTemplate=my-topic-1542379258-2002911578, testMoreReplicasThanAvailableBrokers=my-topic-840747042-63255423}
2022-03-28 19:02:51 [ForkJoinPool-1-worker-21] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-abc5d34b-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-ec1b34f8-kafka-clients, testSendSimpleMessageTls=my-cluster-89673e85-kafka-clients, testConfigurationReflection=my-cluster-188382d8-kafka-clients, testUserWithNameMoreThan64Chars=my-cluster-2d5338bf-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-5dfd606f-kafka-clients, testCapacityFile=my-cluster-855b9105-kafka-clients, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-afe273c9-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-e6343cec-kafka-clients, testTlsExternalUser=my-cluster-45cf8ce4-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-561d4fae-kafka-clients, testCreateTopicAfterUnsupportedOperation=my-cluster-e8a83fb0-kafka-clients, testTopicModificationOfReplicationFactor=my-cluster-15f5f09f-kafka-clients, testKafkaAdminTopicOperations=my-cluster-506fda27-kafka-clients, testConfigurationFileIsCreated=my-cluster-17fc585b-kafka-clients, testCreateTopicViaKafka=my-cluster-75679697-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-f743fdfd-kafka-clients, testUpdateUser=my-cluster-d582eb8c-kafka-clients, testThrottlingQuotasDeleteTopic=my-cluster-836b31f8-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-959c6ed5-kafka-clients, testTlsExternalUserWithQuotas=my-cluster-100f5461-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-e9955717-kafka-clients, testSendingMessagesToNonExistingTopic=my-cluster-a3c1c5ed-kafka-clients, testConfigurationPerformanceOptions=my-cluster-08515847-kafka-clients, testReceiveSimpleMessageTls=my-cluster-634e4d65-kafka-clients, testUserTemplate=my-cluster-927d5a1b-kafka-clients, testMoreReplicasThanAvailableBrokers=my-cluster-22050d08-kafka-clients}
2022-03-28 19:02:51 [ForkJoinPool-1-worker-21] INFO  [ResourceManager:155] Create/Update KafkaUser user-with-correct-nameabcdefghijklmnopqrstuvxyzabcdefghijklmnopq in namespace user-st
2022-03-28 19:02:51 [ForkJoinPool-1-worker-21] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:user-with-correct-nameabcdefghijklmnopqrstuvxyzabcdefghijklmnopq
2022-03-28 19:02:51 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:02:51 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2273537ms till timeout)
2022-03-28 19:02:51 [ForkJoinPool-1-worker-21] INFO  [ResourceManager:433] Wait for KafkaUser: user-with-correct-nameabcdefghijklmnopqrstuvxyzabcdefghijklmnopq will have desired state: Ready
2022-03-28 19:02:51 [ForkJoinPool-1-worker-21] DEBUG [TestUtils:125] Waiting for KafkaUser: user-with-correct-nameabcdefghijklmnopqrstuvxyzabcdefghijklmnopq will have desired state: Ready
2022-03-28 19:02:51 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (30895ms till timeout)
2022-03-28 19:02:51 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (918459ms till timeout)
2022-03-28 19:02:51 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 19:02:51 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:02:51 [ForkJoinPool-1-worker-21] INFO  [ResourceManager:444] KafkaUser: user-with-correct-nameabcdefghijklmnopqrstuvxyzabcdefghijklmnopq is in desired state: Ready
2022-03-28 19:02:51 [ForkJoinPool-1-worker-21] INFO  [KafkaUserUtils:90] Wait until KafkaUser user-with-correct-nameabcdefghijklmnopqrstuvxyzabcdefghijklmnopq status is available
2022-03-28 19:02:51 [ForkJoinPool-1-worker-21] DEBUG [TestUtils:125] Waiting for KafkaUser user-with-correct-nameabcdefghijklmnopqrstuvxyzabcdefghijklmnopq status is available
2022-03-28 19:02:51 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (74053ms till timeout)
2022-03-28 19:02:51 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (102344ms till timeout)
2022-03-28 19:02:51 [ForkJoinPool-1-worker-21] INFO  [KafkaUserUtils:95] KafkaUser user-with-correct-nameabcdefghijklmnopqrstuvxyzabcdefghijklmnopq status is available
2022-03-28 19:02:51 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (196732ms till timeout)
2022-03-28 19:02:51 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 19:02:51 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (72409ms till timeout)
2022-03-28 19:02:51 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (914271ms till timeout)
2022-03-28 19:02:51 [ForkJoinPool-1-worker-21] INFO  [ResourceManager:155] Create/Update KafkaUser sasl-userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdef in namespace user-st
2022-03-28 19:02:51 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 19:02:51 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 19:02:51 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Namespace namespace-9 removal not ready, will try again in 1000 ms (441637ms till timeout)
2022-03-28 19:02:52 [ForkJoinPool-1-worker-21] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:sasl-userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdef
2022-03-28 19:02:52 [ForkJoinPool-1-worker-21] INFO  [ResourceManager:433] Wait for KafkaUser: sasl-userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdef will have desired state: Ready
2022-03-28 19:02:52 [ForkJoinPool-1-worker-21] DEBUG [TestUtils:125] Waiting for KafkaUser: sasl-userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdef will have desired state: Ready
2022-03-28 19:02:52 [ForkJoinPool-1-worker-21] INFO  [ResourceManager:444] KafkaUser: sasl-userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdef is in desired state: Ready
2022-03-28 19:02:52 [ForkJoinPool-1-worker-21] INFO  [ResourceManager:155] Create/Update KafkaUser userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdefghijk in namespace user-st
2022-03-28 19:02:52 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 19:02:52 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 19:02:52 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (472194ms till timeout)
2022-03-28 19:02:52 [ForkJoinPool-1-worker-21] INFO  [KafkaUserUtils:90] Wait until KafkaUser userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdefghijk status is available
2022-03-28 19:02:52 [ForkJoinPool-1-worker-21] DEBUG [TestUtils:125] Waiting for KafkaUser userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdefghijk status is available
2022-03-28 19:02:52 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:02:52 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2272361ms till timeout)
2022-03-28 19:02:52 [ForkJoinPool-1-worker-21] INFO  [KafkaUserUtils:95] KafkaUser userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdefghijk status is available
2022-03-28 19:02:52 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (29722ms till timeout)
2022-03-28 19:02:52 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (917286ms till timeout)
2022-03-28 19:02:52 [ForkJoinPool-1-worker-21] DEBUG [AbstractST:674] ============================================================================
2022-03-28 19:02:52 [ForkJoinPool-1-worker-21] DEBUG [AbstractST:675] [operators.user.UserST - After Each] - Clean up after test
2022-03-28 19:02:52 [ForkJoinPool-1-worker-21] INFO  [ResourceManager:344] ############################################################################
2022-03-28 19:02:52 [ForkJoinPool-1-worker-21] INFO  [ResourceManager:348] Delete all resources for testUserWithNameMoreThan64Chars
2022-03-28 19:02:52 [ForkJoinPool-1-worker-21] INFO  [ResourceManager:241] Delete of KafkaUser sasl-userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdef in namespace user-st
2022-03-28 19:02:52 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (72882ms till timeout)
2022-03-28 19:02:52 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (101171ms till timeout)
2022-03-28 19:02:52 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:02:52 [ForkJoinPool-1-worker-21] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:sasl-userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdef
2022-03-28 19:02:52 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 19:02:53 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (71238ms till timeout)
2022-03-28 19:02:53 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (913100ms till timeout)
2022-03-28 19:02:53 [ForkJoinPool-1-worker-21] INFO  [ResourceManager:241] Delete of KafkaUser userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdefghijk in namespace user-st
2022-03-28 19:02:53 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (195367ms till timeout)
2022-03-28 19:02:53 [ForkJoinPool-1-worker-21] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdefghijk
2022-03-28 19:02:53 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 19:02:53 [ForkJoinPool-1-worker-21] INFO  [ResourceManager:241] Delete of KafkaUser user-with-correct-nameabcdefghijklmnopqrstuvxyzabcdefghijklmnopq in namespace user-st
2022-03-28 19:02:53 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 19:02:53 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 19:02:53 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Namespace namespace-9 removal not ready, will try again in 1000 ms (440212ms till timeout)
2022-03-28 19:02:53 [ForkJoinPool-1-worker-21] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:user-with-correct-nameabcdefghijklmnopqrstuvxyzabcdefghijklmnopq
2022-03-28 19:02:53 [ForkJoinPool-1-worker-21] INFO  [ResourceManager:369] ############################################################################
2022-03-28 19:02:53 [ForkJoinPool-1-worker-21] DEBUG [SuiteThreadController:267] testUserWithNameMoreThan64Chars - Notifies waiting test cases:[testPauseReconciliationInKafkaRebalanceAndTopic, testPauseReconciliationInKafkaAndKafkaConnectWithConnector, testCapacityFile, testConfigurationFileIsCreated, testConfigurationPerformanceOptions, testConfigurationReflection, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testDeployAndUnDeployCruiseControl, testTlsExternalUserWithQuotas, testCreatingUsersWithSecretPrefix, testThrottlingQuotasCreateTopic, testThrottlingQuotasCreateAlterPartitions, testKafkaAdminTopicOperations, testUserTemplate, testUpdateUser, testTlsExternalUser, testSendSimpleMessageTls, testReceiveSimpleMessageTlsScramSha, testThrottlingQuotasDeleteTopic, testReceiveSimpleMessageTls, testTopicModificationOfReplicationFactor, testSendingMessagesToNonExistingTopic, testMoreReplicasThanAvailableBrokers, testCreateTopicViaKafka] to and randomly select one to start execution
2022-03-28 19:02:53 [ForkJoinPool-1-worker-21] DEBUG [SuiteThreadController:93] [operators.user.UserST] - Removing parallel test: testUserWithNameMoreThan64Chars
2022-03-28 19:02:53 [ForkJoinPool-1-worker-21] DEBUG [SuiteThreadController:97] [operators.user.UserST] - Parallel test count: 15
2022-03-28 19:02:53 [ForkJoinPool-1-worker-21] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.user.UserST.testUserWithNameMoreThan64Chars-FINISHED
2022-03-28 19:02:53 [ForkJoinPool-1-worker-21] INFO  [TestSeparator:30] ############################################################################
2022-03-28 19:02:53 [ForkJoinPool-1-worker-21] INFO  [TestSeparator:23] ############################################################################
2022-03-28 19:02:53 [ForkJoinPool-1-worker-21] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.user.UserST.testTlsUserWithQuotas-STARTED
2022-03-28 19:02:53 [ForkJoinPool-1-worker-21] DEBUG [AbstractST:658] ============================================================================
2022-03-28 19:02:53 [ForkJoinPool-1-worker-21] DEBUG [AbstractST:659] [operators.user.UserST - Before Each] - Setup test case environment
2022-03-28 19:02:53 [ForkJoinPool-1-worker-21] DEBUG [SuiteThreadController:77] [operators.user.UserST] - Adding parallel test: testTlsUserWithQuotas
2022-03-28 19:02:53 [ForkJoinPool-1-worker-21] DEBUG [SuiteThreadController:81] [operators.user.UserST] - Parallel test count: 16
2022-03-28 19:02:53 [ForkJoinPool-1-worker-21] DEBUG [SuiteThreadController:205] [testTlsUserWithQuotas] moved to the WaitZone, because current thread exceed maximum of allowed test cases in parallel. (16/15)
2022-03-28 19:02:53 [ForkJoinPool-1-worker-21] TRACE [SuiteThreadController:210] testTlsUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (16/15)
2022-03-28 19:02:53 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:02:53 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2271205ms till timeout)
2022-03-28 19:02:53 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (28566ms till timeout)
2022-03-28 19:02:53 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 19:02:53 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 19:02:53 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (470739ms till timeout)
2022-03-28 19:02:53 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929}
2022-03-28 19:02:53 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (916128ms till timeout)
2022-03-28 19:02:53 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (71721ms till timeout)
2022-03-28 19:02:53 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (100009ms till timeout)
2022-03-28 19:02:54 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929, my-cluster-08515847-cruise-control-dcd7c895b-4pkw7=37920413-f627-4a97-919c-4ff0c818c13a}
2022-03-28 19:02:54 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929}
2022-03-28 19:02:54 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Deployment my-cluster-08515847-cruise-control rolling update in namespace:namespace-4 not ready, will try again in 5000 ms (493717ms till timeout)
2022-03-28 19:02:54 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (70070ms till timeout)
2022-03-28 19:02:54 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (911934ms till timeout)
2022-03-28 19:02:54 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:02:54 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (194106ms till timeout)
2022-03-28 19:02:54 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 19:02:54 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-5dfd606f-kafka-0=a766ef9e-2663-4c21-ad83-6b7d9083122c, my-cluster-5dfd606f-kafka-1=57938984-c88e-45c5-b973-9f953a8e782c, my-cluster-5dfd606f-kafka-2=044285fe-7f53-4c1c-84e1-c51aec79b2ce}
2022-03-28 19:02:54 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:02:54 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2270106ms till timeout)
2022-03-28 19:02:54 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 19:02:54 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-5dfd606f-kafka-0=d3be017c-9e19-4ee7-b0fc-58e8694c30b9, my-cluster-5dfd606f-kafka-1=1187ed10-515a-43c0-be8e-d43991c9d25d, my-cluster-5dfd606f-kafka-2=044285fe-7f53-4c1c-84e1-c51aec79b2ce}
2022-03-28 19:02:54 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-5dfd606f-kafka-0=d3be017c-9e19-4ee7-b0fc-58e8694c30b9, my-cluster-5dfd606f-kafka-1=1187ed10-515a-43c0-be8e-d43991c9d25d, my-cluster-5dfd606f-kafka-2=044285fe-7f53-4c1c-84e1-c51aec79b2ce}
2022-03-28 19:02:54 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:50] At least my-cluster-5dfd606f-kafka-2 hasn't rolled
2022-03-28 19:02:54 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] component with name my-cluster-5dfd606f-kafka rolling update not ready, will try again in 5000 ms (1712298ms till timeout)
2022-03-28 19:02:54 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 19:02:54 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 19:02:54 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Namespace namespace-9 removal not ready, will try again in 1000 ms (438765ms till timeout)
2022-03-28 19:02:54 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (915029ms till timeout)
2022-03-28 19:02:54 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (27373ms till timeout)
2022-03-28 19:02:54 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (70621ms till timeout)
2022-03-28 19:02:55 [ForkJoinPool-1-worker-5] INFO  [Exec:417] Command: oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-03-28 19:02:55 [ForkJoinPool-1-worker-5] INFO  [Exec:417] Return code: 0
2022-03-28 19:02:55 [ForkJoinPool-1-worker-5] INFO  [TopicST:121] Editing topic via Kafka, settings to partitions 5
2022-03-28 19:02:55 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --bootstrap-server localhost:9092 --alter --topic my-topic-1973235938-1730719386 --partitions 5
2022-03-28 19:02:55 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (98908ms till timeout)
2022-03-28 19:02:55 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 19:02:55 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 19:02:55 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (469281ms till timeout)
2022-03-28 19:02:55 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (68878ms till timeout)
2022-03-28 19:02:55 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (910830ms till timeout)
2022-03-28 19:02:55 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:02:55 [ForkJoinPool-1-worker-55] INFO  [ResourceManager:369] ############################################################################
2022-03-28 19:02:55 [ForkJoinPool-1-worker-55] DEBUG [SuiteThreadController:267] testSendingMessagesToNonExistingTopic - Notifies waiting test cases:[testPauseReconciliationInKafkaRebalanceAndTopic, testPauseReconciliationInKafkaAndKafkaConnectWithConnector, testCapacityFile, testConfigurationFileIsCreated, testConfigurationPerformanceOptions, testConfigurationReflection, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testDeployAndUnDeployCruiseControl, testTlsExternalUserWithQuotas, testCreatingUsersWithSecretPrefix, testThrottlingQuotasCreateTopic, testThrottlingQuotasCreateAlterPartitions, testKafkaAdminTopicOperations, testUserTemplate, testUpdateUser, testTlsExternalUser, testSendSimpleMessageTls, testReceiveSimpleMessageTlsScramSha, testThrottlingQuotasDeleteTopic, testReceiveSimpleMessageTls, testTopicModificationOfReplicationFactor, testSendingMessagesToNonExistingTopic, testMoreReplicasThanAvailableBrokers, testCreateTopicViaKafka, testTlsUserWithQuotas] to and randomly select one to start execution
2022-03-28 19:02:55 [ForkJoinPool-1-worker-55] DEBUG [SuiteThreadController:93] [operators.topic.TopicST] - Removing parallel test: testSendingMessagesToNonExistingTopic
2022-03-28 19:02:55 [ForkJoinPool-1-worker-55] DEBUG [SuiteThreadController:97] [operators.topic.TopicST] - Parallel test count: 15
2022-03-28 19:02:55 [ForkJoinPool-1-worker-55] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.topic.TopicST.testSendingMessagesToNonExistingTopic-FINISHED
2022-03-28 19:02:55 [ForkJoinPool-1-worker-55] INFO  [TestSeparator:30] ############################################################################
2022-03-28 19:02:55 [ForkJoinPool-1-worker-55] INFO  [TestSeparator:23] ############################################################################
2022-03-28 19:02:55 [ForkJoinPool-1-worker-55] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.user.UserST.testScramUserWithQuotas-STARTED
2022-03-28 19:02:55 [ForkJoinPool-1-worker-55] DEBUG [AbstractST:658] ============================================================================
2022-03-28 19:02:55 [ForkJoinPool-1-worker-55] DEBUG [AbstractST:659] [operators.user.UserST - Before Each] - Setup test case environment
2022-03-28 19:02:55 [ForkJoinPool-1-worker-55] DEBUG [SuiteThreadController:77] [operators.user.UserST] - Adding parallel test: testScramUserWithQuotas
2022-03-28 19:02:55 [ForkJoinPool-1-worker-55] DEBUG [SuiteThreadController:81] [operators.user.UserST] - Parallel test count: 16
2022-03-28 19:02:55 [ForkJoinPool-1-worker-55] DEBUG [SuiteThreadController:205] [testScramUserWithQuotas] moved to the WaitZone, because current thread exceed maximum of allowed test cases in parallel. (16/15)
2022-03-28 19:02:55 [ForkJoinPool-1-worker-55] TRACE [SuiteThreadController:210] testScramUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (16/15)
2022-03-28 19:02:55 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (192912ms till timeout)
2022-03-28 19:02:55 [ForkJoinPool-1-worker-23] INFO  [Exec:417] Command: oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-03-28 19:02:55 [ForkJoinPool-1-worker-23] INFO  [Exec:417] Return code: 0
2022-03-28 19:02:55 [ForkJoinPool-1-worker-23] INFO  [TopicST:461] Checking in KafkaTopic CR that topic topic-with-replication-to-change exists
2022-03-28 19:02:55 [ForkJoinPool-1-worker-23] INFO  [TopicST:456] Checking topic another-topic in Kafka
2022-03-28 19:02:55 [ForkJoinPool-1-worker-23] TRACE [Exec:248] Running command - oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-03-28 19:02:55 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:02:55 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2269008ms till timeout)
2022-03-28 19:02:55 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 19:02:55 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (913930ms till timeout)
2022-03-28 19:02:56 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (26179ms till timeout)
2022-03-28 19:02:56 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 19:02:56 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (69424ms till timeout)
2022-03-28 19:02:56 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 19:02:56 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 19:02:56 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Namespace namespace-9 removal not ready, will try again in 1000 ms (437337ms till timeout)
2022-03-28 19:02:56 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (97717ms till timeout)
2022-03-28 19:02:56 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (67777ms till timeout)
2022-03-28 19:02:56 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (909640ms till timeout)
2022-03-28 19:02:56 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 19:02:56 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 19:02:56 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (467845ms till timeout)
2022-03-28 19:02:56 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:02:56 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (191689ms till timeout)
2022-03-28 19:02:56 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:02:56 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2267910ms till timeout)
2022-03-28 19:02:57 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (912833ms till timeout)
2022-03-28 19:02:57 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (25079ms till timeout)
2022-03-28 19:02:57 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 19:02:57 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (68326ms till timeout)
2022-03-28 19:02:57 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (96613ms till timeout)
2022-03-28 19:02:57 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (66676ms till timeout)
2022-03-28 19:02:57 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 19:02:57 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (908539ms till timeout)
2022-03-28 19:02:57 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 19:02:57 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 19:02:57 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Namespace namespace-9 removal not ready, will try again in 1000 ms (435873ms till timeout)
2022-03-28 19:02:57 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:02:57 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (190497ms till timeout)
2022-03-28 19:02:58 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 19:02:58 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 19:02:58 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (466414ms till timeout)
2022-03-28 19:02:58 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:02:58 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2266727ms till timeout)
2022-03-28 19:02:58 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (911737ms till timeout)
2022-03-28 19:02:58 [ForkJoinPool-1-worker-35] INFO  [PodUtils:189] Message Successfully removed all 100 found in delete-admin-my-cluster-506fda27-kafka-clients-z8s75 log
2022-03-28 19:02:58 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (67226ms till timeout)
2022-03-28 19:02:58 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (95514ms till timeout)
2022-03-28 19:02:58 [ForkJoinPool-1-worker-35] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment delete-admin-my-cluster-506fda27-kafka-clients deletion
2022-03-28 19:02:58 [ForkJoinPool-1-worker-35] DEBUG [TestUtils:125] Waiting for ReplicaSet delete-admin-my-cluster-506fda27-kafka-clients to be deleted
2022-03-28 19:02:58 [ForkJoinPool-1-worker-21] DEBUG [SuiteThreadController:230] testTlsUserWithQuotas test now can proceed its execution
2022-03-28 19:02:58 [ForkJoinPool-1-worker-21] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 19:02:58 [ForkJoinPool-1-worker-21] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-abc5d34b, testThrottlingQuotasCreateAlterPartitions=my-cluster-ec1b34f8, testSendSimpleMessageTls=my-cluster-89673e85, testConfigurationReflection=my-cluster-188382d8, testUserWithNameMoreThan64Chars=my-cluster-2d5338bf, testDeployAndUnDeployCruiseControl=my-cluster-5dfd606f, testCapacityFile=my-cluster-855b9105, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-afe273c9, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-e6343cec, testTlsExternalUser=my-cluster-45cf8ce4, testSendSimpleMessageTlsScramSha=my-cluster-561d4fae, testCreateTopicAfterUnsupportedOperation=my-cluster-e8a83fb0, testTopicModificationOfReplicationFactor=my-cluster-15f5f09f, testKafkaAdminTopicOperations=my-cluster-506fda27, testConfigurationFileIsCreated=my-cluster-17fc585b, testCreateTopicViaKafka=my-cluster-75679697, testCreatingUsersWithSecretPrefix=my-cluster-f743fdfd, testUpdateUser=my-cluster-d582eb8c, testThrottlingQuotasDeleteTopic=my-cluster-836b31f8, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-959c6ed5, testTlsExternalUserWithQuotas=my-cluster-100f5461, testTlsUserWithQuotas=my-cluster-9bde73ea, testThrottlingQuotasCreateTopic=my-cluster-e9955717, testSendingMessagesToNonExistingTopic=my-cluster-a3c1c5ed, testConfigurationPerformanceOptions=my-cluster-08515847, testReceiveSimpleMessageTls=my-cluster-634e4d65, testUserTemplate=my-cluster-927d5a1b, testMoreReplicasThanAvailableBrokers=my-cluster-22050d08}
2022-03-28 19:02:58 [ForkJoinPool-1-worker-21] TRACE [AbstractST:607] USERS_NAME_MAP: {testReceiveSimpleMessageTlsScramSha=my-user-330562220-1155646423, testThrottlingQuotasCreateAlterPartitions=my-user-333986137-1152182125, testSendSimpleMessageTls=my-user-1887866274-1013125243, testConfigurationReflection=my-user-1890969183-1754686912, testUserWithNameMoreThan64Chars=my-user-1930755541-784294250, testDeployAndUnDeployCruiseControl=my-user-506268651-1309117986, testCapacityFile=my-user-1460643829-103298436, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-user-8456079-650483351, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-2069267102-581538560, testTlsExternalUser=my-user-399414674-1203409092, testSendSimpleMessageTlsScramSha=my-user-1601209876-518796379, testCreateTopicAfterUnsupportedOperation=my-user-1589831352-1271671319, testTopicModificationOfReplicationFactor=my-user-259690203-415132624, testKafkaAdminTopicOperations=my-user-1780746402-846362125, testConfigurationFileIsCreated=my-user-355815510-1316268151, testCreateTopicViaKafka=my-user-541976280-277947468, testCreatingUsersWithSecretPrefix=my-user-607975993-1210920737, testUpdateUser=my-user-1297536277-778093599, testThrottlingQuotasDeleteTopic=my-user-1567019138-1355600812, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-2104248162-45187051, testTlsExternalUserWithQuotas=my-user-1909583883-1106194424, testTlsUserWithQuotas=my-user-1496409761-1456339678, testThrottlingQuotasCreateTopic=my-user-1876060546-1887905718, testSendingMessagesToNonExistingTopic=my-user-134768902-582007171, testConfigurationPerformanceOptions=my-user-98362545-1476587236, testReceiveSimpleMessageTls=my-user-337002809-1497514615, testUserTemplate=my-user-989633374-940717398, testMoreReplicasThanAvailableBrokers=my-user-713898339-1095128639}
2022-03-28 19:02:58 [ForkJoinPool-1-worker-21] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-topic-1027850608-734754781, testThrottlingQuotasCreateAlterPartitions=my-topic-325891726-169328137, testSendSimpleMessageTls=my-topic-874607937-87907051, testConfigurationReflection=my-topic-1394577332-1936226438, testUserWithNameMoreThan64Chars=my-topic-1542907219-2136330275, testDeployAndUnDeployCruiseControl=my-topic-465377127-800134460, testCapacityFile=my-topic-549312506-649708453, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-topic-1687496771-142091227, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-293775067-432059493, testTlsExternalUser=my-topic-707627084-257659651, testSendSimpleMessageTlsScramSha=my-topic-1586843669-332477842, testCreateTopicAfterUnsupportedOperation=my-topic-1619871970-423361410, testTopicModificationOfReplicationFactor=my-topic-1480509436-578706559, testKafkaAdminTopicOperations=my-topic-481231182-953607496, testConfigurationFileIsCreated=my-topic-423850169-855699454, testCreateTopicViaKafka=my-topic-1973235938-1730719386, testCreatingUsersWithSecretPrefix=my-topic-1727421070-668192795, testUpdateUser=my-topic-1325563869-1324403074, testThrottlingQuotasDeleteTopic=my-topic-1674611395-2048879468, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-634125597-1746493046, testTlsExternalUserWithQuotas=my-topic-1986059791-167638004, testTlsUserWithQuotas=my-topic-2040479778-983803186, testThrottlingQuotasCreateTopic=my-topic-1246597695-228473879, testSendingMessagesToNonExistingTopic=my-topic-1692964628-1348460540, testConfigurationPerformanceOptions=my-topic-696588651-1198599652, testReceiveSimpleMessageTls=my-topic-829184226-1519275114, testUserTemplate=my-topic-1542379258-2002911578, testMoreReplicasThanAvailableBrokers=my-topic-840747042-63255423}
2022-03-28 19:02:58 [ForkJoinPool-1-worker-21] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-abc5d34b-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-ec1b34f8-kafka-clients, testSendSimpleMessageTls=my-cluster-89673e85-kafka-clients, testConfigurationReflection=my-cluster-188382d8-kafka-clients, testUserWithNameMoreThan64Chars=my-cluster-2d5338bf-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-5dfd606f-kafka-clients, testCapacityFile=my-cluster-855b9105-kafka-clients, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-afe273c9-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-e6343cec-kafka-clients, testTlsExternalUser=my-cluster-45cf8ce4-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-561d4fae-kafka-clients, testCreateTopicAfterUnsupportedOperation=my-cluster-e8a83fb0-kafka-clients, testTopicModificationOfReplicationFactor=my-cluster-15f5f09f-kafka-clients, testKafkaAdminTopicOperations=my-cluster-506fda27-kafka-clients, testConfigurationFileIsCreated=my-cluster-17fc585b-kafka-clients, testCreateTopicViaKafka=my-cluster-75679697-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-f743fdfd-kafka-clients, testUpdateUser=my-cluster-d582eb8c-kafka-clients, testThrottlingQuotasDeleteTopic=my-cluster-836b31f8-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-959c6ed5-kafka-clients, testTlsExternalUserWithQuotas=my-cluster-100f5461-kafka-clients, testTlsUserWithQuotas=my-cluster-9bde73ea-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-e9955717-kafka-clients, testSendingMessagesToNonExistingTopic=my-cluster-a3c1c5ed-kafka-clients, testConfigurationPerformanceOptions=my-cluster-08515847-kafka-clients, testReceiveSimpleMessageTls=my-cluster-634e4d65-kafka-clients, testUserTemplate=my-cluster-927d5a1b-kafka-clients, testMoreReplicasThanAvailableBrokers=my-cluster-22050d08-kafka-clients}
2022-03-28 19:02:58 [ForkJoinPool-1-worker-21] INFO  [ResourceManager:155] Create/Update KafkaUser encrypted-arnost in namespace user-st
2022-03-28 19:02:58 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] ReplicaSet delete-admin-my-cluster-506fda27-kafka-clients to be deleted not ready, will try again in 5000 ms (179904ms till timeout)
2022-03-28 19:02:58 [ForkJoinPool-1-worker-21] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:encrypted-arnost
2022-03-28 19:02:58 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 19:02:58 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (907439ms till timeout)
2022-03-28 19:02:58 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (65479ms till timeout)
2022-03-28 19:02:58 [ForkJoinPool-1-worker-21] INFO  [ResourceManager:433] Wait for KafkaUser: encrypted-arnost will have desired state: Ready
2022-03-28 19:02:58 [ForkJoinPool-1-worker-21] DEBUG [TestUtils:125] Waiting for KafkaUser: encrypted-arnost will have desired state: Ready
2022-03-28 19:02:58 [ForkJoinPool-1-worker-21] INFO  [ResourceManager:444] KafkaUser: encrypted-arnost is in desired state: Ready
2022-03-28 19:02:59 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 19:02:59 [ForkJoinPool-1-worker-21] DEBUG [UserST:274] Command for kafka-configs.sh bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user CN=encrypted-arnost
2022-03-28 19:02:59 [ForkJoinPool-1-worker-21] TRACE [Exec:248] Running command - oc --namespace user-st exec user-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user CN=encrypted-arnost
2022-03-28 19:02:59 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929}
2022-03-28 19:02:59 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:02:59 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 19:02:59 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 19:02:59 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Namespace namespace-9 removal not ready, will try again in 1000 ms (434440ms till timeout)
2022-03-28 19:02:59 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:02:59 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2265554ms till timeout)
2022-03-28 19:02:59 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (189136ms till timeout)
2022-03-28 19:02:59 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (910575ms till timeout)
2022-03-28 19:02:59 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929, my-cluster-08515847-cruise-control-dcd7c895b-4pkw7=37920413-f627-4a97-919c-4ff0c818c13a}
2022-03-28 19:02:59 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929}
2022-03-28 19:02:59 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Deployment my-cluster-08515847-cruise-control rolling update in namespace:namespace-4 not ready, will try again in 5000 ms (488361ms till timeout)
2022-03-28 19:02:59 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 19:02:59 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 19:02:59 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (464957ms till timeout)
2022-03-28 19:02:59 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (66075ms till timeout)
2022-03-28 19:02:59 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (94363ms till timeout)
2022-03-28 19:02:59 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-5dfd606f-kafka-0=a766ef9e-2663-4c21-ad83-6b7d9083122c, my-cluster-5dfd606f-kafka-1=57938984-c88e-45c5-b973-9f953a8e782c, my-cluster-5dfd606f-kafka-2=044285fe-7f53-4c1c-84e1-c51aec79b2ce}
2022-03-28 19:02:59 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (906342ms till timeout)
2022-03-28 19:02:59 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-5dfd606f-kafka-0=d3be017c-9e19-4ee7-b0fc-58e8694c30b9, my-cluster-5dfd606f-kafka-1=1187ed10-515a-43c0-be8e-d43991c9d25d, my-cluster-5dfd606f-kafka-2=044285fe-7f53-4c1c-84e1-c51aec79b2ce}
2022-03-28 19:02:59 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-5dfd606f-kafka-0=d3be017c-9e19-4ee7-b0fc-58e8694c30b9, my-cluster-5dfd606f-kafka-1=1187ed10-515a-43c0-be8e-d43991c9d25d, my-cluster-5dfd606f-kafka-2=044285fe-7f53-4c1c-84e1-c51aec79b2ce}
2022-03-28 19:02:59 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:50] At least my-cluster-5dfd606f-kafka-2 hasn't rolled
2022-03-28 19:02:59 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] component with name my-cluster-5dfd606f-kafka rolling update not ready, will try again in 5000 ms (1707130ms till timeout)
2022-03-28 19:02:59 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (64286ms till timeout)
2022-03-28 19:03:00 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 19:03:00 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:03:00 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2264456ms till timeout)
2022-03-28 19:03:00 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:03:00 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (909476ms till timeout)
2022-03-28 19:03:00 [ForkJoinPool-1-worker-55] TRACE [SuiteThreadController:210] testScramUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (16/15)
2022-03-28 19:03:00 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 19:03:00 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (187939ms till timeout)
2022-03-28 19:03:00 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 19:03:00 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 1
2022-03-28 19:03:00 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 19:03:00 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Error from server (NotFound): namespaces "namespace-9" not found
2022-03-28 19:03:00 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] ======STDERR END======
2022-03-28 19:03:00 [ForkJoinPool-1-worker-1] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[namespace-8], io.strimzi.test.logs.CollectorElement@c4c0ea0=[], io.strimzi.test.logs.CollectorElement@aec142ef=[namespace-5], io.strimzi.test.logs.CollectorElement@24b97ba9=[namespace-7], io.strimzi.test.logs.CollectorElement@f851b6c3=[namespace-4], io.strimzi.test.logs.CollectorElement@3881d5f2=[namespace-3], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@5c7379cb=[], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[namespace-0], io.strimzi.test.logs.CollectorElement@f4405b3b=[namespace-2], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 19:03:00 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:267] testTlsExternalUser - Notifies waiting test cases:[testPauseReconciliationInKafkaRebalanceAndTopic, testPauseReconciliationInKafkaAndKafkaConnectWithConnector, testCapacityFile, testConfigurationFileIsCreated, testConfigurationPerformanceOptions, testConfigurationReflection, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testDeployAndUnDeployCruiseControl, testTlsExternalUserWithQuotas, testCreatingUsersWithSecretPrefix, testThrottlingQuotasCreateTopic, testThrottlingQuotasCreateAlterPartitions, testKafkaAdminTopicOperations, testUserTemplate, testUpdateUser, testTlsExternalUser, testSendSimpleMessageTls, testReceiveSimpleMessageTlsScramSha, testThrottlingQuotasDeleteTopic, testReceiveSimpleMessageTls, testTopicModificationOfReplicationFactor, testSendingMessagesToNonExistingTopic, testMoreReplicasThanAvailableBrokers, testCreateTopicViaKafka, testScramUserWithQuotas] to and randomly select one to start execution
2022-03-28 19:03:00 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:93] [operators.user.UserST] - Removing parallel test: testTlsExternalUser
2022-03-28 19:03:00 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:97] [operators.user.UserST] - Parallel test count: 15
2022-03-28 19:03:00 [ForkJoinPool-1-worker-1] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.user.UserST.testTlsExternalUser-FINISHED
2022-03-28 19:03:00 [ForkJoinPool-1-worker-1] INFO  [TestSeparator:30] ############################################################################
2022-03-28 19:03:00 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (64975ms till timeout)
2022-03-28 19:03:00 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (93263ms till timeout)
2022-03-28 19:03:00 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 19:03:00 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 19:03:00 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (463538ms till timeout)
2022-03-28 19:03:00 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (905246ms till timeout)
2022-03-28 19:03:01 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (63180ms till timeout)
2022-03-28 19:03:01 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:03:01 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2263357ms till timeout)
2022-03-28 19:03:01 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (908378ms till timeout)
2022-03-28 19:03:01 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:03:01 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (186747ms till timeout)
2022-03-28 19:03:01 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (63781ms till timeout)
2022-03-28 19:03:01 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (92068ms till timeout)
2022-03-28 19:03:01 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 19:03:02 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (904150ms till timeout)
2022-03-28 19:03:02 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (61988ms till timeout)
2022-03-28 19:03:02 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 19:03:02 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 19:03:02 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (462091ms till timeout)
2022-03-28 19:03:02 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:03:02 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2262253ms till timeout)
2022-03-28 19:03:02 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (907272ms till timeout)
2022-03-28 19:03:02 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:03:02 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (185555ms till timeout)
2022-03-28 19:03:03 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (62591ms till timeout)
2022-03-28 19:03:03 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (90878ms till timeout)
2022-03-28 19:03:03 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (902998ms till timeout)
2022-03-28 19:03:03 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (60886ms till timeout)
2022-03-28 19:03:03 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 19:03:03 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:03:03 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2261154ms till timeout)
2022-03-28 19:03:03 [ForkJoinPool-1-worker-35] DEBUG [JobUtils:40] Job delete-admin-my-cluster-506fda27-kafka-clients was deleted
2022-03-28 19:03:03 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (906175ms till timeout)
2022-03-28 19:03:03 [ForkJoinPool-1-worker-35] INFO  [ResourceManager:155] Create/Update Job list-admin-my-cluster-506fda27-kafka-clients in namespace throttling-quota-st
2022-03-28 19:03:03 [ForkJoinPool-1-worker-35] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:list-admin-my-cluster-506fda27-kafka-clients
2022-03-28 19:03:03 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 19:03:03 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 19:03:03 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (460579ms till timeout)
2022-03-28 19:03:03 [ForkJoinPool-1-worker-35] INFO  [JobUtils:81] Waiting for job: list-admin-my-cluster-506fda27-kafka-clients will be in active state
2022-03-28 19:03:03 [ForkJoinPool-1-worker-35] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 19:03:04 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:03:04 [ForkJoinPool-1-worker-35] INFO  [ClientUtils:76] Waiting for producer/consumer:list-admin-my-cluster-506fda27-kafka-clients to finished
2022-03-28 19:03:04 [ForkJoinPool-1-worker-35] DEBUG [TestUtils:125] Waiting for job finished
2022-03-28 19:03:04 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (61474ms till timeout)
2022-03-28 19:03:04 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (184249ms till timeout)
2022-03-28 19:03:04 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (89764ms till timeout)
2022-03-28 19:03:04 [ForkJoinPool-1-worker-35] DEBUG [ClientUtils:79] Job list-admin-my-cluster-506fda27-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:03:04 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (901884ms till timeout)
2022-03-28 19:03:04 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (119716ms till timeout)
2022-03-28 19:03:04 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929}
2022-03-28 19:03:04 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (59734ms till timeout)
2022-03-28 19:03:04 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929, my-cluster-08515847-cruise-control-dcd7c895b-4pkw7=37920413-f627-4a97-919c-4ff0c818c13a}
2022-03-28 19:03:04 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929}
2022-03-28 19:03:04 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Deployment my-cluster-08515847-cruise-control rolling update in namespace:namespace-4 not ready, will try again in 5000 ms (483089ms till timeout)
2022-03-28 19:03:04 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:03:04 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2259998ms till timeout)
2022-03-28 19:03:04 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 19:03:04 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (905018ms till timeout)
2022-03-28 19:03:04 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-5dfd606f-kafka-0=a766ef9e-2663-4c21-ad83-6b7d9083122c, my-cluster-5dfd606f-kafka-1=57938984-c88e-45c5-b973-9f953a8e782c, my-cluster-5dfd606f-kafka-2=044285fe-7f53-4c1c-84e1-c51aec79b2ce}
2022-03-28 19:03:05 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-5dfd606f-kafka-0=d3be017c-9e19-4ee7-b0fc-58e8694c30b9, my-cluster-5dfd606f-kafka-1=1187ed10-515a-43c0-be8e-d43991c9d25d, my-cluster-5dfd606f-kafka-2=044285fe-7f53-4c1c-84e1-c51aec79b2ce}
2022-03-28 19:03:05 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-5dfd606f-kafka-0=d3be017c-9e19-4ee7-b0fc-58e8694c30b9, my-cluster-5dfd606f-kafka-1=1187ed10-515a-43c0-be8e-d43991c9d25d, my-cluster-5dfd606f-kafka-2=044285fe-7f53-4c1c-84e1-c51aec79b2ce}
2022-03-28 19:03:05 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:50] At least my-cluster-5dfd606f-kafka-2 hasn't rolled
2022-03-28 19:03:05 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] component with name my-cluster-5dfd606f-kafka rolling update not ready, will try again in 5000 ms (1702031ms till timeout)
2022-03-28 19:03:05 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (60373ms till timeout)
2022-03-28 19:03:05 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (88660ms till timeout)
2022-03-28 19:03:05 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:03:05 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (900778ms till timeout)
2022-03-28 19:03:05 [ForkJoinPool-1-worker-55] DEBUG [SuiteThreadController:230] testScramUserWithQuotas test now can proceed its execution
2022-03-28 19:03:05 [ForkJoinPool-1-worker-55] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 19:03:05 [ForkJoinPool-1-worker-55] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-abc5d34b, testThrottlingQuotasCreateAlterPartitions=my-cluster-ec1b34f8, testSendSimpleMessageTls=my-cluster-89673e85, testConfigurationReflection=my-cluster-188382d8, testUserWithNameMoreThan64Chars=my-cluster-2d5338bf, testDeployAndUnDeployCruiseControl=my-cluster-5dfd606f, testCapacityFile=my-cluster-855b9105, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-afe273c9, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-e6343cec, testTlsExternalUser=my-cluster-45cf8ce4, testSendSimpleMessageTlsScramSha=my-cluster-561d4fae, testCreateTopicAfterUnsupportedOperation=my-cluster-e8a83fb0, testScramUserWithQuotas=my-cluster-ab753b77, testTopicModificationOfReplicationFactor=my-cluster-15f5f09f, testKafkaAdminTopicOperations=my-cluster-506fda27, testConfigurationFileIsCreated=my-cluster-17fc585b, testCreateTopicViaKafka=my-cluster-75679697, testCreatingUsersWithSecretPrefix=my-cluster-f743fdfd, testUpdateUser=my-cluster-d582eb8c, testThrottlingQuotasDeleteTopic=my-cluster-836b31f8, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-959c6ed5, testTlsExternalUserWithQuotas=my-cluster-100f5461, testTlsUserWithQuotas=my-cluster-9bde73ea, testThrottlingQuotasCreateTopic=my-cluster-e9955717, testSendingMessagesToNonExistingTopic=my-cluster-a3c1c5ed, testConfigurationPerformanceOptions=my-cluster-08515847, testReceiveSimpleMessageTls=my-cluster-634e4d65, testUserTemplate=my-cluster-927d5a1b, testMoreReplicasThanAvailableBrokers=my-cluster-22050d08}
2022-03-28 19:03:05 [ForkJoinPool-1-worker-55] TRACE [AbstractST:607] USERS_NAME_MAP: {testReceiveSimpleMessageTlsScramSha=my-user-330562220-1155646423, testThrottlingQuotasCreateAlterPartitions=my-user-333986137-1152182125, testSendSimpleMessageTls=my-user-1887866274-1013125243, testConfigurationReflection=my-user-1890969183-1754686912, testUserWithNameMoreThan64Chars=my-user-1930755541-784294250, testDeployAndUnDeployCruiseControl=my-user-506268651-1309117986, testCapacityFile=my-user-1460643829-103298436, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-user-8456079-650483351, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-2069267102-581538560, testTlsExternalUser=my-user-399414674-1203409092, testSendSimpleMessageTlsScramSha=my-user-1601209876-518796379, testCreateTopicAfterUnsupportedOperation=my-user-1589831352-1271671319, testScramUserWithQuotas=my-user-941123953-266890403, testTopicModificationOfReplicationFactor=my-user-259690203-415132624, testKafkaAdminTopicOperations=my-user-1780746402-846362125, testConfigurationFileIsCreated=my-user-355815510-1316268151, testCreateTopicViaKafka=my-user-541976280-277947468, testCreatingUsersWithSecretPrefix=my-user-607975993-1210920737, testUpdateUser=my-user-1297536277-778093599, testThrottlingQuotasDeleteTopic=my-user-1567019138-1355600812, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-2104248162-45187051, testTlsExternalUserWithQuotas=my-user-1909583883-1106194424, testTlsUserWithQuotas=my-user-1496409761-1456339678, testThrottlingQuotasCreateTopic=my-user-1876060546-1887905718, testSendingMessagesToNonExistingTopic=my-user-134768902-582007171, testConfigurationPerformanceOptions=my-user-98362545-1476587236, testReceiveSimpleMessageTls=my-user-337002809-1497514615, testUserTemplate=my-user-989633374-940717398, testMoreReplicasThanAvailableBrokers=my-user-713898339-1095128639}
2022-03-28 19:03:05 [ForkJoinPool-1-worker-55] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-topic-1027850608-734754781, testThrottlingQuotasCreateAlterPartitions=my-topic-325891726-169328137, testSendSimpleMessageTls=my-topic-874607937-87907051, testConfigurationReflection=my-topic-1394577332-1936226438, testUserWithNameMoreThan64Chars=my-topic-1542907219-2136330275, testDeployAndUnDeployCruiseControl=my-topic-465377127-800134460, testCapacityFile=my-topic-549312506-649708453, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-topic-1687496771-142091227, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-293775067-432059493, testTlsExternalUser=my-topic-707627084-257659651, testSendSimpleMessageTlsScramSha=my-topic-1586843669-332477842, testCreateTopicAfterUnsupportedOperation=my-topic-1619871970-423361410, testScramUserWithQuotas=my-topic-644624843-1738170872, testTopicModificationOfReplicationFactor=my-topic-1480509436-578706559, testKafkaAdminTopicOperations=my-topic-481231182-953607496, testConfigurationFileIsCreated=my-topic-423850169-855699454, testCreateTopicViaKafka=my-topic-1973235938-1730719386, testCreatingUsersWithSecretPrefix=my-topic-1727421070-668192795, testUpdateUser=my-topic-1325563869-1324403074, testThrottlingQuotasDeleteTopic=my-topic-1674611395-2048879468, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-634125597-1746493046, testTlsExternalUserWithQuotas=my-topic-1986059791-167638004, testTlsUserWithQuotas=my-topic-2040479778-983803186, testThrottlingQuotasCreateTopic=my-topic-1246597695-228473879, testSendingMessagesToNonExistingTopic=my-topic-1692964628-1348460540, testConfigurationPerformanceOptions=my-topic-696588651-1198599652, testReceiveSimpleMessageTls=my-topic-829184226-1519275114, testUserTemplate=my-topic-1542379258-2002911578, testMoreReplicasThanAvailableBrokers=my-topic-840747042-63255423}
2022-03-28 19:03:05 [ForkJoinPool-1-worker-55] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-abc5d34b-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-ec1b34f8-kafka-clients, testSendSimpleMessageTls=my-cluster-89673e85-kafka-clients, testConfigurationReflection=my-cluster-188382d8-kafka-clients, testUserWithNameMoreThan64Chars=my-cluster-2d5338bf-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-5dfd606f-kafka-clients, testCapacityFile=my-cluster-855b9105-kafka-clients, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-afe273c9-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-e6343cec-kafka-clients, testTlsExternalUser=my-cluster-45cf8ce4-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-561d4fae-kafka-clients, testCreateTopicAfterUnsupportedOperation=my-cluster-e8a83fb0-kafka-clients, testScramUserWithQuotas=my-cluster-ab753b77-kafka-clients, testTopicModificationOfReplicationFactor=my-cluster-15f5f09f-kafka-clients, testKafkaAdminTopicOperations=my-cluster-506fda27-kafka-clients, testConfigurationFileIsCreated=my-cluster-17fc585b-kafka-clients, testCreateTopicViaKafka=my-cluster-75679697-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-f743fdfd-kafka-clients, testUpdateUser=my-cluster-d582eb8c-kafka-clients, testThrottlingQuotasDeleteTopic=my-cluster-836b31f8-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-959c6ed5-kafka-clients, testTlsExternalUserWithQuotas=my-cluster-100f5461-kafka-clients, testTlsUserWithQuotas=my-cluster-9bde73ea-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-e9955717-kafka-clients, testSendingMessagesToNonExistingTopic=my-cluster-a3c1c5ed-kafka-clients, testConfigurationPerformanceOptions=my-cluster-08515847-kafka-clients, testReceiveSimpleMessageTls=my-cluster-634e4d65-kafka-clients, testUserTemplate=my-cluster-927d5a1b-kafka-clients, testMoreReplicasThanAvailableBrokers=my-cluster-22050d08-kafka-clients}
2022-03-28 19:03:05 [ForkJoinPool-1-worker-35] DEBUG [ClientUtils:79] Job list-admin-my-cluster-506fda27-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:03:05 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (182952ms till timeout)
2022-03-28 19:03:05 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (58615ms till timeout)
2022-03-28 19:03:05 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (118416ms till timeout)
2022-03-28 19:03:05 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:03:05 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2258900ms till timeout)
2022-03-28 19:03:05 [ForkJoinPool-1-worker-21] INFO  [Exec:417] Command: oc --namespace user-st exec user-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user CN=encrypted-arnost
2022-03-28 19:03:05 [ForkJoinPool-1-worker-21] INFO  [Exec:417] Return code: 0
2022-03-28 19:03:06 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (903911ms till timeout)
2022-03-28 19:03:06 [ForkJoinPool-1-worker-21] INFO  [KafkaUserUtils:62] Waiting for KafkaUser deletion encrypted-arnost
2022-03-28 19:03:06 [ForkJoinPool-1-worker-21] DEBUG [TestUtils:125] Waiting for KafkaUser deletion encrypted-arnost
2022-03-28 19:03:06 [ForkJoinPool-1-worker-21] INFO  [KafkaUserUtils:75] KafkaUser encrypted-arnost deleted
2022-03-28 19:03:06 [ForkJoinPool-1-worker-21] DEBUG [TestUtils:125] Waiting for all KafkaUser CN=encrypted-arnost attributes will be cleaned
2022-03-28 19:03:06 [ForkJoinPool-1-worker-21] TRACE [Exec:248] Running command - oc --namespace user-st exec user-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user CN=encrypted-arnost
2022-03-28 19:03:06 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (59271ms till timeout)
2022-03-28 19:03:06 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (87558ms till timeout)
2022-03-28 19:03:06 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (899678ms till timeout)
2022-03-28 19:03:06 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:03:06 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (181756ms till timeout)
2022-03-28 19:03:06 [ForkJoinPool-1-worker-35] DEBUG [ClientUtils:79] Job list-admin-my-cluster-506fda27-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:03:06 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (57427ms till timeout)
2022-03-28 19:03:06 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (117221ms till timeout)
2022-03-28 19:03:07 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:03:07 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2257792ms till timeout)
2022-03-28 19:03:07 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (902814ms till timeout)
2022-03-28 19:03:07 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (58170ms till timeout)
2022-03-28 19:03:07 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (86458ms till timeout)
2022-03-28 19:03:07 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (898578ms till timeout)
2022-03-28 19:03:07 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:03:07 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (180564ms till timeout)
2022-03-28 19:03:08 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (56148ms till timeout)
2022-03-28 19:03:08 [ForkJoinPool-1-worker-35] DEBUG [ClientUtils:79] Job list-admin-my-cluster-506fda27-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:03:08 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:03:08 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2256601ms till timeout)
2022-03-28 19:03:08 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (115838ms till timeout)
2022-03-28 19:03:08 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (901621ms till timeout)
2022-03-28 19:03:08 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (57070ms till timeout)
2022-03-28 19:03:08 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (85341ms till timeout)
2022-03-28 19:03:08 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (897460ms till timeout)
2022-03-28 19:03:09 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:03:09 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (179372ms till timeout)
2022-03-28 19:03:09 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (55046ms till timeout)
2022-03-28 19:03:09 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:03:09 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2255494ms till timeout)
2022-03-28 19:03:09 [ForkJoinPool-1-worker-5] INFO  [Exec:417] Command: oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --bootstrap-server localhost:9092 --alter --topic my-topic-1973235938-1730719386 --partitions 5
2022-03-28 19:03:09 [ForkJoinPool-1-worker-5] INFO  [Exec:417] Return code: 0
2022-03-28 19:03:09 [ForkJoinPool-1-worker-5] DEBUG [TopicST:124] Topic my-topic-1973235938-1730719386 updated from 3 to 5 partitions
2022-03-28 19:03:09 [ForkJoinPool-1-worker-5] INFO  [KafkaTopicUtils:124] Waiting for KafkaTopic change my-topic-1973235938-1730719386
2022-03-28 19:03:09 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for KafkaTopic change my-topic-1973235938-1730719386
2022-03-28 19:03:09 [ForkJoinPool-1-worker-35] DEBUG [ClientUtils:79] Job list-admin-my-cluster-506fda27-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:03:09 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (900513ms till timeout)
2022-03-28 19:03:09 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Describing topic my-topic-1973235938-1730719386 using pod CLI
2022-03-28 19:03:09 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --bootstrap-server localhost:9092 --describe --topic my-topic-1973235938-1730719386
2022-03-28 19:03:09 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (114514ms till timeout)
2022-03-28 19:03:09 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929}
2022-03-28 19:03:09 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (55868ms till timeout)
2022-03-28 19:03:09 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (84155ms till timeout)
2022-03-28 19:03:09 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (896276ms till timeout)
2022-03-28 19:03:09 [ForkJoinPool-1-worker-23] INFO  [Exec:417] Command: oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-03-28 19:03:09 [ForkJoinPool-1-worker-23] INFO  [Exec:417] Return code: 0
2022-03-28 19:03:09 [ForkJoinPool-1-worker-23] INFO  [TopicST:461] Checking in KafkaTopic CR that topic another-topic exists
2022-03-28 19:03:09 [ForkJoinPool-1-worker-23] TRACE [Exec:248] Running command - oc --namespace topic-st delete kafkatopic topic-with-replication-to-change
2022-03-28 19:03:10 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929, my-cluster-08515847-cruise-control-dcd7c895b-4pkw7=37920413-f627-4a97-919c-4ff0c818c13a}
2022-03-28 19:03:10 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929}
2022-03-28 19:03:10 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Deployment my-cluster-08515847-cruise-control rolling update in namespace:namespace-4 not ready, will try again in 5000 ms (477770ms till timeout)
2022-03-28 19:03:10 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-5dfd606f-kafka-0=a766ef9e-2663-4c21-ad83-6b7d9083122c, my-cluster-5dfd606f-kafka-1=57938984-c88e-45c5-b973-9f953a8e782c, my-cluster-5dfd606f-kafka-2=044285fe-7f53-4c1c-84e1-c51aec79b2ce}
2022-03-28 19:03:10 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-5dfd606f-kafka-0=d3be017c-9e19-4ee7-b0fc-58e8694c30b9, my-cluster-5dfd606f-kafka-1=1187ed10-515a-43c0-be8e-d43991c9d25d, my-cluster-5dfd606f-kafka-2=044285fe-7f53-4c1c-84e1-c51aec79b2ce}
2022-03-28 19:03:10 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-5dfd606f-kafka-0=d3be017c-9e19-4ee7-b0fc-58e8694c30b9, my-cluster-5dfd606f-kafka-1=1187ed10-515a-43c0-be8e-d43991c9d25d, my-cluster-5dfd606f-kafka-2=044285fe-7f53-4c1c-84e1-c51aec79b2ce}
2022-03-28 19:03:10 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:50] At least my-cluster-5dfd606f-kafka-2 hasn't rolled
2022-03-28 19:03:10 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] component with name my-cluster-5dfd606f-kafka rolling update not ready, will try again in 5000 ms (1696932ms till timeout)
2022-03-28 19:03:10 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:03:10 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (53898ms till timeout)
2022-03-28 19:03:10 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 19:03:10 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 19:03:10 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (454037ms till timeout)
2022-03-28 19:03:10 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (178030ms till timeout)
2022-03-28 19:03:10 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:03:10 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2254354ms till timeout)
2022-03-28 19:03:10 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (899376ms till timeout)
2022-03-28 19:03:10 [ForkJoinPool-1-worker-35] DEBUG [ClientUtils:79] Job list-admin-my-cluster-506fda27-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=null, completedIndexes=null, completionTime=2022-03-28T19:03:05Z, conditions=[JobCondition(lastProbeTime=2022-03-28T19:03:05Z, lastTransitionTime=2022-03-28T19:03:05Z, message=null, reason=null, status=True, type=Complete, additionalProperties={})], failed=null, ready=null, startTime=2022-03-28T19:02:59Z, succeeded=1, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:03:10 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (54698ms till timeout)
2022-03-28 19:03:11 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (82994ms till timeout)
2022-03-28 19:03:11 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (895114ms till timeout)
2022-03-28 19:03:11 [ForkJoinPool-1-worker-35] DEBUG [AbstractST:674] ============================================================================
2022-03-28 19:03:11 [ForkJoinPool-1-worker-35] DEBUG [AbstractST:675] [operators.topic.ThrottlingQuotaST - After Each] - Clean up after test
2022-03-28 19:03:11 [ForkJoinPool-1-worker-35] INFO  [ResourceManager:344] ############################################################################
2022-03-28 19:03:11 [ForkJoinPool-1-worker-35] INFO  [ResourceManager:348] Delete all resources for testKafkaAdminTopicOperations
2022-03-28 19:03:11 [ForkJoinPool-1-worker-35] INFO  [ResourceManager:241] Delete of Job list-admin-my-cluster-506fda27-kafka-clients in namespace throttling-quota-st
2022-03-28 19:03:11 [ForkJoinPool-1-worker-35] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:list-admin-my-cluster-506fda27-kafka-clients
2022-03-28 19:03:11 [ForkJoinPool-1-worker-35] INFO  [ResourceManager:241] Delete of Job list-admin-my-cluster-506fda27-kafka-clients in namespace throttling-quota-st
2022-03-28 19:03:11 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 19:03:11 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (52762ms till timeout)
2022-03-28 19:03:11 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:03:11 [ForkJoinPool-1-worker-35] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:list-admin-my-cluster-506fda27-kafka-clients
2022-03-28 19:03:11 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:03:11 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2253216ms till timeout)
2022-03-28 19:03:11 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (898240ms till timeout)
2022-03-28 19:03:11 [ForkJoinPool-1-worker-35] INFO  [ResourceManager:241] Delete of Job delete-admin-my-cluster-506fda27-kafka-clients in namespace throttling-quota-st
2022-03-28 19:03:11 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (176704ms till timeout)
2022-03-28 19:03:11 [ForkJoinPool-1-worker-35] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:delete-admin-my-cluster-506fda27-kafka-clients
2022-03-28 19:03:11 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 19:03:11 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 19:03:11 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (452532ms till timeout)
2022-03-28 19:03:11 [ForkJoinPool-1-worker-35] INFO  [ResourceManager:241] Delete of Job create-admin-my-cluster-506fda27-kafka-clients in namespace throttling-quota-st
2022-03-28 19:03:12 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (53548ms till timeout)
2022-03-28 19:03:12 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (81831ms till timeout)
2022-03-28 19:03:12 [ForkJoinPool-1-worker-35] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:create-admin-my-cluster-506fda27-kafka-clients
2022-03-28 19:03:12 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (893955ms till timeout)
2022-03-28 19:03:12 [ForkJoinPool-1-worker-35] INFO  [ResourceManager:241] Delete of KafkaUser my-user-1780746402-846362125 in namespace throttling-quota-st
2022-03-28 19:03:12 [ForkJoinPool-1-worker-35] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-1780746402-846362125
2022-03-28 19:03:12 [ForkJoinPool-1-worker-35] INFO  [ResourceManager:369] ############################################################################
2022-03-28 19:03:12 [ForkJoinPool-1-worker-35] DEBUG [SuiteThreadController:267] testKafkaAdminTopicOperations - Notifies waiting test cases:[testPauseReconciliationInKafkaRebalanceAndTopic, testPauseReconciliationInKafkaAndKafkaConnectWithConnector, testCapacityFile, testConfigurationFileIsCreated, testConfigurationPerformanceOptions, testConfigurationReflection, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testDeployAndUnDeployCruiseControl, testTlsExternalUserWithQuotas, testCreatingUsersWithSecretPrefix, testThrottlingQuotasCreateTopic, testThrottlingQuotasCreateAlterPartitions, testKafkaAdminTopicOperations, testUserTemplate, testUpdateUser, testTlsExternalUser, testSendSimpleMessageTls, testReceiveSimpleMessageTlsScramSha, testThrottlingQuotasDeleteTopic, testReceiveSimpleMessageTls, testTopicModificationOfReplicationFactor, testSendingMessagesToNonExistingTopic, testMoreReplicasThanAvailableBrokers, testCreateTopicViaKafka] to and randomly select one to start execution
2022-03-28 19:03:12 [ForkJoinPool-1-worker-35] DEBUG [SuiteThreadController:93] [operators.topic.ThrottlingQuotaST] - Removing parallel test: testKafkaAdminTopicOperations
2022-03-28 19:03:12 [ForkJoinPool-1-worker-35] DEBUG [SuiteThreadController:97] [operators.topic.ThrottlingQuotaST] - Parallel test count: 14
2022-03-28 19:03:12 [ForkJoinPool-1-worker-35] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.topic.ThrottlingQuotaST.testKafkaAdminTopicOperations-FINISHED
2022-03-28 19:03:12 [ForkJoinPool-1-worker-35] INFO  [TestSeparator:30] ############################################################################
2022-03-28 19:03:12 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (51605ms till timeout)
2022-03-28 19:03:12 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 19:03:12 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2252060ms till timeout)
2022-03-28 19:03:12 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (897080ms till timeout)
2022-03-28 19:03:12 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:03:12 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 19:03:13 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (175450ms till timeout)
2022-03-28 19:03:13 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (52448ms till timeout)
2022-03-28 19:03:13 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (80734ms till timeout)
2022-03-28 19:03:13 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (892854ms till timeout)
2022-03-28 19:03:13 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (50414ms till timeout)
2022-03-28 19:03:13 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-0 not ready: kafka)
2022-03-28 19:03:13 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-1 not ready: kafka)
2022-03-28 19:03:13 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-2 not ready: kafka)
2022-03-28 19:03:13 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-afe273c9-kafka-3)
2022-03-28 19:03:13 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2250957ms till timeout)
2022-03-28 19:03:13 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 19:03:13 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 19:03:13 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (450583ms till timeout)
2022-03-28 19:03:13 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (895983ms till timeout)
2022-03-28 19:03:14 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:03:14 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (174257ms till timeout)
2022-03-28 19:03:14 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (51292ms till timeout)
2022-03-28 19:03:14 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (79579ms till timeout)
2022-03-28 19:03:14 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (891698ms till timeout)
2022-03-28 19:03:14 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 19:03:15 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929}
2022-03-28 19:03:15 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (49221ms till timeout)
2022-03-28 19:03:15 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (894881ms till timeout)
2022-03-28 19:03:15 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-0 not ready: kafka)
2022-03-28 19:03:15 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-1 not ready: kafka)
2022-03-28 19:03:15 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-2 not ready: kafka)
2022-03-28 19:03:15 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-afe273c9-kafka-3)
2022-03-28 19:03:15 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2249763ms till timeout)
2022-03-28 19:03:15 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-5dfd606f-kafka-0=a766ef9e-2663-4c21-ad83-6b7d9083122c, my-cluster-5dfd606f-kafka-1=57938984-c88e-45c5-b973-9f953a8e782c, my-cluster-5dfd606f-kafka-2=044285fe-7f53-4c1c-84e1-c51aec79b2ce}
2022-03-28 19:03:15 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929, my-cluster-08515847-cruise-control-dcd7c895b-4pkw7=37920413-f627-4a97-919c-4ff0c818c13a}
2022-03-28 19:03:15 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929}
2022-03-28 19:03:15 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Deployment my-cluster-08515847-cruise-control rolling update in namespace:namespace-4 not ready, will try again in 5000 ms (472573ms till timeout)
2022-03-28 19:03:15 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:03:15 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-5dfd606f-kafka-0=d3be017c-9e19-4ee7-b0fc-58e8694c30b9, my-cluster-5dfd606f-kafka-1=1187ed10-515a-43c0-be8e-d43991c9d25d, my-cluster-5dfd606f-kafka-2=044285fe-7f53-4c1c-84e1-c51aec79b2ce}
2022-03-28 19:03:15 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-5dfd606f-kafka-0=d3be017c-9e19-4ee7-b0fc-58e8694c30b9, my-cluster-5dfd606f-kafka-1=1187ed10-515a-43c0-be8e-d43991c9d25d, my-cluster-5dfd606f-kafka-2=044285fe-7f53-4c1c-84e1-c51aec79b2ce}
2022-03-28 19:03:15 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:50] At least my-cluster-5dfd606f-kafka-2 hasn't rolled
2022-03-28 19:03:15 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] component with name my-cluster-5dfd606f-kafka rolling update not ready, will try again in 5000 ms (1691771ms till timeout)
2022-03-28 19:03:15 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 19:03:15 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 19:03:15 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (449064ms till timeout)
2022-03-28 19:03:15 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (50193ms till timeout)
2022-03-28 19:03:15 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (172965ms till timeout)
2022-03-28 19:03:15 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (78479ms till timeout)
2022-03-28 19:03:15 [ForkJoinPool-1-worker-23] DEBUG [Exec:419] Command: oc --namespace topic-st delete kafkatopic topic-with-replication-to-change
2022-03-28 19:03:15 [ForkJoinPool-1-worker-23] DEBUG [Exec:419] Return code: 0
2022-03-28 19:03:15 [ForkJoinPool-1-worker-23] INFO  [KafkaTopicUtils:104] Waiting for KafkaTopic topic-with-replication-to-change deletion
2022-03-28 19:03:15 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for KafkaTopic deletion topic-with-replication-to-change
2022-03-28 19:03:15 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (890601ms till timeout)
2022-03-28 19:03:15 [ForkJoinPool-1-worker-23] TRACE [Exec:248] Running command - oc --namespace topic-st delete kafkatopic another-topic
2022-03-28 19:03:15 [ForkJoinPool-1-worker-21] INFO  [Exec:417] Command: oc --namespace user-st exec user-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user CN=encrypted-arnost
2022-03-28 19:03:15 [ForkJoinPool-1-worker-21] INFO  [Exec:417] Return code: 0
2022-03-28 19:03:15 [ForkJoinPool-1-worker-55] INFO  [ResourceManager:155] Create/Update KafkaUser scramed-arnost in namespace user-st
2022-03-28 19:03:15 [ForkJoinPool-1-worker-21] DEBUG [AbstractST:674] ============================================================================
2022-03-28 19:03:15 [ForkJoinPool-1-worker-21] DEBUG [AbstractST:675] [operators.user.UserST - After Each] - Clean up after test
2022-03-28 19:03:15 [ForkJoinPool-1-worker-21] INFO  [ResourceManager:344] ############################################################################
2022-03-28 19:03:15 [ForkJoinPool-1-worker-21] INFO  [ResourceManager:348] Delete all resources for testTlsUserWithQuotas
2022-03-28 19:03:15 [ForkJoinPool-1-worker-21] INFO  [ResourceManager:241] Delete of KafkaUser encrypted-arnost in namespace user-st
2022-03-28 19:03:15 [ForkJoinPool-1-worker-21] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:encrypted-arnost
2022-03-28 19:03:15 [ForkJoinPool-1-worker-55] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:scramed-arnost
2022-03-28 19:03:16 [ForkJoinPool-1-worker-21] INFO  [ResourceManager:369] ############################################################################
2022-03-28 19:03:16 [ForkJoinPool-1-worker-21] DEBUG [SuiteThreadController:267] testTlsUserWithQuotas - Notifies waiting test cases:[testPauseReconciliationInKafkaRebalanceAndTopic, testPauseReconciliationInKafkaAndKafkaConnectWithConnector, testCapacityFile, testConfigurationFileIsCreated, testConfigurationPerformanceOptions, testConfigurationReflection, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testDeployAndUnDeployCruiseControl, testTlsExternalUserWithQuotas, testCreatingUsersWithSecretPrefix, testThrottlingQuotasCreateTopic, testThrottlingQuotasCreateAlterPartitions, testKafkaAdminTopicOperations, testUserTemplate, testUpdateUser, testTlsExternalUser, testSendSimpleMessageTls, testReceiveSimpleMessageTlsScramSha, testThrottlingQuotasDeleteTopic, testReceiveSimpleMessageTls, testTopicModificationOfReplicationFactor, testSendingMessagesToNonExistingTopic, testMoreReplicasThanAvailableBrokers, testCreateTopicViaKafka] to and randomly select one to start execution
2022-03-28 19:03:16 [ForkJoinPool-1-worker-21] DEBUG [SuiteThreadController:93] [operators.user.UserST] - Removing parallel test: testTlsUserWithQuotas
2022-03-28 19:03:16 [ForkJoinPool-1-worker-21] DEBUG [SuiteThreadController:97] [operators.user.UserST] - Parallel test count: 13
2022-03-28 19:03:16 [ForkJoinPool-1-worker-21] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.user.UserST.testTlsUserWithQuotas-FINISHED
2022-03-28 19:03:16 [ForkJoinPool-1-worker-21] INFO  [TestSeparator:30] ############################################################################
2022-03-28 19:03:16 [ForkJoinPool-1-worker-55] INFO  [ResourceManager:433] Wait for KafkaUser: scramed-arnost will have desired state: Ready
2022-03-28 19:03:16 [ForkJoinPool-1-worker-55] DEBUG [TestUtils:125] Waiting for KafkaUser: scramed-arnost will have desired state: Ready
2022-03-28 19:03:16 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (893738ms till timeout)
2022-03-28 19:03:16 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-0 not ready: kafka)
2022-03-28 19:03:16 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-1 not ready: kafka)
2022-03-28 19:03:16 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-2 not ready: kafka)
2022-03-28 19:03:16 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-afe273c9-kafka-3)
2022-03-28 19:03:16 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2248620ms till timeout)
2022-03-28 19:03:16 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (48037ms till timeout)
2022-03-28 19:03:16 [ForkJoinPool-1-worker-55] TRACE [TestUtils:176] KafkaUser: scramed-arnost will have desired state: Ready not ready, will try again in 1000 ms (179904ms till timeout)
2022-03-28 19:03:16 [ForkJoinPool-1-worker-23] DEBUG [Exec:419] Command: oc --namespace topic-st delete kafkatopic another-topic
2022-03-28 19:03:16 [ForkJoinPool-1-worker-23] DEBUG [Exec:419] Return code: 0
2022-03-28 19:03:16 [ForkJoinPool-1-worker-23] INFO  [KafkaTopicUtils:104] Waiting for KafkaTopic another-topic deletion
2022-03-28 19:03:16 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for KafkaTopic deletion another-topic
2022-03-28 19:03:16 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 19:03:16 [ForkJoinPool-1-worker-23] DEBUG [AbstractST:674] ============================================================================
2022-03-28 19:03:16 [ForkJoinPool-1-worker-23] DEBUG [AbstractST:675] [operators.topic.TopicST - After Each] - Clean up after test
2022-03-28 19:03:16 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:344] ############################################################################
2022-03-28 19:03:16 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:348] Delete all resources for testCreateTopicAfterUnsupportedOperation
2022-03-28 19:03:16 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:241] Delete of KafkaTopic another-topic in namespace topic-st
2022-03-28 19:03:16 [ForkJoinPool-1-worker-21] INFO  [ResourceManager:241] Delete of KafkaTopic topic-with-replication-to-change in namespace topic-st
2022-03-28 19:03:16 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:another-topic
2022-03-28 19:03:16 [ForkJoinPool-1-worker-21] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:topic-with-replication-to-change
2022-03-28 19:03:16 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (49007ms till timeout)
2022-03-28 19:03:16 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (77302ms till timeout)
2022-03-28 19:03:16 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:03:16 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:369] ############################################################################
2022-03-28 19:03:16 [ForkJoinPool-1-worker-23] DEBUG [SuiteThreadController:267] testCreateTopicAfterUnsupportedOperation - Notifies waiting test cases:[testPauseReconciliationInKafkaRebalanceAndTopic, testPauseReconciliationInKafkaAndKafkaConnectWithConnector, testCapacityFile, testConfigurationFileIsCreated, testConfigurationPerformanceOptions, testConfigurationReflection, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testDeployAndUnDeployCruiseControl, testTlsExternalUserWithQuotas, testCreatingUsersWithSecretPrefix, testThrottlingQuotasCreateTopic, testThrottlingQuotasCreateAlterPartitions, testKafkaAdminTopicOperations, testUserTemplate, testUpdateUser, testTlsExternalUser, testSendSimpleMessageTls, testReceiveSimpleMessageTlsScramSha, testThrottlingQuotasDeleteTopic, testReceiveSimpleMessageTls, testTopicModificationOfReplicationFactor, testSendingMessagesToNonExistingTopic, testMoreReplicasThanAvailableBrokers, testCreateTopicViaKafka] to and randomly select one to start execution
2022-03-28 19:03:16 [ForkJoinPool-1-worker-23] DEBUG [SuiteThreadController:93] [operators.topic.TopicST] - Removing parallel test: testCreateTopicAfterUnsupportedOperation
2022-03-28 19:03:16 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (889422ms till timeout)
2022-03-28 19:03:16 [ForkJoinPool-1-worker-23] DEBUG [SuiteThreadController:97] [operators.topic.TopicST] - Parallel test count: 12
2022-03-28 19:03:16 [ForkJoinPool-1-worker-23] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.topic.TopicST.testCreateTopicAfterUnsupportedOperation-FINISHED
2022-03-28 19:03:16 [ForkJoinPool-1-worker-23] INFO  [TestSeparator:30] ############################################################################
2022-03-28 19:03:16 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (171597ms till timeout)
2022-03-28 19:03:17 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (892640ms till timeout)
2022-03-28 19:03:17 [ForkJoinPool-1-worker-55] INFO  [ResourceManager:444] KafkaUser: scramed-arnost is in desired state: Ready
2022-03-28 19:03:17 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-0 not ready: kafka)
2022-03-28 19:03:17 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-1 not ready: kafka)
2022-03-28 19:03:17 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-2 not ready: kafka)
2022-03-28 19:03:17 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-afe273c9-kafka-3)
2022-03-28 19:03:17 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2247427ms till timeout)
2022-03-28 19:03:17 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (46787ms till timeout)
2022-03-28 19:03:17 [ForkJoinPool-1-worker-55] DEBUG [UserST:274] Command for kafka-configs.sh bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user scramed-arnost
2022-03-28 19:03:17 [ForkJoinPool-1-worker-55] TRACE [Exec:248] Running command - oc --namespace user-st exec user-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user scramed-arnost
2022-03-28 19:03:17 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (47907ms till timeout)
2022-03-28 19:03:17 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (76193ms till timeout)
2022-03-28 19:03:17 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (888310ms till timeout)
2022-03-28 19:03:17 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:03:18 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (170389ms till timeout)
2022-03-28 19:03:18 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (891542ms till timeout)
2022-03-28 19:03:18 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-0 not ready: kafka)
2022-03-28 19:03:18 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-1 not ready: kafka)
2022-03-28 19:03:18 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-2 not ready: kafka)
2022-03-28 19:03:18 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-afe273c9-kafka-3)
2022-03-28 19:03:18 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2246328ms till timeout)
2022-03-28 19:03:18 [ForkJoinPool-1-worker-9] INFO  [PodUtils:189] Message org.apache.kafka.common.errors.ThrottlingQuotaExceededException: The throttling quota has been exceeded. found in create-admin-my-cluster-e9955717-kafka-clients-2khnj log
2022-03-28 19:03:18 [ForkJoinPool-1-worker-9] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment create-admin-my-cluster-e9955717-kafka-clients deletion
2022-03-28 19:03:18 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for ReplicaSet create-admin-my-cluster-e9955717-kafka-clients to be deleted
2022-03-28 19:03:18 [ForkJoinPool-1-worker-33] INFO  [PodUtils:189] Message org.apache.kafka.common.errors.ThrottlingQuotaExceededException: The throttling quota has been exceeded. found in create-admin-my-cluster-ec1b34f8-kafka-clients-khzz8 log
2022-03-28 19:03:18 [ForkJoinPool-1-worker-9] DEBUG [JobUtils:40] Job create-admin-my-cluster-e9955717-kafka-clients was deleted
2022-03-28 19:03:18 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (75000ms till timeout)
2022-03-28 19:03:19 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (887121ms till timeout)
2022-03-28 19:03:19 [ForkJoinPool-1-worker-33] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment create-admin-my-cluster-ec1b34f8-kafka-clients deletion
2022-03-28 19:03:19 [ForkJoinPool-1-worker-33] DEBUG [TestUtils:125] Waiting for ReplicaSet create-admin-my-cluster-ec1b34f8-kafka-clients to be deleted
2022-03-28 19:03:19 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:03:19 [ForkJoinPool-1-worker-33] DEBUG [JobUtils:40] Job create-admin-my-cluster-ec1b34f8-kafka-clients was deleted
2022-03-28 19:03:19 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-1
2022-03-28 19:03:19 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (169072ms till timeout)
2022-03-28 19:03:19 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (890417ms till timeout)
2022-03-28 19:03:19 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-0 not ready: kafka)
2022-03-28 19:03:19 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-1 not ready: kafka)
2022-03-28 19:03:19 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-2 not ready: kafka)
2022-03-28 19:03:19 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-afe273c9-kafka-3)
2022-03-28 19:03:19 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2245205ms till timeout)
2022-03-28 19:03:19 [ForkJoinPool-1-worker-33] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-30
2022-03-28 19:03:20 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-1
2022-03-28 19:03:20 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:03:20 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-100
2022-03-28 19:03:20 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (73904ms till timeout)
2022-03-28 19:03:20 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (886024ms till timeout)
2022-03-28 19:03:20 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929}
2022-03-28 19:03:20 [ForkJoinPool-1-worker-33] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-30
2022-03-28 19:03:20 [ForkJoinPool-1-worker-33] DEBUG [Exec:419] Return code: 0
2022-03-28 19:03:20 [ForkJoinPool-1-worker-33] INFO  [ResourceManager:155] Create/Update Job create-admin-my-cluster-ec1b34f8-kafka-clients in namespace throttling-quota-st
2022-03-28 19:03:20 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-5dfd606f-kafka-0=a766ef9e-2663-4c21-ad83-6b7d9083122c, my-cluster-5dfd606f-kafka-1=57938984-c88e-45c5-b973-9f953a8e782c, my-cluster-5dfd606f-kafka-2=044285fe-7f53-4c1c-84e1-c51aec79b2ce}
2022-03-28 19:03:20 [ForkJoinPool-1-worker-33] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:create-admin-my-cluster-ec1b34f8-kafka-clients
2022-03-28 19:03:20 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:03:20 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-5dfd606f-kafka-0=d3be017c-9e19-4ee7-b0fc-58e8694c30b9, my-cluster-5dfd606f-kafka-1=1187ed10-515a-43c0-be8e-d43991c9d25d, my-cluster-5dfd606f-kafka-2=044285fe-7f53-4c1c-84e1-c51aec79b2ce}
2022-03-28 19:03:20 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-5dfd606f-kafka-0=d3be017c-9e19-4ee7-b0fc-58e8694c30b9, my-cluster-5dfd606f-kafka-1=1187ed10-515a-43c0-be8e-d43991c9d25d, my-cluster-5dfd606f-kafka-2=044285fe-7f53-4c1c-84e1-c51aec79b2ce}
2022-03-28 19:03:20 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:50] At least my-cluster-5dfd606f-kafka-2 hasn't rolled
2022-03-28 19:03:20 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] component with name my-cluster-5dfd606f-kafka rolling update not ready, will try again in 5000 ms (1686581ms till timeout)
2022-03-28 19:03:20 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929, my-cluster-08515847-cruise-control-dcd7c895b-4pkw7=37920413-f627-4a97-919c-4ff0c818c13a}
2022-03-28 19:03:20 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929}
2022-03-28 19:03:20 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Deployment my-cluster-08515847-cruise-control rolling update in namespace:namespace-4 not ready, will try again in 5000 ms (467286ms till timeout)
2022-03-28 19:03:20 [ForkJoinPool-1-worker-33] INFO  [JobUtils:81] Waiting for job: create-admin-my-cluster-ec1b34f8-kafka-clients will be in active state
2022-03-28 19:03:20 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (889313ms till timeout)
2022-03-28 19:03:20 [ForkJoinPool-1-worker-33] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 19:03:20 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (167778ms till timeout)
2022-03-28 19:03:20 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-0 not ready: kafka)
2022-03-28 19:03:20 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-1 not ready: kafka)
2022-03-28 19:03:20 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-2 not ready: kafka)
2022-03-28 19:03:20 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-afe273c9-kafka-3)
2022-03-28 19:03:20 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2244101ms till timeout)
2022-03-28 19:03:20 [ForkJoinPool-1-worker-33] INFO  [PodUtils:186] Waiting for message will be in the log
2022-03-28 19:03:20 [ForkJoinPool-1-worker-33] DEBUG [TestUtils:125] Waiting for Waiting for message will be in the log
2022-03-28 19:03:20 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (299901ms till timeout)
2022-03-28 19:03:21 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (72807ms till timeout)
2022-03-28 19:03:21 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (884927ms till timeout)
2022-03-28 19:03:21 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (888216ms till timeout)
2022-03-28 19:03:21 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:03:21 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-0 not ready: kafka)
2022-03-28 19:03:21 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-1 not ready: kafka)
2022-03-28 19:03:21 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-2 not ready: kafka)
2022-03-28 19:03:21 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-afe273c9-kafka-3)
2022-03-28 19:03:21 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2242909ms till timeout)
2022-03-28 19:03:21 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 19:03:21 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 1
2022-03-28 19:03:21 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 19:03:21 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Error from server (NotFound): namespaces "namespace-3" not found
2022-03-28 19:03:21 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] ======STDERR END======
2022-03-28 19:03:21 [ForkJoinPool-1-worker-3] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[namespace-8], io.strimzi.test.logs.CollectorElement@c4c0ea0=[], io.strimzi.test.logs.CollectorElement@aec142ef=[namespace-5], io.strimzi.test.logs.CollectorElement@24b97ba9=[namespace-7], io.strimzi.test.logs.CollectorElement@f851b6c3=[namespace-4], io.strimzi.test.logs.CollectorElement@3881d5f2=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@5c7379cb=[], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[namespace-0], io.strimzi.test.logs.CollectorElement@f4405b3b=[namespace-2], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 19:03:21 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:267] testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods - Notifies waiting test cases:[testPauseReconciliationInKafkaRebalanceAndTopic, testPauseReconciliationInKafkaAndKafkaConnectWithConnector, testCapacityFile, testConfigurationFileIsCreated, testConfigurationPerformanceOptions, testConfigurationReflection, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testDeployAndUnDeployCruiseControl, testTlsExternalUserWithQuotas, testCreatingUsersWithSecretPrefix, testThrottlingQuotasCreateTopic, testThrottlingQuotasCreateAlterPartitions, testKafkaAdminTopicOperations, testUserTemplate, testUpdateUser, testTlsExternalUser, testSendSimpleMessageTls, testReceiveSimpleMessageTlsScramSha, testThrottlingQuotasDeleteTopic, testReceiveSimpleMessageTls, testTopicModificationOfReplicationFactor, testSendingMessagesToNonExistingTopic, testMoreReplicasThanAvailableBrokers, testCreateTopicViaKafka] to and randomly select one to start execution
2022-03-28 19:03:21 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:93] [cruisecontrol.CruiseControlConfigurationST] - Removing parallel test: testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods
2022-03-28 19:03:21 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:97] [cruisecontrol.CruiseControlConfigurationST] - Parallel test count: 11
2022-03-28 19:03:21 [ForkJoinPool-1-worker-3] INFO  [TestSeparator:29] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods-FINISHED
2022-03-28 19:03:21 [ForkJoinPool-1-worker-3] INFO  [TestSeparator:30] ############################################################################
2022-03-28 19:03:21 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (166489ms till timeout)
2022-03-28 19:03:22 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (298802ms till timeout)
2022-03-28 19:03:22 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (71711ms till timeout)
2022-03-28 19:03:22 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (883829ms till timeout)
2022-03-28 19:03:22 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (887118ms till timeout)
2022-03-28 19:03:22 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-0 not ready: kafka)
2022-03-28 19:03:22 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-1 not ready: kafka)
2022-03-28 19:03:22 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-2 not ready: kafka)
2022-03-28 19:03:22 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-afe273c9-kafka-3)
2022-03-28 19:03:22 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2241806ms till timeout)
2022-03-28 19:03:23 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:03:23 [ForkJoinPool-1-worker-5] INFO  [Exec:417] Command: oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --bootstrap-server localhost:9092 --describe --topic my-topic-1973235938-1730719386
2022-03-28 19:03:23 [ForkJoinPool-1-worker-5] INFO  [Exec:417] Return code: 0
2022-03-28 19:03:23 [ForkJoinPool-1-worker-5] INFO  [TopicST:470] Checking topic my-topic-1973235938-1730719386 in Kafka topic-cluster-name
2022-03-28 19:03:23 [ForkJoinPool-1-worker-5] DEBUG [TopicST:471] Topic my-topic-1973235938-1730719386 info: [Topic:my-topic-1973235938-1730719386, TopicId:gC-09C3IQTezb7zNLwoWgg, PartitionCount:5, ReplicationFactor:3, Configs:min.insync.replicas=2,message.format.version=3.0-IV1, Topic:my-topic-1973235938-1730719386, Partition:0, Leader:0, Replicas:0,2,1, Isr:0,2,1, Topic:my-topic-1973235938-1730719386, Partition:1, Leader:2, Replicas:2,1,0, Isr:2,1,0, Topic:my-topic-1973235938-1730719386, Partition:2, Leader:1, Replicas:1,0,2, Isr:1,0,2, Topic:my-topic-1973235938-1730719386, Partition:3, Leader:0, Replicas:0,2,1, Isr:0,2,1, Topic:my-topic-1973235938-1730719386, Partition:4, Leader:1, Replicas:1,0,2, Isr:1,0,2]
2022-03-28 19:03:23 [ForkJoinPool-1-worker-5] DEBUG [AbstractST:674] ============================================================================
2022-03-28 19:03:23 [ForkJoinPool-1-worker-5] DEBUG [AbstractST:675] [operators.topic.TopicST - After Each] - Clean up after test
2022-03-28 19:03:23 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:344] ############################################################################
2022-03-28 19:03:23 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:346] In context testCreateTopicViaKafka is everything deleted.
2022-03-28 19:03:23 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:369] ############################################################################
2022-03-28 19:03:23 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:267] testCreateTopicViaKafka - Notifies waiting test cases:[testPauseReconciliationInKafkaRebalanceAndTopic, testPauseReconciliationInKafkaAndKafkaConnectWithConnector, testCapacityFile, testConfigurationFileIsCreated, testConfigurationPerformanceOptions, testConfigurationReflection, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testDeployAndUnDeployCruiseControl, testTlsExternalUserWithQuotas, testCreatingUsersWithSecretPrefix, testThrottlingQuotasCreateTopic, testThrottlingQuotasCreateAlterPartitions, testKafkaAdminTopicOperations, testUserTemplate, testUpdateUser, testTlsExternalUser, testSendSimpleMessageTls, testReceiveSimpleMessageTlsScramSha, testThrottlingQuotasDeleteTopic, testReceiveSimpleMessageTls, testTopicModificationOfReplicationFactor, testSendingMessagesToNonExistingTopic, testMoreReplicasThanAvailableBrokers, testCreateTopicViaKafka] to and randomly select one to start execution
2022-03-28 19:03:23 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:93] [operators.topic.TopicST] - Removing parallel test: testCreateTopicViaKafka
2022-03-28 19:03:23 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:97] [operators.topic.TopicST] - Parallel test count: 10
2022-03-28 19:03:23 [ForkJoinPool-1-worker-5] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.topic.TopicST.testCreateTopicViaKafka-FINISHED
2022-03-28 19:03:23 [ForkJoinPool-1-worker-5] INFO  [TestSeparator:30] ############################################################################
2022-03-28 19:03:23 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (297703ms till timeout)
2022-03-28 19:03:23 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (165194ms till timeout)
2022-03-28 19:03:23 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (70615ms till timeout)
2022-03-28 19:03:23 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-959c6ed5 will have desired state: Ready not ready, will try again in 1000 ms (882733ms till timeout)
2022-03-28 19:03:23 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (886021ms till timeout)
2022-03-28 19:03:24 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-0 not ready: kafka)
2022-03-28 19:03:24 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-1 not ready: kafka)
2022-03-28 19:03:24 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-2 not ready: kafka)
2022-03-28 19:03:24 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-afe273c9-kafka-3)
2022-03-28 19:03:24 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2240705ms till timeout)
2022-03-28 19:03:24 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (296603ms till timeout)
2022-03-28 19:03:24 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:03:24 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (69512ms till timeout)
2022-03-28 19:03:24 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:444] Kafka: my-cluster-959c6ed5 is in desired state: Ready
2022-03-28 19:03:24 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (163898ms till timeout)
2022-03-28 19:03:24 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update KafkaTopic my-topic-634125597-1746493046 in namespace namespace-9
2022-03-28 19:03:24 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:164] Using Namespace: namespace-0
2022-03-28 19:03:24 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-634125597-1746493046
2022-03-28 19:03:24 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:433] Wait for KafkaTopic: my-topic-634125597-1746493046 will have desired state: Ready
2022-03-28 19:03:24 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for KafkaTopic: my-topic-634125597-1746493046 will have desired state: Ready
2022-03-28 19:03:24 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaTopic: my-topic-634125597-1746493046 will have desired state: Ready not ready, will try again in 1000 ms (179904ms till timeout)
2022-03-28 19:03:24 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (884923ms till timeout)
2022-03-28 19:03:25 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-0 not ready: kafka)
2022-03-28 19:03:25 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-1 not ready: kafka)
2022-03-28 19:03:25 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-2 not ready: kafka)
2022-03-28 19:03:25 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-afe273c9-kafka-3)
2022-03-28 19:03:25 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2239605ms till timeout)
2022-03-28 19:03:25 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (295502ms till timeout)
2022-03-28 19:03:25 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-5dfd606f-kafka-0=a766ef9e-2663-4c21-ad83-6b7d9083122c, my-cluster-5dfd606f-kafka-1=57938984-c88e-45c5-b973-9f953a8e782c, my-cluster-5dfd606f-kafka-2=044285fe-7f53-4c1c-84e1-c51aec79b2ce}
2022-03-28 19:03:25 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929}
2022-03-28 19:03:25 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (68416ms till timeout)
2022-03-28 19:03:25 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-100
2022-03-28 19:03:25 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:03:25 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-102
2022-03-28 19:03:25 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-5dfd606f-kafka-0=d3be017c-9e19-4ee7-b0fc-58e8694c30b9, my-cluster-5dfd606f-kafka-1=1187ed10-515a-43c0-be8e-d43991c9d25d, my-cluster-5dfd606f-kafka-2=044285fe-7f53-4c1c-84e1-c51aec79b2ce}
2022-03-28 19:03:25 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-5dfd606f-kafka-0=d3be017c-9e19-4ee7-b0fc-58e8694c30b9, my-cluster-5dfd606f-kafka-1=1187ed10-515a-43c0-be8e-d43991c9d25d, my-cluster-5dfd606f-kafka-2=044285fe-7f53-4c1c-84e1-c51aec79b2ce}
2022-03-28 19:03:25 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:50] At least my-cluster-5dfd606f-kafka-2 hasn't rolled
2022-03-28 19:03:25 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] component with name my-cluster-5dfd606f-kafka rolling update not ready, will try again in 5000 ms (1681419ms till timeout)
2022-03-28 19:03:25 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:03:25 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929, my-cluster-08515847-cruise-control-dcd7c895b-4pkw7=37920413-f627-4a97-919c-4ff0c818c13a}
2022-03-28 19:03:25 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929}
2022-03-28 19:03:25 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Deployment my-cluster-08515847-cruise-control rolling update in namespace:namespace-4 not ready, will try again in 5000 ms (461936ms till timeout)
2022-03-28 19:03:25 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (162521ms till timeout)
2022-03-28 19:03:26 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:444] KafkaTopic: my-topic-634125597-1746493046 is in desired state: Ready
2022-03-28 19:03:26 [ForkJoinPool-1-worker-7] INFO  [ReconciliationST:147] Adding pause annotation into KafkaTopic resource and changing replication factor
2022-03-28 19:03:26 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (883772ms till timeout)
2022-03-28 19:03:26 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-102
2022-03-28 19:03:26 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:03:26 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-109
2022-03-28 19:03:26 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-0 not ready: kafka)
2022-03-28 19:03:26 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-1 not ready: kafka)
2022-03-28 19:03:26 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-2 not ready: kafka)
2022-03-28 19:03:26 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-afe273c9-kafka-3)
2022-03-28 19:03:26 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2238465ms till timeout)
2022-03-28 19:03:26 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:433] Wait for KafkaTopic: my-topic-634125597-1746493046 will have desired state: ReconciliationPaused
2022-03-28 19:03:26 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for KafkaTopic: my-topic-634125597-1746493046 will have desired state: ReconciliationPaused
2022-03-28 19:03:26 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (294365ms till timeout)
2022-03-28 19:03:26 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:444] KafkaTopic: my-topic-634125597-1746493046 is in desired state: ReconciliationPaused
2022-03-28 19:03:26 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-0 exec my-cluster-959c6ed5-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-634125597-1746493046 --describe --bootstrap-server my-cluster-959c6ed5-kafka-bootstrap:9092
2022-03-28 19:03:26 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (67279ms till timeout)
2022-03-28 19:03:27 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:03:27 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (161322ms till timeout)
2022-03-28 19:03:27 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (882667ms till timeout)
2022-03-28 19:03:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-109
2022-03-28 19:03:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:03:27 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-11
2022-03-28 19:03:27 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-0 not ready: kafka)
2022-03-28 19:03:27 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-1 not ready: kafka)
2022-03-28 19:03:27 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-2 not ready: kafka)
2022-03-28 19:03:27 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-afe273c9-kafka-3)
2022-03-28 19:03:27 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2237364ms till timeout)
2022-03-28 19:03:27 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (293266ms till timeout)
2022-03-28 19:03:27 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (66183ms till timeout)
2022-03-28 19:03:28 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:03:28 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (881570ms till timeout)
2022-03-28 19:03:28 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (160034ms till timeout)
2022-03-28 19:03:28 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-0 not ready: kafka)
2022-03-28 19:03:28 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-1 not ready: kafka)
2022-03-28 19:03:28 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-2 not ready: kafka)
2022-03-28 19:03:28 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-afe273c9-kafka-3)
2022-03-28 19:03:28 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2236262ms till timeout)
2022-03-28 19:03:28 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (292166ms till timeout)
2022-03-28 19:03:28 [ForkJoinPool-1-worker-55] INFO  [Exec:417] Command: oc --namespace user-st exec user-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user scramed-arnost
2022-03-28 19:03:28 [ForkJoinPool-1-worker-55] INFO  [Exec:417] Return code: 0
2022-03-28 19:03:28 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (65087ms till timeout)
2022-03-28 19:03:29 [ForkJoinPool-1-worker-55] INFO  [KafkaUserUtils:62] Waiting for KafkaUser deletion scramed-arnost
2022-03-28 19:03:29 [ForkJoinPool-1-worker-55] DEBUG [TestUtils:125] Waiting for KafkaUser deletion scramed-arnost
2022-03-28 19:03:29 [ForkJoinPool-1-worker-55] INFO  [KafkaUserUtils:75] KafkaUser scramed-arnost deleted
2022-03-28 19:03:29 [ForkJoinPool-1-worker-55] DEBUG [TestUtils:125] Waiting for all KafkaUser scramed-arnost attributes will be cleaned
2022-03-28 19:03:29 [ForkJoinPool-1-worker-55] TRACE [Exec:248] Running command - oc --namespace user-st exec user-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user scramed-arnost
2022-03-28 19:03:29 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (880469ms till timeout)
2022-03-28 19:03:29 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:03:29 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-0 not ready: kafka)
2022-03-28 19:03:29 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-1 not ready: kafka)
2022-03-28 19:03:29 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-2 not ready: kafka)
2022-03-28 19:03:29 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-afe273c9-kafka-3)
2022-03-28 19:03:29 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2235158ms till timeout)
2022-03-28 19:03:29 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (158738ms till timeout)
2022-03-28 19:03:29 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (291048ms till timeout)
2022-03-28 19:03:30 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (63991ms till timeout)
2022-03-28 19:03:30 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (879371ms till timeout)
2022-03-28 19:03:30 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-5dfd606f-kafka-0=a766ef9e-2663-4c21-ad83-6b7d9083122c, my-cluster-5dfd606f-kafka-1=57938984-c88e-45c5-b973-9f953a8e782c, my-cluster-5dfd606f-kafka-2=044285fe-7f53-4c1c-84e1-c51aec79b2ce}
2022-03-28 19:03:30 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-0 not ready: kafka)
2022-03-28 19:03:30 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-1 not ready: kafka)
2022-03-28 19:03:30 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-2 not ready: kafka)
2022-03-28 19:03:30 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-afe273c9-kafka-3)
2022-03-28 19:03:30 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2234059ms till timeout)
2022-03-28 19:03:30 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:03:30 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-5dfd606f-kafka-0=d3be017c-9e19-4ee7-b0fc-58e8694c30b9, my-cluster-5dfd606f-kafka-1=1187ed10-515a-43c0-be8e-d43991c9d25d, my-cluster-5dfd606f-kafka-2=e2c632b2-9aec-4be9-931a-1dd36bf905c0}
2022-03-28 19:03:30 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-5dfd606f-kafka-0=d3be017c-9e19-4ee7-b0fc-58e8694c30b9, my-cluster-5dfd606f-kafka-1=1187ed10-515a-43c0-be8e-d43991c9d25d, my-cluster-5dfd606f-kafka-2=e2c632b2-9aec-4be9-931a-1dd36bf905c0}
2022-03-28 19:03:30 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:55] All pods seem to have rolled
2022-03-28 19:03:30 [ForkJoinPool-1-worker-15] INFO  [RollingUpdateUtils:86] Component with name: my-cluster-5dfd606f-kafka has been successfully rolled
2022-03-28 19:03:30 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:87] Component matching LabelSelector: LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-5dfd606f-kafka, strimzi.io/cluster=my-cluster-5dfd606f, strimzi.io/kind=Kafka}, additionalProperties={}) successfully rolled
2022-03-28 19:03:30 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929}
2022-03-28 19:03:30 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (157544ms till timeout)
2022-03-28 19:03:31 [ForkJoinPool-1-worker-15] INFO  [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-5dfd606f-kafka to be ready
2022-03-28 19:03:31 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-5dfd606f-kafka, strimzi.io/cluster=my-cluster-5dfd606f, strimzi.io/kind=Kafka}, additionalProperties={})to be ready
2022-03-28 19:03:31 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (289859ms till timeout)
2022-03-28 19:03:31 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (62869ms till timeout)
2022-03-28 19:03:31 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929, my-cluster-08515847-cruise-control-dcd7c895b-4pkw7=37920413-f627-4a97-919c-4ff0c818c13a}
2022-03-28 19:03:31 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-0 not ready: kafka)
2022-03-28 19:03:31 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929}
2022-03-28 19:03:31 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-1 not ready: kafka)
2022-03-28 19:03:31 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Deployment my-cluster-08515847-cruise-control rolling update in namespace:namespace-4 not ready, will try again in 5000 ms (456577ms till timeout)
2022-03-28 19:03:31 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-5dfd606f-kafka-2)
2022-03-28 19:03:31 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-5dfd606f-kafka, strimzi.io/cluster=my-cluster-5dfd606f, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1799809ms till timeout)
2022-03-28 19:03:31 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (878274ms till timeout)
2022-03-28 19:03:31 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-0 not ready: kafka)
2022-03-28 19:03:31 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-1 not ready: kafka)
2022-03-28 19:03:31 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-2 not ready: kafka)
2022-03-28 19:03:31 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-afe273c9-kafka-3)
2022-03-28 19:03:31 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2232960ms till timeout)
2022-03-28 19:03:32 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:03:32 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (156351ms till timeout)
2022-03-28 19:03:32 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (61771ms till timeout)
2022-03-28 19:03:32 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (288658ms till timeout)
2022-03-28 19:03:32 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-0 not ready: kafka)
2022-03-28 19:03:32 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-1 not ready: kafka)
2022-03-28 19:03:32 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-5dfd606f-kafka-2)
2022-03-28 19:03:32 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-5dfd606f-kafka, strimzi.io/cluster=my-cluster-5dfd606f, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1798709ms till timeout)
2022-03-28 19:03:32 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (877176ms till timeout)
2022-03-28 19:03:32 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-11
2022-03-28 19:03:32 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:03:32 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-110
2022-03-28 19:03:32 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-0 not ready: kafka)
2022-03-28 19:03:32 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-1 not ready: kafka)
2022-03-28 19:03:32 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-2 not ready: kafka)
2022-03-28 19:03:32 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-afe273c9-kafka-3)
2022-03-28 19:03:32 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2231861ms till timeout)
2022-03-28 19:03:33 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:03:33 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (60675ms till timeout)
2022-03-28 19:03:33 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (155065ms till timeout)
2022-03-28 19:03:33 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (287475ms till timeout)
2022-03-28 19:03:33 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-110
2022-03-28 19:03:33 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:03:33 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-111
2022-03-28 19:03:33 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-0 not ready: kafka)
2022-03-28 19:03:33 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-1 not ready: kafka)
2022-03-28 19:03:33 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-5dfd606f-kafka-2)
2022-03-28 19:03:33 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-5dfd606f-kafka, strimzi.io/cluster=my-cluster-5dfd606f, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1797522ms till timeout)
2022-03-28 19:03:33 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (876078ms till timeout)
2022-03-28 19:03:34 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-0 not ready: kafka)
2022-03-28 19:03:34 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-1 not ready: kafka)
2022-03-28 19:03:34 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-2 not ready: kafka)
2022-03-28 19:03:34 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-afe273c9-kafka-3)
2022-03-28 19:03:34 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2230761ms till timeout)
2022-03-28 19:03:34 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-111
2022-03-28 19:03:34 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:03:34 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-112
2022-03-28 19:03:34 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (59570ms till timeout)
2022-03-28 19:03:34 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:03:34 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (286368ms till timeout)
2022-03-28 19:03:34 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-0 not ready: kafka)
2022-03-28 19:03:34 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-1 not ready: kafka)
2022-03-28 19:03:34 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-5dfd606f-kafka-2)
2022-03-28 19:03:34 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-5dfd606f-kafka, strimzi.io/cluster=my-cluster-5dfd606f, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1796416ms till timeout)
2022-03-28 19:03:34 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-112
2022-03-28 19:03:34 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:03:34 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-123
2022-03-28 19:03:34 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (153768ms till timeout)
2022-03-28 19:03:34 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (874981ms till timeout)
2022-03-28 19:03:35 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-0 not ready: kafka)
2022-03-28 19:03:35 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-1 not ready: kafka)
2022-03-28 19:03:35 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-2 not ready: kafka)
2022-03-28 19:03:35 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-afe273c9-kafka-3)
2022-03-28 19:03:35 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2229661ms till timeout)
2022-03-28 19:03:35 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-123
2022-03-28 19:03:35 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:03:35 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-129
2022-03-28 19:03:35 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaTopic: my-topic-1727421070-668192795 will have desired state: Ready not ready, will try again in 1000 ms (58473ms till timeout)
2022-03-28 19:03:35 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (285266ms till timeout)
2022-03-28 19:03:35 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-0 not ready: kafka)
2022-03-28 19:03:35 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-1 not ready: kafka)
2022-03-28 19:03:35 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-5dfd606f-kafka-2)
2022-03-28 19:03:35 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-5dfd606f-kafka, strimzi.io/cluster=my-cluster-5dfd606f, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1795310ms till timeout)
2022-03-28 19:03:35 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-129
2022-03-28 19:03:35 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:03:35 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-13
2022-03-28 19:03:35 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:03:35 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (152567ms till timeout)
2022-03-28 19:03:36 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (873884ms till timeout)
2022-03-28 19:03:36 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929}
2022-03-28 19:03:36 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-0 not ready: kafka)
2022-03-28 19:03:36 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-1 not ready: kafka)
2022-03-28 19:03:36 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-2 not ready: kafka)
2022-03-28 19:03:36 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-afe273c9-kafka-3)
2022-03-28 19:03:36 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2228562ms till timeout)
2022-03-28 19:03:36 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-13
2022-03-28 19:03:36 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:03:36 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-130
2022-03-28 19:03:36 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929, my-cluster-08515847-cruise-control-dcd7c895b-4pkw7=37920413-f627-4a97-919c-4ff0c818c13a}
2022-03-28 19:03:36 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929}
2022-03-28 19:03:36 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Deployment my-cluster-08515847-cruise-control rolling update in namespace:namespace-4 not ready, will try again in 5000 ms (451367ms till timeout)
2022-03-28 19:03:36 [ForkJoinPool-1-worker-31] INFO  [ResourceManager:444] KafkaTopic: my-topic-1727421070-668192795 is in desired state: Ready
2022-03-28 19:03:36 [ForkJoinPool-1-worker-31] INFO  [ResourceManager:155] Create/Update KafkaUser encrypted-leopold in namespace namespace-9
2022-03-28 19:03:36 [ForkJoinPool-1-worker-31] INFO  [ResourceManager:164] Using Namespace: namespace-8
2022-03-28 19:03:36 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (284164ms till timeout)
2022-03-28 19:03:36 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-0 not ready: kafka)
2022-03-28 19:03:36 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-1 not ready: kafka)
2022-03-28 19:03:36 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-5dfd606f-kafka-2)
2022-03-28 19:03:36 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-5dfd606f-kafka, strimzi.io/cluster=my-cluster-5dfd606f, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1794208ms till timeout)
2022-03-28 19:03:36 [ForkJoinPool-1-worker-31] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:encrypted-leopold
2022-03-28 19:03:36 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-130
2022-03-28 19:03:36 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:03:36 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-132
2022-03-28 19:03:36 [ForkJoinPool-1-worker-31] INFO  [ResourceManager:433] Wait for KafkaUser: encrypted-leopold will have desired state: Ready
2022-03-28 19:03:36 [ForkJoinPool-1-worker-31] DEBUG [TestUtils:125] Waiting for KafkaUser: encrypted-leopold will have desired state: Ready
2022-03-28 19:03:37 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:03:37 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaUser: encrypted-leopold will have desired state: Ready not ready, will try again in 1000 ms (179811ms till timeout)
2022-03-28 19:03:37 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (872710ms till timeout)
2022-03-28 19:03:37 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (151268ms till timeout)
2022-03-28 19:03:37 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-0 not ready: kafka)
2022-03-28 19:03:37 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-1 not ready: kafka)
2022-03-28 19:03:37 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-2 not ready: kafka)
2022-03-28 19:03:37 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-afe273c9-kafka-3)
2022-03-28 19:03:37 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2227463ms till timeout)
2022-03-28 19:03:37 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-132
2022-03-28 19:03:37 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:03:37 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-141
2022-03-28 19:03:37 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (283050ms till timeout)
2022-03-28 19:03:37 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-0 not ready: kafka)
2022-03-28 19:03:37 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-1 not ready: kafka)
2022-03-28 19:03:37 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-5dfd606f-kafka-2)
2022-03-28 19:03:37 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-5dfd606f-kafka, strimzi.io/cluster=my-cluster-5dfd606f, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1793099ms till timeout)
2022-03-28 19:03:37 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-141
2022-03-28 19:03:37 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:03:37 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-145
2022-03-28 19:03:38 [ForkJoinPool-1-worker-7] INFO  [Exec:417] Command: oc --namespace namespace-0 exec my-cluster-959c6ed5-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-634125597-1746493046 --describe --bootstrap-server my-cluster-959c6ed5-kafka-bootstrap:9092
2022-03-28 19:03:38 [ForkJoinPool-1-worker-7] INFO  [Exec:417] Return code: 0
2022-03-28 19:03:38 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for KafkaTopic's spec will be stable
2022-03-28 19:03:38 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-0 exec my-cluster-959c6ed5-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-634125597-1746493046 --describe --bootstrap-server my-cluster-959c6ed5-kafka-bootstrap:9092
2022-03-28 19:03:38 [ForkJoinPool-1-worker-31] INFO  [ResourceManager:444] KafkaUser: encrypted-leopold is in desired state: Ready
2022-03-28 19:03:38 [ForkJoinPool-1-worker-31] INFO  [ResourceManager:155] Create/Update KafkaUser scramed-leopold in namespace namespace-9
2022-03-28 19:03:38 [ForkJoinPool-1-worker-31] INFO  [ResourceManager:164] Using Namespace: namespace-8
2022-03-28 19:03:38 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (871613ms till timeout)
2022-03-28 19:03:38 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:03:38 [ForkJoinPool-1-worker-31] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:scramed-leopold
2022-03-28 19:03:38 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-0 not ready: kafka)
2022-03-28 19:03:38 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-1 not ready: kafka)
2022-03-28 19:03:38 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-2 not ready: kafka)
2022-03-28 19:03:38 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-afe273c9-kafka-3)
2022-03-28 19:03:38 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2226309ms till timeout)
2022-03-28 19:03:38 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-145
2022-03-28 19:03:38 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:03:38 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-146
2022-03-28 19:03:38 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (149890ms till timeout)
2022-03-28 19:03:38 [ForkJoinPool-1-worker-31] INFO  [ResourceManager:433] Wait for KafkaUser: scramed-leopold will have desired state: Ready
2022-03-28 19:03:38 [ForkJoinPool-1-worker-31] DEBUG [TestUtils:125] Waiting for KafkaUser: scramed-leopold will have desired state: Ready
2022-03-28 19:03:38 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] KafkaUser: scramed-leopold will have desired state: Ready not ready, will try again in 1000 ms (179895ms till timeout)
2022-03-28 19:03:38 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (281938ms till timeout)
2022-03-28 19:03:39 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-0 not ready: kafka)
2022-03-28 19:03:39 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-1 not ready: kafka)
2022-03-28 19:03:39 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-5dfd606f-kafka-2)
2022-03-28 19:03:39 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-5dfd606f-kafka, strimzi.io/cluster=my-cluster-5dfd606f, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1791982ms till timeout)
2022-03-28 19:03:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-146
2022-03-28 19:03:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:03:39 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-148
2022-03-28 19:03:39 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (870516ms till timeout)
2022-03-28 19:03:39 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-0 not ready: kafka)
2022-03-28 19:03:39 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-1 not ready: kafka)
2022-03-28 19:03:39 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-2 not ready: kafka)
2022-03-28 19:03:39 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-afe273c9-kafka-3)
2022-03-28 19:03:39 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2225211ms till timeout)
2022-03-28 19:03:39 [ForkJoinPool-1-worker-55] INFO  [Exec:417] Command: oc --namespace user-st exec user-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user scramed-arnost
2022-03-28 19:03:39 [ForkJoinPool-1-worker-55] INFO  [Exec:417] Return code: 0
2022-03-28 19:03:39 [ForkJoinPool-1-worker-55] DEBUG [AbstractST:674] ============================================================================
2022-03-28 19:03:39 [ForkJoinPool-1-worker-55] DEBUG [AbstractST:675] [operators.user.UserST - After Each] - Clean up after test
2022-03-28 19:03:39 [ForkJoinPool-1-worker-55] INFO  [ResourceManager:344] ############################################################################
2022-03-28 19:03:39 [ForkJoinPool-1-worker-55] INFO  [ResourceManager:348] Delete all resources for testScramUserWithQuotas
2022-03-28 19:03:39 [ForkJoinPool-1-worker-55] INFO  [ResourceManager:241] Delete of KafkaUser scramed-arnost in namespace user-st
2022-03-28 19:03:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-148
2022-03-28 19:03:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:03:39 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-149
2022-03-28 19:03:39 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:03:39 [ForkJoinPool-1-worker-55] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:scramed-arnost
2022-03-28 19:03:39 [ForkJoinPool-1-worker-31] INFO  [ResourceManager:444] KafkaUser: scramed-leopold is in desired state: Ready
2022-03-28 19:03:39 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (148603ms till timeout)
2022-03-28 19:03:39 [ForkJoinPool-1-worker-31] INFO  [UserST:346] Deploying KafkaClients pod for TLS listener
2022-03-28 19:03:39 [ForkJoinPool-1-worker-55] INFO  [ResourceManager:369] ############################################################################
2022-03-28 19:03:39 [ForkJoinPool-1-worker-55] DEBUG [SuiteThreadController:267] testScramUserWithQuotas - Notifies waiting test cases:[testPauseReconciliationInKafkaRebalanceAndTopic, testPauseReconciliationInKafkaAndKafkaConnectWithConnector, testCapacityFile, testConfigurationFileIsCreated, testConfigurationPerformanceOptions, testConfigurationReflection, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testDeployAndUnDeployCruiseControl, testTlsExternalUserWithQuotas, testCreatingUsersWithSecretPrefix, testThrottlingQuotasCreateTopic, testThrottlingQuotasCreateAlterPartitions, testKafkaAdminTopicOperations, testUserTemplate, testUpdateUser, testTlsExternalUser, testSendSimpleMessageTls, testReceiveSimpleMessageTlsScramSha, testThrottlingQuotasDeleteTopic, testReceiveSimpleMessageTls, testTopicModificationOfReplicationFactor, testSendingMessagesToNonExistingTopic, testMoreReplicasThanAvailableBrokers, testCreateTopicViaKafka] to and randomly select one to start execution
2022-03-28 19:03:39 [ForkJoinPool-1-worker-55] DEBUG [SuiteThreadController:93] [operators.user.UserST] - Removing parallel test: testScramUserWithQuotas
2022-03-28 19:03:39 [ForkJoinPool-1-worker-55] DEBUG [SuiteThreadController:97] [operators.user.UserST] - Parallel test count: 9
2022-03-28 19:03:39 [ForkJoinPool-1-worker-55] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.user.UserST.testScramUserWithQuotas-FINISHED
2022-03-28 19:03:39 [ForkJoinPool-1-worker-55] INFO  [TestSeparator:30] ############################################################################
2022-03-28 19:03:40 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (280825ms till timeout)
2022-03-28 19:03:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-149
2022-03-28 19:03:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:03:40 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-156
2022-03-28 19:03:40 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-0 not ready: kafka)
2022-03-28 19:03:40 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-1 not ready: kafka)
2022-03-28 19:03:40 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-5dfd606f-kafka-2)
2022-03-28 19:03:40 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-5dfd606f-kafka, strimzi.io/cluster=my-cluster-5dfd606f, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1790869ms till timeout)
2022-03-28 19:03:40 [ForkJoinPool-1-worker-31] INFO  [ResourceManager:155] Create/Update Deployment my-cluster-f743fdfd-tls-kafka-clients in namespace namespace-8
2022-03-28 19:03:40 [ForkJoinPool-1-worker-31] INFO  [ResourceManager:164] Using Namespace: namespace-8
2022-03-28 19:03:40 [ForkJoinPool-1-worker-31] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:my-cluster-f743fdfd-tls-kafka-clients
2022-03-28 19:03:40 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (869366ms till timeout)
2022-03-28 19:03:40 [ForkJoinPool-1-worker-31] INFO  [DeploymentUtils:161] Wait for Deployment: my-cluster-f743fdfd-tls-kafka-clients will be ready
2022-03-28 19:03:40 [ForkJoinPool-1-worker-31] DEBUG [TestUtils:125] Waiting for Wait for Deployment: my-cluster-f743fdfd-tls-kafka-clients will be ready
2022-03-28 19:03:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-156
2022-03-28 19:03:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:03:40 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-158
2022-03-28 19:03:40 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-0 not ready: kafka)
2022-03-28 19:03:40 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-1 not ready: kafka)
2022-03-28 19:03:40 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-2 not ready: kafka)
2022-03-28 19:03:40 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-afe273c9-kafka-3)
2022-03-28 19:03:40 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2224058ms till timeout)
2022-03-28 19:03:40 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Wait for Deployment: my-cluster-f743fdfd-tls-kafka-clients will be ready not ready, will try again in 1000 ms (479810ms till timeout)
2022-03-28 19:03:41 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:03:41 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (147356ms till timeout)
2022-03-28 19:03:41 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (279672ms till timeout)
2022-03-28 19:03:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-158
2022-03-28 19:03:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:03:41 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-159
2022-03-28 19:03:41 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-0 not ready: kafka)
2022-03-28 19:03:41 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-1 not ready: kafka)
2022-03-28 19:03:41 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-5dfd606f-kafka-2)
2022-03-28 19:03:41 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-5dfd606f-kafka, strimzi.io/cluster=my-cluster-5dfd606f, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1789714ms till timeout)
2022-03-28 19:03:41 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929}
2022-03-28 19:03:41 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929, my-cluster-08515847-cruise-control-dcd7c895b-4pkw7=37920413-f627-4a97-919c-4ff0c818c13a}
2022-03-28 19:03:41 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929}
2022-03-28 19:03:41 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Deployment my-cluster-08515847-cruise-control rolling update in namespace:namespace-4 not ready, will try again in 5000 ms (446173ms till timeout)
2022-03-28 19:03:41 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (868198ms till timeout)
2022-03-28 19:03:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-159
2022-03-28 19:03:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:03:41 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-16
2022-03-28 19:03:41 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-0 not ready: kafka)
2022-03-28 19:03:41 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-1 not ready: kafka)
2022-03-28 19:03:41 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-2 not ready: kafka)
2022-03-28 19:03:41 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-afe273c9-kafka-3)
2022-03-28 19:03:41 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2222959ms till timeout)
2022-03-28 19:03:41 [ForkJoinPool-1-worker-7] INFO  [Exec:417] Command: oc --namespace namespace-0 exec my-cluster-959c6ed5-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-634125597-1746493046 --describe --bootstrap-server my-cluster-959c6ed5-kafka-bootstrap:9092
2022-03-28 19:03:41 [ForkJoinPool-1-worker-7] INFO  [Exec:417] Return code: 0
2022-03-28 19:03:41 [ForkJoinPool-1-worker-7] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 19 polls
2022-03-28 19:03:41 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (176248ms till timeout)
2022-03-28 19:03:41 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Wait for Deployment: my-cluster-f743fdfd-tls-kafka-clients will be ready not ready, will try again in 1000 ms (478712ms till timeout)
2022-03-28 19:03:42 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:03:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-16
2022-03-28 19:03:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:03:42 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-165
2022-03-28 19:03:42 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (146165ms till timeout)
2022-03-28 19:03:42 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (278479ms till timeout)
2022-03-28 19:03:42 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-0 not ready: kafka)
2022-03-28 19:03:42 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-1 not ready: kafka)
2022-03-28 19:03:42 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-5dfd606f-kafka-2)
2022-03-28 19:03:42 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-5dfd606f-kafka, strimzi.io/cluster=my-cluster-5dfd606f, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1788522ms till timeout)
2022-03-28 19:03:42 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (867099ms till timeout)
2022-03-28 19:03:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-165
2022-03-28 19:03:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:03:42 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-167
2022-03-28 19:03:42 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-0 exec my-cluster-959c6ed5-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-634125597-1746493046 --describe --bootstrap-server my-cluster-959c6ed5-kafka-bootstrap:9092
2022-03-28 19:03:42 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-0 not ready: kafka)
2022-03-28 19:03:42 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-1 not ready: kafka)
2022-03-28 19:03:42 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-2 not ready: kafka)
2022-03-28 19:03:42 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-afe273c9-kafka-3)
2022-03-28 19:03:42 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2221858ms till timeout)
2022-03-28 19:03:43 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Wait for Deployment: my-cluster-f743fdfd-tls-kafka-clients will be ready not ready, will try again in 1000 ms (477607ms till timeout)
2022-03-28 19:03:43 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:03:43 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (144970ms till timeout)
2022-03-28 19:03:43 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-167
2022-03-28 19:03:43 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:03:43 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-169
2022-03-28 19:03:43 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (277286ms till timeout)
2022-03-28 19:03:43 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-0 not ready: kafka)
2022-03-28 19:03:43 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-1 not ready: kafka)
2022-03-28 19:03:43 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-5dfd606f-kafka-2)
2022-03-28 19:03:43 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-5dfd606f-kafka, strimzi.io/cluster=my-cluster-5dfd606f, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1787329ms till timeout)
2022-03-28 19:03:43 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (866001ms till timeout)
2022-03-28 19:03:44 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-0 not ready: kafka)
2022-03-28 19:03:44 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-1 not ready: kafka)
2022-03-28 19:03:44 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-2 not ready: kafka)
2022-03-28 19:03:44 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-afe273c9-kafka-3)
2022-03-28 19:03:44 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2220759ms till timeout)
2022-03-28 19:03:44 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-169
2022-03-28 19:03:44 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:03:44 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-170
2022-03-28 19:03:44 [ForkJoinPool-1-worker-31] INFO  [DeploymentUtils:168] Deployment: my-cluster-f743fdfd-tls-kafka-clients is ready
2022-03-28 19:03:44 [ForkJoinPool-1-worker-31] INFO  [UserST:350] Deploying KafkaClients pod for PLAIN listener
2022-03-28 19:03:44 [ForkJoinPool-1-worker-31] INFO  [ResourceManager:155] Create/Update Deployment my-cluster-f743fdfd-plain-kafka-clients in namespace namespace-8
2022-03-28 19:03:44 [ForkJoinPool-1-worker-31] INFO  [ResourceManager:164] Using Namespace: namespace-8
2022-03-28 19:03:44 [ForkJoinPool-1-worker-31] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:my-cluster-f743fdfd-plain-kafka-clients
2022-03-28 19:03:44 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:03:44 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-170
2022-03-28 19:03:44 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:03:44 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-173
2022-03-28 19:03:44 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (276152ms till timeout)
2022-03-28 19:03:44 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (143645ms till timeout)
2022-03-28 19:03:44 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-0 not ready: kafka)
2022-03-28 19:03:44 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-1 not ready: kafka)
2022-03-28 19:03:44 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-5dfd606f-kafka-2)
2022-03-28 19:03:44 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-5dfd606f-kafka, strimzi.io/cluster=my-cluster-5dfd606f, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1786197ms till timeout)
2022-03-28 19:03:44 [ForkJoinPool-1-worker-31] INFO  [DeploymentUtils:161] Wait for Deployment: my-cluster-f743fdfd-plain-kafka-clients will be ready
2022-03-28 19:03:44 [ForkJoinPool-1-worker-31] DEBUG [TestUtils:125] Waiting for Wait for Deployment: my-cluster-f743fdfd-plain-kafka-clients will be ready
2022-03-28 19:03:45 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (864896ms till timeout)
2022-03-28 19:03:45 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Wait for Deployment: my-cluster-f743fdfd-plain-kafka-clients will be ready not ready, will try again in 1000 ms (479809ms till timeout)
2022-03-28 19:03:45 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-0 not ready: kafka)
2022-03-28 19:03:45 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-1 not ready: kafka)
2022-03-28 19:03:45 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-2 not ready: kafka)
2022-03-28 19:03:45 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-afe273c9-kafka-3)
2022-03-28 19:03:45 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2219587ms till timeout)
2022-03-28 19:03:45 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-173
2022-03-28 19:03:45 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:03:45 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-176
2022-03-28 19:03:45 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (275047ms till timeout)
2022-03-28 19:03:45 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-176
2022-03-28 19:03:45 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:03:45 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-18
2022-03-28 19:03:45 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:03:45 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-0 not ready: kafka)
2022-03-28 19:03:45 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-1 not ready: kafka)
2022-03-28 19:03:45 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-5dfd606f-kafka-2)
2022-03-28 19:03:45 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-5dfd606f-kafka, strimzi.io/cluster=my-cluster-5dfd606f, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1785085ms till timeout)
2022-03-28 19:03:46 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (142435ms till timeout)
2022-03-28 19:03:46 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (863782ms till timeout)
2022-03-28 19:03:46 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Wait for Deployment: my-cluster-f743fdfd-plain-kafka-clients will be ready not ready, will try again in 1000 ms (478696ms till timeout)
2022-03-28 19:03:46 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-0 not ready: kafka)
2022-03-28 19:03:46 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-1 not ready: kafka)
2022-03-28 19:03:46 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-2 not ready: kafka)
2022-03-28 19:03:46 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-afe273c9-kafka-3)
2022-03-28 19:03:46 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2218474ms till timeout)
2022-03-28 19:03:46 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-18
2022-03-28 19:03:46 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:03:46 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-180
2022-03-28 19:03:46 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-08515847-cruise-control-5ddf79fd84-cjnnb=5d6c4b67-2897-4f53-9c62-a579395d4929}
2022-03-28 19:03:46 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-08515847-cruise-control-dcd7c895b-4pkw7=37920413-f627-4a97-919c-4ff0c818c13a}
2022-03-28 19:03:46 [ForkJoinPool-1-worker-11] DEBUG [DeploymentUtils:119] All pods seem to have rolled
2022-03-28 19:03:46 [ForkJoinPool-1-worker-11] INFO  [DeploymentUtils:161] Wait for Deployment: my-cluster-08515847-cruise-control will be ready
2022-03-28 19:03:46 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Wait for Deployment: my-cluster-08515847-cruise-control will be ready
2022-03-28 19:03:46 [ForkJoinPool-1-worker-11] INFO  [DeploymentUtils:168] Deployment: my-cluster-08515847-cruise-control is ready
2022-03-28 19:03:47 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (273861ms till timeout)
2022-03-28 19:03:47 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-180
2022-03-28 19:03:47 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:03:47 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-183
2022-03-28 19:03:47 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-08515847, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-08515847-cruise-control}, additionalProperties={})to be ready
2022-03-28 19:03:47 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-0 not ready: kafka)
2022-03-28 19:03:47 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-1 not ready: kafka)
2022-03-28 19:03:47 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-5dfd606f-kafka-2)
2022-03-28 19:03:47 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-5dfd606f-kafka, strimzi.io/cluster=my-cluster-5dfd606f, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1783903ms till timeout)
2022-03-28 19:03:47 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:03:47 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (862602ms till timeout)
2022-03-28 19:03:47 [ForkJoinPool-1-worker-11] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-08515847-cruise-control-dcd7c895b-4pkw7 not ready: cruise-control)
2022-03-28 19:03:47 [ForkJoinPool-1-worker-11] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-08515847-cruise-control-dcd7c895b-4pkw7 not ready: tls-sidecar)
2022-03-28 19:03:47 [ForkJoinPool-1-worker-11] DEBUG [PodUtils:106] Pods my-cluster-08515847-cruise-control-dcd7c895b-4pkw7 are ready
2022-03-28 19:03:47 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-08515847, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-08515847-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599807ms till timeout)
2022-03-28 19:03:47 [ForkJoinPool-1-worker-7] INFO  [Exec:417] Command: oc --namespace namespace-0 exec my-cluster-959c6ed5-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-634125597-1746493046 --describe --bootstrap-server my-cluster-959c6ed5-kafka-bootstrap:9092
2022-03-28 19:03:47 [ForkJoinPool-1-worker-7] INFO  [Exec:417] Return code: 0
2022-03-28 19:03:47 [ForkJoinPool-1-worker-7] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 18 polls
2022-03-28 19:03:47 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (170778ms till timeout)
2022-03-28 19:03:47 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Wait for Deployment: my-cluster-f743fdfd-plain-kafka-clients will be ready not ready, will try again in 1000 ms (477516ms till timeout)
2022-03-28 19:03:47 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (141066ms till timeout)
2022-03-28 19:03:47 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-0 not ready: kafka)
2022-03-28 19:03:47 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-1 not ready: kafka)
2022-03-28 19:03:47 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-2 not ready: kafka)
2022-03-28 19:03:47 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-afe273c9-kafka-3)
2022-03-28 19:03:47 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2217294ms till timeout)
2022-03-28 19:03:47 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-183
2022-03-28 19:03:47 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:03:47 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-184
2022-03-28 19:03:48 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (272756ms till timeout)
2022-03-28 19:03:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-184
2022-03-28 19:03:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:03:48 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-186
2022-03-28 19:03:48 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-0 not ready: kafka)
2022-03-28 19:03:48 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-1 not ready: kafka)
2022-03-28 19:03:48 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-5dfd606f-kafka-2)
2022-03-28 19:03:48 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-5dfd606f-kafka, strimzi.io/cluster=my-cluster-5dfd606f, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1782712ms till timeout)
2022-03-28 19:03:48 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-0 exec my-cluster-959c6ed5-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-634125597-1746493046 --describe --bootstrap-server my-cluster-959c6ed5-kafka-bootstrap:9092
2022-03-28 19:03:48 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (861505ms till timeout)
2022-03-28 19:03:48 [ForkJoinPool-1-worker-31] INFO  [DeploymentUtils:168] Deployment: my-cluster-f743fdfd-plain-kafka-clients is ready
2022-03-28 19:03:48 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:03:48 [ForkJoinPool-1-worker-11] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-08515847-cruise-control-dcd7c895b-4pkw7 not ready: cruise-control)
2022-03-28 19:03:48 [ForkJoinPool-1-worker-11] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-08515847-cruise-control-dcd7c895b-4pkw7 not ready: tls-sidecar)
2022-03-28 19:03:48 [ForkJoinPool-1-worker-11] DEBUG [PodUtils:106] Pods my-cluster-08515847-cruise-control-dcd7c895b-4pkw7 are ready
2022-03-28 19:03:48 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-08515847, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-08515847-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598614ms till timeout)
2022-03-28 19:03:48 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-0 not ready: kafka)
2022-03-28 19:03:48 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-1 not ready: kafka)
2022-03-28 19:03:48 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-2 not ready: kafka)
2022-03-28 19:03:48 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-afe273c9-kafka-3)
2022-03-28 19:03:48 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2216196ms till timeout)
2022-03-28 19:03:48 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (139776ms till timeout)
2022-03-28 19:03:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-186
2022-03-28 19:03:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:03:48 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-188
2022-03-28 19:03:48 [ForkJoinPool-1-worker-31] INFO  [UserST:357] Checking if user secrets with secret prefixes exists
2022-03-28 19:03:48 [ForkJoinPool-1-worker-31] INFO  [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-03-28 19:03:48 [ForkJoinPool-1-worker-31] INFO  [UserST:373] Checking if TLS user is able to send messages
2022-03-28 19:03:48 [ForkJoinPool-1-worker-31] DEBUG [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@2ccf3816, which are set.
2022-03-28 19:03:48 [ForkJoinPool-1-worker-31] INFO  [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@136af9e5, messages=[], arguments=[--bootstrap-server, my-cluster-f743fdfd-kafka-bootstrap.namespace-8.svc:9093, --topic, my-topic-1059523275-384667503, USER=top_secret_encrypted_leopold, --max-messages, 100], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-f743fdfd-tls-kafka-clients-fb8b9dbf4-z9vr5', podNamespace='namespace-8', bootstrapServer='my-cluster-f743fdfd-kafka-bootstrap.namespace-8.svc:9093', topicName='my-topic-1059523275-384667503', maxMessages=100, kafkaUsername='top-secret-encrypted-leopold', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@2ccf3816}
2022-03-28 19:03:48 [ForkJoinPool-1-worker-31] INFO  [InternalKafkaClient:124] Producing 100 messages to my-cluster-f743fdfd-kafka-bootstrap.namespace-8.svc:9093:my-topic-1059523275-384667503 from pod my-cluster-f743fdfd-tls-kafka-clients-fb8b9dbf4-z9vr5
2022-03-28 19:03:48 [ForkJoinPool-1-worker-31] INFO  [VerifiableClient:192] Client command: oc exec my-cluster-f743fdfd-tls-kafka-clients-fb8b9dbf4-z9vr5 -n namespace-8 -- /opt/kafka/producer.sh --bootstrap-server my-cluster-f743fdfd-kafka-bootstrap.namespace-8.svc:9093 --topic my-topic-1059523275-384667503 USER=top_secret_encrypted_leopold --max-messages 100
2022-03-28 19:03:48 [ForkJoinPool-1-worker-31] TRACE [Exec:248] Running command - oc exec my-cluster-f743fdfd-tls-kafka-clients-fb8b9dbf4-z9vr5 -n namespace-8 -- /opt/kafka/producer.sh --bootstrap-server my-cluster-f743fdfd-kafka-bootstrap.namespace-8.svc:9093 --topic my-topic-1059523275-384667503 USER=top_secret_encrypted_leopold --max-messages 100
2022-03-28 19:03:49 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (271646ms till timeout)
2022-03-28 19:03:49 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-188
2022-03-28 19:03:49 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:03:49 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-189
2022-03-28 19:03:49 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-0 not ready: kafka)
2022-03-28 19:03:49 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-1 not ready: kafka)
2022-03-28 19:03:49 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-5dfd606f-kafka-2)
2022-03-28 19:03:49 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-5dfd606f-kafka, strimzi.io/cluster=my-cluster-5dfd606f, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1781612ms till timeout)
2022-03-28 19:03:49 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (860405ms till timeout)
2022-03-28 19:03:49 [ForkJoinPool-1-worker-11] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-08515847-cruise-control-dcd7c895b-4pkw7 not ready: cruise-control)
2022-03-28 19:03:49 [ForkJoinPool-1-worker-11] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-08515847-cruise-control-dcd7c895b-4pkw7 not ready: tls-sidecar)
2022-03-28 19:03:49 [ForkJoinPool-1-worker-11] DEBUG [PodUtils:106] Pods my-cluster-08515847-cruise-control-dcd7c895b-4pkw7 are ready
2022-03-28 19:03:49 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-08515847, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-08515847-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597516ms till timeout)
2022-03-28 19:03:49 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-0 not ready: kafka)
2022-03-28 19:03:49 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-1 not ready: kafka)
2022-03-28 19:03:49 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-2 not ready: kafka)
2022-03-28 19:03:49 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-3 not ready: kafka)
2022-03-28 19:03:49 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:106] Pods my-cluster-afe273c9-kafka-0, my-cluster-afe273c9-kafka-1, my-cluster-afe273c9-kafka-2, my-cluster-afe273c9-kafka-3 are ready
2022-03-28 19:03:49 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2215094ms till timeout)
2022-03-28 19:03:49 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:03:49 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-189
2022-03-28 19:03:49 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:03:49 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-19
2022-03-28 19:03:49 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (138574ms till timeout)
2022-03-28 19:03:50 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (270546ms till timeout)
2022-03-28 19:03:50 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-19
2022-03-28 19:03:50 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:03:50 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-190
2022-03-28 19:03:50 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-0 not ready: kafka)
2022-03-28 19:03:50 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-1 not ready: kafka)
2022-03-28 19:03:50 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-2 not ready: kafka)
2022-03-28 19:03:50 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-5dfd606f-kafka-0, my-cluster-5dfd606f-kafka-1, my-cluster-5dfd606f-kafka-2 are ready
2022-03-28 19:03:50 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-5dfd606f-kafka, strimzi.io/cluster=my-cluster-5dfd606f, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1780507ms till timeout)
2022-03-28 19:03:50 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (859299ms till timeout)
2022-03-28 19:03:50 [ForkJoinPool-1-worker-11] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-08515847-cruise-control-dcd7c895b-4pkw7 not ready: cruise-control)
2022-03-28 19:03:50 [ForkJoinPool-1-worker-11] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-08515847-cruise-control-dcd7c895b-4pkw7 not ready: tls-sidecar)
2022-03-28 19:03:50 [ForkJoinPool-1-worker-11] DEBUG [PodUtils:106] Pods my-cluster-08515847-cruise-control-dcd7c895b-4pkw7 are ready
2022-03-28 19:03:50 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-08515847, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-08515847-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596411ms till timeout)
2022-03-28 19:03:50 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-0 not ready: kafka)
2022-03-28 19:03:50 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-1 not ready: kafka)
2022-03-28 19:03:50 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-2 not ready: kafka)
2022-03-28 19:03:50 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-3 not ready: kafka)
2022-03-28 19:03:50 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:106] Pods my-cluster-afe273c9-kafka-0, my-cluster-afe273c9-kafka-1, my-cluster-afe273c9-kafka-2, my-cluster-afe273c9-kafka-3 are ready
2022-03-28 19:03:50 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2213991ms till timeout)
2022-03-28 19:03:50 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-190
2022-03-28 19:03:50 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:03:50 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-195
2022-03-28 19:03:51 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:03:51 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (137382ms till timeout)
2022-03-28 19:03:51 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (269445ms till timeout)
2022-03-28 19:03:51 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-195
2022-03-28 19:03:51 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:03:51 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-199
2022-03-28 19:03:51 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-0 not ready: kafka)
2022-03-28 19:03:51 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-1 not ready: kafka)
2022-03-28 19:03:51 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-2 not ready: kafka)
2022-03-28 19:03:51 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-5dfd606f-kafka-0, my-cluster-5dfd606f-kafka-1, my-cluster-5dfd606f-kafka-2 are ready
2022-03-28 19:03:51 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-5dfd606f-kafka, strimzi.io/cluster=my-cluster-5dfd606f, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1779407ms till timeout)
2022-03-28 19:03:51 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (858200ms till timeout)
2022-03-28 19:03:51 [ForkJoinPool-1-worker-11] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-08515847-cruise-control-dcd7c895b-4pkw7 not ready: cruise-control)
2022-03-28 19:03:51 [ForkJoinPool-1-worker-11] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-08515847-cruise-control-dcd7c895b-4pkw7 not ready: tls-sidecar)
2022-03-28 19:03:51 [ForkJoinPool-1-worker-11] DEBUG [PodUtils:106] Pods my-cluster-08515847-cruise-control-dcd7c895b-4pkw7 are ready
2022-03-28 19:03:51 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-08515847, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-08515847-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595312ms till timeout)
2022-03-28 19:03:51 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-0 not ready: kafka)
2022-03-28 19:03:51 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-1 not ready: kafka)
2022-03-28 19:03:51 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-2 not ready: kafka)
2022-03-28 19:03:51 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-3 not ready: kafka)
2022-03-28 19:03:51 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:106] Pods my-cluster-afe273c9-kafka-0, my-cluster-afe273c9-kafka-1, my-cluster-afe273c9-kafka-2, my-cluster-afe273c9-kafka-3 are ready
2022-03-28 19:03:51 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2212892ms till timeout)
2022-03-28 19:03:52 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-199
2022-03-28 19:03:52 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:03:52 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-2
2022-03-28 19:03:52 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:03:52 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (136183ms till timeout)
2022-03-28 19:03:52 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (268345ms till timeout)
2022-03-28 19:03:52 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-2
2022-03-28 19:03:52 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:03:52 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-200
2022-03-28 19:03:52 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-0 not ready: kafka)
2022-03-28 19:03:52 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-1 not ready: kafka)
2022-03-28 19:03:52 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-2 not ready: kafka)
2022-03-28 19:03:52 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-5dfd606f-kafka-0, my-cluster-5dfd606f-kafka-1, my-cluster-5dfd606f-kafka-2 are ready
2022-03-28 19:03:52 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-5dfd606f-kafka, strimzi.io/cluster=my-cluster-5dfd606f, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1778308ms till timeout)
2022-03-28 19:03:52 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (857101ms till timeout)
2022-03-28 19:03:52 [ForkJoinPool-1-worker-11] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-08515847-cruise-control-dcd7c895b-4pkw7 not ready: cruise-control)
2022-03-28 19:03:52 [ForkJoinPool-1-worker-11] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-08515847-cruise-control-dcd7c895b-4pkw7 not ready: tls-sidecar)
2022-03-28 19:03:52 [ForkJoinPool-1-worker-11] DEBUG [PodUtils:106] Pods my-cluster-08515847-cruise-control-dcd7c895b-4pkw7 are ready
2022-03-28 19:03:52 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-08515847, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-08515847-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594213ms till timeout)
2022-03-28 19:03:53 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-0 not ready: kafka)
2022-03-28 19:03:53 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-1 not ready: kafka)
2022-03-28 19:03:53 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-2 not ready: kafka)
2022-03-28 19:03:53 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-3 not ready: kafka)
2022-03-28 19:03:53 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:106] Pods my-cluster-afe273c9-kafka-0, my-cluster-afe273c9-kafka-1, my-cluster-afe273c9-kafka-2, my-cluster-afe273c9-kafka-3 are ready
2022-03-28 19:03:53 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2211793ms till timeout)
2022-03-28 19:03:53 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-200
2022-03-28 19:03:53 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:03:53 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-201
2022-03-28 19:03:53 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:03:53 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (134991ms till timeout)
2022-03-28 19:03:53 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (267246ms till timeout)
2022-03-28 19:03:53 [ForkJoinPool-1-worker-7] INFO  [Exec:417] Command: oc --namespace namespace-0 exec my-cluster-959c6ed5-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-634125597-1746493046 --describe --bootstrap-server my-cluster-959c6ed5-kafka-bootstrap:9092
2022-03-28 19:03:53 [ForkJoinPool-1-worker-7] INFO  [Exec:417] Return code: 0
2022-03-28 19:03:53 [ForkJoinPool-1-worker-7] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 17 polls
2022-03-28 19:03:53 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (164460ms till timeout)
2022-03-28 19:03:53 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-201
2022-03-28 19:03:53 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:03:53 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-202
2022-03-28 19:03:53 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-0 not ready: kafka)
2022-03-28 19:03:53 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-1 not ready: kafka)
2022-03-28 19:03:53 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-2 not ready: kafka)
2022-03-28 19:03:53 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-5dfd606f-kafka-0, my-cluster-5dfd606f-kafka-1, my-cluster-5dfd606f-kafka-2 are ready
2022-03-28 19:03:53 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-5dfd606f-kafka, strimzi.io/cluster=my-cluster-5dfd606f, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1777208ms till timeout)
2022-03-28 19:03:53 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (855997ms till timeout)
2022-03-28 19:03:54 [ForkJoinPool-1-worker-11] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-08515847-cruise-control-dcd7c895b-4pkw7 not ready: cruise-control)
2022-03-28 19:03:54 [ForkJoinPool-1-worker-11] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-08515847-cruise-control-dcd7c895b-4pkw7 not ready: tls-sidecar)
2022-03-28 19:03:54 [ForkJoinPool-1-worker-11] DEBUG [PodUtils:106] Pods my-cluster-08515847-cruise-control-dcd7c895b-4pkw7 are ready
2022-03-28 19:03:54 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-08515847, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-08515847-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593108ms till timeout)
2022-03-28 19:03:54 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-0 not ready: kafka)
2022-03-28 19:03:54 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-1 not ready: kafka)
2022-03-28 19:03:54 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-2 not ready: kafka)
2022-03-28 19:03:54 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-3 not ready: kafka)
2022-03-28 19:03:54 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:106] Pods my-cluster-afe273c9-kafka-0, my-cluster-afe273c9-kafka-1, my-cluster-afe273c9-kafka-2, my-cluster-afe273c9-kafka-3 are ready
2022-03-28 19:03:54 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2210689ms till timeout)
2022-03-28 19:03:54 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-202
2022-03-28 19:03:54 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:03:54 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-203
2022-03-28 19:03:54 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:03:54 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (133798ms till timeout)
2022-03-28 19:03:54 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-0 exec my-cluster-959c6ed5-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-634125597-1746493046 --describe --bootstrap-server my-cluster-959c6ed5-kafka-bootstrap:9092
2022-03-28 19:03:54 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (266114ms till timeout)
2022-03-28 19:03:54 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-203
2022-03-28 19:03:54 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:03:54 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-204
2022-03-28 19:03:54 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-0 not ready: kafka)
2022-03-28 19:03:54 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-1 not ready: kafka)
2022-03-28 19:03:54 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-2 not ready: kafka)
2022-03-28 19:03:54 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-5dfd606f-kafka-0, my-cluster-5dfd606f-kafka-1, my-cluster-5dfd606f-kafka-2 are ready
2022-03-28 19:03:54 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-5dfd606f-kafka, strimzi.io/cluster=my-cluster-5dfd606f, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1776109ms till timeout)
2022-03-28 19:03:55 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (854900ms till timeout)
2022-03-28 19:03:55 [ForkJoinPool-1-worker-11] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-08515847-cruise-control-dcd7c895b-4pkw7 not ready: cruise-control)
2022-03-28 19:03:55 [ForkJoinPool-1-worker-11] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-08515847-cruise-control-dcd7c895b-4pkw7 not ready: tls-sidecar)
2022-03-28 19:03:55 [ForkJoinPool-1-worker-11] DEBUG [PodUtils:106] Pods my-cluster-08515847-cruise-control-dcd7c895b-4pkw7 are ready
2022-03-28 19:03:55 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-08515847, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-08515847-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592010ms till timeout)
2022-03-28 19:03:55 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-0 not ready: kafka)
2022-03-28 19:03:55 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-1 not ready: kafka)
2022-03-28 19:03:55 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-2 not ready: kafka)
2022-03-28 19:03:55 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-3 not ready: kafka)
2022-03-28 19:03:55 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:106] Pods my-cluster-afe273c9-kafka-0, my-cluster-afe273c9-kafka-1, my-cluster-afe273c9-kafka-2, my-cluster-afe273c9-kafka-3 are ready
2022-03-28 19:03:55 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2209590ms till timeout)
2022-03-28 19:03:55 [ForkJoinPool-1-worker-31] INFO  [InternalKafkaClient:127] Producer finished correctly: true
2022-03-28 19:03:55 [ForkJoinPool-1-worker-31] INFO  [InternalKafkaClient:131] Producer produced 100 messages
2022-03-28 19:03:55 [ForkJoinPool-1-worker-31] DEBUG [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@ca394, which are set.
2022-03-28 19:03:55 [ForkJoinPool-1-worker-31] INFO  [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@57de3272, messages=[], arguments=[--bootstrap-server, my-cluster-f743fdfd-kafka-bootstrap.namespace-8.svc:9093, --topic, my-topic-1059523275-384667503, USER=top_secret_encrypted_leopold, --group-instance-id, instance676671353, --group-id, my-consumer-group-1616692438, --max-messages, 100], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-f743fdfd-tls-kafka-clients-fb8b9dbf4-z9vr5', podNamespace='namespace-8', bootstrapServer='my-cluster-f743fdfd-kafka-bootstrap.namespace-8.svc:9093', topicName='my-topic-1059523275-384667503', maxMessages=100, kafkaUsername='top-secret-encrypted-leopold', consumerGroupName='my-consumer-group-1616692438', consumerInstanceId='instance676671353', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@ca394}
2022-03-28 19:03:55 [ForkJoinPool-1-worker-31] INFO  [InternalKafkaClient:192] Consuming 100 messages from my-cluster-f743fdfd-kafka-bootstrap.namespace-8.svc:9093:my-topic-1059523275-384667503 from pod my-cluster-f743fdfd-tls-kafka-clients-fb8b9dbf4-z9vr5
2022-03-28 19:03:55 [ForkJoinPool-1-worker-31] INFO  [VerifiableClient:192] Client command: oc exec my-cluster-f743fdfd-tls-kafka-clients-fb8b9dbf4-z9vr5 -n namespace-8 -- /opt/kafka/consumer.sh --bootstrap-server my-cluster-f743fdfd-kafka-bootstrap.namespace-8.svc:9093 --topic my-topic-1059523275-384667503 USER=top_secret_encrypted_leopold --group-instance-id instance676671353 --group-id my-consumer-group-1616692438 --max-messages 100
2022-03-28 19:03:55 [ForkJoinPool-1-worker-31] TRACE [Exec:248] Running command - oc exec my-cluster-f743fdfd-tls-kafka-clients-fb8b9dbf4-z9vr5 -n namespace-8 -- /opt/kafka/consumer.sh --bootstrap-server my-cluster-f743fdfd-kafka-bootstrap.namespace-8.svc:9093 --topic my-topic-1059523275-384667503 USER=top_secret_encrypted_leopold --group-instance-id instance676671353 --group-id my-consumer-group-1616692438 --max-messages 100
2022-03-28 19:03:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-204
2022-03-28 19:03:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:03:55 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-205
2022-03-28 19:03:55 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:03:55 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (132606ms till timeout)
2022-03-28 19:03:55 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (264921ms till timeout)
2022-03-28 19:03:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-205
2022-03-28 19:03:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:03:55 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-206
2022-03-28 19:03:56 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-0 not ready: kafka)
2022-03-28 19:03:56 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-1 not ready: kafka)
2022-03-28 19:03:56 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-2 not ready: kafka)
2022-03-28 19:03:56 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-5dfd606f-kafka-0, my-cluster-5dfd606f-kafka-1, my-cluster-5dfd606f-kafka-2 are ready
2022-03-28 19:03:56 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-5dfd606f-kafka, strimzi.io/cluster=my-cluster-5dfd606f, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1774965ms till timeout)
2022-03-28 19:03:56 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (853758ms till timeout)
2022-03-28 19:03:56 [ForkJoinPool-1-worker-11] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-08515847-cruise-control-dcd7c895b-4pkw7 not ready: cruise-control)
2022-03-28 19:03:56 [ForkJoinPool-1-worker-11] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-08515847-cruise-control-dcd7c895b-4pkw7 not ready: tls-sidecar)
2022-03-28 19:03:56 [ForkJoinPool-1-worker-11] DEBUG [PodUtils:106] Pods my-cluster-08515847-cruise-control-dcd7c895b-4pkw7 are ready
2022-03-28 19:03:56 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-08515847, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-08515847-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590870ms till timeout)
2022-03-28 19:03:56 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-0 not ready: kafka)
2022-03-28 19:03:56 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-1 not ready: kafka)
2022-03-28 19:03:56 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-2 not ready: kafka)
2022-03-28 19:03:56 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-3 not ready: kafka)
2022-03-28 19:03:56 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:106] Pods my-cluster-afe273c9-kafka-0, my-cluster-afe273c9-kafka-1, my-cluster-afe273c9-kafka-2, my-cluster-afe273c9-kafka-3 are ready
2022-03-28 19:03:56 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2208450ms till timeout)
2022-03-28 19:03:56 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-206
2022-03-28 19:03:56 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:03:56 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-207
2022-03-28 19:03:56 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:03:57 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (131414ms till timeout)
2022-03-28 19:03:57 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-207
2022-03-28 19:03:57 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:03:57 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-208
2022-03-28 19:03:57 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (263727ms till timeout)
2022-03-28 19:03:57 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-0 not ready: kafka)
2022-03-28 19:03:57 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-1 not ready: kafka)
2022-03-28 19:03:57 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-2 not ready: kafka)
2022-03-28 19:03:57 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-5dfd606f-kafka-0, my-cluster-5dfd606f-kafka-1, my-cluster-5dfd606f-kafka-2 are ready
2022-03-28 19:03:57 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-5dfd606f-kafka, strimzi.io/cluster=my-cluster-5dfd606f, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1773682ms till timeout)
2022-03-28 19:03:57 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (852566ms till timeout)
2022-03-28 19:03:57 [ForkJoinPool-1-worker-11] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-08515847-cruise-control-dcd7c895b-4pkw7 not ready: cruise-control)
2022-03-28 19:03:57 [ForkJoinPool-1-worker-11] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-08515847-cruise-control-dcd7c895b-4pkw7 not ready: tls-sidecar)
2022-03-28 19:03:57 [ForkJoinPool-1-worker-11] DEBUG [PodUtils:106] Pods my-cluster-08515847-cruise-control-dcd7c895b-4pkw7 are ready
2022-03-28 19:03:57 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-08515847, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-08515847-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (589682ms till timeout)
2022-03-28 19:03:57 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-0 not ready: kafka)
2022-03-28 19:03:57 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-1 not ready: kafka)
2022-03-28 19:03:57 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-2 not ready: kafka)
2022-03-28 19:03:57 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-3 not ready: kafka)
2022-03-28 19:03:57 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:106] Pods my-cluster-afe273c9-kafka-0, my-cluster-afe273c9-kafka-1, my-cluster-afe273c9-kafka-2, my-cluster-afe273c9-kafka-3 are ready
2022-03-28 19:03:57 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2207262ms till timeout)
2022-03-28 19:03:57 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-208
2022-03-28 19:03:57 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:03:57 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-209
2022-03-28 19:03:58 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:03:58 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (130220ms till timeout)
2022-03-28 19:03:58 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-209
2022-03-28 19:03:58 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:03:58 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-21
2022-03-28 19:03:58 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (262535ms till timeout)
2022-03-28 19:03:58 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (851466ms till timeout)
2022-03-28 19:03:58 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-0 not ready: kafka)
2022-03-28 19:03:58 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-1 not ready: kafka)
2022-03-28 19:03:58 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-2 not ready: kafka)
2022-03-28 19:03:58 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-5dfd606f-kafka-0, my-cluster-5dfd606f-kafka-1, my-cluster-5dfd606f-kafka-2 are ready
2022-03-28 19:03:58 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-5dfd606f-kafka, strimzi.io/cluster=my-cluster-5dfd606f, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1772486ms till timeout)
2022-03-28 19:03:58 [ForkJoinPool-1-worker-11] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-08515847-cruise-control-dcd7c895b-4pkw7 not ready: cruise-control)
2022-03-28 19:03:58 [ForkJoinPool-1-worker-11] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-08515847-cruise-control-dcd7c895b-4pkw7 not ready: tls-sidecar)
2022-03-28 19:03:58 [ForkJoinPool-1-worker-11] DEBUG [PodUtils:106] Pods my-cluster-08515847-cruise-control-dcd7c895b-4pkw7 are ready
2022-03-28 19:03:58 [ForkJoinPool-1-worker-11] INFO  [DeploymentUtils:141] Deployment my-cluster-08515847-cruise-control rolling update finished
2022-03-28 19:03:58 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-0 not ready: kafka)
2022-03-28 19:03:58 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-1 not ready: kafka)
2022-03-28 19:03:58 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-2 not ready: kafka)
2022-03-28 19:03:58 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-3 not ready: kafka)
2022-03-28 19:03:58 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:106] Pods my-cluster-afe273c9-kafka-0, my-cluster-afe273c9-kafka-1, my-cluster-afe273c9-kafka-2, my-cluster-afe273c9-kafka-3 are ready
2022-03-28 19:03:58 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2206163ms till timeout)
2022-03-28 19:03:58 [ForkJoinPool-1-worker-11] INFO  [CruiseControlConfigurationST:280] Verifying that Kafka pods did not roll
2022-03-28 19:03:58 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Waiting for stability of rolling update will be not triggered
2022-03-28 19:03:58 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:03:58 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-21
2022-03-28 19:03:58 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:03:58 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-211
2022-03-28 19:03:58 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:03:58 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:03:58 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:50] At least my-cluster-08515847-kafka-0 hasn't rolled
2022-03-28 19:03:58 [ForkJoinPool-1-worker-11] INFO  [RollingUpdateUtils:143] {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7} pods didn't roll. Remaining seconds for stability: 50
2022-03-28 19:03:58 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (299894ms till timeout)
2022-03-28 19:03:59 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:03:59 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-211
2022-03-28 19:03:59 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:03:59 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-212
2022-03-28 19:03:59 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (129027ms till timeout)
2022-03-28 19:03:59 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (261343ms till timeout)
2022-03-28 19:03:59 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (850275ms till timeout)
2022-03-28 19:03:59 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-0 not ready: kafka)
2022-03-28 19:03:59 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-1 not ready: kafka)
2022-03-28 19:03:59 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-2 not ready: kafka)
2022-03-28 19:03:59 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-5dfd606f-kafka-0, my-cluster-5dfd606f-kafka-1, my-cluster-5dfd606f-kafka-2 are ready
2022-03-28 19:03:59 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-5dfd606f-kafka, strimzi.io/cluster=my-cluster-5dfd606f, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1771385ms till timeout)
2022-03-28 19:03:59 [ForkJoinPool-1-worker-7] INFO  [Exec:417] Command: oc --namespace namespace-0 exec my-cluster-959c6ed5-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-634125597-1746493046 --describe --bootstrap-server my-cluster-959c6ed5-kafka-bootstrap:9092
2022-03-28 19:03:59 [ForkJoinPool-1-worker-7] INFO  [Exec:417] Return code: 0
2022-03-28 19:03:59 [ForkJoinPool-1-worker-7] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 16 polls
2022-03-28 19:03:59 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (158415ms till timeout)
2022-03-28 19:03:59 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-0 not ready: kafka)
2022-03-28 19:03:59 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-1 not ready: kafka)
2022-03-28 19:03:59 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-2 not ready: kafka)
2022-03-28 19:03:59 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-3 not ready: kafka)
2022-03-28 19:03:59 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:106] Pods my-cluster-afe273c9-kafka-0, my-cluster-afe273c9-kafka-1, my-cluster-afe273c9-kafka-2, my-cluster-afe273c9-kafka-3 are ready
2022-03-28 19:03:59 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-afe273c9-kafka, strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2205062ms till timeout)
2022-03-28 19:03:59 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:03:59 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-212
2022-03-28 19:03:59 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:03:59 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-213
2022-03-28 19:04:00 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:00 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:00 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:50] At least my-cluster-08515847-kafka-0 hasn't rolled
2022-03-28 19:04:00 [ForkJoinPool-1-worker-11] INFO  [RollingUpdateUtils:143] {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7} pods didn't roll. Remaining seconds for stability: 49
2022-03-28 19:04:00 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (298793ms till timeout)
2022-03-28 19:04:00 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-213
2022-03-28 19:04:00 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:00 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-216
2022-03-28 19:04:00 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:04:00 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (127834ms till timeout)
2022-03-28 19:04:00 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-0 exec my-cluster-959c6ed5-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-634125597-1746493046 --describe --bootstrap-server my-cluster-959c6ed5-kafka-bootstrap:9092
2022-03-28 19:04:00 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (260150ms till timeout)
2022-03-28 19:04:00 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (849082ms till timeout)
2022-03-28 19:04:00 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-0 not ready: kafka)
2022-03-28 19:04:00 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-1 not ready: kafka)
2022-03-28 19:04:00 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-2 not ready: kafka)
2022-03-28 19:04:00 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-5dfd606f-kafka-0, my-cluster-5dfd606f-kafka-1, my-cluster-5dfd606f-kafka-2 are ready
2022-03-28 19:04:00 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-5dfd606f-kafka, strimzi.io/cluster=my-cluster-5dfd606f, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1770102ms till timeout)
2022-03-28 19:04:00 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-0 not ready: kafka)
2022-03-28 19:04:00 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-1 not ready: kafka)
2022-03-28 19:04:00 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-2 not ready: kafka)
2022-03-28 19:04:00 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-kafka-3 not ready: kafka)
2022-03-28 19:04:00 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:106] Pods my-cluster-afe273c9-kafka-0, my-cluster-afe273c9-kafka-1, my-cluster-afe273c9-kafka-2, my-cluster-afe273c9-kafka-3 are ready
2022-03-28 19:04:01 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:433] Wait for Kafka: my-cluster-afe273c9 will have desired state: Ready
2022-03-28 19:04:01 [ForkJoinPool-1-worker-25] DEBUG [TestUtils:125] Waiting for Kafka: my-cluster-afe273c9 will have desired state: Ready
2022-03-28 19:04:01 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:01 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-216
2022-03-28 19:04:01 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:01 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-218
2022-03-28 19:04:01 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:444] Kafka: my-cluster-afe273c9 is in desired state: Ready
2022-03-28 19:04:01 [ForkJoinPool-1-worker-25] INFO  [RollingUpdateUtils:132] Kafka: my-cluster-afe273c9 is ready
2022-03-28 19:04:01 [ForkJoinPool-1-worker-25] INFO  [ReconciliationST:94] Deploying KafkaConnect with pause annotation from the start, no pods should appear
2022-03-28 19:04:01 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:155] Create/Update Deployment my-cluster-afe273c9-kafka-clients in namespace namespace-9
2022-03-28 19:04:01 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:164] Using Namespace: namespace-7
2022-03-28 19:04:01 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:01 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:01 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:50] At least my-cluster-08515847-kafka-0 hasn't rolled
2022-03-28 19:04:01 [ForkJoinPool-1-worker-11] INFO  [RollingUpdateUtils:143] {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7} pods didn't roll. Remaining seconds for stability: 48
2022-03-28 19:04:01 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (297606ms till timeout)
2022-03-28 19:04:01 [ForkJoinPool-1-worker-25] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:my-cluster-afe273c9-kafka-clients
2022-03-28 19:04:01 [ForkJoinPool-1-worker-25] INFO  [DeploymentUtils:161] Wait for Deployment: my-cluster-afe273c9-kafka-clients will be ready
2022-03-28 19:04:01 [ForkJoinPool-1-worker-25] DEBUG [TestUtils:125] Waiting for Wait for Deployment: my-cluster-afe273c9-kafka-clients will be ready
2022-03-28 19:04:01 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-218
2022-03-28 19:04:01 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:01 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-219
2022-03-28 19:04:01 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Wait for Deployment: my-cluster-afe273c9-kafka-clients will be ready not ready, will try again in 1000 ms (479905ms till timeout)
2022-03-28 19:04:01 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:04:01 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (126642ms till timeout)
2022-03-28 19:04:01 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (258955ms till timeout)
2022-03-28 19:04:02 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (847889ms till timeout)
2022-03-28 19:04:02 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-0 not ready: kafka)
2022-03-28 19:04:02 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-1 not ready: kafka)
2022-03-28 19:04:02 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-2 not ready: kafka)
2022-03-28 19:04:02 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-5dfd606f-kafka-0, my-cluster-5dfd606f-kafka-1, my-cluster-5dfd606f-kafka-2 are ready
2022-03-28 19:04:02 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:433] Wait for Kafka: my-cluster-5dfd606f will have desired state: Ready
2022-03-28 19:04:02 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Kafka: my-cluster-5dfd606f will have desired state: Ready
2022-03-28 19:04:02 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-219
2022-03-28 19:04:02 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:02 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-22
2022-03-28 19:04:02 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:02 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:444] Kafka: my-cluster-5dfd606f is in desired state: Ready
2022-03-28 19:04:02 [ForkJoinPool-1-worker-15] INFO  [RollingUpdateUtils:132] Kafka: my-cluster-5dfd606f is ready
2022-03-28 19:04:02 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:02 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:02 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:50] At least my-cluster-08515847-kafka-0 hasn't rolled
2022-03-28 19:04:02 [ForkJoinPool-1-worker-11] INFO  [RollingUpdateUtils:143] {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7} pods didn't roll. Remaining seconds for stability: 47
2022-03-28 19:04:02 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (296473ms till timeout)
2022-03-28 19:04:02 [ForkJoinPool-1-worker-15] INFO  [CruiseControlConfigurationST:117] Verifying that in Cruise Control is not present in the Kafka cluster
2022-03-28 19:04:02 [ForkJoinPool-1-worker-15] INFO  [CruiseControlConfigurationST:120] Verifying that my-cluster-5dfd606f-cruise-control- pod is not present
2022-03-28 19:04:02 [ForkJoinPool-1-worker-15] INFO  [PodUtils:209] Wait until Pod my-cluster-5dfd606f-cruise-control- will have stable 0 replicas
2022-03-28 19:04:02 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for  Podmy-cluster-5dfd606f-cruise-control- will have 0 replicas
2022-03-28 19:04:02 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 19 polls
2022-03-28 19:04:02 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-5dfd606f-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (179901ms till timeout)
2022-03-28 19:04:02 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-22
2022-03-28 19:04:02 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:02 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-223
2022-03-28 19:04:02 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Wait for Deployment: my-cluster-afe273c9-kafka-clients will be ready not ready, will try again in 1000 ms (478774ms till timeout)
2022-03-28 19:04:02 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:04:03 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (125450ms till timeout)
2022-03-28 19:04:03 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (257764ms till timeout)
2022-03-28 19:04:03 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (846696ms till timeout)
2022-03-28 19:04:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-223
2022-03-28 19:04:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:03 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-224
2022-03-28 19:04:03 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:03 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:03 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:03 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:50] At least my-cluster-08515847-kafka-0 hasn't rolled
2022-03-28 19:04:03 [ForkJoinPool-1-worker-11] INFO  [RollingUpdateUtils:143] {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7} pods didn't roll. Remaining seconds for stability: 46
2022-03-28 19:04:03 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (295375ms till timeout)
2022-03-28 19:04:03 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 18 polls
2022-03-28 19:04:03 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-5dfd606f-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (178802ms till timeout)
2022-03-28 19:04:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-224
2022-03-28 19:04:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:03 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-225
2022-03-28 19:04:03 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Wait for Deployment: my-cluster-afe273c9-kafka-clients will be ready not ready, will try again in 1000 ms (477674ms till timeout)
2022-03-28 19:04:04 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:04:04 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (124257ms till timeout)
2022-03-28 19:04:04 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (256561ms till timeout)
2022-03-28 19:04:04 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-225
2022-03-28 19:04:04 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:04 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-227
2022-03-28 19:04:04 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (845505ms till timeout)
2022-03-28 19:04:04 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:04 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:04 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:04 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:50] At least my-cluster-08515847-kafka-0 hasn't rolled
2022-03-28 19:04:04 [ForkJoinPool-1-worker-11] INFO  [RollingUpdateUtils:143] {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7} pods didn't roll. Remaining seconds for stability: 45
2022-03-28 19:04:04 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (294276ms till timeout)
2022-03-28 19:04:04 [ForkJoinPool-1-worker-31] INFO  [InternalKafkaClient:195] Consumer finished correctly: true
2022-03-28 19:04:04 [ForkJoinPool-1-worker-31] INFO  [InternalKafkaClient:198] Consumer consumed 100 messages
2022-03-28 19:04:04 [ForkJoinPool-1-worker-31] INFO  [UserST:386] Checking if SCRAM-SHA user is able to send messages
2022-03-28 19:04:04 [ForkJoinPool-1-worker-31] DEBUG [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@284bf17, which are set.
2022-03-28 19:04:04 [ForkJoinPool-1-worker-31] INFO  [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@4665cb73, messages=[], arguments=[--bootstrap-server, my-cluster-f743fdfd-kafka-bootstrap.namespace-8.svc:9092, --topic, my-topic-1059523275-384667503, USER=top_secret_scramed_leopold, --max-messages, 100], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-f743fdfd-plain-kafka-clients-78f78497f7-vh9lx', podNamespace='namespace-8', bootstrapServer='my-cluster-f743fdfd-kafka-bootstrap.namespace-8.svc:9092', topicName='my-topic-1059523275-384667503', maxMessages=100, kafkaUsername='top-secret-scramed-leopold', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@284bf17}
2022-03-28 19:04:04 [ForkJoinPool-1-worker-31] INFO  [InternalKafkaClient:94] Producing 100 messages to my-cluster-f743fdfd-kafka-bootstrap.namespace-8.svc:9092:my-topic-1059523275-384667503 from pod my-cluster-f743fdfd-plain-kafka-clients-78f78497f7-vh9lx
2022-03-28 19:04:04 [ForkJoinPool-1-worker-31] INFO  [VerifiableClient:192] Client command: oc exec my-cluster-f743fdfd-plain-kafka-clients-78f78497f7-vh9lx -n namespace-8 -- /opt/kafka/producer.sh --bootstrap-server my-cluster-f743fdfd-kafka-bootstrap.namespace-8.svc:9092 --topic my-topic-1059523275-384667503 USER=top_secret_scramed_leopold --max-messages 100
2022-03-28 19:04:04 [ForkJoinPool-1-worker-31] TRACE [Exec:248] Running command - oc exec my-cluster-f743fdfd-plain-kafka-clients-78f78497f7-vh9lx -n namespace-8 -- /opt/kafka/producer.sh --bootstrap-server my-cluster-f743fdfd-kafka-bootstrap.namespace-8.svc:9092 --topic my-topic-1059523275-384667503 USER=top_secret_scramed_leopold --max-messages 100
2022-03-28 19:04:04 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-227
2022-03-28 19:04:04 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:04 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-229
2022-03-28 19:04:04 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 17 polls
2022-03-28 19:04:04 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-5dfd606f-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (177608ms till timeout)
2022-03-28 19:04:05 [ForkJoinPool-1-worker-25] INFO  [DeploymentUtils:168] Deployment: my-cluster-afe273c9-kafka-clients is ready
2022-03-28 19:04:05 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:155] Create/Update Deployment my-cluster-afe273c9-scraper in namespace namespace-7
2022-03-28 19:04:05 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:164] Using Namespace: namespace-7
2022-03-28 19:04:05 [ForkJoinPool-1-worker-25] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:my-cluster-afe273c9-scraper
2022-03-28 19:04:05 [ForkJoinPool-1-worker-25] INFO  [DeploymentUtils:161] Wait for Deployment: my-cluster-afe273c9-scraper will be ready
2022-03-28 19:04:05 [ForkJoinPool-1-worker-25] DEBUG [TestUtils:125] Waiting for Wait for Deployment: my-cluster-afe273c9-scraper will be ready
2022-03-28 19:04:05 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:02:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:04:05 [ForkJoinPool-1-worker-7] INFO  [Exec:417] Command: oc --namespace namespace-0 exec my-cluster-959c6ed5-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-634125597-1746493046 --describe --bootstrap-server my-cluster-959c6ed5-kafka-bootstrap:9092
2022-03-28 19:04:05 [ForkJoinPool-1-worker-7] INFO  [Exec:417] Return code: 0
2022-03-28 19:04:05 [ForkJoinPool-1-worker-7] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 15 polls
2022-03-28 19:04:05 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (152673ms till timeout)
2022-03-28 19:04:05 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Wait for Deployment: my-cluster-afe273c9-scraper will be ready not ready, will try again in 1000 ms (479811ms till timeout)
2022-03-28 19:04:05 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (255401ms till timeout)
2022-03-28 19:04:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-229
2022-03-28 19:04:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:05 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-231
2022-03-28 19:04:05 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:05 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (122901ms till timeout)
2022-03-28 19:04:05 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (844339ms till timeout)
2022-03-28 19:04:05 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:05 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:05 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:50] At least my-cluster-08515847-kafka-0 hasn't rolled
2022-03-28 19:04:05 [ForkJoinPool-1-worker-11] INFO  [RollingUpdateUtils:143] {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7} pods didn't roll. Remaining seconds for stability: 44
2022-03-28 19:04:05 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (293160ms till timeout)
2022-03-28 19:04:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-231
2022-03-28 19:04:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:06 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-235
2022-03-28 19:04:06 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 16 polls
2022-03-28 19:04:06 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-5dfd606f-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (176417ms till timeout)
2022-03-28 19:04:06 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-0 exec my-cluster-959c6ed5-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-634125597-1746493046 --describe --bootstrap-server my-cluster-959c6ed5-kafka-bootstrap:9092
2022-03-28 19:04:06 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Wait for Deployment: my-cluster-afe273c9-scraper will be ready not ready, will try again in 1000 ms (478715ms till timeout)
2022-03-28 19:04:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-235
2022-03-28 19:04:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:06 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-237
2022-03-28 19:04:06 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:06 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (254216ms till timeout)
2022-03-28 19:04:06 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=null, completedIndexes=null, completionTime=2022-03-28T19:04:01Z, conditions=[JobCondition(lastProbeTime=2022-03-28T19:04:01Z, lastTransitionTime=2022-03-28T19:04:01Z, message=null, reason=null, status=True, type=Complete, additionalProperties={})], failed=null, ready=null, startTime=2022-03-28T19:02:23Z, succeeded=1, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:04:06 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (843148ms till timeout)
2022-03-28 19:04:06 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:06 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:06 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:50] At least my-cluster-08515847-kafka-0 hasn't rolled
2022-03-28 19:04:06 [ForkJoinPool-1-worker-11] INFO  [RollingUpdateUtils:143] {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7} pods didn't roll. Remaining seconds for stability: 43
2022-03-28 19:04:06 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (291967ms till timeout)
2022-03-28 19:04:07 [ForkJoinPool-1-worker-41] INFO  [PodUtils:186] Waiting for message will be in the log
2022-03-28 19:04:07 [ForkJoinPool-1-worker-41] DEBUG [TestUtils:125] Waiting for Waiting for message will be in the log
2022-03-28 19:04:07 [ForkJoinPool-1-worker-41] INFO  [PodUtils:189] Message All topics created found in create-admin-my-cluster-836b31f8-kafka-clients-qpcms log
2022-03-28 19:04:07 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-237
2022-03-28 19:04:07 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:07 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-238
2022-03-28 19:04:07 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 15 polls
2022-03-28 19:04:07 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-5dfd606f-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (175208ms till timeout)
2022-03-28 19:04:07 [ForkJoinPool-1-worker-41] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment create-admin-my-cluster-836b31f8-kafka-clients deletion
2022-03-28 19:04:07 [ForkJoinPool-1-worker-41] DEBUG [TestUtils:125] Waiting for ReplicaSet create-admin-my-cluster-836b31f8-kafka-clients to be deleted
2022-03-28 19:04:07 [ForkJoinPool-1-worker-41] DEBUG [JobUtils:40] Job create-admin-my-cluster-836b31f8-kafka-clients was deleted
2022-03-28 19:04:07 [ForkJoinPool-1-worker-41] INFO  [ThrottlingQuotaST:112] Executing 3/5 iteration.
2022-03-28 19:04:07 [ForkJoinPool-1-worker-41] INFO  [ResourceManager:155] Create/Update Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st
2022-03-28 19:04:07 [ForkJoinPool-1-worker-41] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:create-admin-my-cluster-836b31f8-kafka-clients
2022-03-28 19:04:07 [ForkJoinPool-1-worker-41] INFO  [JobUtils:81] Waiting for job: create-admin-my-cluster-836b31f8-kafka-clients will be in active state
2022-03-28 19:04:07 [ForkJoinPool-1-worker-41] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 19:04:07 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Wait for Deployment: my-cluster-afe273c9-scraper will be ready not ready, will try again in 1000 ms (477548ms till timeout)
2022-03-28 19:04:07 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-238
2022-03-28 19:04:07 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:07 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-239
2022-03-28 19:04:07 [ForkJoinPool-1-worker-41] INFO  [ClientUtils:76] Waiting for producer/consumer:create-admin-my-cluster-836b31f8-kafka-clients to finished
2022-03-28 19:04:07 [ForkJoinPool-1-worker-41] DEBUG [TestUtils:125] Waiting for job finished
2022-03-28 19:04:07 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (253047ms till timeout)
2022-03-28 19:04:07 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:07 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (841985ms till timeout)
2022-03-28 19:04:08 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:04:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:04:08 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:08 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:08 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:50] At least my-cluster-08515847-kafka-0 hasn't rolled
2022-03-28 19:04:08 [ForkJoinPool-1-worker-11] INFO  [RollingUpdateUtils:143] {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7} pods didn't roll. Remaining seconds for stability: 42
2022-03-28 19:04:08 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (290802ms till timeout)
2022-03-28 19:04:08 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (219711ms till timeout)
2022-03-28 19:04:08 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-239
2022-03-28 19:04:08 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:08 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-240
2022-03-28 19:04:08 [ForkJoinPool-1-worker-31] INFO  [InternalKafkaClient:97] Producer finished correctly: true
2022-03-28 19:04:08 [ForkJoinPool-1-worker-31] INFO  [InternalKafkaClient:101] Producer produced 100 messages
2022-03-28 19:04:08 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 14 polls
2022-03-28 19:04:08 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-5dfd606f-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (174018ms till timeout)
2022-03-28 19:04:08 [ForkJoinPool-1-worker-31] DEBUG [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@2390d028, which are set.
2022-03-28 19:04:08 [ForkJoinPool-1-worker-31] INFO  [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@57b2b8a, messages=[], arguments=[--bootstrap-server, my-cluster-f743fdfd-kafka-bootstrap.namespace-8.svc:9092, --topic, my-topic-1059523275-384667503, USER=top_secret_scramed_leopold, --group-instance-id, instance1210104908, --group-id, my-consumer-group-1616692438, --max-messages, 100], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-f743fdfd-plain-kafka-clients-78f78497f7-vh9lx', podNamespace='namespace-8', bootstrapServer='my-cluster-f743fdfd-kafka-bootstrap.namespace-8.svc:9092', topicName='my-topic-1059523275-384667503', maxMessages=100, kafkaUsername='top-secret-scramed-leopold', consumerGroupName='my-consumer-group-1616692438', consumerInstanceId='instance1210104908', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@2390d028}
2022-03-28 19:04:08 [ForkJoinPool-1-worker-31] INFO  [InternalKafkaClient:157] Consuming 100 messages from my-cluster-f743fdfd-kafka-bootstrap.namespace-8.svc:9092#my-topic-1059523275-384667503 from pod my-cluster-f743fdfd-plain-kafka-clients-78f78497f7-vh9lx
2022-03-28 19:04:08 [ForkJoinPool-1-worker-31] INFO  [VerifiableClient:192] Client command: oc exec my-cluster-f743fdfd-plain-kafka-clients-78f78497f7-vh9lx -n namespace-8 -- /opt/kafka/consumer.sh --bootstrap-server my-cluster-f743fdfd-kafka-bootstrap.namespace-8.svc:9092 --topic my-topic-1059523275-384667503 USER=top_secret_scramed_leopold --group-instance-id instance1210104908 --group-id my-consumer-group-1616692438 --max-messages 100
2022-03-28 19:04:08 [ForkJoinPool-1-worker-31] TRACE [Exec:248] Running command - oc exec my-cluster-f743fdfd-plain-kafka-clients-78f78497f7-vh9lx -n namespace-8 -- /opt/kafka/consumer.sh --bootstrap-server my-cluster-f743fdfd-kafka-bootstrap.namespace-8.svc:9092 --topic my-topic-1059523275-384667503 USER=top_secret_scramed_leopold --group-instance-id instance1210104908 --group-id my-consumer-group-1616692438 --max-messages 100
2022-03-28 19:04:08 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-240
2022-03-28 19:04:08 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:08 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-243
2022-03-28 19:04:08 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Wait for Deployment: my-cluster-afe273c9-scraper will be ready not ready, will try again in 1000 ms (476450ms till timeout)
2022-03-28 19:04:08 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (251947ms till timeout)
2022-03-28 19:04:09 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:09 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (840880ms till timeout)
2022-03-28 19:04:09 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:09 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:09 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:50] At least my-cluster-08515847-kafka-0 hasn't rolled
2022-03-28 19:04:09 [ForkJoinPool-1-worker-11] INFO  [RollingUpdateUtils:143] {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7} pods didn't roll. Remaining seconds for stability: 41
2022-03-28 19:04:09 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (289698ms till timeout)
2022-03-28 19:04:09 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:04:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:04:09 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (218513ms till timeout)
2022-03-28 19:04:09 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-243
2022-03-28 19:04:09 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:09 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-245
2022-03-28 19:04:09 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 13 polls
2022-03-28 19:04:09 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-5dfd606f-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (172828ms till timeout)
2022-03-28 19:04:09 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-245
2022-03-28 19:04:09 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:09 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-249
2022-03-28 19:04:09 [ForkJoinPool-1-worker-25] INFO  [DeploymentUtils:168] Deployment: my-cluster-afe273c9-scraper is ready
2022-03-28 19:04:09 [ForkJoinPool-1-worker-25] INFO  [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-afe273c9-scraper to be ready
2022-03-28 19:04:10 [ForkJoinPool-1-worker-25] DEBUG [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-afe273c9-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready
2022-03-28 19:04:10 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:10 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (250755ms till timeout)
2022-03-28 19:04:10 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (839686ms till timeout)
2022-03-28 19:04:10 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-scraper-676d844d88-rjhqh not ready: my-cluster-afe273c9-scraper)
2022-03-28 19:04:10 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:106] Pods my-cluster-afe273c9-scraper-676d844d88-rjhqh are ready
2022-03-28 19:04:10 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-afe273c9-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599803ms till timeout)
2022-03-28 19:04:10 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:10 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:10 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:50] At least my-cluster-08515847-kafka-0 hasn't rolled
2022-03-28 19:04:10 [ForkJoinPool-1-worker-11] INFO  [RollingUpdateUtils:143] {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7} pods didn't roll. Remaining seconds for stability: 40
2022-03-28 19:04:10 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (288504ms till timeout)
2022-03-28 19:04:10 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:04:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:04:10 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-249
2022-03-28 19:04:10 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:10 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-25
2022-03-28 19:04:10 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (217318ms till timeout)
2022-03-28 19:04:10 [ForkJoinPool-1-worker-7] INFO  [Exec:417] Command: oc --namespace namespace-0 exec my-cluster-959c6ed5-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-634125597-1746493046 --describe --bootstrap-server my-cluster-959c6ed5-kafka-bootstrap:9092
2022-03-28 19:04:10 [ForkJoinPool-1-worker-7] INFO  [Exec:417] Return code: 0
2022-03-28 19:04:10 [ForkJoinPool-1-worker-7] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 14 polls
2022-03-28 19:04:10 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (147281ms till timeout)
2022-03-28 19:04:10 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 12 polls
2022-03-28 19:04:10 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-5dfd606f-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (171638ms till timeout)
2022-03-28 19:04:10 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-25
2022-03-28 19:04:10 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:10 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-252
2022-03-28 19:04:11 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (249653ms till timeout)
2022-03-28 19:04:11 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:11 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (838585ms till timeout)
2022-03-28 19:04:11 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-scraper-676d844d88-rjhqh not ready: my-cluster-afe273c9-scraper)
2022-03-28 19:04:11 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:106] Pods my-cluster-afe273c9-scraper-676d844d88-rjhqh are ready
2022-03-28 19:04:11 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-afe273c9-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598701ms till timeout)
2022-03-28 19:04:11 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:11 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:11 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:50] At least my-cluster-08515847-kafka-0 hasn't rolled
2022-03-28 19:04:11 [ForkJoinPool-1-worker-11] INFO  [RollingUpdateUtils:143] {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7} pods didn't roll. Remaining seconds for stability: 39
2022-03-28 19:04:11 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (287402ms till timeout)
2022-03-28 19:04:11 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-252
2022-03-28 19:04:11 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:11 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-253
2022-03-28 19:04:11 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:04:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:04:11 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (216126ms till timeout)
2022-03-28 19:04:11 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-0 exec my-cluster-959c6ed5-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-634125597-1746493046 --describe --bootstrap-server my-cluster-959c6ed5-kafka-bootstrap:9092
2022-03-28 19:04:12 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 11 polls
2022-03-28 19:04:12 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-5dfd606f-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (170447ms till timeout)
2022-03-28 19:04:12 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-253
2022-03-28 19:04:12 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:12 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-255
2022-03-28 19:04:12 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (248550ms till timeout)
2022-03-28 19:04:12 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:12 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (837481ms till timeout)
2022-03-28 19:04:12 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-scraper-676d844d88-rjhqh not ready: my-cluster-afe273c9-scraper)
2022-03-28 19:04:12 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:106] Pods my-cluster-afe273c9-scraper-676d844d88-rjhqh are ready
2022-03-28 19:04:12 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-afe273c9-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597596ms till timeout)
2022-03-28 19:04:12 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:12 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:12 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:50] At least my-cluster-08515847-kafka-0 hasn't rolled
2022-03-28 19:04:12 [ForkJoinPool-1-worker-11] INFO  [RollingUpdateUtils:143] {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7} pods didn't roll. Remaining seconds for stability: 38
2022-03-28 19:04:12 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (286299ms till timeout)
2022-03-28 19:04:12 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-255
2022-03-28 19:04:12 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:12 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-258
2022-03-28 19:04:12 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:04:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:04:12 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (214931ms till timeout)
2022-03-28 19:04:13 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 10 polls
2022-03-28 19:04:13 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-5dfd606f-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (169256ms till timeout)
2022-03-28 19:04:13 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-258
2022-03-28 19:04:13 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:13 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-26
2022-03-28 19:04:13 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (247449ms till timeout)
2022-03-28 19:04:13 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:13 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (836381ms till timeout)
2022-03-28 19:04:13 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-scraper-676d844d88-rjhqh not ready: my-cluster-afe273c9-scraper)
2022-03-28 19:04:13 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:106] Pods my-cluster-afe273c9-scraper-676d844d88-rjhqh are ready
2022-03-28 19:04:13 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-afe273c9-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596498ms till timeout)
2022-03-28 19:04:13 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:13 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:13 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:50] At least my-cluster-08515847-kafka-0 hasn't rolled
2022-03-28 19:04:13 [ForkJoinPool-1-worker-11] INFO  [RollingUpdateUtils:143] {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7} pods didn't roll. Remaining seconds for stability: 37
2022-03-28 19:04:13 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (285200ms till timeout)
2022-03-28 19:04:13 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-26
2022-03-28 19:04:13 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:13 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-261
2022-03-28 19:04:14 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:04:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:04:14 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (213737ms till timeout)
2022-03-28 19:04:14 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 9 polls
2022-03-28 19:04:14 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-5dfd606f-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (168065ms till timeout)
2022-03-28 19:04:14 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-261
2022-03-28 19:04:14 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:14 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-263
2022-03-28 19:04:14 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (246349ms till timeout)
2022-03-28 19:04:14 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:14 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (835279ms till timeout)
2022-03-28 19:04:14 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-scraper-676d844d88-rjhqh not ready: my-cluster-afe273c9-scraper)
2022-03-28 19:04:14 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:106] Pods my-cluster-afe273c9-scraper-676d844d88-rjhqh are ready
2022-03-28 19:04:14 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-afe273c9-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595395ms till timeout)
2022-03-28 19:04:14 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:14 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:14 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:50] At least my-cluster-08515847-kafka-0 hasn't rolled
2022-03-28 19:04:14 [ForkJoinPool-1-worker-11] INFO  [RollingUpdateUtils:143] {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7} pods didn't roll. Remaining seconds for stability: 36
2022-03-28 19:04:14 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (284095ms till timeout)
2022-03-28 19:04:15 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-263
2022-03-28 19:04:15 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:15 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-264
2022-03-28 19:04:15 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:04:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:04:15 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (212544ms till timeout)
2022-03-28 19:04:15 [ForkJoinPool-1-worker-7] INFO  [Exec:417] Command: oc --namespace namespace-0 exec my-cluster-959c6ed5-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-634125597-1746493046 --describe --bootstrap-server my-cluster-959c6ed5-kafka-bootstrap:9092
2022-03-28 19:04:15 [ForkJoinPool-1-worker-7] INFO  [Exec:417] Return code: 0
2022-03-28 19:04:15 [ForkJoinPool-1-worker-7] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 13 polls
2022-03-28 19:04:15 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (142546ms till timeout)
2022-03-28 19:04:15 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 8 polls
2022-03-28 19:04:15 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-5dfd606f-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (166872ms till timeout)
2022-03-28 19:04:15 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (245206ms till timeout)
2022-03-28 19:04:15 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-264
2022-03-28 19:04:15 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:15 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-267
2022-03-28 19:04:15 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:15 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-scraper-676d844d88-rjhqh not ready: my-cluster-afe273c9-scraper)
2022-03-28 19:04:15 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:106] Pods my-cluster-afe273c9-scraper-676d844d88-rjhqh are ready
2022-03-28 19:04:15 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (834142ms till timeout)
2022-03-28 19:04:15 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-afe273c9-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594259ms till timeout)
2022-03-28 19:04:15 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:15 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:15 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:50] At least my-cluster-08515847-kafka-0 hasn't rolled
2022-03-28 19:04:15 [ForkJoinPool-1-worker-11] INFO  [RollingUpdateUtils:143] {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7} pods didn't roll. Remaining seconds for stability: 35
2022-03-28 19:04:15 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (282961ms till timeout)
2022-03-28 19:04:16 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-267
2022-03-28 19:04:16 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:16 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-271
2022-03-28 19:04:16 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:04:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:04:16 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (211352ms till timeout)
2022-03-28 19:04:16 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-0 exec my-cluster-959c6ed5-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-634125597-1746493046 --describe --bootstrap-server my-cluster-959c6ed5-kafka-bootstrap:9092
2022-03-28 19:04:16 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-271
2022-03-28 19:04:16 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:16 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-272
2022-03-28 19:04:16 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 7 polls
2022-03-28 19:04:16 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-5dfd606f-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (165683ms till timeout)
2022-03-28 19:04:16 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:16 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-scraper-676d844d88-rjhqh not ready: my-cluster-afe273c9-scraper)
2022-03-28 19:04:16 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:106] Pods my-cluster-afe273c9-scraper-676d844d88-rjhqh are ready
2022-03-28 19:04:16 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-afe273c9-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593161ms till timeout)
2022-03-28 19:04:16 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (833044ms till timeout)
2022-03-28 19:04:16 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (244015ms till timeout)
2022-03-28 19:04:16 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:16 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:16 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:50] At least my-cluster-08515847-kafka-0 hasn't rolled
2022-03-28 19:04:16 [ForkJoinPool-1-worker-11] INFO  [RollingUpdateUtils:143] {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7} pods didn't roll. Remaining seconds for stability: 34
2022-03-28 19:04:16 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (281862ms till timeout)
2022-03-28 19:04:17 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-272
2022-03-28 19:04:17 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:17 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-275
2022-03-28 19:04:17 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:04:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:04:17 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (210160ms till timeout)
2022-03-28 19:04:17 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-275
2022-03-28 19:04:17 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:17 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-276
2022-03-28 19:04:17 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:18 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 6 polls
2022-03-28 19:04:18 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-5dfd606f-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (164491ms till timeout)
2022-03-28 19:04:18 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-scraper-676d844d88-rjhqh not ready: my-cluster-afe273c9-scraper)
2022-03-28 19:04:18 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:106] Pods my-cluster-afe273c9-scraper-676d844d88-rjhqh are ready
2022-03-28 19:04:18 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-afe273c9-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591967ms till timeout)
2022-03-28 19:04:18 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (242740ms till timeout)
2022-03-28 19:04:18 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (831766ms till timeout)
2022-03-28 19:04:18 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:18 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:18 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:50] At least my-cluster-08515847-kafka-0 hasn't rolled
2022-03-28 19:04:18 [ForkJoinPool-1-worker-11] INFO  [RollingUpdateUtils:143] {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7} pods didn't roll. Remaining seconds for stability: 33
2022-03-28 19:04:18 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (280676ms till timeout)
2022-03-28 19:04:18 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-276
2022-03-28 19:04:18 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:18 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-279
2022-03-28 19:04:18 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:04:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:04:18 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (208967ms till timeout)
2022-03-28 19:04:19 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-279
2022-03-28 19:04:19 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:19 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-280
2022-03-28 19:04:19 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:19 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 5 polls
2022-03-28 19:04:19 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-5dfd606f-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (163301ms till timeout)
2022-03-28 19:04:19 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (830663ms till timeout)
2022-03-28 19:04:19 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (241548ms till timeout)
2022-03-28 19:04:19 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-scraper-676d844d88-rjhqh not ready: my-cluster-afe273c9-scraper)
2022-03-28 19:04:19 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:106] Pods my-cluster-afe273c9-scraper-676d844d88-rjhqh are ready
2022-03-28 19:04:19 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-afe273c9-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590692ms till timeout)
2022-03-28 19:04:19 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:19 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:19 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:50] At least my-cluster-08515847-kafka-0 hasn't rolled
2022-03-28 19:04:19 [ForkJoinPool-1-worker-11] INFO  [RollingUpdateUtils:143] {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7} pods didn't roll. Remaining seconds for stability: 32
2022-03-28 19:04:19 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (279484ms till timeout)
2022-03-28 19:04:19 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-280
2022-03-28 19:04:19 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:19 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-282
2022-03-28 19:04:19 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:04:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:04:20 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (207773ms till timeout)
2022-03-28 19:04:20 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-282
2022-03-28 19:04:20 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:20 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-284
2022-03-28 19:04:20 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:20 [ForkJoinPool-1-worker-7] INFO  [Exec:417] Command: oc --namespace namespace-0 exec my-cluster-959c6ed5-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-634125597-1746493046 --describe --bootstrap-server my-cluster-959c6ed5-kafka-bootstrap:9092
2022-03-28 19:04:20 [ForkJoinPool-1-worker-7] INFO  [Exec:417] Return code: 0
2022-03-28 19:04:20 [ForkJoinPool-1-worker-7] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 12 polls
2022-03-28 19:04:20 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (137800ms till timeout)
2022-03-28 19:04:20 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 4 polls
2022-03-28 19:04:20 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-5dfd606f-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (162110ms till timeout)
2022-03-28 19:04:20 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (829472ms till timeout)
2022-03-28 19:04:20 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-scraper-676d844d88-rjhqh not ready: my-cluster-afe273c9-scraper)
2022-03-28 19:04:20 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:106] Pods my-cluster-afe273c9-scraper-676d844d88-rjhqh are ready
2022-03-28 19:04:20 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-afe273c9-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (589588ms till timeout)
2022-03-28 19:04:20 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (240358ms till timeout)
2022-03-28 19:04:20 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:20 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:20 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:50] At least my-cluster-08515847-kafka-0 hasn't rolled
2022-03-28 19:04:20 [ForkJoinPool-1-worker-11] INFO  [RollingUpdateUtils:143] {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7} pods didn't roll. Remaining seconds for stability: 31
2022-03-28 19:04:20 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (278294ms till timeout)
2022-03-28 19:04:20 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-284
2022-03-28 19:04:20 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:20 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-288
2022-03-28 19:04:21 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:04:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:04:21 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (206579ms till timeout)
2022-03-28 19:04:21 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-0 exec my-cluster-959c6ed5-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-634125597-1746493046 --describe --bootstrap-server my-cluster-959c6ed5-kafka-bootstrap:9092
2022-03-28 19:04:21 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-288
2022-03-28 19:04:21 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:21 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-29
2022-03-28 19:04:21 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:21 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 3 polls
2022-03-28 19:04:21 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-5dfd606f-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (160919ms till timeout)
2022-03-28 19:04:21 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (828279ms till timeout)
2022-03-28 19:04:21 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-scraper-676d844d88-rjhqh not ready: my-cluster-afe273c9-scraper)
2022-03-28 19:04:21 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:106] Pods my-cluster-afe273c9-scraper-676d844d88-rjhqh are ready
2022-03-28 19:04:21 [ForkJoinPool-1-worker-25] INFO  [DeploymentUtils:197] Deployment my-cluster-afe273c9-scraper is ready
2022-03-28 19:04:21 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (239168ms till timeout)
2022-03-28 19:04:21 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:21 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:21 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:50] At least my-cluster-08515847-kafka-0 hasn't rolled
2022-03-28 19:04:21 [ForkJoinPool-1-worker-11] INFO  [RollingUpdateUtils:143] {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7} pods didn't roll. Remaining seconds for stability: 30
2022-03-28 19:04:21 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (277104ms till timeout)
2022-03-28 19:04:21 [ForkJoinPool-1-worker-25] INFO  [NetworkPolicyResource:187] Apply NetworkPolicy access to my-cluster-afe273c9-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={})
2022-03-28 19:04:21 [ForkJoinPool-1-worker-25] DEBUG [NetworkPolicyResource:227] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=my-cluster-afe273c9-allow, namespace=namespace-7, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[NetworkPolicyIngressRule(from=[NetworkPolicyPeer(ipBlock=null, namespaceSelector=null, podSelector=LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}), additionalProperties={})], ports=[NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=8083, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={}), NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=9404, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={}), NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=8080, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={}), NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=9999, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={})], additionalProperties={})], podSelector=LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=KafkaConnect, strimzi.io/name=my-cluster-afe273c9-connect}, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 19:04:21 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:155] Create/Update NetworkPolicy my-cluster-afe273c9-allow in namespace namespace-7
2022-03-28 19:04:21 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:164] Using Namespace: namespace-7
2022-03-28 19:04:21 [ForkJoinPool-1-worker-25] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource NetworkPolicy:my-cluster-afe273c9-allow
2022-03-28 19:04:22 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-29
2022-03-28 19:04:22 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:22 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-290
2022-03-28 19:04:22 [ForkJoinPool-1-worker-25] INFO  [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}) successfully created
2022-03-28 19:04:22 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:155] Create/Update KafkaConnect my-cluster-afe273c9 in namespace namespace-9
2022-03-28 19:04:22 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:164] Using Namespace: namespace-7
2022-03-28 19:04:22 [ForkJoinPool-1-worker-25] WARN  [VersionUsageUtils:60] The client is using resource type 'kafkaconnects' with unstable version 'v1beta2'
2022-03-28 19:04:22 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:433] Wait for KafkaConnect: my-cluster-afe273c9 will have desired state: ReconciliationPaused
2022-03-28 19:04:22 [ForkJoinPool-1-worker-25] DEBUG [TestUtils:125] Waiting for KafkaConnect: my-cluster-afe273c9 will have desired state: ReconciliationPaused
2022-03-28 19:04:22 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:444] KafkaConnect: my-cluster-afe273c9 is in desired state: ReconciliationPaused
2022-03-28 19:04:22 [ForkJoinPool-1-worker-25] INFO  [PodUtils:209] Wait until Pod my-cluster-afe273c9-connect will have stable 0 replicas
2022-03-28 19:04:22 [ForkJoinPool-1-worker-25] DEBUG [TestUtils:125] Waiting for  Podmy-cluster-afe273c9-connect will have 0 replicas
2022-03-28 19:04:22 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:04:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:04:22 [ForkJoinPool-1-worker-25] INFO  [PodUtils:225] Pod replicas gonna be stable in 19 polls
2022-03-28 19:04:22 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176]  Podmy-cluster-afe273c9-connect will have 0 replicas not ready, will try again in 1000 ms (179806ms till timeout)
2022-03-28 19:04:22 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-290
2022-03-28 19:04:22 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:22 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-294
2022-03-28 19:04:22 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (205204ms till timeout)
2022-03-28 19:04:22 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:22 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 2 polls
2022-03-28 19:04:22 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-5dfd606f-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (159821ms till timeout)
2022-03-28 19:04:22 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (827091ms till timeout)
2022-03-28 19:04:22 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (238057ms till timeout)
2022-03-28 19:04:22 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:22 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:22 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:50] At least my-cluster-08515847-kafka-0 hasn't rolled
2022-03-28 19:04:22 [ForkJoinPool-1-worker-11] INFO  [RollingUpdateUtils:143] {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7} pods didn't roll. Remaining seconds for stability: 29
2022-03-28 19:04:22 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (275910ms till timeout)
2022-03-28 19:04:23 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-294
2022-03-28 19:04:23 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:23 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-297
2022-03-28 19:04:23 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:04:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:04:23 [ForkJoinPool-1-worker-25] INFO  [PodUtils:225] Pod replicas gonna be stable in 18 polls
2022-03-28 19:04:23 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176]  Podmy-cluster-afe273c9-connect will have 0 replicas not ready, will try again in 1000 ms (178588ms till timeout)
2022-03-28 19:04:23 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-297
2022-03-28 19:04:23 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:23 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-298
2022-03-28 19:04:23 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 1 polls
2022-03-28 19:04:23 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-5dfd606f-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (158696ms till timeout)
2022-03-28 19:04:23 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:23 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (203889ms till timeout)
2022-03-28 19:04:23 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (825967ms till timeout)
2022-03-28 19:04:23 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (236935ms till timeout)
2022-03-28 19:04:24 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:24 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:24 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:50] At least my-cluster-08515847-kafka-0 hasn't rolled
2022-03-28 19:04:24 [ForkJoinPool-1-worker-11] INFO  [RollingUpdateUtils:143] {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7} pods didn't roll. Remaining seconds for stability: 28
2022-03-28 19:04:24 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (274786ms till timeout)
2022-03-28 19:04:24 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-298
2022-03-28 19:04:24 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:24 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-299
2022-03-28 19:04:24 [ForkJoinPool-1-worker-7] INFO  [Exec:417] Command: oc --namespace namespace-0 exec my-cluster-959c6ed5-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-634125597-1746493046 --describe --bootstrap-server my-cluster-959c6ed5-kafka-bootstrap:9092
2022-03-28 19:04:24 [ForkJoinPool-1-worker-7] INFO  [Exec:417] Return code: 0
2022-03-28 19:04:24 [ForkJoinPool-1-worker-7] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 11 polls
2022-03-28 19:04:24 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (133360ms till timeout)
2022-03-28 19:04:24 [ForkJoinPool-1-worker-25] INFO  [PodUtils:225] Pod replicas gonna be stable in 17 polls
2022-03-28 19:04:24 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176]  Podmy-cluster-afe273c9-connect will have 0 replicas not ready, will try again in 1000 ms (177395ms till timeout)
2022-03-28 19:04:24 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-299
2022-03-28 19:04:24 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:24 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-3
2022-03-28 19:04:25 [ForkJoinPool-1-worker-15] INFO  [PodUtils:217] Pod replicas are stable for 20 polls intervals
2022-03-28 19:04:25 [ForkJoinPool-1-worker-15] INFO  [PodUtils:228] Pod my-cluster-5dfd606f-cruise-control- has 0 replicas
2022-03-28 19:04:25 [ForkJoinPool-1-worker-15] INFO  [CruiseControlConfigurationST:123] Verifying that in Kafka config map there is no configuration to cruise control metric reporter
2022-03-28 19:04:25 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:04:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:04:25 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:25 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (824780ms till timeout)
2022-03-28 19:04:25 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (235748ms till timeout)
2022-03-28 19:04:25 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties
2022-03-28 19:04:25 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (202605ms till timeout)
2022-03-28 19:04:25 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (120000ms till timeout)
2022-03-28 19:04:25 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:25 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:25 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:50] At least my-cluster-08515847-kafka-0 hasn't rolled
2022-03-28 19:04:25 [ForkJoinPool-1-worker-11] INFO  [RollingUpdateUtils:143] {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7} pods didn't roll. Remaining seconds for stability: 27
2022-03-28 19:04:25 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (273597ms till timeout)
2022-03-28 19:04:25 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-3
2022-03-28 19:04:25 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:25 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-30
2022-03-28 19:04:25 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-0 exec my-cluster-959c6ed5-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-634125597-1746493046 --describe --bootstrap-server my-cluster-959c6ed5-kafka-bootstrap:9092
2022-03-28 19:04:26 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-30
2022-03-28 19:04:26 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:26 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-303
2022-03-28 19:04:26 [ForkJoinPool-1-worker-25] INFO  [PodUtils:225] Pod replicas gonna be stable in 16 polls
2022-03-28 19:04:26 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176]  Podmy-cluster-afe273c9-connect will have 0 replicas not ready, will try again in 1000 ms (176205ms till timeout)
2022-03-28 19:04:26 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (118999ms till timeout)
2022-03-28 19:04:26 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (823683ms till timeout)
2022-03-28 19:04:26 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:26 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:04:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:04:26 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (234558ms till timeout)
2022-03-28 19:04:26 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:26 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:26 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:50] At least my-cluster-08515847-kafka-0 hasn't rolled
2022-03-28 19:04:26 [ForkJoinPool-1-worker-11] INFO  [RollingUpdateUtils:143] {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7} pods didn't roll. Remaining seconds for stability: 26
2022-03-28 19:04:26 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (272407ms till timeout)
2022-03-28 19:04:26 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (201319ms till timeout)
2022-03-28 19:04:26 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-303
2022-03-28 19:04:26 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:26 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-304
2022-03-28 19:04:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-304
2022-03-28 19:04:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:27 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-305
2022-03-28 19:04:27 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (117999ms till timeout)
2022-03-28 19:04:27 [ForkJoinPool-1-worker-25] INFO  [PodUtils:225] Pod replicas gonna be stable in 15 polls
2022-03-28 19:04:27 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176]  Podmy-cluster-afe273c9-connect will have 0 replicas not ready, will try again in 1000 ms (174924ms till timeout)
2022-03-28 19:04:27 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (822494ms till timeout)
2022-03-28 19:04:27 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:27 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (233456ms till timeout)
2022-03-28 19:04:27 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:27 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:27 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:50] At least my-cluster-08515847-kafka-0 hasn't rolled
2022-03-28 19:04:27 [ForkJoinPool-1-worker-11] INFO  [RollingUpdateUtils:143] {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7} pods didn't roll. Remaining seconds for stability: 25
2022-03-28 19:04:27 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (271302ms till timeout)
2022-03-28 19:04:27 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:04:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:04:27 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (200117ms till timeout)
2022-03-28 19:04:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-305
2022-03-28 19:04:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:27 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-308
2022-03-28 19:04:28 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (116998ms till timeout)
2022-03-28 19:04:28 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-308
2022-03-28 19:04:28 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:28 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-309
2022-03-28 19:04:28 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (821397ms till timeout)
2022-03-28 19:04:28 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:28 [ForkJoinPool-1-worker-25] INFO  [PodUtils:225] Pod replicas gonna be stable in 14 polls
2022-03-28 19:04:28 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176]  Podmy-cluster-afe273c9-connect will have 0 replicas not ready, will try again in 1000 ms (173638ms till timeout)
2022-03-28 19:04:28 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (232091ms till timeout)
2022-03-28 19:04:28 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:28 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:28 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:50] At least my-cluster-08515847-kafka-0 hasn't rolled
2022-03-28 19:04:28 [ForkJoinPool-1-worker-11] INFO  [RollingUpdateUtils:143] {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7} pods didn't roll. Remaining seconds for stability: 24
2022-03-28 19:04:28 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (270031ms till timeout)
2022-03-28 19:04:28 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-309
2022-03-28 19:04:28 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:28 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-31
2022-03-28 19:04:28 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:04:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:04:29 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (198803ms till timeout)
2022-03-28 19:04:29 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (115998ms till timeout)
2022-03-28 19:04:29 [ForkJoinPool-1-worker-7] INFO  [Exec:417] Command: oc --namespace namespace-0 exec my-cluster-959c6ed5-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-634125597-1746493046 --describe --bootstrap-server my-cluster-959c6ed5-kafka-bootstrap:9092
2022-03-28 19:04:29 [ForkJoinPool-1-worker-7] INFO  [Exec:417] Return code: 0
2022-03-28 19:04:29 [ForkJoinPool-1-worker-7] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 10 polls
2022-03-28 19:04:29 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (128759ms till timeout)
2022-03-28 19:04:29 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-31
2022-03-28 19:04:29 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:29 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-311
2022-03-28 19:04:29 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (820298ms till timeout)
2022-03-28 19:04:29 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:29 [ForkJoinPool-1-worker-25] INFO  [PodUtils:225] Pod replicas gonna be stable in 13 polls
2022-03-28 19:04:29 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176]  Podmy-cluster-afe273c9-connect will have 0 replicas not ready, will try again in 1000 ms (172446ms till timeout)
2022-03-28 19:04:29 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (230900ms till timeout)
2022-03-28 19:04:29 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:29 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:29 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:50] At least my-cluster-08515847-kafka-0 hasn't rolled
2022-03-28 19:04:29 [ForkJoinPool-1-worker-11] INFO  [RollingUpdateUtils:143] {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7} pods didn't roll. Remaining seconds for stability: 23
2022-03-28 19:04:29 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (268841ms till timeout)
2022-03-28 19:04:30 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-311
2022-03-28 19:04:30 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:30 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-312
2022-03-28 19:04:30 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:04:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:04:30 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (114997ms till timeout)
2022-03-28 19:04:30 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (197579ms till timeout)
2022-03-28 19:04:30 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-0 exec my-cluster-959c6ed5-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-634125597-1746493046 --describe --bootstrap-server my-cluster-959c6ed5-kafka-bootstrap:9092
2022-03-28 19:04:30 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-312
2022-03-28 19:04:30 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:30 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-313
2022-03-28 19:04:30 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (819201ms till timeout)
2022-03-28 19:04:30 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:31 [ForkJoinPool-1-worker-25] INFO  [PodUtils:225] Pod replicas gonna be stable in 12 polls
2022-03-28 19:04:31 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176]  Podmy-cluster-afe273c9-connect will have 0 replicas not ready, will try again in 1000 ms (171255ms till timeout)
2022-03-28 19:04:31 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:31 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:31 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:50] At least my-cluster-08515847-kafka-0 hasn't rolled
2022-03-28 19:04:31 [ForkJoinPool-1-worker-11] INFO  [RollingUpdateUtils:143] {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7} pods didn't roll. Remaining seconds for stability: 22
2022-03-28 19:04:31 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (267735ms till timeout)
2022-03-28 19:04:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-313
2022-03-28 19:04:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:31 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-320
2022-03-28 19:04:31 [ForkJoinPool-1-worker-33] INFO  [PodUtils:189] Message All topics created found in create-admin-my-cluster-ec1b34f8-kafka-clients-wt5gv log
2022-03-28 19:04:31 [ForkJoinPool-1-worker-33] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment create-admin-my-cluster-ec1b34f8-kafka-clients deletion
2022-03-28 19:04:31 [ForkJoinPool-1-worker-33] DEBUG [TestUtils:125] Waiting for ReplicaSet create-admin-my-cluster-ec1b34f8-kafka-clients to be deleted
2022-03-28 19:04:31 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (113997ms till timeout)
2022-03-28 19:04:31 [ForkJoinPool-1-worker-33] DEBUG [JobUtils:40] Job create-admin-my-cluster-ec1b34f8-kafka-clients was deleted
2022-03-28 19:04:31 [ForkJoinPool-1-worker-33] INFO  [ResourceManager:155] Create/Update Job alter-admin-my-cluster-ec1b34f8-kafka-clients in namespace throttling-quota-st
2022-03-28 19:04:31 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:04:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:04:31 [ForkJoinPool-1-worker-33] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:alter-admin-my-cluster-ec1b34f8-kafka-clients
2022-03-28 19:04:31 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (196258ms till timeout)
2022-03-28 19:04:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-320
2022-03-28 19:04:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:31 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-321
2022-03-28 19:04:31 [ForkJoinPool-1-worker-33] INFO  [JobUtils:81] Waiting for job: alter-admin-my-cluster-ec1b34f8-kafka-clients will be in active state
2022-03-28 19:04:31 [ForkJoinPool-1-worker-33] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 19:04:31 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (818051ms till timeout)
2022-03-28 19:04:31 [ForkJoinPool-1-worker-33] INFO  [PodUtils:186] Waiting for message will be in the log
2022-03-28 19:04:31 [ForkJoinPool-1-worker-33] DEBUG [TestUtils:125] Waiting for Waiting for message will be in the log
2022-03-28 19:04:32 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (299899ms till timeout)
2022-03-28 19:04:32 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:32 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-321
2022-03-28 19:04:32 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:32 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-322
2022-03-28 19:04:32 [ForkJoinPool-1-worker-25] INFO  [PodUtils:225] Pod replicas gonna be stable in 11 polls
2022-03-28 19:04:32 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176]  Podmy-cluster-afe273c9-connect will have 0 replicas not ready, will try again in 1000 ms (170154ms till timeout)
2022-03-28 19:04:32 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (112996ms till timeout)
2022-03-28 19:04:32 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:32 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:32 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:50] At least my-cluster-08515847-kafka-0 hasn't rolled
2022-03-28 19:04:32 [ForkJoinPool-1-worker-11] INFO  [RollingUpdateUtils:143] {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7} pods didn't roll. Remaining seconds for stability: 21
2022-03-28 19:04:32 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (266544ms till timeout)
2022-03-28 19:04:32 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:04:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:04:32 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-322
2022-03-28 19:04:32 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:32 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-323
2022-03-28 19:04:32 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (195067ms till timeout)
2022-03-28 19:04:32 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (816955ms till timeout)
2022-03-28 19:04:33 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (298800ms till timeout)
2022-03-28 19:04:33 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (111996ms till timeout)
2022-03-28 19:04:33 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:33 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-323
2022-03-28 19:04:33 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:33 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-324
2022-03-28 19:04:33 [ForkJoinPool-1-worker-25] INFO  [PodUtils:225] Pod replicas gonna be stable in 10 polls
2022-03-28 19:04:33 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176]  Podmy-cluster-afe273c9-connect will have 0 replicas not ready, will try again in 1000 ms (168964ms till timeout)
2022-03-28 19:04:33 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:33 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:33 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:50] At least my-cluster-08515847-kafka-0 hasn't rolled
2022-03-28 19:04:33 [ForkJoinPool-1-worker-11] INFO  [RollingUpdateUtils:143] {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7} pods didn't roll. Remaining seconds for stability: 20
2022-03-28 19:04:33 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (265444ms till timeout)
2022-03-28 19:04:33 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-324
2022-03-28 19:04:33 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:33 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-325
2022-03-28 19:04:33 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:04:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:04:33 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (193870ms till timeout)
2022-03-28 19:04:34 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (815841ms till timeout)
2022-03-28 19:04:34 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (110995ms till timeout)
2022-03-28 19:04:34 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (297692ms till timeout)
2022-03-28 19:04:34 [ForkJoinPool-1-worker-7] INFO  [Exec:417] Command: oc --namespace namespace-0 exec my-cluster-959c6ed5-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-634125597-1746493046 --describe --bootstrap-server my-cluster-959c6ed5-kafka-bootstrap:9092
2022-03-28 19:04:34 [ForkJoinPool-1-worker-7] INFO  [Exec:417] Return code: 0
2022-03-28 19:04:34 [ForkJoinPool-1-worker-7] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 9 polls
2022-03-28 19:04:34 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (123806ms till timeout)
2022-03-28 19:04:34 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-325
2022-03-28 19:04:34 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:34 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-326
2022-03-28 19:04:34 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:34 [ForkJoinPool-1-worker-25] INFO  [PodUtils:225] Pod replicas gonna be stable in 9 polls
2022-03-28 19:04:34 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176]  Podmy-cluster-afe273c9-connect will have 0 replicas not ready, will try again in 1000 ms (167774ms till timeout)
2022-03-28 19:04:34 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:34 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:34 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:50] At least my-cluster-08515847-kafka-0 hasn't rolled
2022-03-28 19:04:34 [ForkJoinPool-1-worker-11] INFO  [RollingUpdateUtils:143] {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7} pods didn't roll. Remaining seconds for stability: 19
2022-03-28 19:04:34 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (264254ms till timeout)
2022-03-28 19:04:34 [ForkJoinPool-1-worker-31] INFO  [InternalKafkaClient:160] Consumer finished correctly: true
2022-03-28 19:04:34 [ForkJoinPool-1-worker-31] INFO  [InternalKafkaClient:163] Consumer consumed 100 messages
2022-03-28 19:04:34 [ForkJoinPool-1-worker-31] INFO  [UserST:392] Checking owner reference - if the secret will be deleted when we delete KafkaUser
2022-03-28 19:04:34 [ForkJoinPool-1-worker-31] INFO  [UserST:394] Deleting KafkaUser:encrypted-leopold
2022-03-28 19:04:34 [ForkJoinPool-1-worker-31] INFO  [KafkaUserUtils:62] Waiting for KafkaUser deletion encrypted-leopold
2022-03-28 19:04:34 [ForkJoinPool-1-worker-31] DEBUG [TestUtils:125] Waiting for KafkaUser deletion encrypted-leopold
2022-03-28 19:04:34 [ForkJoinPool-1-worker-31] INFO  [KafkaUserUtils:75] KafkaUser encrypted-leopold deleted
2022-03-28 19:04:34 [ForkJoinPool-1-worker-31] INFO  [UserST:398] Deleting KafkaUser:scramed-leopold
2022-03-28 19:04:34 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-326
2022-03-28 19:04:34 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:34 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-327
2022-03-28 19:04:34 [ForkJoinPool-1-worker-31] INFO  [KafkaUserUtils:62] Waiting for KafkaUser deletion scramed-leopold
2022-03-28 19:04:34 [ForkJoinPool-1-worker-31] DEBUG [TestUtils:125] Waiting for KafkaUser deletion scramed-leopold
2022-03-28 19:04:35 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:04:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:04:35 [ForkJoinPool-1-worker-31] INFO  [KafkaUserUtils:75] KafkaUser scramed-leopold deleted
2022-03-28 19:04:35 [ForkJoinPool-1-worker-31] INFO  [UserST:402] Checking if secrets are deleted
2022-03-28 19:04:35 [ForkJoinPool-1-worker-31] INFO  [SecretUtils:54] Waiting for Secret deletion top-secret-encrypted-leopold
2022-03-28 19:04:35 [ForkJoinPool-1-worker-31] DEBUG [TestUtils:125] Waiting for Expected secret top-secret-encrypted-leopold deleted
2022-03-28 19:04:35 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (109995ms till timeout)
2022-03-28 19:04:35 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (192579ms till timeout)
2022-03-28 19:04:35 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (814658ms till timeout)
2022-03-28 19:04:35 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-0 exec my-cluster-959c6ed5-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-634125597-1746493046 --describe --bootstrap-server my-cluster-959c6ed5-kafka-bootstrap:9092
2022-03-28 19:04:35 [ForkJoinPool-1-worker-31] INFO  [SecretUtils:58] Secret top-secret-encrypted-leopold deleted
2022-03-28 19:04:35 [ForkJoinPool-1-worker-31] INFO  [SecretUtils:54] Waiting for Secret deletion top-secret-scramed-leopold
2022-03-28 19:04:35 [ForkJoinPool-1-worker-31] DEBUG [TestUtils:125] Waiting for Expected secret top-secret-scramed-leopold deleted
2022-03-28 19:04:35 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-327
2022-03-28 19:04:35 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:35 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-328
2022-03-28 19:04:35 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (296509ms till timeout)
2022-03-28 19:04:35 [ForkJoinPool-1-worker-31] INFO  [SecretUtils:58] Secret top-secret-scramed-leopold deleted
2022-03-28 19:04:35 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:35 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:35 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:35 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:50] At least my-cluster-08515847-kafka-0 hasn't rolled
2022-03-28 19:04:35 [ForkJoinPool-1-worker-11] INFO  [RollingUpdateUtils:143] {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7} pods didn't roll. Remaining seconds for stability: 18
2022-03-28 19:04:35 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (263094ms till timeout)
2022-03-28 19:04:35 [ForkJoinPool-1-worker-25] INFO  [PodUtils:225] Pod replicas gonna be stable in 8 polls
2022-03-28 19:04:35 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176]  Podmy-cluster-afe273c9-connect will have 0 replicas not ready, will try again in 1000 ms (166608ms till timeout)
2022-03-28 19:04:35 [ForkJoinPool-1-worker-31] DEBUG [AbstractST:674] ============================================================================
2022-03-28 19:04:35 [ForkJoinPool-1-worker-31] DEBUG [AbstractST:675] [operators.user.UserST - After Each] - Clean up after test
2022-03-28 19:04:35 [ForkJoinPool-1-worker-31] INFO  [ResourceManager:344] ############################################################################
2022-03-28 19:04:35 [ForkJoinPool-1-worker-31] INFO  [ResourceManager:348] Delete all resources for testCreatingUsersWithSecretPrefix
2022-03-28 19:04:35 [ForkJoinPool-1-worker-31] INFO  [ResourceManager:241] Delete of KafkaUser scramed-leopold in namespace namespace-8
2022-03-28 19:04:35 [ForkJoinPool-1-worker-55] INFO  [ResourceManager:241] Delete of KafkaTopic my-topic-1727421070-668192795 in namespace namespace-8
2022-03-28 19:04:35 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:241] Delete of Kafka my-cluster-f743fdfd in namespace namespace-8
2022-03-28 19:04:35 [ForkJoinPool-1-worker-35] INFO  [ResourceManager:241] Delete of Deployment my-cluster-f743fdfd-tls-kafka-clients in namespace namespace-8
2022-03-28 19:04:35 [ForkJoinPool-1-worker-21] INFO  [ResourceManager:241] Delete of KafkaUser encrypted-leopold in namespace namespace-8
2022-03-28 19:04:35 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:241] Delete of Deployment my-cluster-f743fdfd-plain-kafka-clients in namespace namespace-8
2022-03-28 19:04:35 [ForkJoinPool-1-worker-31] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:scramed-leopold
2022-03-28 19:04:36 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-328
2022-03-28 19:04:36 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:36 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-329
2022-03-28 19:04:36 [ForkJoinPool-1-worker-21] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:encrypted-leopold
2022-03-28 19:04:36 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:my-cluster-f743fdfd-plain-kafka-clients
2022-03-28 19:04:36 [ForkJoinPool-1-worker-35] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:my-cluster-f743fdfd-tls-kafka-clients
2022-03-28 19:04:36 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-f743fdfd
2022-03-28 19:04:36 [ForkJoinPool-1-worker-55] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1727421070-668192795
2022-03-28 19:04:36 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (108994ms till timeout)
2022-03-28 19:04:36 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:04:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:04:36 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (813495ms till timeout)
2022-03-28 19:04:36 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-f743fdfd-tls-kafka-clients not ready, will try again in 10000 ms (479500ms till timeout)
2022-03-28 19:04:36 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-329
2022-03-28 19:04:36 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:36 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-330
2022-03-28 19:04:36 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (191224ms till timeout)
2022-03-28 19:04:36 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-f743fdfd-plain-kafka-clients not ready, will try again in 10000 ms (479411ms till timeout)
2022-03-28 19:04:36 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (295344ms till timeout)
2022-03-28 19:04:36 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:36 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:36 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:36 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:50] At least my-cluster-08515847-kafka-0 hasn't rolled
2022-03-28 19:04:36 [ForkJoinPool-1-worker-11] INFO  [RollingUpdateUtils:143] {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7} pods didn't roll. Remaining seconds for stability: 17
2022-03-28 19:04:36 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (261987ms till timeout)
2022-03-28 19:04:36 [ForkJoinPool-1-worker-25] INFO  [PodUtils:225] Pod replicas gonna be stable in 7 polls
2022-03-28 19:04:36 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176]  Podmy-cluster-afe273c9-connect will have 0 replicas not ready, will try again in 1000 ms (165501ms till timeout)
2022-03-28 19:04:37 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-330
2022-03-28 19:04:37 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:37 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-333
2022-03-28 19:04:37 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (107994ms till timeout)
2022-03-28 19:04:37 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (812393ms till timeout)
2022-03-28 19:04:37 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-333
2022-03-28 19:04:37 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:37 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-334
2022-03-28 19:04:37 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:04:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:04:37 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (294151ms till timeout)
2022-03-28 19:04:37 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:37 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (189932ms till timeout)
2022-03-28 19:04:38 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:38 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:38 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:50] At least my-cluster-08515847-kafka-0 hasn't rolled
2022-03-28 19:04:38 [ForkJoinPool-1-worker-11] INFO  [RollingUpdateUtils:143] {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7} pods didn't roll. Remaining seconds for stability: 16
2022-03-28 19:04:38 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (260831ms till timeout)
2022-03-28 19:04:38 [ForkJoinPool-1-worker-25] INFO  [PodUtils:225] Pod replicas gonna be stable in 6 polls
2022-03-28 19:04:38 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176]  Podmy-cluster-afe273c9-connect will have 0 replicas not ready, will try again in 1000 ms (164251ms till timeout)
2022-03-28 19:04:38 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-334
2022-03-28 19:04:38 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:38 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-337
2022-03-28 19:04:38 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (106993ms till timeout)
2022-03-28 19:04:38 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (811293ms till timeout)
2022-03-28 19:04:38 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-337
2022-03-28 19:04:38 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:38 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-340
2022-03-28 19:04:38 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (293050ms till timeout)
2022-03-28 19:04:39 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:39 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:04:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:04:39 [ForkJoinPool-1-worker-7] INFO  [Exec:417] Command: oc --namespace namespace-0 exec my-cluster-959c6ed5-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-634125597-1746493046 --describe --bootstrap-server my-cluster-959c6ed5-kafka-bootstrap:9092
2022-03-28 19:04:39 [ForkJoinPool-1-worker-7] INFO  [Exec:417] Return code: 0
2022-03-28 19:04:39 [ForkJoinPool-1-worker-7] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 8 polls
2022-03-28 19:04:39 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (119116ms till timeout)
2022-03-28 19:04:39 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:39 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:39 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:50] At least my-cluster-08515847-kafka-0 hasn't rolled
2022-03-28 19:04:39 [ForkJoinPool-1-worker-11] INFO  [RollingUpdateUtils:143] {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7} pods didn't roll. Remaining seconds for stability: 15
2022-03-28 19:04:39 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (259729ms till timeout)
2022-03-28 19:04:39 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (188639ms till timeout)
2022-03-28 19:04:39 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (105993ms till timeout)
2022-03-28 19:04:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-340
2022-03-28 19:04:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:39 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-346
2022-03-28 19:04:39 [ForkJoinPool-1-worker-25] INFO  [PodUtils:225] Pod replicas gonna be stable in 5 polls
2022-03-28 19:04:39 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176]  Podmy-cluster-afe273c9-connect will have 0 replicas not ready, will try again in 1000 ms (163056ms till timeout)
2022-03-28 19:04:39 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (810195ms till timeout)
2022-03-28 19:04:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-346
2022-03-28 19:04:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:39 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-349
2022-03-28 19:04:40 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (291948ms till timeout)
2022-03-28 19:04:40 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-0 exec my-cluster-959c6ed5-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-634125597-1746493046 --describe --bootstrap-server my-cluster-959c6ed5-kafka-bootstrap:9092
2022-03-28 19:04:40 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:40 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:40 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:40 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:50] At least my-cluster-08515847-kafka-0 hasn't rolled
2022-03-28 19:04:40 [ForkJoinPool-1-worker-11] INFO  [RollingUpdateUtils:143] {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7} pods didn't roll. Remaining seconds for stability: 14
2022-03-28 19:04:40 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (258628ms till timeout)
2022-03-28 19:04:40 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (104992ms till timeout)
2022-03-28 19:04:40 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:04:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:04:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-349
2022-03-28 19:04:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:40 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-353
2022-03-28 19:04:40 [ForkJoinPool-1-worker-25] INFO  [PodUtils:225] Pod replicas gonna be stable in 4 polls
2022-03-28 19:04:40 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176]  Podmy-cluster-afe273c9-connect will have 0 replicas not ready, will try again in 1000 ms (161860ms till timeout)
2022-03-28 19:04:40 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (187348ms till timeout)
2022-03-28 19:04:40 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (809099ms till timeout)
2022-03-28 19:04:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-353
2022-03-28 19:04:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:40 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-356
2022-03-28 19:04:41 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (290849ms till timeout)
2022-03-28 19:04:41 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:41 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (103991ms till timeout)
2022-03-28 19:04:41 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:41 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:41 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:50] At least my-cluster-08515847-kafka-0 hasn't rolled
2022-03-28 19:04:41 [ForkJoinPool-1-worker-11] INFO  [RollingUpdateUtils:143] {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7} pods didn't roll. Remaining seconds for stability: 13
2022-03-28 19:04:41 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (257529ms till timeout)
2022-03-28 19:04:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-356
2022-03-28 19:04:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:41 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-357
2022-03-28 19:04:41 [ForkJoinPool-1-worker-25] INFO  [PodUtils:225] Pod replicas gonna be stable in 3 polls
2022-03-28 19:04:41 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176]  Podmy-cluster-afe273c9-connect will have 0 replicas not ready, will try again in 1000 ms (160670ms till timeout)
2022-03-28 19:04:41 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:04:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:04:41 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (186056ms till timeout)
2022-03-28 19:04:41 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (808001ms till timeout)
2022-03-28 19:04:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-357
2022-03-28 19:04:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:41 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-358
2022-03-28 19:04:42 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (289749ms till timeout)
2022-03-28 19:04:42 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (102991ms till timeout)
2022-03-28 19:04:42 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:42 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:42 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:42 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:50] At least my-cluster-08515847-kafka-0 hasn't rolled
2022-03-28 19:04:42 [ForkJoinPool-1-worker-11] INFO  [RollingUpdateUtils:143] {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7} pods didn't roll. Remaining seconds for stability: 12
2022-03-28 19:04:42 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (256429ms till timeout)
2022-03-28 19:04:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-358
2022-03-28 19:04:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:42 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-361
2022-03-28 19:04:42 [ForkJoinPool-1-worker-25] INFO  [PodUtils:225] Pod replicas gonna be stable in 2 polls
2022-03-28 19:04:42 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176]  Podmy-cluster-afe273c9-connect will have 0 replicas not ready, will try again in 1000 ms (159479ms till timeout)
2022-03-28 19:04:42 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:04:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:04:42 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (184860ms till timeout)
2022-03-28 19:04:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-361
2022-03-28 19:04:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:42 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-362
2022-03-28 19:04:43 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (806844ms till timeout)
2022-03-28 19:04:43 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (101990ms till timeout)
2022-03-28 19:04:43 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (288567ms till timeout)
2022-03-28 19:04:43 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:43 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:43 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:43 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:50] At least my-cluster-08515847-kafka-0 hasn't rolled
2022-03-28 19:04:43 [ForkJoinPool-1-worker-11] INFO  [RollingUpdateUtils:143] {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7} pods didn't roll. Remaining seconds for stability: 11
2022-03-28 19:04:43 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (255329ms till timeout)
2022-03-28 19:04:43 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-362
2022-03-28 19:04:43 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:43 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-366
2022-03-28 19:04:43 [ForkJoinPool-1-worker-7] INFO  [Exec:417] Command: oc --namespace namespace-0 exec my-cluster-959c6ed5-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-634125597-1746493046 --describe --bootstrap-server my-cluster-959c6ed5-kafka-bootstrap:9092
2022-03-28 19:04:43 [ForkJoinPool-1-worker-7] INFO  [Exec:417] Return code: 0
2022-03-28 19:04:43 [ForkJoinPool-1-worker-7] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 7 polls
2022-03-28 19:04:43 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (114489ms till timeout)
2022-03-28 19:04:44 [ForkJoinPool-1-worker-25] INFO  [PodUtils:225] Pod replicas gonna be stable in 1 polls
2022-03-28 19:04:44 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176]  Podmy-cluster-afe273c9-connect will have 0 replicas not ready, will try again in 1000 ms (158285ms till timeout)
2022-03-28 19:04:44 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:04:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:04:44 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-366
2022-03-28 19:04:44 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:44 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-367
2022-03-28 19:04:44 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (183669ms till timeout)
2022-03-28 19:04:44 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (100990ms till timeout)
2022-03-28 19:04:44 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (805653ms till timeout)
2022-03-28 19:04:44 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (287467ms till timeout)
2022-03-28 19:04:44 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:44 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:44 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:44 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:50] At least my-cluster-08515847-kafka-0 hasn't rolled
2022-03-28 19:04:44 [ForkJoinPool-1-worker-11] INFO  [RollingUpdateUtils:143] {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7} pods didn't roll. Remaining seconds for stability: 10
2022-03-28 19:04:44 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (254222ms till timeout)
2022-03-28 19:04:44 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-367
2022-03-28 19:04:44 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:44 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-368
2022-03-28 19:04:44 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-0 exec my-cluster-959c6ed5-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-634125597-1746493046 --describe --bootstrap-server my-cluster-959c6ed5-kafka-bootstrap:9092
2022-03-28 19:04:45 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-368
2022-03-28 19:04:45 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:45 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-371
2022-03-28 19:04:45 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (99989ms till timeout)
2022-03-28 19:04:45 [ForkJoinPool-1-worker-25] INFO  [PodUtils:217] Pod replicas are stable for 20 polls intervals
2022-03-28 19:04:45 [ForkJoinPool-1-worker-25] INFO  [PodUtils:228] Pod my-cluster-afe273c9-connect has 0 replicas
2022-03-28 19:04:45 [ForkJoinPool-1-worker-25] INFO  [ReconciliationST:108] Setting annotation to "false" and creating KafkaConnector
2022-03-28 19:04:45 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:04:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:04:45 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (182383ms till timeout)
2022-03-28 19:04:45 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (804462ms till timeout)
2022-03-28 19:04:45 [ForkJoinPool-1-worker-25] INFO  [DeploymentUtils:161] Wait for Deployment: my-cluster-afe273c9-connect will be ready
2022-03-28 19:04:45 [ForkJoinPool-1-worker-25] DEBUG [TestUtils:125] Waiting for Wait for Deployment: my-cluster-afe273c9-connect will be ready
2022-03-28 19:04:45 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:45 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (286310ms till timeout)
2022-03-28 19:04:45 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:45 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:45 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:50] At least my-cluster-08515847-kafka-0 hasn't rolled
2022-03-28 19:04:45 [ForkJoinPool-1-worker-11] INFO  [RollingUpdateUtils:143] {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7} pods didn't roll. Remaining seconds for stability: 9
2022-03-28 19:04:45 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (253084ms till timeout)
2022-03-28 19:04:45 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-371
2022-03-28 19:04:45 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:45 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-372
2022-03-28 19:04:45 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Wait for Deployment: my-cluster-afe273c9-connect will be ready not ready, will try again in 1000 ms (479714ms till timeout)
2022-03-28 19:04:46 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (98989ms till timeout)
2022-03-28 19:04:46 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-372
2022-03-28 19:04:46 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:46 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-375
2022-03-28 19:04:46 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:04:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:04:46 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (803271ms till timeout)
2022-03-28 19:04:46 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (181095ms till timeout)
2022-03-28 19:04:46 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:46 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (285122ms till timeout)
2022-03-28 19:04:46 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-375
2022-03-28 19:04:46 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:46 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-376
2022-03-28 19:04:46 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:46 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:46 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:50] At least my-cluster-08515847-kafka-0 hasn't rolled
2022-03-28 19:04:46 [ForkJoinPool-1-worker-11] INFO  [RollingUpdateUtils:143] {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7} pods didn't roll. Remaining seconds for stability: 8
2022-03-28 19:04:46 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (251897ms till timeout)
2022-03-28 19:04:47 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Wait for Deployment: my-cluster-afe273c9-connect will be ready not ready, will try again in 1000 ms (478528ms till timeout)
2022-03-28 19:04:47 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-f743fdfd-tls-kafka-clients not ready, will try again in 10000 ms (469002ms till timeout)
2022-03-28 19:04:47 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-f743fdfd-plain-kafka-clients not ready, will try again in 10000 ms (468901ms till timeout)
2022-03-28 19:04:47 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (97989ms till timeout)
2022-03-28 19:04:47 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-376
2022-03-28 19:04:47 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:47 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-382
2022-03-28 19:04:47 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (802172ms till timeout)
2022-03-28 19:04:47 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:04:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:04:47 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:47 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (284022ms till timeout)
2022-03-28 19:04:48 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (179804ms till timeout)
2022-03-28 19:04:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-382
2022-03-28 19:04:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:48 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-384
2022-03-28 19:04:48 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:48 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:48 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:50] At least my-cluster-08515847-kafka-0 hasn't rolled
2022-03-28 19:04:48 [ForkJoinPool-1-worker-11] INFO  [RollingUpdateUtils:143] {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7} pods didn't roll. Remaining seconds for stability: 7
2022-03-28 19:04:48 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (250707ms till timeout)
2022-03-28 19:04:48 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Wait for Deployment: my-cluster-afe273c9-connect will be ready not ready, will try again in 1000 ms (477424ms till timeout)
2022-03-28 19:04:48 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (96988ms till timeout)
2022-03-28 19:04:48 [ForkJoinPool-1-worker-7] INFO  [Exec:417] Command: oc --namespace namespace-0 exec my-cluster-959c6ed5-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-634125597-1746493046 --describe --bootstrap-server my-cluster-959c6ed5-kafka-bootstrap:9092
2022-03-28 19:04:48 [ForkJoinPool-1-worker-7] INFO  [Exec:417] Return code: 0
2022-03-28 19:04:48 [ForkJoinPool-1-worker-7] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 6 polls
2022-03-28 19:04:48 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (109737ms till timeout)
2022-03-28 19:04:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-384
2022-03-28 19:04:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:48 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-385
2022-03-28 19:04:48 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (801073ms till timeout)
2022-03-28 19:04:49 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (282885ms till timeout)
2022-03-28 19:04:49 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:49 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:04:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:04:49 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-385
2022-03-28 19:04:49 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:49 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-387
2022-03-28 19:04:49 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (95987ms till timeout)
2022-03-28 19:04:49 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Wait for Deployment: my-cluster-afe273c9-connect will be ready not ready, will try again in 1000 ms (476295ms till timeout)
2022-03-28 19:04:49 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:49 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:49 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:50] At least my-cluster-08515847-kafka-0 hasn't rolled
2022-03-28 19:04:49 [ForkJoinPool-1-worker-11] INFO  [RollingUpdateUtils:143] {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7} pods didn't roll. Remaining seconds for stability: 6
2022-03-28 19:04:49 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (249480ms till timeout)
2022-03-28 19:04:49 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (178484ms till timeout)
2022-03-28 19:04:49 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-0 exec my-cluster-959c6ed5-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-634125597-1746493046 --describe --bootstrap-server my-cluster-959c6ed5-kafka-bootstrap:9092
2022-03-28 19:04:49 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-387
2022-03-28 19:04:49 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:49 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-388
2022-03-28 19:04:49 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (799975ms till timeout)
2022-03-28 19:04:50 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (281786ms till timeout)
2022-03-28 19:04:50 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (94987ms till timeout)
2022-03-28 19:04:50 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-388
2022-03-28 19:04:50 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:50 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-39
2022-03-28 19:04:50 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:50 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Wait for Deployment: my-cluster-afe273c9-connect will be ready not ready, will try again in 1000 ms (475197ms till timeout)
2022-03-28 19:04:50 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:04:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:04:50 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:50 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:50 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:50] At least my-cluster-08515847-kafka-0 hasn't rolled
2022-03-28 19:04:50 [ForkJoinPool-1-worker-11] INFO  [RollingUpdateUtils:143] {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7} pods didn't roll. Remaining seconds for stability: 5
2022-03-28 19:04:50 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (248379ms till timeout)
2022-03-28 19:04:50 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (177290ms till timeout)
2022-03-28 19:04:50 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-39
2022-03-28 19:04:50 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:50 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-391
2022-03-28 19:04:51 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (798877ms till timeout)
2022-03-28 19:04:51 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (93986ms till timeout)
2022-03-28 19:04:51 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (280685ms till timeout)
2022-03-28 19:04:51 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Wait for Deployment: my-cluster-afe273c9-connect will be ready not ready, will try again in 1000 ms (474099ms till timeout)
2022-03-28 19:04:51 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:51 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-391
2022-03-28 19:04:51 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:51 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-395
2022-03-28 19:04:51 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:51 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:51 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:50] At least my-cluster-08515847-kafka-0 hasn't rolled
2022-03-28 19:04:51 [ForkJoinPool-1-worker-11] INFO  [RollingUpdateUtils:143] {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7} pods didn't roll. Remaining seconds for stability: 4
2022-03-28 19:04:51 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (247275ms till timeout)
2022-03-28 19:04:51 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:04:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:04:51 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (176091ms till timeout)
2022-03-28 19:04:52 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-395
2022-03-28 19:04:52 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:52 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-398
2022-03-28 19:04:52 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (797777ms till timeout)
2022-03-28 19:04:52 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (92985ms till timeout)
2022-03-28 19:04:52 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (279585ms till timeout)
2022-03-28 19:04:52 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Wait for Deployment: my-cluster-afe273c9-connect will be ready not ready, will try again in 1000 ms (473002ms till timeout)
2022-03-28 19:04:52 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:52 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-398
2022-03-28 19:04:52 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:52 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-4
2022-03-28 19:04:52 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:52 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:52 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:50] At least my-cluster-08515847-kafka-0 hasn't rolled
2022-03-28 19:04:52 [ForkJoinPool-1-worker-11] INFO  [RollingUpdateUtils:143] {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7} pods didn't roll. Remaining seconds for stability: 3
2022-03-28 19:04:52 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (246176ms till timeout)
2022-03-28 19:04:52 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:04:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:04:52 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (174899ms till timeout)
2022-03-28 19:04:53 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-4
2022-03-28 19:04:53 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:53 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-401
2022-03-28 19:04:53 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (796679ms till timeout)
2022-03-28 19:04:53 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (91985ms till timeout)
2022-03-28 19:04:53 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (278485ms till timeout)
2022-03-28 19:04:53 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Wait for Deployment: my-cluster-afe273c9-connect will be ready not ready, will try again in 1000 ms (471904ms till timeout)
2022-03-28 19:04:53 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:53 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-401
2022-03-28 19:04:53 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:53 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-406
2022-03-28 19:04:53 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:53 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:53 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:50] At least my-cluster-08515847-kafka-0 hasn't rolled
2022-03-28 19:04:53 [ForkJoinPool-1-worker-11] INFO  [RollingUpdateUtils:143] {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7} pods didn't roll. Remaining seconds for stability: 2
2022-03-28 19:04:53 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (245078ms till timeout)
2022-03-28 19:04:54 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:04:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:04:54 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (173700ms till timeout)
2022-03-28 19:04:54 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (90984ms till timeout)
2022-03-28 19:04:54 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-406
2022-03-28 19:04:54 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:54 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-408
2022-03-28 19:04:54 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (795562ms till timeout)
2022-03-28 19:04:54 [ForkJoinPool-1-worker-7] INFO  [Exec:417] Command: oc --namespace namespace-0 exec my-cluster-959c6ed5-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-634125597-1746493046 --describe --bootstrap-server my-cluster-959c6ed5-kafka-bootstrap:9092
2022-03-28 19:04:54 [ForkJoinPool-1-worker-7] INFO  [Exec:417] Return code: 0
2022-03-28 19:04:54 [ForkJoinPool-1-worker-7] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 5 polls
2022-03-28 19:04:54 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (103666ms till timeout)
2022-03-28 19:04:54 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (277385ms till timeout)
2022-03-28 19:04:54 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Wait for Deployment: my-cluster-afe273c9-connect will be ready not ready, will try again in 1000 ms (470807ms till timeout)
2022-03-28 19:04:54 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:54 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-408
2022-03-28 19:04:54 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:54 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-41
2022-03-28 19:04:54 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:54 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:54 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:50] At least my-cluster-08515847-kafka-0 hasn't rolled
2022-03-28 19:04:54 [ForkJoinPool-1-worker-11] INFO  [RollingUpdateUtils:143] {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7} pods didn't roll. Remaining seconds for stability: 1
2022-03-28 19:04:54 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (243978ms till timeout)
2022-03-28 19:04:55 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:04:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:04:55 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (89984ms till timeout)
2022-03-28 19:04:55 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (172508ms till timeout)
2022-03-28 19:04:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-41
2022-03-28 19:04:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:55 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-410
2022-03-28 19:04:55 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (794466ms till timeout)
2022-03-28 19:04:55 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-0 exec my-cluster-959c6ed5-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-634125597-1746493046 --describe --bootstrap-server my-cluster-959c6ed5-kafka-bootstrap:9092
2022-03-28 19:04:55 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (276285ms till timeout)
2022-03-28 19:04:55 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Wait for Deployment: my-cluster-afe273c9-connect will be ready not ready, will try again in 1000 ms (469710ms till timeout)
2022-03-28 19:04:55 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-410
2022-03-28 19:04:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:55 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-412
2022-03-28 19:04:55 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:55 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7}
2022-03-28 19:04:55 [ForkJoinPool-1-worker-11] DEBUG [RollingUpdateUtils:50] At least my-cluster-08515847-kafka-0 hasn't rolled
2022-03-28 19:04:55 [ForkJoinPool-1-worker-11] INFO  [RollingUpdateUtils:143] {my-cluster-08515847-kafka-0=f7e44241-5a94-470c-b9a1-888c48a8871d, my-cluster-08515847-kafka-1=b11d4ab3-b683-4f67-8541-998e29e2039f, my-cluster-08515847-kafka-2=6301b283-88a0-49f8-b5ff-5d9b47c375d7} pods didn't roll. Remaining seconds for stability: 0
2022-03-28 19:04:55 [ForkJoinPool-1-worker-11] INFO  [CruiseControlConfigurationST:283] Verifying new configuration in the Kafka CR
2022-03-28 19:04:56 [ForkJoinPool-1-worker-11] INFO  [CruiseControlConfigurationST:300] Verifying Cruise control performance options are set in Kafka CR
2022-03-28 19:04:56 [ForkJoinPool-1-worker-11] DEBUG [AbstractST:674] ============================================================================
2022-03-28 19:04:56 [ForkJoinPool-1-worker-11] DEBUG [AbstractST:675] [cruisecontrol.CruiseControlConfigurationST - After Each] - Clean up after test
2022-03-28 19:04:56 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:344] ############################################################################
2022-03-28 19:04:56 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:348] Delete all resources for testConfigurationPerformanceOptions
2022-03-28 19:04:56 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:241] Delete of Kafka my-cluster-08515847 in namespace namespace-4
2022-03-28 19:04:56 [ForkJoinPool-1-worker-11] INFO  [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-4, for cruise control Kafka cluster my-cluster-08515847
2022-03-28 19:04:56 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (88983ms till timeout)
2022-03-28 19:04:56 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-412
2022-03-28 19:04:56 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:56 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-413
2022-03-28 19:04:56 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:04:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:04:56 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (171304ms till timeout)
2022-03-28 19:04:56 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (793288ms till timeout)
2022-03-28 19:04:56 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-08515847
2022-03-28 19:04:56 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster-08515847 not ready, will try again in 10000 ms (839887ms till timeout)
2022-03-28 19:04:56 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (275082ms till timeout)
2022-03-28 19:04:57 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Wait for Deployment: my-cluster-afe273c9-connect will be ready not ready, will try again in 1000 ms (468550ms till timeout)
2022-03-28 19:04:57 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-413
2022-03-28 19:04:57 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:57 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-414
2022-03-28 19:04:57 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (87983ms till timeout)
2022-03-28 19:04:57 [ForkJoinPool-1-worker-35] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-f743fdfd-tls-kafka-clients not ready, will try again in 10000 ms (458508ms till timeout)
2022-03-28 19:04:57 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-f743fdfd-plain-kafka-clients not ready, will try again in 10000 ms (458410ms till timeout)
2022-03-28 19:04:57 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-414
2022-03-28 19:04:57 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:57 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-417
2022-03-28 19:04:57 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:04:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:04:57 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (792115ms till timeout)
2022-03-28 19:04:57 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (169941ms till timeout)
2022-03-28 19:04:58 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (273961ms till timeout)
2022-03-28 19:04:58 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Wait for Deployment: my-cluster-afe273c9-connect will be ready not ready, will try again in 1000 ms (467454ms till timeout)
2022-03-28 19:04:58 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-417
2022-03-28 19:04:58 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:58 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-418
2022-03-28 19:04:58 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (86982ms till timeout)
2022-03-28 19:04:58 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-418
2022-03-28 19:04:58 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:58 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-419
2022-03-28 19:04:58 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (791017ms till timeout)
2022-03-28 19:04:58 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:04:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:04:59 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (168744ms till timeout)
2022-03-28 19:04:59 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (272771ms till timeout)
2022-03-28 19:04:59 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (85982ms till timeout)
2022-03-28 19:04:59 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-419
2022-03-28 19:04:59 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:59 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-424
2022-03-28 19:04:59 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Wait for Deployment: my-cluster-afe273c9-connect will be ready not ready, will try again in 1000 ms (466269ms till timeout)
2022-03-28 19:04:59 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-424
2022-03-28 19:04:59 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:04:59 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-427
2022-03-28 19:05:00 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (789919ms till timeout)
2022-03-28 19:05:00 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:04:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:05:00 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (84981ms till timeout)
2022-03-28 19:05:00 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (167552ms till timeout)
2022-03-28 19:05:00 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-427
2022-03-28 19:05:00 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:00 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-428
2022-03-28 19:05:00 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (271577ms till timeout)
2022-03-28 19:05:00 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Wait for Deployment: my-cluster-afe273c9-connect will be ready not ready, will try again in 1000 ms (465076ms till timeout)
2022-03-28 19:05:00 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-428
2022-03-28 19:05:00 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:00 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-430
2022-03-28 19:05:01 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (788821ms till timeout)
2022-03-28 19:05:01 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (83981ms till timeout)
2022-03-28 19:05:01 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:04:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:05:01 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-430
2022-03-28 19:05:01 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:01 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-431
2022-03-28 19:05:01 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (166360ms till timeout)
2022-03-28 19:05:01 [ForkJoinPool-1-worker-7] INFO  [Exec:417] Command: oc --namespace namespace-0 exec my-cluster-959c6ed5-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-634125597-1746493046 --describe --bootstrap-server my-cluster-959c6ed5-kafka-bootstrap:9092
2022-03-28 19:05:01 [ForkJoinPool-1-worker-7] INFO  [Exec:417] Return code: 0
2022-03-28 19:05:01 [ForkJoinPool-1-worker-7] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 4 polls
2022-03-28 19:05:01 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (96611ms till timeout)
2022-03-28 19:05:01 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (270385ms till timeout)
2022-03-28 19:05:01 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Wait for Deployment: my-cluster-afe273c9-connect will be ready not ready, will try again in 1000 ms (463884ms till timeout)
2022-03-28 19:05:01 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-431
2022-03-28 19:05:01 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:01 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-432
2022-03-28 19:05:02 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (787724ms till timeout)
2022-03-28 19:05:02 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (82980ms till timeout)
2022-03-28 19:05:02 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-432
2022-03-28 19:05:02 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:02 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-433
2022-03-28 19:05:02 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-0 exec my-cluster-959c6ed5-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-634125597-1746493046 --describe --bootstrap-server my-cluster-959c6ed5-kafka-bootstrap:9092
2022-03-28 19:05:02 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:04:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:05:02 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (165166ms till timeout)
2022-03-28 19:05:02 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (269193ms till timeout)
2022-03-28 19:05:02 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Wait for Deployment: my-cluster-afe273c9-connect will be ready not ready, will try again in 1000 ms (462692ms till timeout)
2022-03-28 19:05:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-433
2022-03-28 19:05:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:03 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-434
2022-03-28 19:05:03 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (81980ms till timeout)
2022-03-28 19:05:03 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (786627ms till timeout)
2022-03-28 19:05:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-434
2022-03-28 19:05:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:03 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-437
2022-03-28 19:05:03 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:04:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:05:03 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (163974ms till timeout)
2022-03-28 19:05:03 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (267998ms till timeout)
2022-03-28 19:05:04 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-437
2022-03-28 19:05:04 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:04 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-440
2022-03-28 19:05:04 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Wait for Deployment: my-cluster-afe273c9-connect will be ready not ready, will try again in 1000 ms (461497ms till timeout)
2022-03-28 19:05:04 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (80979ms till timeout)
2022-03-28 19:05:04 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (785529ms till timeout)
2022-03-28 19:05:04 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-440
2022-03-28 19:05:04 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:04 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-441
2022-03-28 19:05:04 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:04:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:05:05 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (162781ms till timeout)
2022-03-28 19:05:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-441
2022-03-28 19:05:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:05 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-442
2022-03-28 19:05:05 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (266806ms till timeout)
2022-03-28 19:05:05 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Wait for Deployment: my-cluster-afe273c9-connect will be ready not ready, will try again in 1000 ms (460305ms till timeout)
2022-03-28 19:05:05 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (79979ms till timeout)
2022-03-28 19:05:05 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (784432ms till timeout)
2022-03-28 19:05:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-442
2022-03-28 19:05:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:05 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-443
2022-03-28 19:05:06 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:04:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:05:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-443
2022-03-28 19:05:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:06 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-444
2022-03-28 19:05:06 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (161588ms till timeout)
2022-03-28 19:05:06 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (78978ms till timeout)
2022-03-28 19:05:06 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (265613ms till timeout)
2022-03-28 19:05:06 [ForkJoinPool-1-worker-7] INFO  [Exec:417] Command: oc --namespace namespace-0 exec my-cluster-959c6ed5-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-634125597-1746493046 --describe --bootstrap-server my-cluster-959c6ed5-kafka-bootstrap:9092
2022-03-28 19:05:06 [ForkJoinPool-1-worker-7] INFO  [Exec:417] Return code: 0
2022-03-28 19:05:06 [ForkJoinPool-1-worker-7] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 3 polls
2022-03-28 19:05:06 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (91800ms till timeout)
2022-03-28 19:05:06 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Wait for Deployment: my-cluster-afe273c9-connect will be ready not ready, will try again in 1000 ms (459111ms till timeout)
2022-03-28 19:05:06 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (783335ms till timeout)
2022-03-28 19:05:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-444
2022-03-28 19:05:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:06 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-445
2022-03-28 19:05:06 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:369] ############################################################################
2022-03-28 19:05:06 [ForkJoinPool-1-worker-11] INFO  [TestSuiteNamespaceManager:200] Deleting namespace:namespace-4 for test case:testConfigurationPerformanceOptions
2022-03-28 19:05:06 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Namespace namespace-4 removal
2022-03-28 19:05:06 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-4 -o yaml
2022-03-28 19:05:07 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (77978ms till timeout)
2022-03-28 19:05:07 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-445
2022-03-28 19:05:07 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:07 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-446
2022-03-28 19:05:07 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:04:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:05:07 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-0 exec my-cluster-959c6ed5-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-634125597-1746493046 --describe --bootstrap-server my-cluster-959c6ed5-kafka-bootstrap:9092
2022-03-28 19:05:07 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-4 -o yaml
2022-03-28 19:05:07 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:07 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (479565ms till timeout)
2022-03-28 19:05:07 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (160396ms till timeout)
2022-03-28 19:05:07 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (264419ms till timeout)
2022-03-28 19:05:07 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Wait for Deployment: my-cluster-afe273c9-connect will be ready not ready, will try again in 1000 ms (457920ms till timeout)
2022-03-28 19:05:07 [ForkJoinPool-1-worker-31] INFO  [ResourceManager:369] ############################################################################
2022-03-28 19:05:07 [ForkJoinPool-1-worker-31] INFO  [TestSuiteNamespaceManager:200] Deleting namespace:namespace-8 for test case:testCreatingUsersWithSecretPrefix
2022-03-28 19:05:07 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (782189ms till timeout)
2022-03-28 19:05:07 [ForkJoinPool-1-worker-31] DEBUG [TestUtils:125] Waiting for Namespace namespace-8 removal
2022-03-28 19:05:07 [ForkJoinPool-1-worker-31] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 19:05:07 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-446
2022-03-28 19:05:07 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:07 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-447
2022-03-28 19:05:08 [ForkJoinPool-1-worker-31] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 19:05:08 [ForkJoinPool-1-worker-31] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:08 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (479578ms till timeout)
2022-03-28 19:05:08 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (76977ms till timeout)
2022-03-28 19:05:08 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-4 -o yaml
2022-03-28 19:05:08 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-447
2022-03-28 19:05:08 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:08 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-448
2022-03-28 19:05:08 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:04:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:05:08 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (159202ms till timeout)
2022-03-28 19:05:08 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (263228ms till timeout)
2022-03-28 19:05:08 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-4 -o yaml
2022-03-28 19:05:08 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:08 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (478130ms till timeout)
2022-03-28 19:05:08 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (781090ms till timeout)
2022-03-28 19:05:08 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Wait for Deployment: my-cluster-afe273c9-connect will be ready not ready, will try again in 1000 ms (456726ms till timeout)
2022-03-28 19:05:08 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-448
2022-03-28 19:05:08 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:08 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-449
2022-03-28 19:05:09 [ForkJoinPool-1-worker-31] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 19:05:09 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (75977ms till timeout)
2022-03-28 19:05:09 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-449
2022-03-28 19:05:09 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:09 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-45
2022-03-28 19:05:09 [ForkJoinPool-1-worker-31] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 19:05:09 [ForkJoinPool-1-worker-31] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:09 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (478121ms till timeout)
2022-03-28 19:05:09 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:04:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:05:09 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-4 -o yaml
2022-03-28 19:05:09 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (158010ms till timeout)
2022-03-28 19:05:09 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (262035ms till timeout)
2022-03-28 19:05:09 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-45
2022-03-28 19:05:09 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:09 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-450
2022-03-28 19:05:10 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Wait for Deployment: my-cluster-afe273c9-connect will be ready not ready, will try again in 1000 ms (455532ms till timeout)
2022-03-28 19:05:10 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (779895ms till timeout)
2022-03-28 19:05:10 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (74976ms till timeout)
2022-03-28 19:05:10 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-4 -o yaml
2022-03-28 19:05:10 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:10 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (476686ms till timeout)
2022-03-28 19:05:10 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-450
2022-03-28 19:05:10 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:10 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-451
2022-03-28 19:05:10 [ForkJoinPool-1-worker-31] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 19:05:10 [ForkJoinPool-1-worker-7] INFO  [Exec:417] Command: oc --namespace namespace-0 exec my-cluster-959c6ed5-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-634125597-1746493046 --describe --bootstrap-server my-cluster-959c6ed5-kafka-bootstrap:9092
2022-03-28 19:05:10 [ForkJoinPool-1-worker-7] INFO  [Exec:417] Return code: 0
2022-03-28 19:05:10 [ForkJoinPool-1-worker-7] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 2 polls
2022-03-28 19:05:10 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (87207ms till timeout)
2022-03-28 19:05:10 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:04:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:05:11 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-451
2022-03-28 19:05:11 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:11 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-454
2022-03-28 19:05:11 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (260862ms till timeout)
2022-03-28 19:05:11 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Wait for Deployment: my-cluster-afe273c9-connect will be ready not ready, will try again in 1000 ms (454395ms till timeout)
2022-03-28 19:05:11 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (156677ms till timeout)
2022-03-28 19:05:11 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (778757ms till timeout)
2022-03-28 19:05:11 [ForkJoinPool-1-worker-31] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 19:05:11 [ForkJoinPool-1-worker-31] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:11 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (476667ms till timeout)
2022-03-28 19:05:11 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (73976ms till timeout)
2022-03-28 19:05:11 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-4 -o yaml
2022-03-28 19:05:11 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-454
2022-03-28 19:05:11 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:11 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-455
2022-03-28 19:05:11 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-4 -o yaml
2022-03-28 19:05:11 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:11 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (475227ms till timeout)
2022-03-28 19:05:11 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-0 exec my-cluster-959c6ed5-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-634125597-1746493046 --describe --bootstrap-server my-cluster-959c6ed5-kafka-bootstrap:9092
2022-03-28 19:05:12 [ForkJoinPool-1-worker-31] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 19:05:12 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-455
2022-03-28 19:05:12 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:12 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-456
2022-03-28 19:05:12 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (259752ms till timeout)
2022-03-28 19:05:12 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (72975ms till timeout)
2022-03-28 19:05:12 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:04:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:05:12 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Wait for Deployment: my-cluster-afe273c9-connect will be ready not ready, will try again in 1000 ms (453253ms till timeout)
2022-03-28 19:05:12 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (777611ms till timeout)
2022-03-28 19:05:12 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (155435ms till timeout)
2022-03-28 19:05:12 [ForkJoinPool-1-worker-31] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 19:05:12 [ForkJoinPool-1-worker-31] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:12 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (475206ms till timeout)
2022-03-28 19:05:12 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-4 -o yaml
2022-03-28 19:05:12 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-456
2022-03-28 19:05:12 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:12 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-462
2022-03-28 19:05:13 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-4 -o yaml
2022-03-28 19:05:13 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:13 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (473717ms till timeout)
2022-03-28 19:05:13 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (71975ms till timeout)
2022-03-28 19:05:13 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (258649ms till timeout)
2022-03-28 19:05:13 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-462
2022-03-28 19:05:13 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:13 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-464
2022-03-28 19:05:13 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Wait for Deployment: my-cluster-afe273c9-connect will be ready not ready, will try again in 1000 ms (452148ms till timeout)
2022-03-28 19:05:13 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (776509ms till timeout)
2022-03-28 19:05:13 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:04:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:05:13 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (154241ms till timeout)
2022-03-28 19:05:13 [ForkJoinPool-1-worker-31] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 19:05:13 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-464
2022-03-28 19:05:13 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:13 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-466
2022-03-28 19:05:14 [ForkJoinPool-1-worker-31] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 19:05:14 [ForkJoinPool-1-worker-31] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:14 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (473712ms till timeout)
2022-03-28 19:05:14 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-4 -o yaml
2022-03-28 19:05:14 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (70974ms till timeout)
2022-03-28 19:05:14 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (257549ms till timeout)
2022-03-28 19:05:14 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Wait for Deployment: my-cluster-afe273c9-connect will be ready not ready, will try again in 1000 ms (451044ms till timeout)
2022-03-28 19:05:14 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (775407ms till timeout)
2022-03-28 19:05:14 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-466
2022-03-28 19:05:14 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:14 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-469
2022-03-28 19:05:14 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-4 -o yaml
2022-03-28 19:05:14 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:14 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (472271ms till timeout)
2022-03-28 19:05:14 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:04:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:05:14 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (153045ms till timeout)
2022-03-28 19:05:15 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-469
2022-03-28 19:05:15 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:15 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-471
2022-03-28 19:05:15 [ForkJoinPool-1-worker-31] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 19:05:15 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (69972ms till timeout)
2022-03-28 19:05:15 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (256450ms till timeout)
2022-03-28 19:05:15 [ForkJoinPool-1-worker-31] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 19:05:15 [ForkJoinPool-1-worker-31] DEBUG [Exec:419] Return code: 1
2022-03-28 19:05:15 [ForkJoinPool-1-worker-31] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 19:05:15 [ForkJoinPool-1-worker-31] DEBUG [Exec:419] Error from server (NotFound): namespaces "namespace-8" not found
2022-03-28 19:05:15 [ForkJoinPool-1-worker-31] DEBUG [Exec:419] ======STDERR END======
2022-03-28 19:05:15 [ForkJoinPool-1-worker-31] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[], io.strimzi.test.logs.CollectorElement@c4c0ea0=[], io.strimzi.test.logs.CollectorElement@aec142ef=[namespace-5], io.strimzi.test.logs.CollectorElement@24b97ba9=[namespace-7], io.strimzi.test.logs.CollectorElement@f851b6c3=[namespace-4], io.strimzi.test.logs.CollectorElement@3881d5f2=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@5c7379cb=[], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[namespace-0], io.strimzi.test.logs.CollectorElement@f4405b3b=[namespace-2], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 19:05:15 [ForkJoinPool-1-worker-31] DEBUG [SuiteThreadController:267] testCreatingUsersWithSecretPrefix - Notifies waiting test cases:[testPauseReconciliationInKafkaRebalanceAndTopic, testPauseReconciliationInKafkaAndKafkaConnectWithConnector, testCapacityFile, testConfigurationFileIsCreated, testConfigurationPerformanceOptions, testConfigurationReflection, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testDeployAndUnDeployCruiseControl, testTlsExternalUserWithQuotas, testCreatingUsersWithSecretPrefix, testThrottlingQuotasCreateTopic, testThrottlingQuotasCreateAlterPartitions, testKafkaAdminTopicOperations, testUserTemplate, testUpdateUser, testTlsExternalUser, testSendSimpleMessageTls, testReceiveSimpleMessageTlsScramSha, testThrottlingQuotasDeleteTopic, testReceiveSimpleMessageTls, testTopicModificationOfReplicationFactor, testSendingMessagesToNonExistingTopic, testMoreReplicasThanAvailableBrokers, testCreateTopicViaKafka] to and randomly select one to start execution
2022-03-28 19:05:15 [ForkJoinPool-1-worker-31] DEBUG [SuiteThreadController:93] [operators.user.UserST] - Removing parallel test: testCreatingUsersWithSecretPrefix
2022-03-28 19:05:15 [ForkJoinPool-1-worker-31] DEBUG [SuiteThreadController:97] [operators.user.UserST] - Parallel test count: 8
2022-03-28 19:05:15 [ForkJoinPool-1-worker-31] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.user.UserST.testCreatingUsersWithSecretPrefix-FINISHED
2022-03-28 19:05:15 [ForkJoinPool-1-worker-31] INFO  [TestSeparator:30] ############################################################################
2022-03-28 19:05:15 [ForkJoinPool-1-worker-31] DEBUG [AbstractST:689] ============================================================================
2022-03-28 19:05:15 [ForkJoinPool-1-worker-31] DEBUG [AbstractST:690] [operators.user.UserST - After All] - Clean up after test suite
2022-03-28 19:05:15 [ForkJoinPool-1-worker-31] INFO  [ResourceManager:344] ############################################################################
2022-03-28 19:05:15 [ForkJoinPool-1-worker-31] INFO  [ResourceManager:348] Delete all resources for UserST
2022-03-28 19:05:15 [ForkJoinPool-1-worker-31] INFO  [ResourceManager:241] Delete of Kafka user-cluster-name in namespace user-st
2022-03-28 19:05:15 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (774309ms till timeout)
2022-03-28 19:05:15 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-471
2022-03-28 19:05:15 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:15 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-472
2022-03-28 19:05:15 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-4 -o yaml
2022-03-28 19:05:15 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Wait for Deployment: my-cluster-afe273c9-connect will be ready not ready, will try again in 1000 ms (449852ms till timeout)
2022-03-28 19:05:15 [ForkJoinPool-1-worker-31] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:user-cluster-name
2022-03-28 19:05:15 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:user-cluster-name not ready, will try again in 10000 ms (839891ms till timeout)
2022-03-28 19:05:15 [ForkJoinPool-1-worker-7] INFO  [Exec:417] Command: oc --namespace namespace-0 exec my-cluster-959c6ed5-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-634125597-1746493046 --describe --bootstrap-server my-cluster-959c6ed5-kafka-bootstrap:9092
2022-03-28 19:05:15 [ForkJoinPool-1-worker-7] INFO  [Exec:417] Return code: 0
2022-03-28 19:05:15 [ForkJoinPool-1-worker-7] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 1 polls
2022-03-28 19:05:15 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (82220ms till timeout)
2022-03-28 19:05:16 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:04:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:05:16 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-4 -o yaml
2022-03-28 19:05:16 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:16 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (470814ms till timeout)
2022-03-28 19:05:16 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (151617ms till timeout)
2022-03-28 19:05:16 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (68971ms till timeout)
2022-03-28 19:05:16 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-472
2022-03-28 19:05:16 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:16 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-473
2022-03-28 19:05:16 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (255345ms till timeout)
2022-03-28 19:05:16 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Kafka: my-cluster-17fc585b will have desired state: Ready not ready, will try again in 1000 ms (773210ms till timeout)
2022-03-28 19:05:16 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Wait for Deployment: my-cluster-afe273c9-connect will be ready not ready, will try again in 1000 ms (448752ms till timeout)
2022-03-28 19:05:16 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-0 exec my-cluster-959c6ed5-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-634125597-1746493046 --describe --bootstrap-server my-cluster-959c6ed5-kafka-bootstrap:9092
2022-03-28 19:05:16 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-473
2022-03-28 19:05:16 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:16 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-476
2022-03-28 19:05:17 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-4 -o yaml
2022-03-28 19:05:17 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (67971ms till timeout)
2022-03-28 19:05:17 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:04:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:05:17 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (150421ms till timeout)
2022-03-28 19:05:17 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-476
2022-03-28 19:05:17 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:17 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-478
2022-03-28 19:05:17 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-4 -o yaml
2022-03-28 19:05:17 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:17 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (469339ms till timeout)
2022-03-28 19:05:17 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (254246ms till timeout)
2022-03-28 19:05:17 [ForkJoinPool-1-worker-29] INFO  [ResourceManager:444] Kafka: my-cluster-17fc585b is in desired state: Ready
2022-03-28 19:05:17 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Wait for Deployment: my-cluster-afe273c9-connect will be ready not ready, will try again in 1000 ms (447623ms till timeout)
2022-03-28 19:05:18 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-478
2022-03-28 19:05:18 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:18 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-479
2022-03-28 19:05:18 [ForkJoinPool-1-worker-29] TRACE [Exec:248] Running command - oc --namespace namespace-5 exec my-cluster-17fc585b-cruise-control-6b86b78c78-5t8h7 -- /bin/bash -c cat /tmp/cruisecontrol.properties
2022-03-28 19:05:18 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (66970ms till timeout)
2022-03-28 19:05:18 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:04:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:05:18 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-4 -o yaml
2022-03-28 19:05:18 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (149227ms till timeout)
2022-03-28 19:05:18 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-479
2022-03-28 19:05:18 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:18 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-483
2022-03-28 19:05:18 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (253137ms till timeout)
2022-03-28 19:05:19 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Wait for Deployment: my-cluster-afe273c9-connect will be ready not ready, will try again in 1000 ms (446526ms till timeout)
2022-03-28 19:05:19 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-4 -o yaml
2022-03-28 19:05:19 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:19 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (467843ms till timeout)
2022-03-28 19:05:19 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-483
2022-03-28 19:05:19 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:19 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-486
2022-03-28 19:05:19 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (65970ms till timeout)
2022-03-28 19:05:19 [ForkJoinPool-1-worker-29] INFO  [Exec:417] Command: oc --namespace namespace-5 exec my-cluster-17fc585b-cruise-control-6b86b78c78-5t8h7 -- /bin/bash -c cat /tmp/cruisecontrol.properties
2022-03-28 19:05:19 [ForkJoinPool-1-worker-29] INFO  [Exec:417] Return code: 0
2022-03-28 19:05:19 [ForkJoinPool-1-worker-29] DEBUG [AbstractST:674] ============================================================================
2022-03-28 19:05:19 [ForkJoinPool-1-worker-29] DEBUG [AbstractST:675] [cruisecontrol.CruiseControlConfigurationST - After Each] - Clean up after test
2022-03-28 19:05:19 [ForkJoinPool-1-worker-29] INFO  [ResourceManager:344] ############################################################################
2022-03-28 19:05:19 [ForkJoinPool-1-worker-29] INFO  [ResourceManager:348] Delete all resources for testConfigurationFileIsCreated
2022-03-28 19:05:19 [ForkJoinPool-1-worker-29] INFO  [ResourceManager:241] Delete of Kafka my-cluster-17fc585b in namespace namespace-5
2022-03-28 19:05:19 [ForkJoinPool-1-worker-29] INFO  [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-5, for cruise control Kafka cluster my-cluster-17fc585b
2022-03-28 19:05:19 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-486
2022-03-28 19:05:19 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:04:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:05:19 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:19 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-49
2022-03-28 19:05:19 [ForkJoinPool-1-worker-29] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-17fc585b
2022-03-28 19:05:20 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (251956ms till timeout)
2022-03-28 19:05:20 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (147831ms till timeout)
2022-03-28 19:05:20 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-4 -o yaml
2022-03-28 19:05:20 [ForkJoinPool-1-worker-29] INFO  [ResourceManager:369] ############################################################################
2022-03-28 19:05:20 [ForkJoinPool-1-worker-29] INFO  [TestSuiteNamespaceManager:200] Deleting namespace:namespace-5 for test case:testConfigurationFileIsCreated
2022-03-28 19:05:20 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Wait for Deployment: my-cluster-afe273c9-connect will be ready not ready, will try again in 1000 ms (445335ms till timeout)
2022-03-28 19:05:20 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (64969ms till timeout)
2022-03-28 19:05:20 [ForkJoinPool-1-worker-29] DEBUG [TestUtils:125] Waiting for Namespace namespace-5 removal
2022-03-28 19:05:20 [ForkJoinPool-1-worker-29] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-5 -o yaml
2022-03-28 19:05:20 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-49
2022-03-28 19:05:20 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:20 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-490
2022-03-28 19:05:20 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-9 get Namespace namespace-4 -o yaml
2022-03-28 19:05:20 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 1
2022-03-28 19:05:20 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 19:05:20 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Error from server (NotFound): namespaces "namespace-4" not found
2022-03-28 19:05:20 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] ======STDERR END======
2022-03-28 19:05:20 [ForkJoinPool-1-worker-11] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[], io.strimzi.test.logs.CollectorElement@c4c0ea0=[], io.strimzi.test.logs.CollectorElement@aec142ef=[namespace-5], io.strimzi.test.logs.CollectorElement@24b97ba9=[namespace-7], io.strimzi.test.logs.CollectorElement@f851b6c3=[], io.strimzi.test.logs.CollectorElement@3881d5f2=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@5c7379cb=[], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[namespace-0], io.strimzi.test.logs.CollectorElement@f4405b3b=[namespace-2], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 19:05:20 [ForkJoinPool-1-worker-11] DEBUG [SuiteThreadController:267] testConfigurationPerformanceOptions - Notifies waiting test cases:[testPauseReconciliationInKafkaRebalanceAndTopic, testPauseReconciliationInKafkaAndKafkaConnectWithConnector, testCapacityFile, testConfigurationFileIsCreated, testConfigurationPerformanceOptions, testConfigurationReflection, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testDeployAndUnDeployCruiseControl, testTlsExternalUserWithQuotas, testCreatingUsersWithSecretPrefix, testThrottlingQuotasCreateTopic, testThrottlingQuotasCreateAlterPartitions, testKafkaAdminTopicOperations, testUserTemplate, testUpdateUser, testTlsExternalUser, testSendSimpleMessageTls, testReceiveSimpleMessageTlsScramSha, testThrottlingQuotasDeleteTopic, testReceiveSimpleMessageTls, testTopicModificationOfReplicationFactor, testSendingMessagesToNonExistingTopic, testMoreReplicasThanAvailableBrokers, testCreateTopicViaKafka] to and randomly select one to start execution
2022-03-28 19:05:20 [ForkJoinPool-1-worker-11] DEBUG [SuiteThreadController:93] [cruisecontrol.CruiseControlConfigurationST] - Removing parallel test: testConfigurationPerformanceOptions
2022-03-28 19:05:20 [ForkJoinPool-1-worker-11] DEBUG [SuiteThreadController:97] [cruisecontrol.CruiseControlConfigurationST] - Parallel test count: 7
2022-03-28 19:05:20 [ForkJoinPool-1-worker-11] INFO  [TestSeparator:29] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testConfigurationPerformanceOptions-FINISHED
2022-03-28 19:05:20 [ForkJoinPool-1-worker-11] INFO  [TestSeparator:30] ############################################################################
2022-03-28 19:05:20 [ForkJoinPool-1-worker-29] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-5 -o yaml
2022-03-28 19:05:20 [ForkJoinPool-1-worker-29] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:20 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (479535ms till timeout)
2022-03-28 19:05:20 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-490
2022-03-28 19:05:20 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:20 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-492
2022-03-28 19:05:21 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (250855ms till timeout)
2022-03-28 19:05:21 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:04:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:05:21 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (63968ms till timeout)
2022-03-28 19:05:21 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (146533ms till timeout)
2022-03-28 19:05:21 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Wait for Deployment: my-cluster-afe273c9-connect will be ready not ready, will try again in 1000 ms (444153ms till timeout)
2022-03-28 19:05:21 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-492
2022-03-28 19:05:21 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:21 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-493
2022-03-28 19:05:21 [ForkJoinPool-1-worker-29] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-5 -o yaml
2022-03-28 19:05:21 [ForkJoinPool-1-worker-7] INFO  [Exec:417] Command: oc --namespace namespace-0 exec my-cluster-959c6ed5-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-634125597-1746493046 --describe --bootstrap-server my-cluster-959c6ed5-kafka-bootstrap:9092
2022-03-28 19:05:21 [ForkJoinPool-1-worker-7] INFO  [Exec:417] Return code: 0
2022-03-28 19:05:21 [ForkJoinPool-1-worker-7] INFO  [KafkaTopicUtils:197] KafkaTopic's spec is stable for 20 polls intervals
2022-03-28 19:05:21 [ForkJoinPool-1-worker-7] INFO  [ReconciliationST:156] Setting annotation to "false", partitions should be scaled to 4
2022-03-28 19:05:22 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-493
2022-03-28 19:05:22 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:22 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-496
2022-03-28 19:05:22 [ForkJoinPool-1-worker-7] INFO  [KafkaTopicUtils:124] Waiting for KafkaTopic change my-topic-634125597-1746493046
2022-03-28 19:05:22 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for KafkaTopic change my-topic-634125597-1746493046
2022-03-28 19:05:22 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (62968ms till timeout)
2022-03-28 19:05:22 [ForkJoinPool-1-worker-29] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-5 -o yaml
2022-03-28 19:05:22 [ForkJoinPool-1-worker-29] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:22 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (478057ms till timeout)
2022-03-28 19:05:22 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (249679ms till timeout)
2022-03-28 19:05:22 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update KafkaRebalance my-cluster-959c6ed5 in namespace namespace-9
2022-03-28 19:05:22 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:164] Using Namespace: namespace-0
2022-03-28 19:05:22 [ForkJoinPool-1-worker-7] WARN  [VersionUsageUtils:60] The client is using resource type 'kafkarebalances' with unstable version 'v1beta2'
2022-03-28 19:05:22 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:04:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:05:22 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Wait for Deployment: my-cluster-afe273c9-connect will be ready not ready, will try again in 1000 ms (442985ms till timeout)
2022-03-28 19:05:22 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaRebalance:my-cluster-959c6ed5
2022-03-28 19:05:22 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-496
2022-03-28 19:05:22 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:22 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-498
2022-03-28 19:05:22 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (145175ms till timeout)
2022-03-28 19:05:22 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:433] Wait for KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal
2022-03-28 19:05:22 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal
2022-03-28 19:05:22 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (359903ms till timeout)
2022-03-28 19:05:23 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-498
2022-03-28 19:05:23 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:23 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-5
2022-03-28 19:05:23 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (61967ms till timeout)
2022-03-28 19:05:23 [ForkJoinPool-1-worker-29] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-5 -o yaml
2022-03-28 19:05:23 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (248578ms till timeout)
2022-03-28 19:05:23 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Wait for Deployment: my-cluster-afe273c9-connect will be ready not ready, will try again in 1000 ms (441887ms till timeout)
2022-03-28 19:05:23 [ForkJoinPool-1-worker-29] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-5 -o yaml
2022-03-28 19:05:23 [ForkJoinPool-1-worker-29] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:23 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (476579ms till timeout)
2022-03-28 19:05:23 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:04:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:05:23 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-5
2022-03-28 19:05:23 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:23 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-50
2022-03-28 19:05:23 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (143980ms till timeout)
2022-03-28 19:05:24 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (358807ms till timeout)
2022-03-28 19:05:24 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (60967ms till timeout)
2022-03-28 19:05:24 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-50
2022-03-28 19:05:24 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:24 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-51
2022-03-28 19:05:24 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (247478ms till timeout)
2022-03-28 19:05:24 [ForkJoinPool-1-worker-29] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-5 -o yaml
2022-03-28 19:05:24 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Wait for Deployment: my-cluster-afe273c9-connect will be ready not ready, will try again in 1000 ms (440790ms till timeout)
2022-03-28 19:05:24 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:04:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:05:25 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (142786ms till timeout)
2022-03-28 19:05:25 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (357709ms till timeout)
2022-03-28 19:05:25 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-51
2022-03-28 19:05:25 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:25 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-54
2022-03-28 19:05:25 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (59966ms till timeout)
2022-03-28 19:05:25 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (246378ms till timeout)
2022-03-28 19:05:25 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Wait for Deployment: my-cluster-afe273c9-connect will be ready not ready, will try again in 1000 ms (439692ms till timeout)
2022-03-28 19:05:25 [ForkJoinPool-1-worker-31] INFO  [ResourceManager:369] ############################################################################
2022-03-28 19:05:26 [ForkJoinPool-1-worker-31] DEBUG [TestUtils:125] Waiting for Namespace user-st removal
2022-03-28 19:05:26 [ForkJoinPool-1-worker-31] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace user-st -o yaml
2022-03-28 19:05:26 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:04:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:05:26 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (356613ms till timeout)
2022-03-28 19:05:26 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (58966ms till timeout)
2022-03-28 19:05:26 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (141498ms till timeout)
2022-03-28 19:05:26 [ForkJoinPool-1-worker-31] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace user-st -o yaml
2022-03-28 19:05:26 [ForkJoinPool-1-worker-31] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:26 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (479485ms till timeout)
2022-03-28 19:05:26 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (245278ms till timeout)
2022-03-28 19:05:26 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Wait for Deployment: my-cluster-afe273c9-connect will be ready not ready, will try again in 1000 ms (438595ms till timeout)
2022-03-28 19:05:27 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (57965ms till timeout)
2022-03-28 19:05:27 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (355516ms till timeout)
2022-03-28 19:05:27 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:04:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:05:27 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (140304ms till timeout)
2022-03-28 19:05:27 [ForkJoinPool-1-worker-31] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace user-st -o yaml
2022-03-28 19:05:27 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (244178ms till timeout)
2022-03-28 19:05:28 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Wait for Deployment: my-cluster-afe273c9-connect will be ready not ready, will try again in 1000 ms (437498ms till timeout)
2022-03-28 19:05:28 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (56964ms till timeout)
2022-03-28 19:05:28 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (354419ms till timeout)
2022-03-28 19:05:28 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:04:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:05:28 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (139111ms till timeout)
2022-03-28 19:05:28 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (243079ms till timeout)
2022-03-28 19:05:29 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Wait for Deployment: my-cluster-afe273c9-connect will be ready not ready, will try again in 1000 ms (436395ms till timeout)
2022-03-28 19:05:29 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (55964ms till timeout)
2022-03-28 19:05:29 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (353322ms till timeout)
2022-03-28 19:05:29 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:04:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:05:29 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (137918ms till timeout)
2022-03-28 19:05:30 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (241942ms till timeout)
2022-03-28 19:05:30 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Wait for Deployment: my-cluster-afe273c9-connect will be ready not ready, will try again in 1000 ms (435298ms till timeout)
2022-03-28 19:05:30 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (54963ms till timeout)
2022-03-28 19:05:30 [ForkJoinPool-1-worker-29] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-5 -o yaml
2022-03-28 19:05:30 [ForkJoinPool-1-worker-29] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:30 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (470028ms till timeout)
2022-03-28 19:05:30 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (352225ms till timeout)
2022-03-28 19:05:30 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-54
2022-03-28 19:05:30 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:30 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-55
2022-03-28 19:05:31 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:04:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:05:31 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (136725ms till timeout)
2022-03-28 19:05:31 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (240717ms till timeout)
2022-03-28 19:05:31 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (53963ms till timeout)
2022-03-28 19:05:31 [ForkJoinPool-1-worker-29] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-5 -o yaml
2022-03-28 19:05:31 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Wait for Deployment: my-cluster-afe273c9-connect will be ready not ready, will try again in 1000 ms (434198ms till timeout)
2022-03-28 19:05:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-55
2022-03-28 19:05:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:31 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-58
2022-03-28 19:05:31 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (351127ms till timeout)
2022-03-28 19:05:32 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:04:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:05:32 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (52962ms till timeout)
2022-03-28 19:05:32 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (239601ms till timeout)
2022-03-28 19:05:32 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (135383ms till timeout)
2022-03-28 19:05:32 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Wait for Deployment: my-cluster-afe273c9-connect will be ready not ready, will try again in 1000 ms (433000ms till timeout)
2022-03-28 19:05:32 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (350030ms till timeout)
2022-03-28 19:05:33 [ForkJoinPool-1-worker-31] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-9 get Namespace user-st -o yaml
2022-03-28 19:05:33 [ForkJoinPool-1-worker-31] DEBUG [Exec:419] Return code: 1
2022-03-28 19:05:33 [ForkJoinPool-1-worker-31] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 19:05:33 [ForkJoinPool-1-worker-31] DEBUG [Exec:419] Error from server (NotFound): namespaces "user-st" not found
2022-03-28 19:05:33 [ForkJoinPool-1-worker-31] DEBUG [Exec:419] ======STDERR END======
2022-03-28 19:05:33 [ForkJoinPool-1-worker-31] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[], io.strimzi.test.logs.CollectorElement@c4c0ea0=[], io.strimzi.test.logs.CollectorElement@aec142ef=[namespace-5], io.strimzi.test.logs.CollectorElement@24b97ba9=[namespace-7], io.strimzi.test.logs.CollectorElement@f851b6c3=[], io.strimzi.test.logs.CollectorElement@3881d5f2=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@5c7379cb=[], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[namespace-0], io.strimzi.test.logs.CollectorElement@f4405b3b=[namespace-2], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 19:05:33 [ForkJoinPool-1-worker-31] DEBUG [SuiteThreadController:254] UserST - Notifies waiting test suites:[TopicST, ThrottlingQuotaST, UserST, HttpBridgeTlsST, HttpBridgeScramShaST, ReconciliationST, CruiseControlConfigurationST] to and randomly select one to start execution
2022-03-28 19:05:33 [ForkJoinPool-1-worker-31] DEBUG [SuiteThreadController:85] [operators.user.UserST] - Removing parallel suite: UserST
2022-03-28 19:05:33 [ForkJoinPool-1-worker-31] DEBUG [SuiteThreadController:89] [operators.user.UserST] - Parallel suites count: 4
[INFO] Tests run: 12, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 628.656 s - in io.strimzi.systemtest.operators.user.UserST
2022-03-28 19:05:33 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (51962ms till timeout)
2022-03-28 19:05:33 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (238500ms till timeout)
2022-03-28 19:05:33 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:04:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:05:33 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Wait for Deployment: my-cluster-afe273c9-connect will be ready not ready, will try again in 1000 ms (431902ms till timeout)
2022-03-28 19:05:33 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (134090ms till timeout)
2022-03-28 19:05:33 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (348933ms till timeout)
2022-03-28 19:05:34 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (50961ms till timeout)
2022-03-28 19:05:34 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (237400ms till timeout)
2022-03-28 19:05:34 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Wait for Deployment: my-cluster-afe273c9-connect will be ready not ready, will try again in 1000 ms (430803ms till timeout)
2022-03-28 19:05:34 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:04:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:05:34 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (132895ms till timeout)
2022-03-28 19:05:35 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (347819ms till timeout)
2022-03-28 19:05:35 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (49961ms till timeout)
2022-03-28 19:05:35 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (236301ms till timeout)
2022-03-28 19:05:35 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Wait for Deployment: my-cluster-afe273c9-connect will be ready not ready, will try again in 1000 ms (429706ms till timeout)
2022-03-28 19:05:36 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:04:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:05:36 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (346722ms till timeout)
2022-03-28 19:05:36 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (131608ms till timeout)
2022-03-28 19:05:36 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (48960ms till timeout)
2022-03-28 19:05:36 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (235201ms till timeout)
2022-03-28 19:05:36 [ForkJoinPool-1-worker-29] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-5 -o yaml
2022-03-28 19:05:36 [ForkJoinPool-1-worker-29] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:36 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (463519ms till timeout)
2022-03-28 19:05:36 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Wait for Deployment: my-cluster-afe273c9-connect will be ready not ready, will try again in 1000 ms (428610ms till timeout)
2022-03-28 19:05:37 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-58
2022-03-28 19:05:37 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:37 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-6
2022-03-28 19:05:37 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (345626ms till timeout)
2022-03-28 19:05:37 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (47959ms till timeout)
2022-03-28 19:05:37 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:04:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:05:37 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (130416ms till timeout)
2022-03-28 19:05:37 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-6
2022-03-28 19:05:37 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:37 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-61
2022-03-28 19:05:37 [ForkJoinPool-1-worker-29] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-5 -o yaml
2022-03-28 19:05:37 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (234102ms till timeout)
2022-03-28 19:05:38 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Wait for Deployment: my-cluster-afe273c9-connect will be ready not ready, will try again in 1000 ms (427513ms till timeout)
2022-03-28 19:05:38 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (46959ms till timeout)
2022-03-28 19:05:38 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (344529ms till timeout)
2022-03-28 19:05:38 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:04:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:05:38 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (129224ms till timeout)
2022-03-28 19:05:38 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (233002ms till timeout)
2022-03-28 19:05:39 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Wait for Deployment: my-cluster-afe273c9-connect will be ready not ready, will try again in 1000 ms (426416ms till timeout)
2022-03-28 19:05:39 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (45958ms till timeout)
2022-03-28 19:05:39 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (343430ms till timeout)
2022-03-28 19:05:39 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:04:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:05:39 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (128031ms till timeout)
2022-03-28 19:05:40 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (231892ms till timeout)
2022-03-28 19:05:40 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Wait for Deployment: my-cluster-afe273c9-connect will be ready not ready, will try again in 1000 ms (425318ms till timeout)
2022-03-28 19:05:40 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (44958ms till timeout)
2022-03-28 19:05:40 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (342332ms till timeout)
2022-03-28 19:05:40 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:04:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:05:41 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (126774ms till timeout)
2022-03-28 19:05:41 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (230791ms till timeout)
2022-03-28 19:05:41 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (43957ms till timeout)
2022-03-28 19:05:41 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Wait for Deployment: my-cluster-afe273c9-connect will be ready not ready, will try again in 1000 ms (424222ms till timeout)
2022-03-28 19:05:41 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (341235ms till timeout)
2022-03-28 19:05:42 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:04:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:05:42 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (125581ms till timeout)
2022-03-28 19:05:42 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (42957ms till timeout)
2022-03-28 19:05:42 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (229607ms till timeout)
2022-03-28 19:05:42 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Wait for Deployment: my-cluster-afe273c9-connect will be ready not ready, will try again in 1000 ms (423105ms till timeout)
2022-03-28 19:05:42 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (340139ms till timeout)
2022-03-28 19:05:43 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-61
2022-03-28 19:05:43 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:43 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-62
2022-03-28 19:05:43 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (41956ms till timeout)
2022-03-28 19:05:43 [ForkJoinPool-1-worker-29] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-5 -o yaml
2022-03-28 19:05:43 [ForkJoinPool-1-worker-29] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:43 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (457049ms till timeout)
2022-03-28 19:05:43 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:04:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:05:43 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (124389ms till timeout)
2022-03-28 19:05:43 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (228415ms till timeout)
2022-03-28 19:05:43 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Wait for Deployment: my-cluster-afe273c9-connect will be ready not ready, will try again in 1000 ms (421914ms till timeout)
2022-03-28 19:05:43 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (339041ms till timeout)
2022-03-28 19:05:44 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (40956ms till timeout)
2022-03-28 19:05:44 [ForkJoinPool-1-worker-29] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-5 -o yaml
2022-03-28 19:05:44 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:04:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:05:44 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (123196ms till timeout)
2022-03-28 19:05:44 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (227223ms till timeout)
2022-03-28 19:05:44 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Wait for Deployment: my-cluster-afe273c9-connect will be ready not ready, will try again in 1000 ms (420721ms till timeout)
2022-03-28 19:05:44 [ForkJoinPool-1-worker-29] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-5 -o yaml
2022-03-28 19:05:44 [ForkJoinPool-1-worker-29] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:44 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (455481ms till timeout)
2022-03-28 19:05:44 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (337929ms till timeout)
2022-03-28 19:05:45 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (39955ms till timeout)
2022-03-28 19:05:45 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:04:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:05:45 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (122003ms till timeout)
2022-03-28 19:05:45 [ForkJoinPool-1-worker-29] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-5 -o yaml
2022-03-28 19:05:45 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (226026ms till timeout)
2022-03-28 19:05:46 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (336828ms till timeout)
2022-03-28 19:05:46 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Wait for Deployment: my-cluster-afe273c9-connect will be ready not ready, will try again in 1000 ms (419524ms till timeout)
2022-03-28 19:05:46 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (38955ms till timeout)
2022-03-28 19:05:46 [ForkJoinPool-1-worker-29] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-5 -o yaml
2022-03-28 19:05:46 [ForkJoinPool-1-worker-29] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:46 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (453900ms till timeout)
2022-03-28 19:05:46 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=null, completedIndexes=null, completionTime=2022-03-28T19:05:41Z, conditions=[JobCondition(lastProbeTime=2022-03-28T19:05:41Z, lastTransitionTime=2022-03-28T19:05:41Z, message=null, reason=null, status=True, type=Complete, additionalProperties={})], failed=null, ready=null, startTime=2022-03-28T19:04:02Z, succeeded=1, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:05:47 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (224833ms till timeout)
2022-03-28 19:05:47 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Wait for Deployment: my-cluster-afe273c9-connect will be ready not ready, will try again in 1000 ms (418329ms till timeout)
2022-03-28 19:05:47 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (335631ms till timeout)
2022-03-28 19:05:47 [ForkJoinPool-1-worker-41] INFO  [PodUtils:186] Waiting for message will be in the log
2022-03-28 19:05:47 [ForkJoinPool-1-worker-41] DEBUG [TestUtils:125] Waiting for Waiting for message will be in the log
2022-03-28 19:05:47 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (37954ms till timeout)
2022-03-28 19:05:47 [ForkJoinPool-1-worker-41] INFO  [PodUtils:189] Message All topics created found in create-admin-my-cluster-836b31f8-kafka-clients-lbkv7 log
2022-03-28 19:05:47 [ForkJoinPool-1-worker-29] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-5 -o yaml
2022-03-28 19:05:47 [ForkJoinPool-1-worker-41] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment create-admin-my-cluster-836b31f8-kafka-clients deletion
2022-03-28 19:05:47 [ForkJoinPool-1-worker-41] DEBUG [TestUtils:125] Waiting for ReplicaSet create-admin-my-cluster-836b31f8-kafka-clients to be deleted
2022-03-28 19:05:47 [ForkJoinPool-1-worker-41] DEBUG [JobUtils:40] Job create-admin-my-cluster-836b31f8-kafka-clients was deleted
2022-03-28 19:05:47 [ForkJoinPool-1-worker-41] INFO  [ThrottlingQuotaST:112] Executing 4/5 iteration.
2022-03-28 19:05:47 [ForkJoinPool-1-worker-41] INFO  [ResourceManager:155] Create/Update Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st
2022-03-28 19:05:47 [ForkJoinPool-1-worker-41] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:create-admin-my-cluster-836b31f8-kafka-clients
2022-03-28 19:05:47 [ForkJoinPool-1-worker-41] INFO  [JobUtils:81] Waiting for job: create-admin-my-cluster-836b31f8-kafka-clients will be in active state
2022-03-28 19:05:47 [ForkJoinPool-1-worker-41] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 19:05:47 [ForkJoinPool-1-worker-41] INFO  [ClientUtils:76] Waiting for producer/consumer:create-admin-my-cluster-836b31f8-kafka-clients to finished
2022-03-28 19:05:47 [ForkJoinPool-1-worker-41] DEBUG [TestUtils:125] Waiting for job finished
2022-03-28 19:05:47 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:05:48 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (219808ms till timeout)
2022-03-28 19:05:48 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (223733ms till timeout)
2022-03-28 19:05:48 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (36953ms till timeout)
2022-03-28 19:05:48 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Wait for Deployment: my-cluster-afe273c9-connect will be ready not ready, will try again in 1000 ms (417231ms till timeout)
2022-03-28 19:05:48 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (334533ms till timeout)
2022-03-28 19:05:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-62
2022-03-28 19:05:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:48 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-63
2022-03-28 19:05:49 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:05:49 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (218615ms till timeout)
2022-03-28 19:05:49 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (35953ms till timeout)
2022-03-28 19:05:49 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (222633ms till timeout)
2022-03-28 19:05:49 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Wait for Deployment: my-cluster-afe273c9-connect will be ready not ready, will try again in 1000 ms (416131ms till timeout)
2022-03-28 19:05:49 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (333433ms till timeout)
2022-03-28 19:05:50 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (34952ms till timeout)
2022-03-28 19:05:50 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:05:50 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (217422ms till timeout)
2022-03-28 19:05:50 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (221459ms till timeout)
2022-03-28 19:05:50 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Wait for Deployment: my-cluster-afe273c9-connect will be ready not ready, will try again in 1000 ms (414956ms till timeout)
2022-03-28 19:05:50 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (332257ms till timeout)
2022-03-28 19:05:51 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (33952ms till timeout)
2022-03-28 19:05:51 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:05:51 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (216228ms till timeout)
2022-03-28 19:05:51 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (220233ms till timeout)
2022-03-28 19:05:51 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Wait for Deployment: my-cluster-afe273c9-connect will be ready not ready, will try again in 1000 ms (413764ms till timeout)
2022-03-28 19:05:51 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (331066ms till timeout)
2022-03-28 19:05:52 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (32951ms till timeout)
2022-03-28 19:05:52 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:05:52 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (215035ms till timeout)
2022-03-28 19:05:52 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (219070ms till timeout)
2022-03-28 19:05:52 [ForkJoinPool-1-worker-29] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-5 -o yaml
2022-03-28 19:05:52 [ForkJoinPool-1-worker-29] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:52 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (447350ms till timeout)
2022-03-28 19:05:52 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Wait for Deployment: my-cluster-afe273c9-connect will be ready not ready, will try again in 1000 ms (412569ms till timeout)
2022-03-28 19:05:52 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (329872ms till timeout)
2022-03-28 19:05:53 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (31951ms till timeout)
2022-03-28 19:05:53 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:05:53 [ForkJoinPool-1-worker-29] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-5 -o yaml
2022-03-28 19:05:53 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (213842ms till timeout)
2022-03-28 19:05:54 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (217879ms till timeout)
2022-03-28 19:05:54 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Wait for Deployment: my-cluster-afe273c9-connect will be ready not ready, will try again in 1000 ms (411377ms till timeout)
2022-03-28 19:05:54 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (328679ms till timeout)
2022-03-28 19:05:54 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (30950ms till timeout)
2022-03-28 19:05:54 [ForkJoinPool-1-worker-29] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-5 -o yaml
2022-03-28 19:05:54 [ForkJoinPool-1-worker-29] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:54 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (445897ms till timeout)
2022-03-28 19:05:54 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-63
2022-03-28 19:05:54 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:54 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-65
2022-03-28 19:05:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-65
2022-03-28 19:05:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:55 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-66
2022-03-28 19:05:55 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:05:55 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (212652ms till timeout)
2022-03-28 19:05:55 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (29950ms till timeout)
2022-03-28 19:05:55 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (216656ms till timeout)
2022-03-28 19:05:55 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Wait for Deployment: my-cluster-afe273c9-connect will be ready not ready, will try again in 1000 ms (410188ms till timeout)
2022-03-28 19:05:55 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (327490ms till timeout)
2022-03-28 19:05:55 [ForkJoinPool-1-worker-29] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-5 -o yaml
2022-03-28 19:05:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-66
2022-03-28 19:05:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:55 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-68
2022-03-28 19:05:55 [ForkJoinPool-1-worker-29] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-9 get Namespace namespace-5 -o yaml
2022-03-28 19:05:55 [ForkJoinPool-1-worker-29] DEBUG [Exec:419] Return code: 1
2022-03-28 19:05:55 [ForkJoinPool-1-worker-29] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 19:05:55 [ForkJoinPool-1-worker-29] DEBUG [Exec:419] Error from server (NotFound): namespaces "namespace-5" not found
2022-03-28 19:05:55 [ForkJoinPool-1-worker-29] DEBUG [Exec:419] ======STDERR END======
2022-03-28 19:05:55 [ForkJoinPool-1-worker-29] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[], io.strimzi.test.logs.CollectorElement@c4c0ea0=[], io.strimzi.test.logs.CollectorElement@aec142ef=[], io.strimzi.test.logs.CollectorElement@24b97ba9=[namespace-7], io.strimzi.test.logs.CollectorElement@f851b6c3=[], io.strimzi.test.logs.CollectorElement@3881d5f2=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@5c7379cb=[], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[namespace-0], io.strimzi.test.logs.CollectorElement@f4405b3b=[namespace-2], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 19:05:55 [ForkJoinPool-1-worker-29] DEBUG [SuiteThreadController:267] testConfigurationFileIsCreated - Notifies waiting test cases:[testPauseReconciliationInKafkaRebalanceAndTopic, testPauseReconciliationInKafkaAndKafkaConnectWithConnector, testCapacityFile, testConfigurationFileIsCreated, testConfigurationPerformanceOptions, testConfigurationReflection, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testDeployAndUnDeployCruiseControl, testTlsExternalUserWithQuotas, testCreatingUsersWithSecretPrefix, testThrottlingQuotasCreateTopic, testThrottlingQuotasCreateAlterPartitions, testKafkaAdminTopicOperations, testUserTemplate, testUpdateUser, testTlsExternalUser, testSendSimpleMessageTls, testReceiveSimpleMessageTlsScramSha, testThrottlingQuotasDeleteTopic, testReceiveSimpleMessageTls, testTopicModificationOfReplicationFactor, testSendingMessagesToNonExistingTopic, testMoreReplicasThanAvailableBrokers, testCreateTopicViaKafka] to and randomly select one to start execution
2022-03-28 19:05:55 [ForkJoinPool-1-worker-29] DEBUG [SuiteThreadController:93] [cruisecontrol.CruiseControlConfigurationST] - Removing parallel test: testConfigurationFileIsCreated
2022-03-28 19:05:55 [ForkJoinPool-1-worker-29] DEBUG [SuiteThreadController:97] [cruisecontrol.CruiseControlConfigurationST] - Parallel test count: 6
2022-03-28 19:05:55 [ForkJoinPool-1-worker-29] INFO  [TestSeparator:29] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testConfigurationFileIsCreated-FINISHED
2022-03-28 19:05:55 [ForkJoinPool-1-worker-29] INFO  [TestSeparator:30] ############################################################################
2022-03-28 19:05:56 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-68
2022-03-28 19:05:56 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:56 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-71
2022-03-28 19:05:56 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:05:56 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (28949ms till timeout)
2022-03-28 19:05:56 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (211457ms till timeout)
2022-03-28 19:05:56 [ForkJoinPool-1-worker-25] INFO  [DeploymentUtils:168] Deployment: my-cluster-afe273c9-connect is ready
2022-03-28 19:05:56 [ForkJoinPool-1-worker-25] INFO  [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-afe273c9-connect to be ready
2022-03-28 19:05:56 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (215407ms till timeout)
2022-03-28 19:05:56 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (326299ms till timeout)
2022-03-28 19:05:56 [ForkJoinPool-1-worker-25] DEBUG [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=KafkaConnect, strimzi.io/name=my-cluster-afe273c9-connect}, additionalProperties={})to be ready
2022-03-28 19:05:56 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-71
2022-03-28 19:05:56 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:56 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-72
2022-03-28 19:05:56 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-connect-795c55f45c-jjrk7 not ready: my-cluster-afe273c9-connect)
2022-03-28 19:05:56 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:106] Pods my-cluster-afe273c9-connect-795c55f45c-jjrk7 are ready
2022-03-28 19:05:56 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=KafkaConnect, strimzi.io/name=my-cluster-afe273c9-connect}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599903ms till timeout)
2022-03-28 19:05:57 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-72
2022-03-28 19:05:57 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:57 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-73
2022-03-28 19:05:57 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (27949ms till timeout)
2022-03-28 19:05:57 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:05:57 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (210260ms till timeout)
2022-03-28 19:05:57 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (325195ms till timeout)
2022-03-28 19:05:57 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (214208ms till timeout)
2022-03-28 19:05:57 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-73
2022-03-28 19:05:57 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:57 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-74
2022-03-28 19:05:57 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-connect-795c55f45c-jjrk7 not ready: my-cluster-afe273c9-connect)
2022-03-28 19:05:57 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:106] Pods my-cluster-afe273c9-connect-795c55f45c-jjrk7 are ready
2022-03-28 19:05:57 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=KafkaConnect, strimzi.io/name=my-cluster-afe273c9-connect}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598804ms till timeout)
2022-03-28 19:05:58 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (26948ms till timeout)
2022-03-28 19:05:58 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-74
2022-03-28 19:05:58 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:58 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-75
2022-03-28 19:05:58 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:05:58 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (324097ms till timeout)
2022-03-28 19:05:58 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (208970ms till timeout)
2022-03-28 19:05:58 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (213014ms till timeout)
2022-03-28 19:05:58 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-connect-795c55f45c-jjrk7 not ready: my-cluster-afe273c9-connect)
2022-03-28 19:05:58 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:106] Pods my-cluster-afe273c9-connect-795c55f45c-jjrk7 are ready
2022-03-28 19:05:58 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=KafkaConnect, strimzi.io/name=my-cluster-afe273c9-connect}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597703ms till timeout)
2022-03-28 19:05:59 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-75
2022-03-28 19:05:59 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:59 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-8
2022-03-28 19:05:59 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (25947ms till timeout)
2022-03-28 19:05:59 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-8
2022-03-28 19:05:59 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:05:59 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-80
2022-03-28 19:05:59 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (323000ms till timeout)
2022-03-28 19:05:59 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:06:00 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (211897ms till timeout)
2022-03-28 19:06:00 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-80
2022-03-28 19:06:00 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:06:00 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-81
2022-03-28 19:06:00 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (207680ms till timeout)
2022-03-28 19:06:00 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-connect-795c55f45c-jjrk7 not ready: my-cluster-afe273c9-connect)
2022-03-28 19:06:00 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:106] Pods my-cluster-afe273c9-connect-795c55f45c-jjrk7 are ready
2022-03-28 19:06:00 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=KafkaConnect, strimzi.io/name=my-cluster-afe273c9-connect}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596509ms till timeout)
2022-03-28 19:06:00 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (24947ms till timeout)
2022-03-28 19:06:00 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-81
2022-03-28 19:06:00 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:06:00 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-82
2022-03-28 19:06:00 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (321903ms till timeout)
2022-03-28 19:06:01 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (210796ms till timeout)
2022-03-28 19:06:01 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-82
2022-03-28 19:06:01 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:06:01 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-85
2022-03-28 19:06:01 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:06:01 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-connect-795c55f45c-jjrk7 not ready: my-cluster-afe273c9-connect)
2022-03-28 19:06:01 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:106] Pods my-cluster-afe273c9-connect-795c55f45c-jjrk7 are ready
2022-03-28 19:06:01 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=KafkaConnect, strimzi.io/name=my-cluster-afe273c9-connect}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595394ms till timeout)
2022-03-28 19:06:01 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (23946ms till timeout)
2022-03-28 19:06:01 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (206469ms till timeout)
2022-03-28 19:06:01 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-85
2022-03-28 19:06:01 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:06:01 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-86
2022-03-28 19:06:02 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (320806ms till timeout)
2022-03-28 19:06:02 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (209696ms till timeout)
2022-03-28 19:06:02 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (22946ms till timeout)
2022-03-28 19:06:02 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-86
2022-03-28 19:06:02 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:06:02 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-87
2022-03-28 19:06:02 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-connect-795c55f45c-jjrk7 not ready: my-cluster-afe273c9-connect)
2022-03-28 19:06:02 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:106] Pods my-cluster-afe273c9-connect-795c55f45c-jjrk7 are ready
2022-03-28 19:06:02 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=KafkaConnect, strimzi.io/name=my-cluster-afe273c9-connect}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594294ms till timeout)
2022-03-28 19:06:02 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:06:02 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (205272ms till timeout)
2022-03-28 19:06:02 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-87
2022-03-28 19:06:02 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:06:02 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-9
2022-03-28 19:06:03 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (319708ms till timeout)
2022-03-28 19:06:03 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (21945ms till timeout)
2022-03-28 19:06:03 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (208597ms till timeout)
2022-03-28 19:06:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-9
2022-03-28 19:06:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:06:03 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-96
2022-03-28 19:06:03 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-connect-795c55f45c-jjrk7 not ready: my-cluster-afe273c9-connect)
2022-03-28 19:06:03 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:106] Pods my-cluster-afe273c9-connect-795c55f45c-jjrk7 are ready
2022-03-28 19:06:03 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=KafkaConnect, strimzi.io/name=my-cluster-afe273c9-connect}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593195ms till timeout)
2022-03-28 19:06:03 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:06:03 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (204070ms till timeout)
2022-03-28 19:06:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-96
2022-03-28 19:06:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:06:03 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-97
2022-03-28 19:06:04 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (318611ms till timeout)
2022-03-28 19:06:04 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (20944ms till timeout)
2022-03-28 19:06:04 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (207498ms till timeout)
2022-03-28 19:06:04 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-connect-795c55f45c-jjrk7 not ready: my-cluster-afe273c9-connect)
2022-03-28 19:06:04 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:106] Pods my-cluster-afe273c9-connect-795c55f45c-jjrk7 are ready
2022-03-28 19:06:04 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-97
2022-03-28 19:06:04 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=KafkaConnect, strimzi.io/name=my-cluster-afe273c9-connect}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592095ms till timeout)
2022-03-28 19:06:04 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:06:04 [ForkJoinPool-1-worker-9] DEBUG [AbstractST:674] ============================================================================
2022-03-28 19:06:04 [ForkJoinPool-1-worker-9] DEBUG [AbstractST:675] [operators.topic.ThrottlingQuotaST - After Each] - Clean up after test
2022-03-28 19:06:04 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:344] ############################################################################
2022-03-28 19:06:04 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:348] Delete all resources for testThrottlingQuotasCreateTopic
2022-03-28 19:06:04 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:241] Delete of Job create-admin-my-cluster-e9955717-kafka-clients in namespace throttling-quota-st
2022-03-28 19:06:04 [ForkJoinPool-1-worker-29] INFO  [ResourceManager:241] Delete of KafkaUser my-user-1876060546-1887905718 in namespace throttling-quota-st
2022-03-28 19:06:04 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:create-admin-my-cluster-e9955717-kafka-clients
2022-03-28 19:06:04 [ForkJoinPool-1-worker-29] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-1876060546-1887905718
2022-03-28 19:06:04 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:369] ############################################################################
2022-03-28 19:06:04 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:267] testThrottlingQuotasCreateTopic - Notifies waiting test cases:[testPauseReconciliationInKafkaRebalanceAndTopic, testPauseReconciliationInKafkaAndKafkaConnectWithConnector, testCapacityFile, testConfigurationFileIsCreated, testConfigurationPerformanceOptions, testConfigurationReflection, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testDeployAndUnDeployCruiseControl, testTlsExternalUserWithQuotas, testCreatingUsersWithSecretPrefix, testThrottlingQuotasCreateTopic, testThrottlingQuotasCreateAlterPartitions, testKafkaAdminTopicOperations, testUserTemplate, testUpdateUser, testTlsExternalUser, testSendSimpleMessageTls, testReceiveSimpleMessageTlsScramSha, testThrottlingQuotasDeleteTopic, testReceiveSimpleMessageTls, testTopicModificationOfReplicationFactor, testSendingMessagesToNonExistingTopic, testMoreReplicasThanAvailableBrokers, testCreateTopicViaKafka] to and randomly select one to start execution
2022-03-28 19:06:04 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:93] [operators.topic.ThrottlingQuotaST] - Removing parallel test: testThrottlingQuotasCreateTopic
2022-03-28 19:06:04 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:97] [operators.topic.ThrottlingQuotaST] - Parallel test count: 5
2022-03-28 19:06:04 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:06:04 [ForkJoinPool-1-worker-9] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.topic.ThrottlingQuotaST.testThrottlingQuotasCreateTopic-FINISHED
2022-03-28 19:06:04 [ForkJoinPool-1-worker-9] INFO  [TestSeparator:30] ############################################################################
2022-03-28 19:06:05 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (202791ms till timeout)
2022-03-28 19:06:05 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (19944ms till timeout)
2022-03-28 19:06:05 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (317515ms till timeout)
2022-03-28 19:06:05 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (206397ms till timeout)
2022-03-28 19:06:05 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-connect-795c55f45c-jjrk7 not ready: my-cluster-afe273c9-connect)
2022-03-28 19:06:05 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:106] Pods my-cluster-afe273c9-connect-795c55f45c-jjrk7 are ready
2022-03-28 19:06:05 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=KafkaConnect, strimzi.io/name=my-cluster-afe273c9-connect}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590995ms till timeout)
2022-03-28 19:06:06 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:06:06 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (201598ms till timeout)
2022-03-28 19:06:06 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (18943ms till timeout)
2022-03-28 19:06:06 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (316416ms till timeout)
2022-03-28 19:06:06 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (205298ms till timeout)
2022-03-28 19:06:06 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-connect-795c55f45c-jjrk7 not ready: my-cluster-afe273c9-connect)
2022-03-28 19:06:06 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:106] Pods my-cluster-afe273c9-connect-795c55f45c-jjrk7 are ready
2022-03-28 19:06:06 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-afe273c9, strimzi.io/kind=KafkaConnect, strimzi.io/name=my-cluster-afe273c9-connect}, additionalProperties={})to be ready not ready, will try again in 1000 ms (589896ms till timeout)
2022-03-28 19:06:07 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (17943ms till timeout)
2022-03-28 19:06:07 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:06:07 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (200406ms till timeout)
2022-03-28 19:06:07 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (315320ms till timeout)
2022-03-28 19:06:07 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (204198ms till timeout)
2022-03-28 19:06:07 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-afe273c9-connect-795c55f45c-jjrk7 not ready: my-cluster-afe273c9-connect)
2022-03-28 19:06:07 [ForkJoinPool-1-worker-25] DEBUG [PodUtils:106] Pods my-cluster-afe273c9-connect-795c55f45c-jjrk7 are ready
2022-03-28 19:06:07 [ForkJoinPool-1-worker-25] INFO  [DeploymentUtils:197] Deployment my-cluster-afe273c9-connect is ready
2022-03-28 19:06:07 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:155] Create/Update KafkaConnector my-cluster-afe273c9 in namespace namespace-9
2022-03-28 19:06:07 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:164] Using Namespace: namespace-7
2022-03-28 19:06:07 [ForkJoinPool-1-worker-25] WARN  [VersionUsageUtils:60] The client is using resource type 'kafkaconnectors' with unstable version 'v1beta2'
2022-03-28 19:06:07 [ForkJoinPool-1-worker-25] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaConnector:my-cluster-afe273c9
2022-03-28 19:06:08 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:433] Wait for KafkaConnector: my-cluster-afe273c9 will have desired state: Ready
2022-03-28 19:06:08 [ForkJoinPool-1-worker-25] DEBUG [TestUtils:125] Waiting for KafkaConnector: my-cluster-afe273c9 will have desired state: Ready
2022-03-28 19:06:08 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] KafkaConnector: my-cluster-afe273c9 will have desired state: Ready not ready, will try again in 1000 ms (239905ms till timeout)
2022-03-28 19:06:08 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (16942ms till timeout)
2022-03-28 19:06:08 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:06:08 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (199211ms till timeout)
2022-03-28 19:06:08 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (314148ms till timeout)
2022-03-28 19:06:08 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (203097ms till timeout)
2022-03-28 19:06:09 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (15942ms till timeout)
2022-03-28 19:06:09 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:444] KafkaConnector: my-cluster-afe273c9 is in desired state: Ready
2022-03-28 19:06:09 [ForkJoinPool-1-worker-25] TRACE [Exec:248] Running command - oc --namespace namespace-7 exec my-cluster-afe273c9-connect-795c55f45c-jjrk7 -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-afe273c9
2022-03-28 19:06:09 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:06:09 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (313052ms till timeout)
2022-03-28 19:06:09 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (197925ms till timeout)
2022-03-28 19:06:10 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (201963ms till timeout)
2022-03-28 19:06:10 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (14941ms till timeout)
2022-03-28 19:06:10 [ForkJoinPool-1-worker-25] INFO  [Exec:417] Command: oc --namespace namespace-7 exec my-cluster-afe273c9-connect-795c55f45c-jjrk7 -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-afe273c9
2022-03-28 19:06:10 [ForkJoinPool-1-worker-25] INFO  [Exec:417] Return code: 0
2022-03-28 19:06:10 [ForkJoinPool-1-worker-25] INFO  [ReconciliationST:118] Adding pause annotation into the KafkaConnector and scaling taskMax to 4
2022-03-28 19:06:10 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (311906ms till timeout)
2022-03-28 19:06:11 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:06:11 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:433] Wait for KafkaConnector: my-cluster-afe273c9 will have desired state: ReconciliationPaused
2022-03-28 19:06:11 [ForkJoinPool-1-worker-25] DEBUG [TestUtils:125] Waiting for KafkaConnector: my-cluster-afe273c9 will have desired state: ReconciliationPaused
2022-03-28 19:06:11 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (200819ms till timeout)
2022-03-28 19:06:11 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (196591ms till timeout)
2022-03-28 19:06:11 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:444] KafkaConnector: my-cluster-afe273c9 is in desired state: ReconciliationPaused
2022-03-28 19:06:11 [ForkJoinPool-1-worker-25] DEBUG [TestUtils:125] Waiting for Connector's spec will be stable
2022-03-28 19:06:11 [ForkJoinPool-1-worker-25] TRACE [Exec:248] Running command - oc --namespace namespace-7 exec my-cluster-afe273c9-connect-795c55f45c-jjrk7 -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-afe273c9
2022-03-28 19:06:11 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (13941ms till timeout)
2022-03-28 19:06:12 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (310809ms till timeout)
2022-03-28 19:06:12 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (199718ms till timeout)
2022-03-28 19:06:12 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (12940ms till timeout)
2022-03-28 19:06:12 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:06:12 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (195392ms till timeout)
2022-03-28 19:06:12 [ForkJoinPool-1-worker-25] INFO  [Exec:417] Command: oc --namespace namespace-7 exec my-cluster-afe273c9-connect-795c55f45c-jjrk7 -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-afe273c9
2022-03-28 19:06:12 [ForkJoinPool-1-worker-25] INFO  [Exec:417] Return code: 0
2022-03-28 19:06:12 [ForkJoinPool-1-worker-25] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 19 polls
2022-03-28 19:06:12 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (178656ms till timeout)
2022-03-28 19:06:13 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (309713ms till timeout)
2022-03-28 19:06:13 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (11940ms till timeout)
2022-03-28 19:06:13 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (198619ms till timeout)
2022-03-28 19:06:13 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:06:13 [ForkJoinPool-1-worker-25] TRACE [Exec:248] Running command - oc --namespace namespace-7 exec my-cluster-afe273c9-connect-795c55f45c-jjrk7 -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-afe273c9
2022-03-28 19:06:13 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (194197ms till timeout)
2022-03-28 19:06:14 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (308616ms till timeout)
2022-03-28 19:06:14 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (10939ms till timeout)
2022-03-28 19:06:14 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (197520ms till timeout)
2022-03-28 19:06:14 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:06:14 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (193003ms till timeout)
2022-03-28 19:06:15 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (9939ms till timeout)
2022-03-28 19:06:15 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (307518ms till timeout)
2022-03-28 19:06:15 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (196419ms till timeout)
2022-03-28 19:06:15 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:06:16 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (191808ms till timeout)
2022-03-28 19:06:16 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (8938ms till timeout)
2022-03-28 19:06:16 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (306421ms till timeout)
2022-03-28 19:06:16 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (195319ms till timeout)
2022-03-28 19:06:17 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:06:17 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (190614ms till timeout)
2022-03-28 19:06:17 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (7938ms till timeout)
2022-03-28 19:06:17 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (305324ms till timeout)
2022-03-28 19:06:17 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (194217ms till timeout)
2022-03-28 19:06:18 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (6937ms till timeout)
2022-03-28 19:06:18 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:06:18 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (189420ms till timeout)
2022-03-28 19:06:18 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (304226ms till timeout)
2022-03-28 19:06:18 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (193117ms till timeout)
2022-03-28 19:06:19 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (5936ms till timeout)
2022-03-28 19:06:19 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:06:19 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (188228ms till timeout)
2022-03-28 19:06:19 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (303129ms till timeout)
2022-03-28 19:06:19 [ForkJoinPool-1-worker-25] INFO  [Exec:417] Command: oc --namespace namespace-7 exec my-cluster-afe273c9-connect-795c55f45c-jjrk7 -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-afe273c9
2022-03-28 19:06:19 [ForkJoinPool-1-worker-25] INFO  [Exec:417] Return code: 0
2022-03-28 19:06:19 [ForkJoinPool-1-worker-25] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 18 polls
2022-03-28 19:06:19 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (171363ms till timeout)
2022-03-28 19:06:19 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (192015ms till timeout)
2022-03-28 19:06:20 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (4935ms till timeout)
2022-03-28 19:06:20 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:06:20 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (187033ms till timeout)
2022-03-28 19:06:20 [ForkJoinPool-1-worker-25] TRACE [Exec:248] Running command - oc --namespace namespace-7 exec my-cluster-afe273c9-connect-795c55f45c-jjrk7 -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-afe273c9
2022-03-28 19:06:20 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (301969ms till timeout)
2022-03-28 19:06:21 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (190916ms till timeout)
2022-03-28 19:06:21 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (3935ms till timeout)
2022-03-28 19:06:21 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:06:21 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (300872ms till timeout)
2022-03-28 19:06:22 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (185745ms till timeout)
2022-03-28 19:06:22 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (189782ms till timeout)
2022-03-28 19:06:22 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (2934ms till timeout)
2022-03-28 19:06:23 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (299775ms till timeout)
2022-03-28 19:06:23 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:06:23 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (188678ms till timeout)
2022-03-28 19:06:23 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (1933ms till timeout)
2022-03-28 19:06:23 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (184447ms till timeout)
2022-03-28 19:06:24 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (298677ms till timeout)
2022-03-28 19:06:24 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties not ready, will try again in 933 ms (933ms till timeout)
2022-03-28 19:06:24 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (187578ms till timeout)
2022-03-28 19:06:24 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:06:24 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (183253ms till timeout)
2022-03-28 19:06:25 [ForkJoinPool-1-worker-15] ERROR [TestUtils:162] Exception waiting for Verify that kafka configuration {cluster-name=my-cluster-5dfd606f} has correct cruise control metric reporter properties, null
2022-03-28 19:06:25 [ForkJoinPool-1-worker-15] INFO  [CruiseControlConfigurationST:126] Cruise Control topics will not be deleted and will stay in the Kafka cluster
2022-03-28 19:06:25 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Verify that kafka contains cruise control topics with related configuration.
2022-03-28 19:06:25 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (297579ms till timeout)
2022-03-28 19:06:25 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Verify that kafka contains cruise control topics with related configuration.
2022-03-28 19:06:25 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (186360ms till timeout)
2022-03-28 19:06:25 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:06:25 [ForkJoinPool-1-worker-15] INFO  [CruiseControlConfigurationST:130] Adding Cruise Control to the classic Kafka.
2022-03-28 19:06:25 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (181872ms till timeout)
2022-03-28 19:06:26 [ForkJoinPool-1-worker-15] INFO  [RollingUpdateUtils:73] Waiting for component with name: my-cluster-5dfd606f-kafka rolling update
2022-03-28 19:06:26 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:74] Waiting for rolling update of component matching LabelSelector: LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-5dfd606f-kafka, strimzi.io/cluster=my-cluster-5dfd606f, strimzi.io/kind=Kafka}, additionalProperties={})
2022-03-28 19:06:26 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for component with name my-cluster-5dfd606f-kafka rolling update
2022-03-28 19:06:26 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-5dfd606f-kafka-0=d3be017c-9e19-4ee7-b0fc-58e8694c30b9, my-cluster-5dfd606f-kafka-1=1187ed10-515a-43c0-be8e-d43991c9d25d, my-cluster-5dfd606f-kafka-2=e2c632b2-9aec-4be9-931a-1dd36bf905c0}
2022-03-28 19:06:26 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-5dfd606f-kafka-0=d3be017c-9e19-4ee7-b0fc-58e8694c30b9, my-cluster-5dfd606f-kafka-1=1187ed10-515a-43c0-be8e-d43991c9d25d, my-cluster-5dfd606f-kafka-2=e2c632b2-9aec-4be9-931a-1dd36bf905c0}
2022-03-28 19:06:26 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-5dfd606f-kafka-0=d3be017c-9e19-4ee7-b0fc-58e8694c30b9, my-cluster-5dfd606f-kafka-1=1187ed10-515a-43c0-be8e-d43991c9d25d, my-cluster-5dfd606f-kafka-2=e2c632b2-9aec-4be9-931a-1dd36bf905c0}
2022-03-28 19:06:26 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:50] At least my-cluster-5dfd606f-kafka-1 hasn't rolled
2022-03-28 19:06:26 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] component with name my-cluster-5dfd606f-kafka rolling update not ready, will try again in 5000 ms (1799810ms till timeout)
2022-03-28 19:06:26 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (296457ms till timeout)
2022-03-28 19:06:26 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (185250ms till timeout)
2022-03-28 19:06:27 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:06:27 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (180678ms till timeout)
2022-03-28 19:06:27 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (295359ms till timeout)
2022-03-28 19:06:27 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (184149ms till timeout)
2022-03-28 19:06:28 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:06:28 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (179485ms till timeout)
2022-03-28 19:06:28 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (294262ms till timeout)
2022-03-28 19:06:28 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (183050ms till timeout)
2022-03-28 19:06:29 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:06:29 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (178292ms till timeout)
2022-03-28 19:06:29 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (293165ms till timeout)
2022-03-28 19:06:30 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (181949ms till timeout)
2022-03-28 19:06:30 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:06:30 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (177097ms till timeout)
2022-03-28 19:06:30 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (292034ms till timeout)
2022-03-28 19:06:31 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (180848ms till timeout)
2022-03-28 19:06:31 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-5dfd606f-kafka-0=d3be017c-9e19-4ee7-b0fc-58e8694c30b9, my-cluster-5dfd606f-kafka-1=1187ed10-515a-43c0-be8e-d43991c9d25d, my-cluster-5dfd606f-kafka-2=e2c632b2-9aec-4be9-931a-1dd36bf905c0}
2022-03-28 19:06:31 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-5dfd606f-kafka-0=d3be017c-9e19-4ee7-b0fc-58e8694c30b9, my-cluster-5dfd606f-kafka-1=1187ed10-515a-43c0-be8e-d43991c9d25d, my-cluster-5dfd606f-kafka-2=e2c632b2-9aec-4be9-931a-1dd36bf905c0}
2022-03-28 19:06:31 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-5dfd606f-kafka-0=d3be017c-9e19-4ee7-b0fc-58e8694c30b9, my-cluster-5dfd606f-kafka-1=1187ed10-515a-43c0-be8e-d43991c9d25d, my-cluster-5dfd606f-kafka-2=e2c632b2-9aec-4be9-931a-1dd36bf905c0}
2022-03-28 19:06:31 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:50] At least my-cluster-5dfd606f-kafka-1 hasn't rolled
2022-03-28 19:06:31 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] component with name my-cluster-5dfd606f-kafka rolling update not ready, will try again in 5000 ms (1794619ms till timeout)
2022-03-28 19:06:31 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:06:31 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (290938ms till timeout)
2022-03-28 19:06:32 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (175811ms till timeout)
2022-03-28 19:06:32 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (179749ms till timeout)
2022-03-28 19:06:32 [ForkJoinPool-1-worker-25] INFO  [Exec:417] Command: oc --namespace namespace-7 exec my-cluster-afe273c9-connect-795c55f45c-jjrk7 -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-afe273c9
2022-03-28 19:06:32 [ForkJoinPool-1-worker-25] INFO  [Exec:417] Return code: 0
2022-03-28 19:06:32 [ForkJoinPool-1-worker-25] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 17 polls
2022-03-28 19:06:32 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (159010ms till timeout)
2022-03-28 19:06:33 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (289840ms till timeout)
2022-03-28 19:06:33 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:06:33 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (174616ms till timeout)
2022-03-28 19:06:33 [ForkJoinPool-1-worker-25] TRACE [Exec:248] Running command - oc --namespace namespace-7 exec my-cluster-afe273c9-connect-795c55f45c-jjrk7 -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-afe273c9
2022-03-28 19:06:33 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (178648ms till timeout)
2022-03-28 19:06:34 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (288742ms till timeout)
2022-03-28 19:06:34 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:06:34 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (173423ms till timeout)
2022-03-28 19:06:34 [ForkJoinPool-1-worker-25] INFO  [Exec:417] Command: oc --namespace namespace-7 exec my-cluster-afe273c9-connect-795c55f45c-jjrk7 -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-afe273c9
2022-03-28 19:06:34 [ForkJoinPool-1-worker-25] INFO  [Exec:417] Return code: 0
2022-03-28 19:06:34 [ForkJoinPool-1-worker-25] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 16 polls
2022-03-28 19:06:34 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (156803ms till timeout)
2022-03-28 19:06:34 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (177461ms till timeout)
2022-03-28 19:06:35 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (287644ms till timeout)
2022-03-28 19:06:35 [ForkJoinPool-1-worker-25] TRACE [Exec:248] Running command - oc --namespace namespace-7 exec my-cluster-afe273c9-connect-795c55f45c-jjrk7 -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-afe273c9
2022-03-28 19:06:35 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:06:35 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (172231ms till timeout)
2022-03-28 19:06:35 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (176269ms till timeout)
2022-03-28 19:06:36 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (286546ms till timeout)
2022-03-28 19:06:36 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-5dfd606f-kafka-0=d3be017c-9e19-4ee7-b0fc-58e8694c30b9, my-cluster-5dfd606f-kafka-1=1187ed10-515a-43c0-be8e-d43991c9d25d, my-cluster-5dfd606f-kafka-2=e2c632b2-9aec-4be9-931a-1dd36bf905c0}
2022-03-28 19:06:36 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-5dfd606f-kafka-0=627b83da-e657-4469-b634-fe4ac182abcc, my-cluster-5dfd606f-kafka-1=1187ed10-515a-43c0-be8e-d43991c9d25d, my-cluster-5dfd606f-kafka-2=e2c632b2-9aec-4be9-931a-1dd36bf905c0}
2022-03-28 19:06:36 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-5dfd606f-kafka-0=627b83da-e657-4469-b634-fe4ac182abcc, my-cluster-5dfd606f-kafka-1=1187ed10-515a-43c0-be8e-d43991c9d25d, my-cluster-5dfd606f-kafka-2=e2c632b2-9aec-4be9-931a-1dd36bf905c0}
2022-03-28 19:06:36 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:50] At least my-cluster-5dfd606f-kafka-1 hasn't rolled
2022-03-28 19:06:36 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] component with name my-cluster-5dfd606f-kafka rolling update not ready, will try again in 5000 ms (1789421ms till timeout)
2022-03-28 19:06:36 [ForkJoinPool-1-worker-25] INFO  [Exec:417] Command: oc --namespace namespace-7 exec my-cluster-afe273c9-connect-795c55f45c-jjrk7 -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-afe273c9
2022-03-28 19:06:36 [ForkJoinPool-1-worker-25] INFO  [Exec:417] Return code: 0
2022-03-28 19:06:36 [ForkJoinPool-1-worker-25] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 15 polls
2022-03-28 19:06:36 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (154552ms till timeout)
2022-03-28 19:06:36 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:06:36 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (175169ms till timeout)
2022-03-28 19:06:36 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (170938ms till timeout)
2022-03-28 19:06:37 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (285448ms till timeout)
2022-03-28 19:06:37 [ForkJoinPool-1-worker-25] TRACE [Exec:248] Running command - oc --namespace namespace-7 exec my-cluster-afe273c9-connect-795c55f45c-jjrk7 -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-afe273c9
2022-03-28 19:06:37 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (174069ms till timeout)
2022-03-28 19:06:37 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:06:38 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (169742ms till timeout)
2022-03-28 19:06:38 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (284351ms till timeout)
2022-03-28 19:06:38 [ForkJoinPool-1-worker-25] INFO  [Exec:417] Command: oc --namespace namespace-7 exec my-cluster-afe273c9-connect-795c55f45c-jjrk7 -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-afe273c9
2022-03-28 19:06:38 [ForkJoinPool-1-worker-25] INFO  [Exec:417] Return code: 0
2022-03-28 19:06:38 [ForkJoinPool-1-worker-25] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 14 polls
2022-03-28 19:06:38 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (152289ms till timeout)
2022-03-28 19:06:38 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (172968ms till timeout)
2022-03-28 19:06:39 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:06:39 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (168550ms till timeout)
2022-03-28 19:06:39 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (283254ms till timeout)
2022-03-28 19:06:39 [ForkJoinPool-1-worker-25] TRACE [Exec:248] Running command - oc --namespace namespace-7 exec my-cluster-afe273c9-connect-795c55f45c-jjrk7 -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-afe273c9
2022-03-28 19:06:40 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (171861ms till timeout)
2022-03-28 19:06:40 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:06:40 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (167356ms till timeout)
2022-03-28 19:06:40 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (282157ms till timeout)
2022-03-28 19:06:41 [ForkJoinPool-1-worker-25] INFO  [Exec:417] Command: oc --namespace namespace-7 exec my-cluster-afe273c9-connect-795c55f45c-jjrk7 -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-afe273c9
2022-03-28 19:06:41 [ForkJoinPool-1-worker-25] INFO  [Exec:417] Return code: 0
2022-03-28 19:06:41 [ForkJoinPool-1-worker-25] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 13 polls
2022-03-28 19:06:41 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (150096ms till timeout)
2022-03-28 19:06:41 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (170762ms till timeout)
2022-03-28 19:06:41 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:06:41 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-5dfd606f-kafka-0=d3be017c-9e19-4ee7-b0fc-58e8694c30b9, my-cluster-5dfd606f-kafka-1=1187ed10-515a-43c0-be8e-d43991c9d25d, my-cluster-5dfd606f-kafka-2=e2c632b2-9aec-4be9-931a-1dd36bf905c0}
2022-03-28 19:06:41 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (166162ms till timeout)
2022-03-28 19:06:41 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-5dfd606f-kafka-0=627b83da-e657-4469-b634-fe4ac182abcc, my-cluster-5dfd606f-kafka-1=1187ed10-515a-43c0-be8e-d43991c9d25d, my-cluster-5dfd606f-kafka-2=e2c632b2-9aec-4be9-931a-1dd36bf905c0}
2022-03-28 19:06:41 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-5dfd606f-kafka-0=627b83da-e657-4469-b634-fe4ac182abcc, my-cluster-5dfd606f-kafka-1=1187ed10-515a-43c0-be8e-d43991c9d25d, my-cluster-5dfd606f-kafka-2=e2c632b2-9aec-4be9-931a-1dd36bf905c0}
2022-03-28 19:06:41 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:50] At least my-cluster-5dfd606f-kafka-1 hasn't rolled
2022-03-28 19:06:41 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] component with name my-cluster-5dfd606f-kafka rolling update not ready, will try again in 5000 ms (1784204ms till timeout)
2022-03-28 19:06:41 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (280998ms till timeout)
2022-03-28 19:06:42 [ForkJoinPool-1-worker-25] TRACE [Exec:248] Running command - oc --namespace namespace-7 exec my-cluster-afe273c9-connect-795c55f45c-jjrk7 -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-afe273c9
2022-03-28 19:06:42 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (169662ms till timeout)
2022-03-28 19:06:42 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:06:42 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (164968ms till timeout)
2022-03-28 19:06:42 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (279902ms till timeout)
2022-03-28 19:06:43 [ForkJoinPool-1-worker-25] INFO  [Exec:417] Command: oc --namespace namespace-7 exec my-cluster-afe273c9-connect-795c55f45c-jjrk7 -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-afe273c9
2022-03-28 19:06:43 [ForkJoinPool-1-worker-25] INFO  [Exec:417] Return code: 0
2022-03-28 19:06:43 [ForkJoinPool-1-worker-25] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 12 polls
2022-03-28 19:06:43 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (147873ms till timeout)
2022-03-28 19:06:43 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (168563ms till timeout)
2022-03-28 19:06:43 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:06:44 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (278806ms till timeout)
2022-03-28 19:06:44 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (163679ms till timeout)
2022-03-28 19:06:44 [ForkJoinPool-1-worker-25] TRACE [Exec:248] Running command - oc --namespace namespace-7 exec my-cluster-afe273c9-connect-795c55f45c-jjrk7 -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-afe273c9
2022-03-28 19:06:44 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (167463ms till timeout)
2022-03-28 19:06:45 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (277709ms till timeout)
2022-03-28 19:06:45 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:06:45 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (162485ms till timeout)
2022-03-28 19:06:45 [ForkJoinPool-1-worker-25] INFO  [Exec:417] Command: oc --namespace namespace-7 exec my-cluster-afe273c9-connect-795c55f45c-jjrk7 -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-afe273c9
2022-03-28 19:06:45 [ForkJoinPool-1-worker-25] INFO  [Exec:417] Return code: 0
2022-03-28 19:06:45 [ForkJoinPool-1-worker-25] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 11 polls
2022-03-28 19:06:45 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (145671ms till timeout)
2022-03-28 19:06:45 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (166363ms till timeout)
2022-03-28 19:06:46 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (276609ms till timeout)
2022-03-28 19:06:46 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:06:46 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (161290ms till timeout)
2022-03-28 19:06:46 [ForkJoinPool-1-worker-25] TRACE [Exec:248] Running command - oc --namespace namespace-7 exec my-cluster-afe273c9-connect-795c55f45c-jjrk7 -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-afe273c9
2022-03-28 19:06:46 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (165263ms till timeout)
2022-03-28 19:06:46 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-5dfd606f-kafka-0=d3be017c-9e19-4ee7-b0fc-58e8694c30b9, my-cluster-5dfd606f-kafka-1=1187ed10-515a-43c0-be8e-d43991c9d25d, my-cluster-5dfd606f-kafka-2=e2c632b2-9aec-4be9-931a-1dd36bf905c0}
2022-03-28 19:06:47 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-5dfd606f-kafka-0=627b83da-e657-4469-b634-fe4ac182abcc, my-cluster-5dfd606f-kafka-1=1187ed10-515a-43c0-be8e-d43991c9d25d, my-cluster-5dfd606f-kafka-2=e2c632b2-9aec-4be9-931a-1dd36bf905c0}
2022-03-28 19:06:47 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-5dfd606f-kafka-0=627b83da-e657-4469-b634-fe4ac182abcc, my-cluster-5dfd606f-kafka-1=1187ed10-515a-43c0-be8e-d43991c9d25d, my-cluster-5dfd606f-kafka-2=e2c632b2-9aec-4be9-931a-1dd36bf905c0}
2022-03-28 19:06:47 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:50] At least my-cluster-5dfd606f-kafka-1 hasn't rolled
2022-03-28 19:06:47 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] component with name my-cluster-5dfd606f-kafka rolling update not ready, will try again in 5000 ms (1779013ms till timeout)
2022-03-28 19:06:47 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (275512ms till timeout)
2022-03-28 19:06:47 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:06:47 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (160095ms till timeout)
2022-03-28 19:06:47 [ForkJoinPool-1-worker-25] INFO  [Exec:417] Command: oc --namespace namespace-7 exec my-cluster-afe273c9-connect-795c55f45c-jjrk7 -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-afe273c9
2022-03-28 19:06:47 [ForkJoinPool-1-worker-25] INFO  [Exec:417] Return code: 0
2022-03-28 19:06:47 [ForkJoinPool-1-worker-25] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 10 polls
2022-03-28 19:06:47 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (143456ms till timeout)
2022-03-28 19:06:47 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (164132ms till timeout)
2022-03-28 19:06:48 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (274415ms till timeout)
2022-03-28 19:06:48 [ForkJoinPool-1-worker-25] TRACE [Exec:248] Running command - oc --namespace namespace-7 exec my-cluster-afe273c9-connect-795c55f45c-jjrk7 -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-afe273c9
2022-03-28 19:06:48 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:06:48 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (158902ms till timeout)
2022-03-28 19:06:49 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (162941ms till timeout)
2022-03-28 19:06:49 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (273318ms till timeout)
2022-03-28 19:06:49 [ForkJoinPool-1-worker-25] INFO  [Exec:417] Command: oc --namespace namespace-7 exec my-cluster-afe273c9-connect-795c55f45c-jjrk7 -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-afe273c9
2022-03-28 19:06:49 [ForkJoinPool-1-worker-25] INFO  [Exec:417] Return code: 0
2022-03-28 19:06:49 [ForkJoinPool-1-worker-25] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 9 polls
2022-03-28 19:06:49 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (141271ms till timeout)
2022-03-28 19:06:50 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:06:50 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (157710ms till timeout)
2022-03-28 19:06:50 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (161748ms till timeout)
2022-03-28 19:06:50 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (272212ms till timeout)
2022-03-28 19:06:50 [ForkJoinPool-1-worker-25] TRACE [Exec:248] Running command - oc --namespace namespace-7 exec my-cluster-afe273c9-connect-795c55f45c-jjrk7 -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-afe273c9
2022-03-28 19:06:51 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:06:51 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (156517ms till timeout)
2022-03-28 19:06:51 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (160551ms till timeout)
2022-03-28 19:06:51 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (271115ms till timeout)
2022-03-28 19:06:52 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-5dfd606f-kafka-0=d3be017c-9e19-4ee7-b0fc-58e8694c30b9, my-cluster-5dfd606f-kafka-1=1187ed10-515a-43c0-be8e-d43991c9d25d, my-cluster-5dfd606f-kafka-2=e2c632b2-9aec-4be9-931a-1dd36bf905c0}
2022-03-28 19:06:52 [ForkJoinPool-1-worker-25] INFO  [Exec:417] Command: oc --namespace namespace-7 exec my-cluster-afe273c9-connect-795c55f45c-jjrk7 -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-afe273c9
2022-03-28 19:06:52 [ForkJoinPool-1-worker-25] INFO  [Exec:417] Return code: 0
2022-03-28 19:06:52 [ForkJoinPool-1-worker-25] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 8 polls
2022-03-28 19:06:52 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (139012ms till timeout)
2022-03-28 19:06:52 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-5dfd606f-kafka-0=627b83da-e657-4469-b634-fe4ac182abcc, my-cluster-5dfd606f-kafka-1=1187ed10-515a-43c0-be8e-d43991c9d25d, my-cluster-5dfd606f-kafka-2=e2c632b2-9aec-4be9-931a-1dd36bf905c0}
2022-03-28 19:06:52 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-5dfd606f-kafka-0=627b83da-e657-4469-b634-fe4ac182abcc, my-cluster-5dfd606f-kafka-1=1187ed10-515a-43c0-be8e-d43991c9d25d, my-cluster-5dfd606f-kafka-2=e2c632b2-9aec-4be9-931a-1dd36bf905c0}
2022-03-28 19:06:52 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:50] At least my-cluster-5dfd606f-kafka-1 hasn't rolled
2022-03-28 19:06:52 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] component with name my-cluster-5dfd606f-kafka rolling update not ready, will try again in 5000 ms (1773820ms till timeout)
2022-03-28 19:06:52 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:06:52 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (155311ms till timeout)
2022-03-28 19:06:52 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (159349ms till timeout)
2022-03-28 19:06:52 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (270018ms till timeout)
2022-03-28 19:06:53 [ForkJoinPool-1-worker-25] TRACE [Exec:248] Running command - oc --namespace namespace-7 exec my-cluster-afe273c9-connect-795c55f45c-jjrk7 -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-afe273c9
2022-03-28 19:06:53 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:06:53 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (154118ms till timeout)
2022-03-28 19:06:53 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (158155ms till timeout)
2022-03-28 19:06:53 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (268922ms till timeout)
2022-03-28 19:06:54 [ForkJoinPool-1-worker-25] INFO  [Exec:417] Command: oc --namespace namespace-7 exec my-cluster-afe273c9-connect-795c55f45c-jjrk7 -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-afe273c9
2022-03-28 19:06:54 [ForkJoinPool-1-worker-25] INFO  [Exec:417] Return code: 0
2022-03-28 19:06:54 [ForkJoinPool-1-worker-25] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 7 polls
2022-03-28 19:06:54 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (136781ms till timeout)
2022-03-28 19:06:54 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:06:54 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (152924ms till timeout)
2022-03-28 19:06:55 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (156928ms till timeout)
2022-03-28 19:06:55 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (267759ms till timeout)
2022-03-28 19:06:55 [ForkJoinPool-1-worker-25] TRACE [Exec:248] Running command - oc --namespace namespace-7 exec my-cluster-afe273c9-connect-795c55f45c-jjrk7 -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-afe273c9
2022-03-28 19:06:56 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:06:56 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (151729ms till timeout)
2022-03-28 19:06:56 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (155768ms till timeout)
2022-03-28 19:06:56 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (266569ms till timeout)
2022-03-28 19:06:57 [ForkJoinPool-1-worker-25] INFO  [Exec:417] Command: oc --namespace namespace-7 exec my-cluster-afe273c9-connect-795c55f45c-jjrk7 -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-afe273c9
2022-03-28 19:06:57 [ForkJoinPool-1-worker-25] INFO  [Exec:417] Return code: 0
2022-03-28 19:06:57 [ForkJoinPool-1-worker-25] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 6 polls
2022-03-28 19:06:57 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (134209ms till timeout)
2022-03-28 19:06:57 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:06:57 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-5dfd606f-kafka-0=d3be017c-9e19-4ee7-b0fc-58e8694c30b9, my-cluster-5dfd606f-kafka-1=1187ed10-515a-43c0-be8e-d43991c9d25d, my-cluster-5dfd606f-kafka-2=e2c632b2-9aec-4be9-931a-1dd36bf905c0}
2022-03-28 19:06:57 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (154667ms till timeout)
2022-03-28 19:06:57 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (150437ms till timeout)
2022-03-28 19:06:57 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (265469ms till timeout)
2022-03-28 19:06:57 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-5dfd606f-kafka-0=627b83da-e657-4469-b634-fe4ac182abcc, my-cluster-5dfd606f-kafka-1=1187ed10-515a-43c0-be8e-d43991c9d25d, my-cluster-5dfd606f-kafka-2=e2c632b2-9aec-4be9-931a-1dd36bf905c0}
2022-03-28 19:06:57 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-5dfd606f-kafka-0=627b83da-e657-4469-b634-fe4ac182abcc, my-cluster-5dfd606f-kafka-1=1187ed10-515a-43c0-be8e-d43991c9d25d, my-cluster-5dfd606f-kafka-2=e2c632b2-9aec-4be9-931a-1dd36bf905c0}
2022-03-28 19:06:57 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:50] At least my-cluster-5dfd606f-kafka-1 hasn't rolled
2022-03-28 19:06:57 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] component with name my-cluster-5dfd606f-kafka rolling update not ready, will try again in 5000 ms (1768574ms till timeout)
2022-03-28 19:06:58 [ForkJoinPool-1-worker-25] TRACE [Exec:248] Running command - oc --namespace namespace-7 exec my-cluster-afe273c9-connect-795c55f45c-jjrk7 -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-afe273c9
2022-03-28 19:06:58 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (153568ms till timeout)
2022-03-28 19:06:58 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:06:58 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (264367ms till timeout)
2022-03-28 19:06:58 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (149239ms till timeout)
2022-03-28 19:06:59 [ForkJoinPool-1-worker-25] INFO  [Exec:417] Command: oc --namespace namespace-7 exec my-cluster-afe273c9-connect-795c55f45c-jjrk7 -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-afe273c9
2022-03-28 19:06:59 [ForkJoinPool-1-worker-25] INFO  [Exec:417] Return code: 0
2022-03-28 19:06:59 [ForkJoinPool-1-worker-25] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 5 polls
2022-03-28 19:06:59 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (132033ms till timeout)
2022-03-28 19:06:59 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (152468ms till timeout)
2022-03-28 19:06:59 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (263269ms till timeout)
2022-03-28 19:06:59 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:06:59 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (148046ms till timeout)
2022-03-28 19:07:00 [ForkJoinPool-1-worker-25] TRACE [Exec:248] Running command - oc --namespace namespace-7 exec my-cluster-afe273c9-connect-795c55f45c-jjrk7 -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-afe273c9
2022-03-28 19:07:00 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (151368ms till timeout)
2022-03-28 19:07:00 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (262170ms till timeout)
2022-03-28 19:07:00 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:07:00 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (146853ms till timeout)
2022-03-28 19:07:01 [ForkJoinPool-1-worker-25] INFO  [Exec:417] Command: oc --namespace namespace-7 exec my-cluster-afe273c9-connect-795c55f45c-jjrk7 -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-afe273c9
2022-03-28 19:07:01 [ForkJoinPool-1-worker-25] INFO  [Exec:417] Return code: 0
2022-03-28 19:07:01 [ForkJoinPool-1-worker-25] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 4 polls
2022-03-28 19:07:01 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (129817ms till timeout)
2022-03-28 19:07:01 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (150267ms till timeout)
2022-03-28 19:07:01 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (261068ms till timeout)
2022-03-28 19:07:02 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:07:02 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (145661ms till timeout)
2022-03-28 19:07:02 [ForkJoinPool-1-worker-25] TRACE [Exec:248] Running command - oc --namespace namespace-7 exec my-cluster-afe273c9-connect-795c55f45c-jjrk7 -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-afe273c9
2022-03-28 19:07:02 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-5dfd606f-kafka-0=d3be017c-9e19-4ee7-b0fc-58e8694c30b9, my-cluster-5dfd606f-kafka-1=1187ed10-515a-43c0-be8e-d43991c9d25d, my-cluster-5dfd606f-kafka-2=e2c632b2-9aec-4be9-931a-1dd36bf905c0}
2022-03-28 19:07:02 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-5dfd606f-kafka-0=627b83da-e657-4469-b634-fe4ac182abcc, my-cluster-5dfd606f-kafka-1=1187ed10-515a-43c0-be8e-d43991c9d25d, my-cluster-5dfd606f-kafka-2=908bb47e-9f95-441e-bd49-e4a01f6d902c}
2022-03-28 19:07:02 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-5dfd606f-kafka-0=627b83da-e657-4469-b634-fe4ac182abcc, my-cluster-5dfd606f-kafka-1=1187ed10-515a-43c0-be8e-d43991c9d25d, my-cluster-5dfd606f-kafka-2=908bb47e-9f95-441e-bd49-e4a01f6d902c}
2022-03-28 19:07:02 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:50] At least my-cluster-5dfd606f-kafka-1 hasn't rolled
2022-03-28 19:07:02 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] component with name my-cluster-5dfd606f-kafka rolling update not ready, will try again in 5000 ms (1763384ms till timeout)
2022-03-28 19:07:02 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (149166ms till timeout)
2022-03-28 19:07:02 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (259966ms till timeout)
2022-03-28 19:07:03 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:07:03 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (144469ms till timeout)
2022-03-28 19:07:03 [ForkJoinPool-1-worker-25] INFO  [Exec:417] Command: oc --namespace namespace-7 exec my-cluster-afe273c9-connect-795c55f45c-jjrk7 -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-afe273c9
2022-03-28 19:07:03 [ForkJoinPool-1-worker-25] INFO  [Exec:417] Return code: 0
2022-03-28 19:07:03 [ForkJoinPool-1-worker-25] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 3 polls
2022-03-28 19:07:03 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (127643ms till timeout)
2022-03-28 19:07:03 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (148067ms till timeout)
2022-03-28 19:07:03 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (258867ms till timeout)
2022-03-28 19:07:04 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:07:04 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (143277ms till timeout)
2022-03-28 19:07:04 [ForkJoinPool-1-worker-25] TRACE [Exec:248] Running command - oc --namespace namespace-7 exec my-cluster-afe273c9-connect-795c55f45c-jjrk7 -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-afe273c9
2022-03-28 19:07:04 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (146967ms till timeout)
2022-03-28 19:07:05 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (257769ms till timeout)
2022-03-28 19:07:05 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:07:05 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (142083ms till timeout)
2022-03-28 19:07:05 [ForkJoinPool-1-worker-25] INFO  [Exec:417] Command: oc --namespace namespace-7 exec my-cluster-afe273c9-connect-795c55f45c-jjrk7 -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-afe273c9
2022-03-28 19:07:05 [ForkJoinPool-1-worker-25] INFO  [Exec:417] Return code: 0
2022-03-28 19:07:05 [ForkJoinPool-1-worker-25] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 2 polls
2022-03-28 19:07:05 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (125481ms till timeout)
2022-03-28 19:07:06 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (145866ms till timeout)
2022-03-28 19:07:06 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (256668ms till timeout)
2022-03-28 19:07:06 [ForkJoinPool-1-worker-25] TRACE [Exec:248] Running command - oc --namespace namespace-7 exec my-cluster-afe273c9-connect-795c55f45c-jjrk7 -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-afe273c9
2022-03-28 19:07:06 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:07:06 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (140891ms till timeout)
2022-03-28 19:07:07 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (144766ms till timeout)
2022-03-28 19:07:07 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (255567ms till timeout)
2022-03-28 19:07:07 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-5dfd606f-kafka-0=d3be017c-9e19-4ee7-b0fc-58e8694c30b9, my-cluster-5dfd606f-kafka-1=1187ed10-515a-43c0-be8e-d43991c9d25d, my-cluster-5dfd606f-kafka-2=e2c632b2-9aec-4be9-931a-1dd36bf905c0}
2022-03-28 19:07:07 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-5dfd606f-kafka-0=627b83da-e657-4469-b634-fe4ac182abcc, my-cluster-5dfd606f-kafka-1=1187ed10-515a-43c0-be8e-d43991c9d25d, my-cluster-5dfd606f-kafka-2=908bb47e-9f95-441e-bd49-e4a01f6d902c}
2022-03-28 19:07:07 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-5dfd606f-kafka-0=627b83da-e657-4469-b634-fe4ac182abcc, my-cluster-5dfd606f-kafka-1=1187ed10-515a-43c0-be8e-d43991c9d25d, my-cluster-5dfd606f-kafka-2=908bb47e-9f95-441e-bd49-e4a01f6d902c}
2022-03-28 19:07:07 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:50] At least my-cluster-5dfd606f-kafka-1 hasn't rolled
2022-03-28 19:07:07 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] component with name my-cluster-5dfd606f-kafka rolling update not ready, will try again in 5000 ms (1758190ms till timeout)
2022-03-28 19:07:07 [ForkJoinPool-1-worker-25] INFO  [Exec:417] Command: oc --namespace namespace-7 exec my-cluster-afe273c9-connect-795c55f45c-jjrk7 -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-afe273c9
2022-03-28 19:07:07 [ForkJoinPool-1-worker-25] INFO  [Exec:417] Return code: 0
2022-03-28 19:07:07 [ForkJoinPool-1-worker-25] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 1 polls
2022-03-28 19:07:07 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (123283ms till timeout)
2022-03-28 19:07:08 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:07:08 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (139698ms till timeout)
2022-03-28 19:07:08 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (143664ms till timeout)
2022-03-28 19:07:08 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (254467ms till timeout)
2022-03-28 19:07:08 [ForkJoinPool-1-worker-25] TRACE [Exec:248] Running command - oc --namespace namespace-7 exec my-cluster-afe273c9-connect-795c55f45c-jjrk7 -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-afe273c9
2022-03-28 19:07:09 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:07:09 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (138505ms till timeout)
2022-03-28 19:07:09 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (142542ms till timeout)
2022-03-28 19:07:09 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (253343ms till timeout)
2022-03-28 19:07:10 [ForkJoinPool-1-worker-25] INFO  [Exec:417] Command: oc --namespace namespace-7 exec my-cluster-afe273c9-connect-795c55f45c-jjrk7 -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-afe273c9
2022-03-28 19:07:10 [ForkJoinPool-1-worker-25] INFO  [Exec:417] Return code: 0
2022-03-28 19:07:10 [ForkJoinPool-1-worker-25] INFO  [KafkaConnectorUtils:154] Connector's spec is stable for 20 polls intervals
2022-03-28 19:07:10 [ForkJoinPool-1-worker-25] INFO  [ReconciliationST:127] Setting annotation to "false", taskMax should be increased to 4
2022-03-28 19:07:10 [ForkJoinPool-1-worker-25] DEBUG [TestUtils:125] Waiting for Wait for KafkaConnector config will contain desired config
2022-03-28 19:07:10 [ForkJoinPool-1-worker-25] TRACE [Exec:248] Running command - oc --namespace namespace-7 exec my-cluster-afe273c9-connect-795c55f45c-jjrk7 -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-afe273c9/config
2022-03-28 19:07:10 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:07:10 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (141385ms till timeout)
2022-03-28 19:07:10 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (137155ms till timeout)
2022-03-28 19:07:10 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (252186ms till timeout)
2022-03-28 19:07:11 [ForkJoinPool-1-worker-25] INFO  [Exec:417] Command: oc --namespace namespace-7 exec my-cluster-afe273c9-connect-795c55f45c-jjrk7 -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-afe273c9/config
2022-03-28 19:07:11 [ForkJoinPool-1-worker-25] INFO  [Exec:417] Return code: 0
2022-03-28 19:07:11 [ForkJoinPool-1-worker-25] TRACE [Exec:248] Running command - oc --namespace namespace-7 exec my-cluster-afe273c9-connect-795c55f45c-jjrk7 -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-afe273c9/config
2022-03-28 19:07:11 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (140285ms till timeout)
2022-03-28 19:07:11 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:07:11 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (251084ms till timeout)
2022-03-28 19:07:11 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (135956ms till timeout)
2022-03-28 19:07:12 [ForkJoinPool-1-worker-25] INFO  [Exec:417] Command: oc --namespace namespace-7 exec my-cluster-afe273c9-connect-795c55f45c-jjrk7 -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-afe273c9/config
2022-03-28 19:07:12 [ForkJoinPool-1-worker-25] INFO  [Exec:417] Return code: 0
2022-03-28 19:07:12 [ForkJoinPool-1-worker-25] DEBUG [AbstractST:674] ============================================================================
2022-03-28 19:07:12 [ForkJoinPool-1-worker-25] DEBUG [AbstractST:675] [operators.ReconciliationST - After Each] - Clean up after test
2022-03-28 19:07:12 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:344] ############################################################################
2022-03-28 19:07:12 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:348] Delete all resources for testPauseReconciliationInKafkaAndKafkaConnectWithConnector
2022-03-28 19:07:12 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:241] Delete of NetworkPolicy my-cluster-afe273c9-allow in namespace namespace-7
2022-03-28 19:07:12 [ForkJoinPool-1-worker-29] INFO  [ResourceManager:241] Delete of Deployment my-cluster-afe273c9-kafka-clients in namespace namespace-7
2022-03-28 19:07:12 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:241] Delete of KafkaConnect my-cluster-afe273c9 in namespace namespace-7
2022-03-28 19:07:12 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:241] Delete of Deployment my-cluster-afe273c9-scraper in namespace namespace-7
2022-03-28 19:07:12 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:241] Delete of Kafka my-cluster-afe273c9 in namespace namespace-7
2022-03-28 19:07:12 [ForkJoinPool-1-worker-31] INFO  [ResourceManager:241] Delete of KafkaConnector my-cluster-afe273c9 in namespace namespace-7
2022-03-28 19:07:12 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (139185ms till timeout)
2022-03-28 19:07:12 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-5dfd606f-kafka-0=d3be017c-9e19-4ee7-b0fc-58e8694c30b9, my-cluster-5dfd606f-kafka-1=1187ed10-515a-43c0-be8e-d43991c9d25d, my-cluster-5dfd606f-kafka-2=e2c632b2-9aec-4be9-931a-1dd36bf905c0}
2022-03-28 19:07:12 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (249981ms till timeout)
2022-03-28 19:07:12 [ForkJoinPool-1-worker-31] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaConnector:my-cluster-afe273c9
2022-03-28 19:07:12 [ForkJoinPool-1-worker-25] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource NetworkPolicy:my-cluster-afe273c9-allow
2022-03-28 19:07:12 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:my-cluster-afe273c9-scraper
2022-03-28 19:07:12 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-afe273c9
2022-03-28 19:07:12 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaConnect:my-cluster-afe273c9
2022-03-28 19:07:12 [ForkJoinPool-1-worker-29] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:my-cluster-afe273c9-kafka-clients
2022-03-28 19:07:13 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-5dfd606f-kafka-0=627b83da-e657-4469-b634-fe4ac182abcc, my-cluster-5dfd606f-kafka-1=1187ed10-515a-43c0-be8e-d43991c9d25d, my-cluster-5dfd606f-kafka-2=908bb47e-9f95-441e-bd49-e4a01f6d902c}
2022-03-28 19:07:13 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-5dfd606f-kafka-0=627b83da-e657-4469-b634-fe4ac182abcc, my-cluster-5dfd606f-kafka-1=1187ed10-515a-43c0-be8e-d43991c9d25d, my-cluster-5dfd606f-kafka-2=908bb47e-9f95-441e-bd49-e4a01f6d902c}
2022-03-28 19:07:13 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:50] At least my-cluster-5dfd606f-kafka-1 hasn't rolled
2022-03-28 19:07:13 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] component with name my-cluster-5dfd606f-kafka rolling update not ready, will try again in 5000 ms (1752953ms till timeout)
2022-03-28 19:07:13 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster-afe273c9 not ready, will try again in 10000 ms (839736ms till timeout)
2022-03-28 19:07:13 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:07:13 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-afe273c9-scraper not ready, will try again in 10000 ms (479566ms till timeout)
2022-03-28 19:07:13 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (134304ms till timeout)
2022-03-28 19:07:13 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaConnect:my-cluster-afe273c9 not ready, will try again in 10000 ms (599441ms till timeout)
2022-03-28 19:07:13 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-afe273c9-kafka-clients not ready, will try again in 10000 ms (479328ms till timeout)
2022-03-28 19:07:13 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (138076ms till timeout)
2022-03-28 19:07:13 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (248876ms till timeout)
2022-03-28 19:07:14 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:07:14 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (133109ms till timeout)
2022-03-28 19:07:14 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (136976ms till timeout)
2022-03-28 19:07:15 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (247776ms till timeout)
2022-03-28 19:07:15 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:07:15 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (131915ms till timeout)
2022-03-28 19:07:16 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (135874ms till timeout)
2022-03-28 19:07:16 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (246673ms till timeout)
2022-03-28 19:07:17 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:07:17 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (130718ms till timeout)
2022-03-28 19:07:17 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (134753ms till timeout)
2022-03-28 19:07:17 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (245552ms till timeout)
2022-03-28 19:07:18 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-5dfd606f-kafka-0=d3be017c-9e19-4ee7-b0fc-58e8694c30b9, my-cluster-5dfd606f-kafka-1=1187ed10-515a-43c0-be8e-d43991c9d25d, my-cluster-5dfd606f-kafka-2=e2c632b2-9aec-4be9-931a-1dd36bf905c0}
2022-03-28 19:07:18 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-5dfd606f-kafka-0=627b83da-e657-4469-b634-fe4ac182abcc, my-cluster-5dfd606f-kafka-1=1187ed10-515a-43c0-be8e-d43991c9d25d, my-cluster-5dfd606f-kafka-2=908bb47e-9f95-441e-bd49-e4a01f6d902c}
2022-03-28 19:07:18 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-5dfd606f-kafka-0=627b83da-e657-4469-b634-fe4ac182abcc, my-cluster-5dfd606f-kafka-1=1187ed10-515a-43c0-be8e-d43991c9d25d, my-cluster-5dfd606f-kafka-2=908bb47e-9f95-441e-bd49-e4a01f6d902c}
2022-03-28 19:07:18 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:50] At least my-cluster-5dfd606f-kafka-1 hasn't rolled
2022-03-28 19:07:18 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] component with name my-cluster-5dfd606f-kafka rolling update not ready, will try again in 5000 ms (1747760ms till timeout)
2022-03-28 19:07:18 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:07:18 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (133569ms till timeout)
2022-03-28 19:07:18 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (129339ms till timeout)
2022-03-28 19:07:18 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (244371ms till timeout)
2022-03-28 19:07:19 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (132469ms till timeout)
2022-03-28 19:07:19 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (243270ms till timeout)
2022-03-28 19:07:19 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:07:19 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (128138ms till timeout)
2022-03-28 19:07:20 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (131368ms till timeout)
2022-03-28 19:07:20 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (242144ms till timeout)
2022-03-28 19:07:20 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:07:20 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (126920ms till timeout)
2022-03-28 19:07:21 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (130269ms till timeout)
2022-03-28 19:07:21 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (241043ms till timeout)
2022-03-28 19:07:22 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:07:22 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (125727ms till timeout)
2022-03-28 19:07:22 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (129169ms till timeout)
2022-03-28 19:07:22 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (239945ms till timeout)
2022-03-28 19:07:23 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:07:23 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-5dfd606f-kafka-0=d3be017c-9e19-4ee7-b0fc-58e8694c30b9, my-cluster-5dfd606f-kafka-1=1187ed10-515a-43c0-be8e-d43991c9d25d, my-cluster-5dfd606f-kafka-2=e2c632b2-9aec-4be9-931a-1dd36bf905c0}
2022-03-28 19:07:23 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (124439ms till timeout)
2022-03-28 19:07:23 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-5dfd606f-kafka-0=627b83da-e657-4469-b634-fe4ac182abcc, my-cluster-5dfd606f-kafka-1=1187ed10-515a-43c0-be8e-d43991c9d25d, my-cluster-5dfd606f-kafka-2=908bb47e-9f95-441e-bd49-e4a01f6d902c}
2022-03-28 19:07:23 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-5dfd606f-kafka-0=627b83da-e657-4469-b634-fe4ac182abcc, my-cluster-5dfd606f-kafka-1=1187ed10-515a-43c0-be8e-d43991c9d25d, my-cluster-5dfd606f-kafka-2=908bb47e-9f95-441e-bd49-e4a01f6d902c}
2022-03-28 19:07:23 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:50] At least my-cluster-5dfd606f-kafka-1 hasn't rolled
2022-03-28 19:07:23 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] component with name my-cluster-5dfd606f-kafka rolling update not ready, will try again in 5000 ms (1742482ms till timeout)
2022-03-28 19:07:23 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-afe273c9-scraper not ready, will try again in 10000 ms (469301ms till timeout)
2022-03-28 19:07:23 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (128002ms till timeout)
2022-03-28 19:07:24 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (238804ms till timeout)
2022-03-28 19:07:24 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-afe273c9-kafka-clients not ready, will try again in 10000 ms (468904ms till timeout)
2022-03-28 19:07:24 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:07:24 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (123246ms till timeout)
2022-03-28 19:07:25 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (126902ms till timeout)
2022-03-28 19:07:25 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (237702ms till timeout)
2022-03-28 19:07:25 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:07:25 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (122052ms till timeout)
2022-03-28 19:07:26 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (125801ms till timeout)
2022-03-28 19:07:26 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (236600ms till timeout)
2022-03-28 19:07:26 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=null, completedIndexes=null, completionTime=2022-03-28T19:07:21Z, conditions=[JobCondition(lastProbeTime=2022-03-28T19:07:21Z, lastTransitionTime=2022-03-28T19:07:21Z, message=null, reason=null, status=True, type=Complete, additionalProperties={})], failed=null, ready=null, startTime=2022-03-28T19:05:43Z, succeeded=1, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:07:27 [ForkJoinPool-1-worker-41] INFO  [PodUtils:186] Waiting for message will be in the log
2022-03-28 19:07:27 [ForkJoinPool-1-worker-41] DEBUG [TestUtils:125] Waiting for Waiting for message will be in the log
2022-03-28 19:07:27 [ForkJoinPool-1-worker-41] INFO  [PodUtils:189] Message All topics created found in create-admin-my-cluster-836b31f8-kafka-clients-jgng8 log
2022-03-28 19:07:27 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (124698ms till timeout)
2022-03-28 19:07:27 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (235499ms till timeout)
2022-03-28 19:07:27 [ForkJoinPool-1-worker-41] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment create-admin-my-cluster-836b31f8-kafka-clients deletion
2022-03-28 19:07:27 [ForkJoinPool-1-worker-41] DEBUG [TestUtils:125] Waiting for ReplicaSet create-admin-my-cluster-836b31f8-kafka-clients to be deleted
2022-03-28 19:07:27 [ForkJoinPool-1-worker-41] DEBUG [JobUtils:40] Job create-admin-my-cluster-836b31f8-kafka-clients was deleted
2022-03-28 19:07:27 [ForkJoinPool-1-worker-41] INFO  [ThrottlingQuotaST:112] Executing 5/5 iteration.
2022-03-28 19:07:27 [ForkJoinPool-1-worker-41] INFO  [ResourceManager:155] Create/Update Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st
2022-03-28 19:07:27 [ForkJoinPool-1-worker-41] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:create-admin-my-cluster-836b31f8-kafka-clients
2022-03-28 19:07:27 [ForkJoinPool-1-worker-41] INFO  [JobUtils:81] Waiting for job: create-admin-my-cluster-836b31f8-kafka-clients will be in active state
2022-03-28 19:07:27 [ForkJoinPool-1-worker-41] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 19:07:27 [ForkJoinPool-1-worker-41] INFO  [ClientUtils:76] Waiting for producer/consumer:create-admin-my-cluster-836b31f8-kafka-clients to finished
2022-03-28 19:07:27 [ForkJoinPool-1-worker-41] DEBUG [TestUtils:125] Waiting for job finished
2022-03-28 19:07:27 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:07:27 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (219808ms till timeout)
2022-03-28 19:07:28 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (123593ms till timeout)
2022-03-28 19:07:28 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (234392ms till timeout)
2022-03-28 19:07:28 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-5dfd606f-kafka-0=d3be017c-9e19-4ee7-b0fc-58e8694c30b9, my-cluster-5dfd606f-kafka-1=1187ed10-515a-43c0-be8e-d43991c9d25d, my-cluster-5dfd606f-kafka-2=e2c632b2-9aec-4be9-931a-1dd36bf905c0}
2022-03-28 19:07:28 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-5dfd606f-kafka-0=627b83da-e657-4469-b634-fe4ac182abcc, my-cluster-5dfd606f-kafka-1=1187ed10-515a-43c0-be8e-d43991c9d25d, my-cluster-5dfd606f-kafka-2=908bb47e-9f95-441e-bd49-e4a01f6d902c}
2022-03-28 19:07:28 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-5dfd606f-kafka-0=627b83da-e657-4469-b634-fe4ac182abcc, my-cluster-5dfd606f-kafka-1=1187ed10-515a-43c0-be8e-d43991c9d25d, my-cluster-5dfd606f-kafka-2=908bb47e-9f95-441e-bd49-e4a01f6d902c}
2022-03-28 19:07:28 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:50] At least my-cluster-5dfd606f-kafka-1 hasn't rolled
2022-03-28 19:07:28 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] component with name my-cluster-5dfd606f-kafka rolling update not ready, will try again in 5000 ms (1737291ms till timeout)
2022-03-28 19:07:29 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:07:29 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (218617ms till timeout)
2022-03-28 19:07:29 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (122493ms till timeout)
2022-03-28 19:07:29 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (233294ms till timeout)
2022-03-28 19:07:30 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:07:30 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (217423ms till timeout)
2022-03-28 19:07:30 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (121394ms till timeout)
2022-03-28 19:07:30 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (232195ms till timeout)
2022-03-28 19:07:31 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:07:31 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (216231ms till timeout)
2022-03-28 19:07:31 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (120294ms till timeout)
2022-03-28 19:07:31 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (231095ms till timeout)
2022-03-28 19:07:32 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:07:32 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (215038ms till timeout)
2022-03-28 19:07:32 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (119147ms till timeout)
2022-03-28 19:07:32 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (229942ms till timeout)
2022-03-28 19:07:33 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-5dfd606f-kafka-0=d3be017c-9e19-4ee7-b0fc-58e8694c30b9, my-cluster-5dfd606f-kafka-1=1187ed10-515a-43c0-be8e-d43991c9d25d, my-cluster-5dfd606f-kafka-2=e2c632b2-9aec-4be9-931a-1dd36bf905c0}
2022-03-28 19:07:33 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-afe273c9-scraper not ready, will try again in 10000 ms (459107ms till timeout)
2022-03-28 19:07:33 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:07:33 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-5dfd606f-kafka-0=627b83da-e657-4469-b634-fe4ac182abcc, my-cluster-5dfd606f-kafka-1=1187ed10-515a-43c0-be8e-d43991c9d25d, my-cluster-5dfd606f-kafka-2=908bb47e-9f95-441e-bd49-e4a01f6d902c}
2022-03-28 19:07:33 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-5dfd606f-kafka-0=627b83da-e657-4469-b634-fe4ac182abcc, my-cluster-5dfd606f-kafka-1=1187ed10-515a-43c0-be8e-d43991c9d25d, my-cluster-5dfd606f-kafka-2=908bb47e-9f95-441e-bd49-e4a01f6d902c}
2022-03-28 19:07:33 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:50] At least my-cluster-5dfd606f-kafka-1 hasn't rolled
2022-03-28 19:07:33 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] component with name my-cluster-5dfd606f-kafka rolling update not ready, will try again in 5000 ms (1732100ms till timeout)
2022-03-28 19:07:33 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (117999ms till timeout)
2022-03-28 19:07:34 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (213705ms till timeout)
2022-03-28 19:07:34 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (228807ms till timeout)
2022-03-28 19:07:34 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-afe273c9-kafka-clients not ready, will try again in 10000 ms (458613ms till timeout)
2022-03-28 19:07:35 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (116900ms till timeout)
2022-03-28 19:07:35 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:07:35 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (227700ms till timeout)
2022-03-28 19:07:35 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (212501ms till timeout)
2022-03-28 19:07:36 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (115800ms till timeout)
2022-03-28 19:07:36 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (226602ms till timeout)
2022-03-28 19:07:36 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:07:36 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (211307ms till timeout)
2022-03-28 19:07:37 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (114701ms till timeout)
2022-03-28 19:07:37 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (225500ms till timeout)
2022-03-28 19:07:37 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:07:37 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (210114ms till timeout)
2022-03-28 19:07:38 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (113600ms till timeout)
2022-03-28 19:07:38 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (224399ms till timeout)
2022-03-28 19:07:38 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:07:38 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (208923ms till timeout)
2022-03-28 19:07:38 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-5dfd606f-kafka-0=d3be017c-9e19-4ee7-b0fc-58e8694c30b9, my-cluster-5dfd606f-kafka-1=1187ed10-515a-43c0-be8e-d43991c9d25d, my-cluster-5dfd606f-kafka-2=e2c632b2-9aec-4be9-931a-1dd36bf905c0}
2022-03-28 19:07:39 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-5dfd606f-kafka-0=627b83da-e657-4469-b634-fe4ac182abcc, my-cluster-5dfd606f-kafka-1=3b5862df-223e-457c-bb27-6647e9ca37b9, my-cluster-5dfd606f-kafka-2=908bb47e-9f95-441e-bd49-e4a01f6d902c}
2022-03-28 19:07:39 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-5dfd606f-kafka-0=627b83da-e657-4469-b634-fe4ac182abcc, my-cluster-5dfd606f-kafka-1=3b5862df-223e-457c-bb27-6647e9ca37b9, my-cluster-5dfd606f-kafka-2=908bb47e-9f95-441e-bd49-e4a01f6d902c}
2022-03-28 19:07:39 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:55] All pods seem to have rolled
2022-03-28 19:07:39 [ForkJoinPool-1-worker-15] INFO  [RollingUpdateUtils:86] Component with name: my-cluster-5dfd606f-kafka has been successfully rolled
2022-03-28 19:07:39 [ForkJoinPool-1-worker-15] DEBUG [RollingUpdateUtils:87] Component matching LabelSelector: LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-5dfd606f-kafka, strimzi.io/cluster=my-cluster-5dfd606f, strimzi.io/kind=Kafka}, additionalProperties={}) successfully rolled
2022-03-28 19:07:39 [ForkJoinPool-1-worker-15] INFO  [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-5dfd606f-kafka to be ready
2022-03-28 19:07:39 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-5dfd606f-kafka, strimzi.io/cluster=my-cluster-5dfd606f, strimzi.io/kind=Kafka}, additionalProperties={})to be ready
2022-03-28 19:07:39 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-0 not ready: kafka)
2022-03-28 19:07:39 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-5dfd606f-kafka-1)
2022-03-28 19:07:39 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-5dfd606f-kafka, strimzi.io/cluster=my-cluster-5dfd606f, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1799901ms till timeout)
2022-03-28 19:07:39 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (112500ms till timeout)
2022-03-28 19:07:39 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (223302ms till timeout)
2022-03-28 19:07:39 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:07:40 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (207731ms till timeout)
2022-03-28 19:07:40 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-0 not ready: kafka)
2022-03-28 19:07:40 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-5dfd606f-kafka-1)
2022-03-28 19:07:40 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-5dfd606f-kafka, strimzi.io/cluster=my-cluster-5dfd606f, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1798711ms till timeout)
2022-03-28 19:07:40 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (111400ms till timeout)
2022-03-28 19:07:40 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (222202ms till timeout)
2022-03-28 19:07:41 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:07:41 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (206521ms till timeout)
2022-03-28 19:07:41 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-0 not ready: kafka)
2022-03-28 19:07:41 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-5dfd606f-kafka-1)
2022-03-28 19:07:41 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-5dfd606f-kafka, strimzi.io/cluster=my-cluster-5dfd606f, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1797521ms till timeout)
2022-03-28 19:07:41 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (110234ms till timeout)
2022-03-28 19:07:41 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (221042ms till timeout)
2022-03-28 19:07:42 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:07:42 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (205328ms till timeout)
2022-03-28 19:07:42 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-0 not ready: kafka)
2022-03-28 19:07:42 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-5dfd606f-kafka-1)
2022-03-28 19:07:42 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-5dfd606f-kafka, strimzi.io/cluster=my-cluster-5dfd606f, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1796332ms till timeout)
2022-03-28 19:07:42 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (219943ms till timeout)
2022-03-28 19:07:43 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (108961ms till timeout)
2022-03-28 19:07:43 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:07:43 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (204136ms till timeout)
2022-03-28 19:07:43 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-afe273c9-scraper not ready, will try again in 10000 ms (448912ms till timeout)
2022-03-28 19:07:44 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (218799ms till timeout)
2022-03-28 19:07:44 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-0 not ready: kafka)
2022-03-28 19:07:44 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-5dfd606f-kafka-1)
2022-03-28 19:07:44 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-5dfd606f-kafka, strimzi.io/cluster=my-cluster-5dfd606f, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1795092ms till timeout)
2022-03-28 19:07:44 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (107805ms till timeout)
2022-03-28 19:07:44 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-afe273c9-kafka-clients not ready, will try again in 10000 ms (448317ms till timeout)
2022-03-28 19:07:44 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:07:44 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (202914ms till timeout)
2022-03-28 19:07:45 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (217702ms till timeout)
2022-03-28 19:07:45 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-0 not ready: kafka)
2022-03-28 19:07:45 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-5dfd606f-kafka-1)
2022-03-28 19:07:45 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-5dfd606f-kafka, strimzi.io/cluster=my-cluster-5dfd606f, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1793990ms till timeout)
2022-03-28 19:07:45 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (106611ms till timeout)
2022-03-28 19:07:45 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:07:46 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (201720ms till timeout)
2022-03-28 19:07:46 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (216606ms till timeout)
2022-03-28 19:07:46 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-0 not ready: kafka)
2022-03-28 19:07:46 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-5dfd606f-kafka-1)
2022-03-28 19:07:46 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-5dfd606f-kafka, strimzi.io/cluster=my-cluster-5dfd606f, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1792893ms till timeout)
2022-03-28 19:07:46 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (105512ms till timeout)
2022-03-28 19:07:47 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:07:47 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (200521ms till timeout)
2022-03-28 19:07:47 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (215510ms till timeout)
2022-03-28 19:07:47 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-0 not ready: kafka)
2022-03-28 19:07:47 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-5dfd606f-kafka-1)
2022-03-28 19:07:47 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-5dfd606f-kafka, strimzi.io/cluster=my-cluster-5dfd606f, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1791701ms till timeout)
2022-03-28 19:07:47 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (104404ms till timeout)
2022-03-28 19:07:48 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:07:48 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (199328ms till timeout)
2022-03-28 19:07:48 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (214336ms till timeout)
2022-03-28 19:07:48 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-0 not ready: kafka)
2022-03-28 19:07:48 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-5dfd606f-kafka-1)
2022-03-28 19:07:48 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-5dfd606f-kafka, strimzi.io/cluster=my-cluster-5dfd606f, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1790510ms till timeout)
2022-03-28 19:07:48 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (103224ms till timeout)
2022-03-28 19:07:49 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:07:49 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (213239ms till timeout)
2022-03-28 19:07:49 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (198039ms till timeout)
2022-03-28 19:07:49 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-0 not ready: kafka)
2022-03-28 19:07:49 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-5dfd606f-kafka-1)
2022-03-28 19:07:49 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-5dfd606f-kafka, strimzi.io/cluster=my-cluster-5dfd606f, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1789321ms till timeout)
2022-03-28 19:07:49 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (102001ms till timeout)
2022-03-28 19:07:50 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (212141ms till timeout)
2022-03-28 19:07:50 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:07:50 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (196847ms till timeout)
2022-03-28 19:07:51 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-0 not ready: kafka)
2022-03-28 19:07:51 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-5dfd606f-kafka-1)
2022-03-28 19:07:51 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-5dfd606f-kafka, strimzi.io/cluster=my-cluster-5dfd606f, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1788131ms till timeout)
2022-03-28 19:07:51 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (100761ms till timeout)
2022-03-28 19:07:51 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (211042ms till timeout)
2022-03-28 19:07:52 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:07:52 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (195654ms till timeout)
2022-03-28 19:07:52 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-0 not ready: kafka)
2022-03-28 19:07:52 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-5dfd606f-kafka-1)
2022-03-28 19:07:52 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-5dfd606f-kafka, strimzi.io/cluster=my-cluster-5dfd606f, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1786941ms till timeout)
2022-03-28 19:07:52 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (99570ms till timeout)
2022-03-28 19:07:52 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (209944ms till timeout)
2022-03-28 19:07:53 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:07:53 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (194461ms till timeout)
2022-03-28 19:07:53 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-0 not ready: kafka)
2022-03-28 19:07:53 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-5dfd606f-kafka-1)
2022-03-28 19:07:53 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-5dfd606f-kafka, strimzi.io/cluster=my-cluster-5dfd606f, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1785751ms till timeout)
2022-03-28 19:07:53 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (98464ms till timeout)
2022-03-28 19:07:54 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (208846ms till timeout)
2022-03-28 19:07:54 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:07:54 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (193268ms till timeout)
2022-03-28 19:07:54 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-0 not ready: kafka)
2022-03-28 19:07:54 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-5dfd606f-kafka-1)
2022-03-28 19:07:54 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-5dfd606f-kafka, strimzi.io/cluster=my-cluster-5dfd606f, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1784562ms till timeout)
2022-03-28 19:07:54 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (97275ms till timeout)
2022-03-28 19:07:54 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:369] ############################################################################
2022-03-28 19:07:54 [ForkJoinPool-1-worker-25] INFO  [TestSuiteNamespaceManager:200] Deleting namespace:namespace-7 for test case:testPauseReconciliationInKafkaAndKafkaConnectWithConnector
2022-03-28 19:07:54 [ForkJoinPool-1-worker-25] DEBUG [TestUtils:125] Waiting for Namespace namespace-7 removal
2022-03-28 19:07:54 [ForkJoinPool-1-worker-25] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-7 -o yaml
2022-03-28 19:07:55 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (207748ms till timeout)
2022-03-28 19:07:55 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-7 -o yaml
2022-03-28 19:07:55 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Return code: 0
2022-03-28 19:07:55 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (479500ms till timeout)
2022-03-28 19:07:55 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:07:55 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (192074ms till timeout)
2022-03-28 19:07:55 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-0 not ready: kafka)
2022-03-28 19:07:55 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-1 not ready: kafka)
2022-03-28 19:07:55 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-2 not ready: kafka)
2022-03-28 19:07:55 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-5dfd606f-kafka-0, my-cluster-5dfd606f-kafka-1, my-cluster-5dfd606f-kafka-2 are ready
2022-03-28 19:07:55 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-5dfd606f-kafka, strimzi.io/cluster=my-cluster-5dfd606f, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1783354ms till timeout)
2022-03-28 19:07:56 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (95906ms till timeout)
2022-03-28 19:07:56 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (206647ms till timeout)
2022-03-28 19:07:56 [ForkJoinPool-1-worker-25] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-7 -o yaml
2022-03-28 19:07:56 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:07:56 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-7 -o yaml
2022-03-28 19:07:56 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Return code: 0
2022-03-28 19:07:56 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (478014ms till timeout)
2022-03-28 19:07:56 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (190881ms till timeout)
2022-03-28 19:07:57 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-0 not ready: kafka)
2022-03-28 19:07:57 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-1 not ready: kafka)
2022-03-28 19:07:57 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-2 not ready: kafka)
2022-03-28 19:07:57 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-5dfd606f-kafka-0, my-cluster-5dfd606f-kafka-1, my-cluster-5dfd606f-kafka-2 are ready
2022-03-28 19:07:57 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-5dfd606f-kafka, strimzi.io/cluster=my-cluster-5dfd606f, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1782164ms till timeout)
2022-03-28 19:07:57 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (94806ms till timeout)
2022-03-28 19:07:57 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (205549ms till timeout)
2022-03-28 19:07:57 [ForkJoinPool-1-worker-25] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-7 -o yaml
2022-03-28 19:07:57 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:07:58 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (189688ms till timeout)
2022-03-28 19:07:58 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-0 not ready: kafka)
2022-03-28 19:07:58 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-1 not ready: kafka)
2022-03-28 19:07:58 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-2 not ready: kafka)
2022-03-28 19:07:58 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-5dfd606f-kafka-0, my-cluster-5dfd606f-kafka-1, my-cluster-5dfd606f-kafka-2 are ready
2022-03-28 19:07:58 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-5dfd606f-kafka, strimzi.io/cluster=my-cluster-5dfd606f, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1780974ms till timeout)
2022-03-28 19:07:58 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (93686ms till timeout)
2022-03-28 19:07:58 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-7 -o yaml
2022-03-28 19:07:58 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Return code: 0
2022-03-28 19:07:58 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (476575ms till timeout)
2022-03-28 19:07:58 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (204452ms till timeout)
2022-03-28 19:07:59 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:07:59 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (188495ms till timeout)
2022-03-28 19:07:59 [ForkJoinPool-1-worker-25] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-7 -o yaml
2022-03-28 19:07:59 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-0 not ready: kafka)
2022-03-28 19:07:59 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-1 not ready: kafka)
2022-03-28 19:07:59 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-2 not ready: kafka)
2022-03-28 19:07:59 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-5dfd606f-kafka-0, my-cluster-5dfd606f-kafka-1, my-cluster-5dfd606f-kafka-2 are ready
2022-03-28 19:07:59 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-5dfd606f-kafka, strimzi.io/cluster=my-cluster-5dfd606f, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1779782ms till timeout)
2022-03-28 19:07:59 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (92497ms till timeout)
2022-03-28 19:07:59 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (203304ms till timeout)
2022-03-28 19:07:59 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-7 -o yaml
2022-03-28 19:07:59 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Return code: 0
2022-03-28 19:07:59 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (475119ms till timeout)
2022-03-28 19:08:00 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:08:00 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (187300ms till timeout)
2022-03-28 19:08:00 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-0 not ready: kafka)
2022-03-28 19:08:00 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-1 not ready: kafka)
2022-03-28 19:08:00 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-2 not ready: kafka)
2022-03-28 19:08:00 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-5dfd606f-kafka-0, my-cluster-5dfd606f-kafka-1, my-cluster-5dfd606f-kafka-2 are ready
2022-03-28 19:08:00 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-5dfd606f-kafka, strimzi.io/cluster=my-cluster-5dfd606f, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1778590ms till timeout)
2022-03-28 19:08:00 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (202199ms till timeout)
2022-03-28 19:08:00 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (91219ms till timeout)
2022-03-28 19:08:00 [ForkJoinPool-1-worker-25] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-7 -o yaml
2022-03-28 19:08:01 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-7 -o yaml
2022-03-28 19:08:01 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Return code: 0
2022-03-28 19:08:01 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (473665ms till timeout)
2022-03-28 19:08:01 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:08:01 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (186108ms till timeout)
2022-03-28 19:08:01 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-0 not ready: kafka)
2022-03-28 19:08:01 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-1 not ready: kafka)
2022-03-28 19:08:01 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-2 not ready: kafka)
2022-03-28 19:08:01 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-5dfd606f-kafka-0, my-cluster-5dfd606f-kafka-1, my-cluster-5dfd606f-kafka-2 are ready
2022-03-28 19:08:01 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-5dfd606f-kafka, strimzi.io/cluster=my-cluster-5dfd606f, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1777401ms till timeout)
2022-03-28 19:08:01 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (201012ms till timeout)
2022-03-28 19:08:01 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (90029ms till timeout)
2022-03-28 19:08:02 [ForkJoinPool-1-worker-25] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-7 -o yaml
2022-03-28 19:08:02 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-9 get Namespace namespace-7 -o yaml
2022-03-28 19:08:02 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Return code: 1
2022-03-28 19:08:02 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 19:08:02 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Error from server (NotFound): namespaces "namespace-7" not found
2022-03-28 19:08:02 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] ======STDERR END======
2022-03-28 19:08:02 [ForkJoinPool-1-worker-25] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[], io.strimzi.test.logs.CollectorElement@c4c0ea0=[], io.strimzi.test.logs.CollectorElement@aec142ef=[], io.strimzi.test.logs.CollectorElement@24b97ba9=[], io.strimzi.test.logs.CollectorElement@f851b6c3=[], io.strimzi.test.logs.CollectorElement@3881d5f2=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@5c7379cb=[], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[namespace-0], io.strimzi.test.logs.CollectorElement@f4405b3b=[namespace-2], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 19:08:02 [ForkJoinPool-1-worker-25] DEBUG [SuiteThreadController:267] testPauseReconciliationInKafkaAndKafkaConnectWithConnector - Notifies waiting test cases:[testPauseReconciliationInKafkaRebalanceAndTopic, testPauseReconciliationInKafkaAndKafkaConnectWithConnector, testCapacityFile, testConfigurationFileIsCreated, testConfigurationPerformanceOptions, testConfigurationReflection, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testDeployAndUnDeployCruiseControl, testTlsExternalUserWithQuotas, testCreatingUsersWithSecretPrefix, testThrottlingQuotasCreateTopic, testThrottlingQuotasCreateAlterPartitions, testKafkaAdminTopicOperations, testUserTemplate, testUpdateUser, testTlsExternalUser, testSendSimpleMessageTls, testReceiveSimpleMessageTlsScramSha, testThrottlingQuotasDeleteTopic, testReceiveSimpleMessageTls, testTopicModificationOfReplicationFactor, testSendingMessagesToNonExistingTopic, testMoreReplicasThanAvailableBrokers, testCreateTopicViaKafka] to and randomly select one to start execution
2022-03-28 19:08:02 [ForkJoinPool-1-worker-25] DEBUG [SuiteThreadController:93] [operators.ReconciliationST] - Removing parallel test: testPauseReconciliationInKafkaAndKafkaConnectWithConnector
2022-03-28 19:08:02 [ForkJoinPool-1-worker-25] DEBUG [SuiteThreadController:97] [operators.ReconciliationST] - Parallel test count: 4
2022-03-28 19:08:02 [ForkJoinPool-1-worker-25] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.ReconciliationST.testPauseReconciliationInKafkaAndKafkaConnectWithConnector-FINISHED
2022-03-28 19:08:02 [ForkJoinPool-1-worker-25] INFO  [TestSeparator:30] ############################################################################
2022-03-28 19:08:02 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:08:02 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (184912ms till timeout)
2022-03-28 19:08:03 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-0 not ready: kafka)
2022-03-28 19:08:03 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-1 not ready: kafka)
2022-03-28 19:08:03 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-2 not ready: kafka)
2022-03-28 19:08:03 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-5dfd606f-kafka-0, my-cluster-5dfd606f-kafka-1, my-cluster-5dfd606f-kafka-2 are ready
2022-03-28 19:08:03 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-5dfd606f-kafka, strimzi.io/cluster=my-cluster-5dfd606f, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1776211ms till timeout)
2022-03-28 19:08:03 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (199823ms till timeout)
2022-03-28 19:08:03 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (88840ms till timeout)
2022-03-28 19:08:03 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:08:04 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (183719ms till timeout)
2022-03-28 19:08:04 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-0 not ready: kafka)
2022-03-28 19:08:04 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-1 not ready: kafka)
2022-03-28 19:08:04 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-2 not ready: kafka)
2022-03-28 19:08:04 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-5dfd606f-kafka-0, my-cluster-5dfd606f-kafka-1, my-cluster-5dfd606f-kafka-2 are ready
2022-03-28 19:08:04 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-5dfd606f-kafka, strimzi.io/cluster=my-cluster-5dfd606f, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1775019ms till timeout)
2022-03-28 19:08:04 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (198634ms till timeout)
2022-03-28 19:08:04 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (87701ms till timeout)
2022-03-28 19:08:05 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:08:05 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (182526ms till timeout)
2022-03-28 19:08:05 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (197534ms till timeout)
2022-03-28 19:08:05 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-0 not ready: kafka)
2022-03-28 19:08:05 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-1 not ready: kafka)
2022-03-28 19:08:05 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-2 not ready: kafka)
2022-03-28 19:08:05 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-5dfd606f-kafka-0, my-cluster-5dfd606f-kafka-1, my-cluster-5dfd606f-kafka-2 are ready
2022-03-28 19:08:05 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-5dfd606f-kafka, strimzi.io/cluster=my-cluster-5dfd606f, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1773826ms till timeout)
2022-03-28 19:08:05 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (86455ms till timeout)
2022-03-28 19:08:06 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:08:06 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (196436ms till timeout)
2022-03-28 19:08:06 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (181238ms till timeout)
2022-03-28 19:08:06 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-0 not ready: kafka)
2022-03-28 19:08:06 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-1 not ready: kafka)
2022-03-28 19:08:06 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-2 not ready: kafka)
2022-03-28 19:08:06 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-5dfd606f-kafka-0, my-cluster-5dfd606f-kafka-1, my-cluster-5dfd606f-kafka-2 are ready
2022-03-28 19:08:06 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-5dfd606f-kafka, strimzi.io/cluster=my-cluster-5dfd606f, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1772634ms till timeout)
2022-03-28 19:08:06 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (85262ms till timeout)
2022-03-28 19:08:07 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (195336ms till timeout)
2022-03-28 19:08:07 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:08:07 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-0 not ready: kafka)
2022-03-28 19:08:07 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-1 not ready: kafka)
2022-03-28 19:08:07 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-5dfd606f-kafka-2 not ready: kafka)
2022-03-28 19:08:07 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-5dfd606f-kafka-0, my-cluster-5dfd606f-kafka-1, my-cluster-5dfd606f-kafka-2 are ready
2022-03-28 19:08:07 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (179947ms till timeout)
2022-03-28 19:08:07 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (84066ms till timeout)
2022-03-28 19:08:07 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:433] Wait for Kafka: my-cluster-5dfd606f will have desired state: Ready
2022-03-28 19:08:07 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Kafka: my-cluster-5dfd606f will have desired state: Ready
2022-03-28 19:08:08 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:444] Kafka: my-cluster-5dfd606f is in desired state: Ready
2022-03-28 19:08:08 [ForkJoinPool-1-worker-15] INFO  [RollingUpdateUtils:132] Kafka: my-cluster-5dfd606f is ready
2022-03-28 19:08:08 [ForkJoinPool-1-worker-15] INFO  [CruiseControlConfigurationST:136] Verifying that in Kafka config map there is configuration to cruise control metric reporter
2022-03-28 19:08:08 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Verify that kafka configuration {cruise.control.metrics.reporter.ssl.keystore.location=/tmp/kafka/cluster.keystore.p12, cruise.control.metrics.reporter.ssl.endpoint.identification.algorithm=HTTPS, cruise.control.metrics.reporter.ssl.truststore.password=${CERTS_STORE_PASSWORD}, cruise.control.metrics.reporter.ssl.truststore.type=PKCS12, cruise.control.metrics.reporter.ssl.keystore.password=${CERTS_STORE_PASSWORD}, cruise.control.metrics.topic.replication.factor=3, cluster-name=my-cluster-5dfd606f, cruise.control.metrics.reporter.security.protocol=SSL, cruise.control.metrics.reporter.ssl.keystore.type=PKCS12, cruise.control.metrics.topic.num.partitions=1, cruise.control.metrics.reporter.ssl.truststore.location=/tmp/kafka/cluster.truststore.p12, cruise.control.metrics.topic=strimzi.cruisecontrol.metrics, cruise.control.metrics.topic.min.insync.replicas=1, cruise.control.metrics.reporter.bootstrap.servers=my-cluster-5dfd606f-kafka-brokers:9091, cruise.control.metrics.topic.auto.create=true} has correct cruise control metric reporter properties
2022-03-28 19:08:08 [ForkJoinPool-1-worker-15] INFO  [CruiseControlConfigurationST:139] Verifying that Cruise Control topics are created after CC is instantiated.
2022-03-28 19:08:08 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Verify that kafka contains cruise control topics with related configuration.
2022-03-28 19:08:08 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Verify that kafka contains cruise control topics with related configuration.
2022-03-28 19:08:08 [ForkJoinPool-1-worker-15] DEBUG [AbstractST:674] ============================================================================
2022-03-28 19:08:08 [ForkJoinPool-1-worker-15] DEBUG [AbstractST:675] [cruisecontrol.CruiseControlConfigurationST - After Each] - Clean up after test
2022-03-28 19:08:08 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:344] ############################################################################
2022-03-28 19:08:08 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:348] Delete all resources for testDeployAndUnDeployCruiseControl
2022-03-28 19:08:08 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:241] Delete of Kafka my-cluster-5dfd606f in namespace namespace-2
2022-03-28 19:08:08 [ForkJoinPool-1-worker-15] INFO  [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-2, for cruise control Kafka cluster my-cluster-5dfd606f
2022-03-28 19:08:08 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (194181ms till timeout)
2022-03-28 19:08:08 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-5dfd606f
2022-03-28 19:08:08 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:08:09 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:369] ############################################################################
2022-03-28 19:08:09 [ForkJoinPool-1-worker-15] INFO  [TestSuiteNamespaceManager:200] Deleting namespace:namespace-2 for test case:testDeployAndUnDeployCruiseControl
2022-03-28 19:08:09 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (82869ms till timeout)
2022-03-28 19:08:09 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (178525ms till timeout)
2022-03-28 19:08:09 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Namespace namespace-2 removal
2022-03-28 19:08:09 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-2 -o yaml
2022-03-28 19:08:09 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-2 -o yaml
2022-03-28 19:08:09 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 19:08:09 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (479580ms till timeout)
2022-03-28 19:08:09 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (193085ms till timeout)
2022-03-28 19:08:10 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (81769ms till timeout)
2022-03-28 19:08:10 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:08:10 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (177333ms till timeout)
2022-03-28 19:08:10 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-2 -o yaml
2022-03-28 19:08:10 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (191989ms till timeout)
2022-03-28 19:08:11 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-2 -o yaml
2022-03-28 19:08:11 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 19:08:11 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (478128ms till timeout)
2022-03-28 19:08:11 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (80669ms till timeout)
2022-03-28 19:08:11 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:08:11 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (176142ms till timeout)
2022-03-28 19:08:11 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (190892ms till timeout)
2022-03-28 19:08:12 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-2 -o yaml
2022-03-28 19:08:12 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (79569ms till timeout)
2022-03-28 19:08:12 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-2 -o yaml
2022-03-28 19:08:12 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 19:08:12 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (476699ms till timeout)
2022-03-28 19:08:12 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:08:12 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (174950ms till timeout)
2022-03-28 19:08:13 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (189795ms till timeout)
2022-03-28 19:08:13 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (78469ms till timeout)
2022-03-28 19:08:13 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-2 -o yaml
2022-03-28 19:08:13 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:08:14 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (173743ms till timeout)
2022-03-28 19:08:14 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-2 -o yaml
2022-03-28 19:08:14 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 19:08:14 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (475250ms till timeout)
2022-03-28 19:08:14 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (188699ms till timeout)
2022-03-28 19:08:14 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (77369ms till timeout)
2022-03-28 19:08:15 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-2 -o yaml
2022-03-28 19:08:15 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:08:15 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (172551ms till timeout)
2022-03-28 19:08:15 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (187558ms till timeout)
2022-03-28 19:08:15 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-2 -o yaml
2022-03-28 19:08:15 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 19:08:15 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (473805ms till timeout)
2022-03-28 19:08:15 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (76262ms till timeout)
2022-03-28 19:08:16 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:08:16 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (186461ms till timeout)
2022-03-28 19:08:16 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (171264ms till timeout)
2022-03-28 19:08:16 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-2 -o yaml
2022-03-28 19:08:16 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (75162ms till timeout)
2022-03-28 19:08:16 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-2 -o yaml
2022-03-28 19:08:16 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 19:08:16 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (472349ms till timeout)
2022-03-28 19:08:17 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (185363ms till timeout)
2022-03-28 19:08:17 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:08:17 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (170070ms till timeout)
2022-03-28 19:08:17 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (74062ms till timeout)
2022-03-28 19:08:17 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-2 -o yaml
2022-03-28 19:08:18 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-2 -o yaml
2022-03-28 19:08:18 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 19:08:18 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (470890ms till timeout)
2022-03-28 19:08:18 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (184266ms till timeout)
2022-03-28 19:08:18 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:08:18 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (168877ms till timeout)
2022-03-28 19:08:19 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (72958ms till timeout)
2022-03-28 19:08:19 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-2 -o yaml
2022-03-28 19:08:19 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (183168ms till timeout)
2022-03-28 19:08:19 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-2 -o yaml
2022-03-28 19:08:19 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 19:08:19 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (469433ms till timeout)
2022-03-28 19:08:19 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:08:20 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (167685ms till timeout)
2022-03-28 19:08:20 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (71793ms till timeout)
2022-03-28 19:08:20 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (182070ms till timeout)
2022-03-28 19:08:20 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-2 -o yaml
2022-03-28 19:08:21 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:08:21 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (166491ms till timeout)
2022-03-28 19:08:21 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (70600ms till timeout)
2022-03-28 19:08:21 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-2 -o yaml
2022-03-28 19:08:21 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 19:08:21 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (467948ms till timeout)
2022-03-28 19:08:21 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (180972ms till timeout)
2022-03-28 19:08:22 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:08:22 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-2 -o yaml
2022-03-28 19:08:22 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (69500ms till timeout)
2022-03-28 19:08:22 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (165198ms till timeout)
2022-03-28 19:08:22 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-2 -o yaml
2022-03-28 19:08:22 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 19:08:22 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (466480ms till timeout)
2022-03-28 19:08:22 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (179873ms till timeout)
2022-03-28 19:08:23 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (68400ms till timeout)
2022-03-28 19:08:23 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:08:23 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (164002ms till timeout)
2022-03-28 19:08:23 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-2 -o yaml
2022-03-28 19:08:24 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (178776ms till timeout)
2022-03-28 19:08:24 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-2 -o yaml
2022-03-28 19:08:24 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 19:08:24 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (465038ms till timeout)
2022-03-28 19:08:24 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (67301ms till timeout)
2022-03-28 19:08:24 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:08:24 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (162809ms till timeout)
2022-03-28 19:08:25 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (177676ms till timeout)
2022-03-28 19:08:25 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-2 -o yaml
2022-03-28 19:08:25 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (66202ms till timeout)
2022-03-28 19:08:25 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-2 -o yaml
2022-03-28 19:08:25 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 19:08:25 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (463580ms till timeout)
2022-03-28 19:08:26 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:08:26 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (161617ms till timeout)
2022-03-28 19:08:26 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (176580ms till timeout)
2022-03-28 19:08:26 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-2 -o yaml
2022-03-28 19:08:26 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (65101ms till timeout)
2022-03-28 19:08:27 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-2 -o yaml
2022-03-28 19:08:27 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 19:08:27 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (462143ms till timeout)
2022-03-28 19:08:27 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:08:27 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (160424ms till timeout)
2022-03-28 19:08:27 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (175431ms till timeout)
2022-03-28 19:08:27 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (64001ms till timeout)
2022-03-28 19:08:28 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-2 -o yaml
2022-03-28 19:08:28 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:08:28 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (174335ms till timeout)
2022-03-28 19:08:28 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (159137ms till timeout)
2022-03-28 19:08:28 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-2 -o yaml
2022-03-28 19:08:28 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 19:08:28 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (460707ms till timeout)
2022-03-28 19:08:29 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (62902ms till timeout)
2022-03-28 19:08:29 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (173238ms till timeout)
2022-03-28 19:08:29 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-2 -o yaml
2022-03-28 19:08:29 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:08:29 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (157944ms till timeout)
2022-03-28 19:08:30 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-2 -o yaml
2022-03-28 19:08:30 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 19:08:30 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (459268ms till timeout)
2022-03-28 19:08:30 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (61800ms till timeout)
2022-03-28 19:08:30 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (172140ms till timeout)
2022-03-28 19:08:30 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:08:31 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (156752ms till timeout)
2022-03-28 19:08:31 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-2 -o yaml
2022-03-28 19:08:31 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (60701ms till timeout)
2022-03-28 19:08:31 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-2 -o yaml
2022-03-28 19:08:31 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 19:08:31 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (457827ms till timeout)
2022-03-28 19:08:31 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (171042ms till timeout)
2022-03-28 19:08:32 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:08:32 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (155558ms till timeout)
2022-03-28 19:08:32 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (59600ms till timeout)
2022-03-28 19:08:32 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-2 -o yaml
2022-03-28 19:08:32 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (169944ms till timeout)
2022-03-28 19:08:32 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-2 -o yaml
2022-03-28 19:08:32 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 19:08:32 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (456356ms till timeout)
2022-03-28 19:08:33 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:08:33 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (154365ms till timeout)
2022-03-28 19:08:33 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (58475ms till timeout)
2022-03-28 19:08:33 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-2 -o yaml
2022-03-28 19:08:34 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (168846ms till timeout)
2022-03-28 19:08:34 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:08:34 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-2 -o yaml
2022-03-28 19:08:34 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 19:08:34 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (454845ms till timeout)
2022-03-28 19:08:34 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (153172ms till timeout)
2022-03-28 19:08:34 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (57282ms till timeout)
2022-03-28 19:08:35 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (167749ms till timeout)
2022-03-28 19:08:35 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-2 -o yaml
2022-03-28 19:08:35 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:08:35 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (151979ms till timeout)
2022-03-28 19:08:35 [ForkJoinPool-1-worker-33] INFO  [PodUtils:189] Message org.apache.kafka.common.errors.ThrottlingQuotaExceededException: The throttling quota has been exceeded. found in alter-admin-my-cluster-ec1b34f8-kafka-clients-zd8vg log
2022-03-28 19:08:35 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-2 -o yaml
2022-03-28 19:08:35 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 19:08:35 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (453372ms till timeout)
2022-03-28 19:08:36 [ForkJoinPool-1-worker-33] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment alter-admin-my-cluster-ec1b34f8-kafka-clients deletion
2022-03-28 19:08:36 [ForkJoinPool-1-worker-33] DEBUG [TestUtils:125] Waiting for ReplicaSet alter-admin-my-cluster-ec1b34f8-kafka-clients to be deleted
2022-03-28 19:08:36 [ForkJoinPool-1-worker-33] DEBUG [JobUtils:40] Job alter-admin-my-cluster-ec1b34f8-kafka-clients was deleted
2022-03-28 19:08:36 [ForkJoinPool-1-worker-33] INFO  [ResourceManager:155] Create/Update Job teardown-delete in namespace throttling-quota-st
2022-03-28 19:08:36 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (166599ms till timeout)
2022-03-28 19:08:36 [ForkJoinPool-1-worker-33] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:teardown-delete
2022-03-28 19:08:36 [ForkJoinPool-1-worker-33] INFO  [JobUtils:81] Waiting for job: teardown-delete will be in active state
2022-03-28 19:08:36 [ForkJoinPool-1-worker-33] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 19:08:36 [ForkJoinPool-1-worker-33] INFO  [ClientUtils:76] Waiting for producer/consumer:teardown-delete to finished
2022-03-28 19:08:36 [ForkJoinPool-1-worker-33] DEBUG [TestUtils:125] Waiting for job finished
2022-03-28 19:08:36 [ForkJoinPool-1-worker-33] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:08:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:08:36 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (129807ms till timeout)
2022-03-28 19:08:36 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:08:36 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (150786ms till timeout)
2022-03-28 19:08:36 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-2 -o yaml
2022-03-28 19:08:37 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (165501ms till timeout)
2022-03-28 19:08:37 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-2 -o yaml
2022-03-28 19:08:37 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 19:08:37 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (451927ms till timeout)
2022-03-28 19:08:37 [ForkJoinPool-1-worker-33] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:08:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:08:37 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (128614ms till timeout)
2022-03-28 19:08:38 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:08:38 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (149593ms till timeout)
2022-03-28 19:08:38 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-2 -o yaml
2022-03-28 19:08:38 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (164404ms till timeout)
2022-03-28 19:08:38 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-2 -o yaml
2022-03-28 19:08:38 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 19:08:38 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (450485ms till timeout)
2022-03-28 19:08:39 [ForkJoinPool-1-worker-33] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:08:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:08:39 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (127416ms till timeout)
2022-03-28 19:08:39 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:08:39 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (148400ms till timeout)
2022-03-28 19:08:39 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (163308ms till timeout)
2022-03-28 19:08:39 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-2 -o yaml
2022-03-28 19:08:40 [ForkJoinPool-1-worker-33] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:08:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:08:40 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-2 -o yaml
2022-03-28 19:08:40 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 19:08:40 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (449024ms till timeout)
2022-03-28 19:08:40 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (126224ms till timeout)
2022-03-28 19:08:40 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:08:40 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (147209ms till timeout)
2022-03-28 19:08:40 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (162207ms till timeout)
2022-03-28 19:08:41 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-2 -o yaml
2022-03-28 19:08:41 [ForkJoinPool-1-worker-33] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:08:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:08:41 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (125029ms till timeout)
2022-03-28 19:08:41 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:08:41 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (146016ms till timeout)
2022-03-28 19:08:41 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-9 get Namespace namespace-2 -o yaml
2022-03-28 19:08:41 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 1
2022-03-28 19:08:41 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 19:08:41 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Error from server (NotFound): namespaces "namespace-2" not found
2022-03-28 19:08:41 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] ======STDERR END======
2022-03-28 19:08:41 [ForkJoinPool-1-worker-15] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[], io.strimzi.test.logs.CollectorElement@c4c0ea0=[], io.strimzi.test.logs.CollectorElement@aec142ef=[], io.strimzi.test.logs.CollectorElement@24b97ba9=[], io.strimzi.test.logs.CollectorElement@f851b6c3=[], io.strimzi.test.logs.CollectorElement@3881d5f2=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@5c7379cb=[], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[namespace-0], io.strimzi.test.logs.CollectorElement@f4405b3b=[], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 19:08:41 [ForkJoinPool-1-worker-15] DEBUG [SuiteThreadController:267] testDeployAndUnDeployCruiseControl - Notifies waiting test cases:[testPauseReconciliationInKafkaRebalanceAndTopic, testPauseReconciliationInKafkaAndKafkaConnectWithConnector, testCapacityFile, testConfigurationFileIsCreated, testConfigurationPerformanceOptions, testConfigurationReflection, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testDeployAndUnDeployCruiseControl, testTlsExternalUserWithQuotas, testCreatingUsersWithSecretPrefix, testThrottlingQuotasCreateTopic, testThrottlingQuotasCreateAlterPartitions, testKafkaAdminTopicOperations, testUserTemplate, testUpdateUser, testTlsExternalUser, testSendSimpleMessageTls, testReceiveSimpleMessageTlsScramSha, testThrottlingQuotasDeleteTopic, testReceiveSimpleMessageTls, testTopicModificationOfReplicationFactor, testSendingMessagesToNonExistingTopic, testMoreReplicasThanAvailableBrokers, testCreateTopicViaKafka] to and randomly select one to start execution
2022-03-28 19:08:41 [ForkJoinPool-1-worker-15] DEBUG [SuiteThreadController:93] [cruisecontrol.CruiseControlConfigurationST] - Removing parallel test: testDeployAndUnDeployCruiseControl
2022-03-28 19:08:41 [ForkJoinPool-1-worker-15] DEBUG [SuiteThreadController:97] [cruisecontrol.CruiseControlConfigurationST] - Parallel test count: 3
2022-03-28 19:08:41 [ForkJoinPool-1-worker-15] INFO  [TestSeparator:29] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testDeployAndUnDeployCruiseControl-FINISHED
2022-03-28 19:08:41 [ForkJoinPool-1-worker-15] INFO  [TestSeparator:30] ############################################################################
2022-03-28 19:08:41 [ForkJoinPool-1-worker-17] DEBUG [AbstractST:689] ============================================================================
2022-03-28 19:08:41 [ForkJoinPool-1-worker-17] DEBUG [AbstractST:690] [cruisecontrol.CruiseControlConfigurationST - After All] - Clean up after test suite
2022-03-28 19:08:41 [ForkJoinPool-1-worker-17] INFO  [ResourceManager:344] ############################################################################
2022-03-28 19:08:41 [ForkJoinPool-1-worker-17] INFO  [ResourceManager:346] In context CruiseControlConfigurationST is everything deleted.
2022-03-28 19:08:41 [ForkJoinPool-1-worker-17] INFO  [ResourceManager:369] ############################################################################
2022-03-28 19:08:41 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (161024ms till timeout)
2022-03-28 19:08:41 [ForkJoinPool-1-worker-17] DEBUG [TestUtils:125] Waiting for Namespace cruise-control-configuration-st removal
2022-03-28 19:08:41 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace cruise-control-configuration-st -o yaml
2022-03-28 19:08:42 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace cruise-control-configuration-st -o yaml
2022-03-28 19:08:42 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 19:08:42 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Namespace cruise-control-configuration-st removal not ready, will try again in 1000 ms (479555ms till timeout)
2022-03-28 19:08:42 [ForkJoinPool-1-worker-33] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:08:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:08:42 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (123836ms till timeout)
2022-03-28 19:08:42 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:08:42 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (159928ms till timeout)
2022-03-28 19:08:43 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (144730ms till timeout)
2022-03-28 19:08:43 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace cruise-control-configuration-st -o yaml
2022-03-28 19:08:43 [ForkJoinPool-1-worker-33] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:08:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:08:43 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace cruise-control-configuration-st -o yaml
2022-03-28 19:08:43 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 19:08:43 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Namespace cruise-control-configuration-st removal not ready, will try again in 1000 ms (478092ms till timeout)
2022-03-28 19:08:43 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (122642ms till timeout)
2022-03-28 19:08:44 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (158831ms till timeout)
2022-03-28 19:08:44 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:08:44 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (143538ms till timeout)
2022-03-28 19:08:44 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace cruise-control-configuration-st -o yaml
2022-03-28 19:08:45 [ForkJoinPool-1-worker-33] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:08:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:08:45 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (121449ms till timeout)
2022-03-28 19:08:45 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (157653ms till timeout)
2022-03-28 19:08:45 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:08:45 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace cruise-control-configuration-st -o yaml
2022-03-28 19:08:45 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 19:08:45 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Namespace cruise-control-configuration-st removal not ready, will try again in 1000 ms (476618ms till timeout)
2022-03-28 19:08:45 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (142347ms till timeout)
2022-03-28 19:08:46 [ForkJoinPool-1-worker-33] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:08:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:08:46 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (156556ms till timeout)
2022-03-28 19:08:46 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace cruise-control-configuration-st -o yaml
2022-03-28 19:08:46 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (120162ms till timeout)
2022-03-28 19:08:46 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:08:46 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (141153ms till timeout)
2022-03-28 19:08:46 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace cruise-control-configuration-st -o yaml
2022-03-28 19:08:46 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 19:08:46 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Namespace cruise-control-configuration-st removal not ready, will try again in 1000 ms (475170ms till timeout)
2022-03-28 19:08:47 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (155413ms till timeout)
2022-03-28 19:08:47 [ForkJoinPool-1-worker-33] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:08:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:08:47 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (118924ms till timeout)
2022-03-28 19:08:47 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:08:47 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace cruise-control-configuration-st -o yaml
2022-03-28 19:08:47 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (139930ms till timeout)
2022-03-28 19:08:48 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace cruise-control-configuration-st -o yaml
2022-03-28 19:08:48 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 19:08:48 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Namespace cruise-control-configuration-st removal not ready, will try again in 1000 ms (473696ms till timeout)
2022-03-28 19:08:48 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (154311ms till timeout)
2022-03-28 19:08:48 [ForkJoinPool-1-worker-33] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:08:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:08:48 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (117730ms till timeout)
2022-03-28 19:08:48 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:08:49 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (138736ms till timeout)
2022-03-28 19:08:49 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace cruise-control-configuration-st -o yaml
2022-03-28 19:08:49 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (153213ms till timeout)
2022-03-28 19:08:49 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-9 get Namespace cruise-control-configuration-st -o yaml
2022-03-28 19:08:49 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 1
2022-03-28 19:08:49 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 19:08:49 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Error from server (NotFound): namespaces "cruise-control-configuration-st" not found
2022-03-28 19:08:49 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] ======STDERR END======
2022-03-28 19:08:49 [ForkJoinPool-1-worker-17] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[], io.strimzi.test.logs.CollectorElement@c4c0ea0=[], io.strimzi.test.logs.CollectorElement@aec142ef=[], io.strimzi.test.logs.CollectorElement@24b97ba9=[], io.strimzi.test.logs.CollectorElement@f851b6c3=[], io.strimzi.test.logs.CollectorElement@3881d5f2=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@e3067dd1=[], io.strimzi.test.logs.CollectorElement@5c7379cb=[], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[namespace-0], io.strimzi.test.logs.CollectorElement@f4405b3b=[], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 19:08:49 [ForkJoinPool-1-worker-17] DEBUG [SuiteThreadController:254] CruiseControlConfigurationST - Notifies waiting test suites:[TopicST, ThrottlingQuotaST, UserST, HttpBridgeTlsST, HttpBridgeScramShaST, ReconciliationST, CruiseControlConfigurationST] to and randomly select one to start execution
2022-03-28 19:08:49 [ForkJoinPool-1-worker-17] DEBUG [SuiteThreadController:85] [cruisecontrol.CruiseControlConfigurationST] - Removing parallel suite: CruiseControlConfigurationST
2022-03-28 19:08:49 [ForkJoinPool-1-worker-17] DEBUG [SuiteThreadController:89] [cruisecontrol.CruiseControlConfigurationST] - Parallel suites count: 3
[INFO] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 825.237 s - in io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST
2022-03-28 19:08:49 [ForkJoinPool-1-worker-33] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:08:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:08:50 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (116539ms till timeout)
2022-03-28 19:08:50 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:08:50 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (137544ms till timeout)
2022-03-28 19:08:50 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (152115ms till timeout)
2022-03-28 19:08:51 [ForkJoinPool-1-worker-33] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:08:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:08:51 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (115346ms till timeout)
2022-03-28 19:08:51 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:08:51 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (136350ms till timeout)
2022-03-28 19:08:51 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (151017ms till timeout)
2022-03-28 19:08:52 [ForkJoinPool-1-worker-33] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:08:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:08:52 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (114153ms till timeout)
2022-03-28 19:08:52 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:08:52 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (135158ms till timeout)
2022-03-28 19:08:52 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (149920ms till timeout)
2022-03-28 19:08:53 [ForkJoinPool-1-worker-33] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:08:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:08:53 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (112960ms till timeout)
2022-03-28 19:08:53 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:08:53 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (133966ms till timeout)
2022-03-28 19:08:54 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (148823ms till timeout)
2022-03-28 19:08:54 [ForkJoinPool-1-worker-33] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:08:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:08:54 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (111761ms till timeout)
2022-03-28 19:08:54 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:08:54 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (132767ms till timeout)
2022-03-28 19:08:55 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (147727ms till timeout)
2022-03-28 19:08:55 [ForkJoinPool-1-worker-33] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:08:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:08:55 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (110567ms till timeout)
2022-03-28 19:08:56 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:08:56 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (131574ms till timeout)
2022-03-28 19:08:56 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (146581ms till timeout)
2022-03-28 19:08:57 [ForkJoinPool-1-worker-33] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:08:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:08:57 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (109374ms till timeout)
2022-03-28 19:08:57 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:08:57 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (145484ms till timeout)
2022-03-28 19:08:57 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (130286ms till timeout)
2022-03-28 19:08:58 [ForkJoinPool-1-worker-33] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:08:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:08:58 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (108181ms till timeout)
2022-03-28 19:08:58 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (144385ms till timeout)
2022-03-28 19:08:58 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:08:58 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (129086ms till timeout)
2022-03-28 19:08:59 [ForkJoinPool-1-worker-33] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:08:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:08:59 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (143288ms till timeout)
2022-03-28 19:08:59 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (106894ms till timeout)
2022-03-28 19:08:59 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:08:59 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (127894ms till timeout)
2022-03-28 19:09:00 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (142190ms till timeout)
2022-03-28 19:09:00 [ForkJoinPool-1-worker-33] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:08:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:09:00 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (105698ms till timeout)
2022-03-28 19:09:00 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:09:01 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (126702ms till timeout)
2022-03-28 19:09:01 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (141093ms till timeout)
2022-03-28 19:09:01 [ForkJoinPool-1-worker-33] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:08:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:09:02 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (104505ms till timeout)
2022-03-28 19:09:02 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:09:02 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (125506ms till timeout)
2022-03-28 19:09:02 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (139995ms till timeout)
2022-03-28 19:09:03 [ForkJoinPool-1-worker-33] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:08:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:09:03 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (103313ms till timeout)
2022-03-28 19:09:03 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:09:03 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (124315ms till timeout)
2022-03-28 19:09:03 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (138897ms till timeout)
2022-03-28 19:09:04 [ForkJoinPool-1-worker-33] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:08:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:09:04 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (102120ms till timeout)
2022-03-28 19:09:04 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:09:04 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (123122ms till timeout)
2022-03-28 19:09:05 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (137799ms till timeout)
2022-03-28 19:09:05 [ForkJoinPool-1-worker-33] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:08:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:09:05 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (100928ms till timeout)
2022-03-28 19:09:05 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:09:05 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (121929ms till timeout)
2022-03-28 19:09:06 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (136703ms till timeout)
2022-03-28 19:09:06 [ForkJoinPool-1-worker-33] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:08:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:09:06 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (99735ms till timeout)
2022-03-28 19:09:06 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=null, completedIndexes=null, completionTime=2022-03-28T19:09:01Z, conditions=[JobCondition(lastProbeTime=2022-03-28T19:09:01Z, lastTransitionTime=2022-03-28T19:09:01Z, message=null, reason=null, status=True, type=Complete, additionalProperties={})], failed=null, ready=null, startTime=2022-03-28T19:07:22Z, succeeded=1, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:09:07 [ForkJoinPool-1-worker-41] INFO  [PodUtils:186] Waiting for message will be in the log
2022-03-28 19:09:07 [ForkJoinPool-1-worker-41] DEBUG [TestUtils:125] Waiting for Waiting for message will be in the log
2022-03-28 19:09:07 [ForkJoinPool-1-worker-41] INFO  [PodUtils:189] Message All topics created found in create-admin-my-cluster-836b31f8-kafka-clients-m9ggg log
2022-03-28 19:09:07 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (135538ms till timeout)
2022-03-28 19:09:07 [ForkJoinPool-1-worker-41] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment create-admin-my-cluster-836b31f8-kafka-clients deletion
2022-03-28 19:09:07 [ForkJoinPool-1-worker-41] DEBUG [TestUtils:125] Waiting for ReplicaSet create-admin-my-cluster-836b31f8-kafka-clients to be deleted
2022-03-28 19:09:07 [ForkJoinPool-1-worker-41] DEBUG [JobUtils:40] Job create-admin-my-cluster-836b31f8-kafka-clients was deleted
2022-03-28 19:09:07 [ForkJoinPool-1-worker-41] INFO  [ResourceManager:155] Create/Update Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st
2022-03-28 19:09:07 [ForkJoinPool-1-worker-41] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:delete-admin-my-cluster-836b31f8-kafka-clients
2022-03-28 19:09:07 [ForkJoinPool-1-worker-41] INFO  [JobUtils:81] Waiting for job: delete-admin-my-cluster-836b31f8-kafka-clients will be in active state
2022-03-28 19:09:07 [ForkJoinPool-1-worker-41] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 19:09:08 [ForkJoinPool-1-worker-33] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:08:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:09:08 [ForkJoinPool-1-worker-41] INFO  [PodUtils:186] Waiting for message will be in the log
2022-03-28 19:09:08 [ForkJoinPool-1-worker-41] DEBUG [TestUtils:125] Waiting for Waiting for message will be in the log
2022-03-28 19:09:08 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (98362ms till timeout)
2022-03-28 19:09:08 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (299791ms till timeout)
2022-03-28 19:09:08 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (134436ms till timeout)
2022-03-28 19:09:09 [ForkJoinPool-1-worker-33] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:08:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:09:09 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (97163ms till timeout)
2022-03-28 19:09:09 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (298610ms till timeout)
2022-03-28 19:09:09 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (133270ms till timeout)
2022-03-28 19:09:10 [ForkJoinPool-1-worker-33] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:08:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:09:10 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (297500ms till timeout)
2022-03-28 19:09:10 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (132160ms till timeout)
2022-03-28 19:09:10 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (95860ms till timeout)
2022-03-28 19:09:11 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (296218ms till timeout)
2022-03-28 19:09:11 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (130973ms till timeout)
2022-03-28 19:09:11 [ForkJoinPool-1-worker-33] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:08:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:09:11 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (94577ms till timeout)
2022-03-28 19:09:13 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (294935ms till timeout)
2022-03-28 19:09:13 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (129691ms till timeout)
2022-03-28 19:09:13 [ForkJoinPool-1-worker-33] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:08:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:09:13 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (93295ms till timeout)
2022-03-28 19:09:14 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (128592ms till timeout)
2022-03-28 19:09:14 [ForkJoinPool-1-worker-33] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:08:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:09:14 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (293554ms till timeout)
2022-03-28 19:09:14 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (92009ms till timeout)
2022-03-28 19:09:15 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (127495ms till timeout)
2022-03-28 19:09:15 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (292272ms till timeout)
2022-03-28 19:09:15 [ForkJoinPool-1-worker-33] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:08:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:09:15 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (90632ms till timeout)
2022-03-28 19:09:16 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (126397ms till timeout)
2022-03-28 19:09:17 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (290990ms till timeout)
2022-03-28 19:09:17 [ForkJoinPool-1-worker-33] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:08:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:09:17 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (89350ms till timeout)
2022-03-28 19:09:17 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (125301ms till timeout)
2022-03-28 19:09:18 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (289707ms till timeout)
2022-03-28 19:09:18 [ForkJoinPool-1-worker-33] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:08:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:09:18 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (88066ms till timeout)
2022-03-28 19:09:18 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (124200ms till timeout)
2022-03-28 19:09:19 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (288425ms till timeout)
2022-03-28 19:09:19 [ForkJoinPool-1-worker-33] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:08:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:09:19 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (123104ms till timeout)
2022-03-28 19:09:19 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (86708ms till timeout)
2022-03-28 19:09:20 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (287142ms till timeout)
2022-03-28 19:09:20 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (121897ms till timeout)
2022-03-28 19:09:20 [ForkJoinPool-1-worker-33] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:08:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:09:21 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (85496ms till timeout)
2022-03-28 19:09:22 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (285857ms till timeout)
2022-03-28 19:09:22 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (120612ms till timeout)
2022-03-28 19:09:22 [ForkJoinPool-1-worker-33] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:08:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:09:22 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (84215ms till timeout)
2022-03-28 19:09:23 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (284542ms till timeout)
2022-03-28 19:09:23 [ForkJoinPool-1-worker-33] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:08:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:09:23 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (119298ms till timeout)
2022-03-28 19:09:23 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (82902ms till timeout)
2022-03-28 19:09:24 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (283259ms till timeout)
2022-03-28 19:09:24 [ForkJoinPool-1-worker-33] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:08:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:09:24 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (118014ms till timeout)
2022-03-28 19:09:24 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (81619ms till timeout)
2022-03-28 19:09:26 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (281973ms till timeout)
2022-03-28 19:09:26 [ForkJoinPool-1-worker-33] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:08:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:09:26 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (116729ms till timeout)
2022-03-28 19:09:26 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (80328ms till timeout)
2022-03-28 19:09:27 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (280690ms till timeout)
2022-03-28 19:09:27 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (115445ms till timeout)
2022-03-28 19:09:27 [ForkJoinPool-1-worker-33] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:08:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:09:27 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (79048ms till timeout)
2022-03-28 19:09:28 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (279407ms till timeout)
2022-03-28 19:09:28 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (114162ms till timeout)
2022-03-28 19:09:28 [ForkJoinPool-1-worker-33] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:08:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:09:28 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (77764ms till timeout)
2022-03-28 19:09:29 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (278124ms till timeout)
2022-03-28 19:09:29 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (112880ms till timeout)
2022-03-28 19:09:29 [ForkJoinPool-1-worker-33] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:08:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:09:30 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (76482ms till timeout)
2022-03-28 19:09:31 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (276841ms till timeout)
2022-03-28 19:09:31 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (111597ms till timeout)
2022-03-28 19:09:31 [ForkJoinPool-1-worker-33] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:08:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:09:31 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (75201ms till timeout)
2022-03-28 19:09:32 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (275558ms till timeout)
2022-03-28 19:09:32 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (110314ms till timeout)
2022-03-28 19:09:32 [ForkJoinPool-1-worker-33] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:08:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:09:32 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (73917ms till timeout)
2022-03-28 19:09:33 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (274275ms till timeout)
2022-03-28 19:09:33 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (109031ms till timeout)
2022-03-28 19:09:33 [ForkJoinPool-1-worker-33] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:08:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:09:33 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (72634ms till timeout)
2022-03-28 19:09:35 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (272993ms till timeout)
2022-03-28 19:09:35 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (107749ms till timeout)
2022-03-28 19:09:35 [ForkJoinPool-1-worker-33] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:08:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:09:35 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (71352ms till timeout)
2022-03-28 19:09:36 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (271710ms till timeout)
2022-03-28 19:09:36 [ForkJoinPool-1-worker-33] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:08:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:09:36 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (106466ms till timeout)
2022-03-28 19:09:36 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (70070ms till timeout)
2022-03-28 19:09:37 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (270428ms till timeout)
2022-03-28 19:09:37 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (105183ms till timeout)
2022-03-28 19:09:37 [ForkJoinPool-1-worker-33] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:08:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:09:37 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (68787ms till timeout)
2022-03-28 19:09:38 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (269145ms till timeout)
2022-03-28 19:09:38 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (103901ms till timeout)
2022-03-28 19:09:38 [ForkJoinPool-1-worker-33] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:08:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:09:39 [ForkJoinPool-1-worker-33] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (67506ms till timeout)
2022-03-28 19:09:40 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (267862ms till timeout)
2022-03-28 19:09:40 [ForkJoinPool-1-worker-33] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=null, completedIndexes=null, completionTime=2022-03-28T19:09:35Z, conditions=[JobCondition(lastProbeTime=2022-03-28T19:09:35Z, lastTransitionTime=2022-03-28T19:09:35Z, message=null, reason=null, status=True, type=Complete, additionalProperties={})], failed=null, ready=null, startTime=2022-03-28T19:08:31Z, succeeded=1, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:09:40 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (102618ms till timeout)
2022-03-28 19:09:40 [ForkJoinPool-1-worker-33] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment teardown-delete deletion
2022-03-28 19:09:40 [ForkJoinPool-1-worker-33] DEBUG [TestUtils:125] Waiting for ReplicaSet teardown-delete to be deleted
2022-03-28 19:09:40 [ForkJoinPool-1-worker-33] DEBUG [JobUtils:40] Job teardown-delete was deleted
2022-03-28 19:09:40 [ForkJoinPool-1-worker-33] DEBUG [AbstractST:674] ============================================================================
2022-03-28 19:09:40 [ForkJoinPool-1-worker-33] DEBUG [AbstractST:675] [operators.topic.ThrottlingQuotaST - After Each] - Clean up after test
2022-03-28 19:09:40 [ForkJoinPool-1-worker-33] INFO  [ResourceManager:344] ############################################################################
2022-03-28 19:09:40 [ForkJoinPool-1-worker-33] INFO  [ResourceManager:348] Delete all resources for testThrottlingQuotasCreateAlterPartitions
2022-03-28 19:09:40 [ForkJoinPool-1-worker-17] INFO  [ResourceManager:241] Delete of Job create-admin-my-cluster-ec1b34f8-kafka-clients in namespace throttling-quota-st
2022-03-28 19:09:40 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:241] Delete of KafkaUser my-user-333986137-1152182125 in namespace throttling-quota-st
2022-03-28 19:09:40 [ForkJoinPool-1-worker-33] INFO  [ResourceManager:241] Delete of Job create-admin-my-cluster-ec1b34f8-kafka-clients in namespace throttling-quota-st
2022-03-28 19:09:40 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:241] Delete of Job teardown-delete in namespace throttling-quota-st
2022-03-28 19:09:40 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:241] Delete of Job alter-admin-my-cluster-ec1b34f8-kafka-clients in namespace throttling-quota-st
2022-03-28 19:09:40 [ForkJoinPool-1-worker-17] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:create-admin-my-cluster-ec1b34f8-kafka-clients
2022-03-28 19:09:40 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:teardown-delete
2022-03-28 19:09:40 [ForkJoinPool-1-worker-25] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:alter-admin-my-cluster-ec1b34f8-kafka-clients
2022-03-28 19:09:40 [ForkJoinPool-1-worker-33] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:create-admin-my-cluster-ec1b34f8-kafka-clients
2022-03-28 19:09:40 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-333986137-1152182125
2022-03-28 19:09:40 [ForkJoinPool-1-worker-33] INFO  [ResourceManager:369] ############################################################################
2022-03-28 19:09:40 [ForkJoinPool-1-worker-33] DEBUG [SuiteThreadController:267] testThrottlingQuotasCreateAlterPartitions - Notifies waiting test cases:[testPauseReconciliationInKafkaRebalanceAndTopic, testPauseReconciliationInKafkaAndKafkaConnectWithConnector, testCapacityFile, testConfigurationFileIsCreated, testConfigurationPerformanceOptions, testConfigurationReflection, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testDeployAndUnDeployCruiseControl, testTlsExternalUserWithQuotas, testCreatingUsersWithSecretPrefix, testThrottlingQuotasCreateTopic, testThrottlingQuotasCreateAlterPartitions, testKafkaAdminTopicOperations, testUserTemplate, testUpdateUser, testTlsExternalUser, testSendSimpleMessageTls, testReceiveSimpleMessageTlsScramSha, testThrottlingQuotasDeleteTopic, testReceiveSimpleMessageTls, testTopicModificationOfReplicationFactor, testSendingMessagesToNonExistingTopic, testMoreReplicasThanAvailableBrokers, testCreateTopicViaKafka] to and randomly select one to start execution
2022-03-28 19:09:40 [ForkJoinPool-1-worker-33] DEBUG [SuiteThreadController:93] [operators.topic.ThrottlingQuotaST] - Removing parallel test: testThrottlingQuotasCreateAlterPartitions
2022-03-28 19:09:40 [ForkJoinPool-1-worker-33] DEBUG [SuiteThreadController:97] [operators.topic.ThrottlingQuotaST] - Parallel test count: 2
2022-03-28 19:09:40 [ForkJoinPool-1-worker-33] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.topic.ThrottlingQuotaST.testThrottlingQuotasCreateAlterPartitions-FINISHED
2022-03-28 19:09:40 [ForkJoinPool-1-worker-33] INFO  [TestSeparator:30] ############################################################################
2022-03-28 19:09:41 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (266672ms till timeout)
2022-03-28 19:09:41 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (101424ms till timeout)
2022-03-28 19:09:42 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (265387ms till timeout)
2022-03-28 19:09:42 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (100143ms till timeout)
2022-03-28 19:09:43 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (264105ms till timeout)
2022-03-28 19:09:43 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (98860ms till timeout)
2022-03-28 19:09:45 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (262822ms till timeout)
2022-03-28 19:09:45 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (97578ms till timeout)
2022-03-28 19:09:46 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (261539ms till timeout)
2022-03-28 19:09:46 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (96294ms till timeout)
2022-03-28 19:09:47 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (95011ms till timeout)
2022-03-28 19:09:47 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (260253ms till timeout)
2022-03-28 19:09:48 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (93910ms till timeout)
2022-03-28 19:09:49 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (258848ms till timeout)
2022-03-28 19:09:50 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (92813ms till timeout)
2022-03-28 19:09:50 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (257564ms till timeout)
2022-03-28 19:09:51 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (91715ms till timeout)
2022-03-28 19:09:51 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (256275ms till timeout)
2022-03-28 19:09:52 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (90616ms till timeout)
2022-03-28 19:09:53 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (254990ms till timeout)
2022-03-28 19:09:53 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (89518ms till timeout)
2022-03-28 19:09:54 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (253709ms till timeout)
2022-03-28 19:09:54 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (88418ms till timeout)
2022-03-28 19:09:55 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (252425ms till timeout)
2022-03-28 19:09:55 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (87181ms till timeout)
2022-03-28 19:09:56 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (251143ms till timeout)
2022-03-28 19:09:56 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (85899ms till timeout)
2022-03-28 19:09:58 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (249861ms till timeout)
2022-03-28 19:09:58 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (84617ms till timeout)
2022-03-28 19:09:59 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (248578ms till timeout)
2022-03-28 19:09:59 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (83334ms till timeout)
2022-03-28 19:10:00 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (247295ms till timeout)
2022-03-28 19:10:00 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (82050ms till timeout)
2022-03-28 19:10:02 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (246012ms till timeout)
2022-03-28 19:10:02 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (80768ms till timeout)
2022-03-28 19:10:03 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (244730ms till timeout)
2022-03-28 19:10:03 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (79485ms till timeout)
2022-03-28 19:10:04 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (243446ms till timeout)
2022-03-28 19:10:04 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (78202ms till timeout)
2022-03-28 19:10:05 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (242163ms till timeout)
2022-03-28 19:10:05 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (76918ms till timeout)
2022-03-28 19:10:07 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (240881ms till timeout)
2022-03-28 19:10:07 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (75636ms till timeout)
2022-03-28 19:10:08 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (239598ms till timeout)
2022-03-28 19:10:08 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (74353ms till timeout)
2022-03-28 19:10:09 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (238312ms till timeout)
2022-03-28 19:10:09 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (73068ms till timeout)
2022-03-28 19:10:10 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (71889ms till timeout)
2022-03-28 19:10:11 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (236948ms till timeout)
2022-03-28 19:10:12 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (70789ms till timeout)
2022-03-28 19:10:12 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (235665ms till timeout)
2022-03-28 19:10:13 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (69691ms till timeout)
2022-03-28 19:10:13 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (234383ms till timeout)
2022-03-28 19:10:14 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (68588ms till timeout)
2022-03-28 19:10:15 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (233101ms till timeout)
2022-03-28 19:10:15 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (67491ms till timeout)
2022-03-28 19:10:16 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (231818ms till timeout)
2022-03-28 19:10:16 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (66393ms till timeout)
2022-03-28 19:10:17 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (230535ms till timeout)
2022-03-28 19:10:17 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (65290ms till timeout)
2022-03-28 19:10:18 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (229247ms till timeout)
2022-03-28 19:10:18 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (64002ms till timeout)
2022-03-28 19:10:20 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (227964ms till timeout)
2022-03-28 19:10:20 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (62720ms till timeout)
2022-03-28 19:10:21 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (226682ms till timeout)
2022-03-28 19:10:21 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (61437ms till timeout)
2022-03-28 19:10:22 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (225390ms till timeout)
2022-03-28 19:10:22 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (60146ms till timeout)
2022-03-28 19:10:23 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (224108ms till timeout)
2022-03-28 19:10:23 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (58863ms till timeout)
2022-03-28 19:10:25 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (222826ms till timeout)
2022-03-28 19:10:25 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (57581ms till timeout)
2022-03-28 19:10:26 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (221543ms till timeout)
2022-03-28 19:10:26 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (56298ms till timeout)
2022-03-28 19:10:27 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (220260ms till timeout)
2022-03-28 19:10:27 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (55016ms till timeout)
2022-03-28 19:10:28 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (53917ms till timeout)
2022-03-28 19:10:29 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (218879ms till timeout)
2022-03-28 19:10:30 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (52819ms till timeout)
2022-03-28 19:10:30 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (217596ms till timeout)
2022-03-28 19:10:31 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (51720ms till timeout)
2022-03-28 19:10:31 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (216313ms till timeout)
2022-03-28 19:10:32 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (50621ms till timeout)
2022-03-28 19:10:33 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (215027ms till timeout)
2022-03-28 19:10:33 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (49522ms till timeout)
2022-03-28 19:10:34 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (213745ms till timeout)
2022-03-28 19:10:34 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (48423ms till timeout)
2022-03-28 19:10:35 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (212461ms till timeout)
2022-03-28 19:10:35 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (47216ms till timeout)
2022-03-28 19:10:36 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (211177ms till timeout)
2022-03-28 19:10:36 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (45933ms till timeout)
2022-03-28 19:10:38 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (209894ms till timeout)
2022-03-28 19:10:38 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (44650ms till timeout)
2022-03-28 19:10:39 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (208610ms till timeout)
2022-03-28 19:10:39 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (43365ms till timeout)
2022-03-28 19:10:40 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (207326ms till timeout)
2022-03-28 19:10:40 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (42082ms till timeout)
2022-03-28 19:10:42 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (206042ms till timeout)
2022-03-28 19:10:42 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (40798ms till timeout)
2022-03-28 19:10:43 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (204760ms till timeout)
2022-03-28 19:10:43 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (39516ms till timeout)
2022-03-28 19:10:44 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (203477ms till timeout)
2022-03-28 19:10:44 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (38233ms till timeout)
2022-03-28 19:10:45 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (202193ms till timeout)
2022-03-28 19:10:45 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (36949ms till timeout)
2022-03-28 19:10:47 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (200910ms till timeout)
2022-03-28 19:10:47 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (35665ms till timeout)
2022-03-28 19:10:48 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (199626ms till timeout)
2022-03-28 19:10:48 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (34382ms till timeout)
2022-03-28 19:10:49 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (198344ms till timeout)
2022-03-28 19:10:49 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (33099ms till timeout)
2022-03-28 19:10:51 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (197060ms till timeout)
2022-03-28 19:10:51 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (31815ms till timeout)
2022-03-28 19:10:52 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (195777ms till timeout)
2022-03-28 19:10:52 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (30532ms till timeout)
2022-03-28 19:10:53 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (194494ms till timeout)
2022-03-28 19:10:53 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (29250ms till timeout)
2022-03-28 19:10:54 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (193203ms till timeout)
2022-03-28 19:10:54 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (27959ms till timeout)
2022-03-28 19:10:56 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (191920ms till timeout)
2022-03-28 19:10:56 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (26675ms till timeout)
2022-03-28 19:10:57 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (190636ms till timeout)
2022-03-28 19:10:57 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (25392ms till timeout)
2022-03-28 19:10:58 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (189352ms till timeout)
2022-03-28 19:10:58 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (24108ms till timeout)
2022-03-28 19:11:00 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (188070ms till timeout)
2022-03-28 19:11:00 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (22826ms till timeout)
2022-03-28 19:11:01 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (186787ms till timeout)
2022-03-28 19:11:01 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (21542ms till timeout)
2022-03-28 19:11:02 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (185504ms till timeout)
2022-03-28 19:11:02 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (20259ms till timeout)
2022-03-28 19:11:03 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (184222ms till timeout)
2022-03-28 19:11:03 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (18978ms till timeout)
2022-03-28 19:11:05 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (182939ms till timeout)
2022-03-28 19:11:05 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (17695ms till timeout)
2022-03-28 19:11:06 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (181655ms till timeout)
2022-03-28 19:11:06 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (16410ms till timeout)
2022-03-28 19:11:07 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (180371ms till timeout)
2022-03-28 19:11:07 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (15126ms till timeout)
2022-03-28 19:11:09 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (179089ms till timeout)
2022-03-28 19:11:09 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (13844ms till timeout)
2022-03-28 19:11:10 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (177807ms till timeout)
2022-03-28 19:11:10 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (12562ms till timeout)
2022-03-28 19:11:11 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (176523ms till timeout)
2022-03-28 19:11:11 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (11278ms till timeout)
2022-03-28 19:11:12 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (175237ms till timeout)
2022-03-28 19:11:12 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (9992ms till timeout)
2022-03-28 19:11:14 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (173950ms till timeout)
2022-03-28 19:11:14 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (8705ms till timeout)
2022-03-28 19:11:15 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (172667ms till timeout)
2022-03-28 19:11:15 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (7422ms till timeout)
2022-03-28 19:11:16 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (171383ms till timeout)
2022-03-28 19:11:16 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (6138ms till timeout)
2022-03-28 19:11:17 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (5039ms till timeout)
2022-03-28 19:11:18 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (170002ms till timeout)
2022-03-28 19:11:18 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (3941ms till timeout)
2022-03-28 19:11:19 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (168719ms till timeout)
2022-03-28 19:11:20 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (2843ms till timeout)
2022-03-28 19:11:20 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (167433ms till timeout)
2022-03-28 19:11:21 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 1000 ms (1746ms till timeout)
2022-03-28 19:11:21 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (166148ms till timeout)
2022-03-28 19:11:22 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaRebalance: my-cluster-959c6ed5 will have desired state: PendingProposal not ready, will try again in 645 ms (645ms till timeout)
2022-03-28 19:11:23 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (164861ms till timeout)
2022-03-28 19:11:23 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:414] KafkaRebalance status:

Conditions:

Pods with conditions and messages:

my-cluster-959c6ed5-cruise-control-84cff755f9-h7w28:
	Type: <EMPTY>
	Message: <EMPTY>

	Type: <EMPTY>
	Message: <EMPTY>

	Type: <EMPTY>
	Message: <EMPTY>

	Type: <EMPTY>
	Message: <EMPTY>


my-cluster-959c6ed5-entity-operator-5f994dc47d-6m5zr:
	Type: <EMPTY>
	Message: <EMPTY>

	Type: <EMPTY>
	Message: <EMPTY>

	Type: <EMPTY>
	Message: <EMPTY>

	Type: <EMPTY>
	Message: <EMPTY>


my-cluster-959c6ed5-kafka-0:
	Type: <EMPTY>
	Message: <EMPTY>

	Type: <EMPTY>
	Message: <EMPTY>

	Type: <EMPTY>
	Message: <EMPTY>

	Type: <EMPTY>
	Message: <EMPTY>


my-cluster-959c6ed5-kafka-1:
	Type: <EMPTY>
	Message: <EMPTY>

	Type: <EMPTY>
	Message: <EMPTY>

	Type: <EMPTY>
	Message: <EMPTY>

	Type: <EMPTY>
	Message: <EMPTY>


my-cluster-959c6ed5-kafka-2:
	Type: <EMPTY>
	Message: <EMPTY>

	Type: <EMPTY>
	Message: <EMPTY>

	Type: <EMPTY>
	Message: <EMPTY>

	Type: <EMPTY>
	Message: <EMPTY>


my-cluster-959c6ed5-zookeeper-0:
	Type: <EMPTY>
	Message: <EMPTY>

	Type: <EMPTY>
	Message: <EMPTY>

	Type: <EMPTY>
	Message: <EMPTY>

	Type: <EMPTY>
	Message: <EMPTY>


my-cluster-959c6ed5-zookeeper-1:
	Type: <EMPTY>
	Message: <EMPTY>

	Type: <EMPTY>
	Message: <EMPTY>

	Type: <EMPTY>
	Message: <EMPTY>

	Type: <EMPTY>
	Message: <EMPTY>


my-cluster-959c6ed5-zookeeper-2:
	Type: <EMPTY>
	Message: <EMPTY>

	Type: <EMPTY>
	Message: <EMPTY>

	Type: <EMPTY>
	Message: <EMPTY>

	Type: <EMPTY>
	Message: <EMPTY>
2022-03-28 19:11:23 [ForkJoinPool-1-worker-7] ERROR [TestExecutionWatcher:28] ReconciliationST - Exception Timeout after 180000 ms waiting for Resource condition: readiness is fulfilled for resource KafkaRebalance:my-cluster-959c6ed5 has been thrown in @Test. Going to collect logs from components.
2022-03-28 19:11:23 [ForkJoinPool-1-worker-7] DEBUG [LogCollector:158] [reconciliation-st] adding to all namespaces, which should be collected: [infra-namespace, reconciliation-st, namespace-0]
2022-03-28 19:11:23 [ForkJoinPool-1-worker-7] INFO  [LogCollector:252] Collecting events in Namespace infra-namespace
2022-03-28 19:11:23 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace infra-namespace get events
2022-03-28 19:11:24 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (163760ms till timeout)
2022-03-28 19:11:24 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace infra-namespace get events
2022-03-28 19:11:24 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 19:11:24 [ForkJoinPool-1-worker-7] INFO  [LogCollector:259] Collecting ConfigMaps in Namespace infra-namespace
2022-03-28 19:11:24 [ForkJoinPool-1-worker-7] INFO  [LogCollector:217] Collecting logs for Pod(s) in Namespace infra-namespace
2022-03-28 19:11:25 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (162539ms till timeout)
2022-03-28 19:11:25 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace infra-namespace describe pod strimzi-cluster-operator-5dc4cd9447-4npc2
2022-03-28 19:11:26 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace infra-namespace describe pod strimzi-cluster-operator-5dc4cd9447-4npc2
2022-03-28 19:11:26 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 19:11:26 [ForkJoinPool-1-worker-7] INFO  [LogCollector:266] Collecting Deployments in Namespace infra-namespace
2022-03-28 19:11:26 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace infra-namespace get Deployment -o yaml
2022-03-28 19:11:26 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (161394ms till timeout)
2022-03-28 19:11:27 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace infra-namespace get Deployment -o yaml
2022-03-28 19:11:27 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 19:11:27 [ForkJoinPool-1-worker-7] INFO  [LogCollector:271] Collecting StatefulSets in Namespace infra-namespace
2022-03-28 19:11:27 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace infra-namespace get StatefulSet -o yaml
2022-03-28 19:11:27 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace infra-namespace get StatefulSet -o yaml
2022-03-28 19:11:27 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 19:11:27 [ForkJoinPool-1-worker-7] INFO  [LogCollector:276] Collecting ReplicaSets in Namespace infra-namespace
2022-03-28 19:11:27 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace infra-namespace get replicaset -o yaml
2022-03-28 19:11:27 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (160203ms till timeout)
2022-03-28 19:11:28 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace infra-namespace get replicaset -o yaml
2022-03-28 19:11:28 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 19:11:28 [ForkJoinPool-1-worker-7] INFO  [LogCollector:281] Collecting Strimzi in Namespace infra-namespace
2022-03-28 19:11:28 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc get strimzi -o yaml -n infra-namespace
2022-03-28 19:11:29 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (158920ms till timeout)
2022-03-28 19:11:30 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (157638ms till timeout)
2022-03-28 19:11:30 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc get strimzi -o yaml -n infra-namespace
2022-03-28 19:11:30 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 19:11:30 [ForkJoinPool-1-worker-7] INFO  [LogCollector:287] Collecting cluster status
2022-03-28 19:11:30 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc describe nodes
2022-03-28 19:11:31 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (156355ms till timeout)
2022-03-28 19:11:33 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (155072ms till timeout)
2022-03-28 19:11:34 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc describe nodes
2022-03-28 19:11:34 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 19:11:34 [ForkJoinPool-1-worker-7] INFO  [LogCollector:252] Collecting events in Namespace reconciliation-st
2022-03-28 19:11:34 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace reconciliation-st get events
2022-03-28 19:11:34 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (153790ms till timeout)
2022-03-28 19:11:34 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace reconciliation-st get events
2022-03-28 19:11:34 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 19:11:34 [ForkJoinPool-1-worker-7] INFO  [LogCollector:259] Collecting ConfigMaps in Namespace reconciliation-st
2022-03-28 19:11:34 [ForkJoinPool-1-worker-7] INFO  [LogCollector:217] Collecting logs for Pod(s) in Namespace reconciliation-st
2022-03-28 19:11:34 [ForkJoinPool-1-worker-7] INFO  [LogCollector:266] Collecting Deployments in Namespace reconciliation-st
2022-03-28 19:11:34 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace reconciliation-st get Deployment -o yaml
2022-03-28 19:11:35 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace reconciliation-st get Deployment -o yaml
2022-03-28 19:11:35 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 19:11:35 [ForkJoinPool-1-worker-7] INFO  [LogCollector:271] Collecting StatefulSets in Namespace reconciliation-st
2022-03-28 19:11:35 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace reconciliation-st get StatefulSet -o yaml
2022-03-28 19:11:35 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (152507ms till timeout)
2022-03-28 19:11:36 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (151225ms till timeout)
2022-03-28 19:11:38 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (149940ms till timeout)
2022-03-28 19:11:39 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (148657ms till timeout)
2022-03-28 19:11:40 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (147372ms till timeout)
2022-03-28 19:11:40 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace reconciliation-st get StatefulSet -o yaml
2022-03-28 19:11:40 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 19:11:40 [ForkJoinPool-1-worker-7] INFO  [LogCollector:276] Collecting ReplicaSets in Namespace reconciliation-st
2022-03-28 19:11:40 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace reconciliation-st get replicaset -o yaml
2022-03-28 19:11:41 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace reconciliation-st get replicaset -o yaml
2022-03-28 19:11:41 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 19:11:41 [ForkJoinPool-1-worker-7] INFO  [LogCollector:281] Collecting Strimzi in Namespace reconciliation-st
2022-03-28 19:11:41 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc get strimzi -o yaml -n reconciliation-st
2022-03-28 19:11:42 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (146089ms till timeout)
2022-03-28 19:11:43 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (144807ms till timeout)
2022-03-28 19:11:44 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc get strimzi -o yaml -n reconciliation-st
2022-03-28 19:11:44 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 19:11:44 [ForkJoinPool-1-worker-7] INFO  [LogCollector:287] Collecting cluster status
2022-03-28 19:11:44 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc describe nodes
2022-03-28 19:11:44 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (143524ms till timeout)
2022-03-28 19:11:45 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (142234ms till timeout)
2022-03-28 19:11:47 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (140949ms till timeout)
2022-03-28 19:11:48 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (139664ms till timeout)
2022-03-28 19:11:49 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (138382ms till timeout)
2022-03-28 19:11:51 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (137097ms till timeout)
2022-03-28 19:11:52 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (135814ms till timeout)
2022-03-28 19:11:53 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc describe nodes
2022-03-28 19:11:53 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 19:11:53 [ForkJoinPool-1-worker-7] INFO  [LogCollector:252] Collecting events in Namespace namespace-0
2022-03-28 19:11:53 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-0 get events
2022-03-28 19:11:53 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (134531ms till timeout)
2022-03-28 19:11:53 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-0 get events
2022-03-28 19:11:53 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 19:11:53 [ForkJoinPool-1-worker-7] INFO  [LogCollector:259] Collecting ConfigMaps in Namespace namespace-0
2022-03-28 19:11:53 [ForkJoinPool-1-worker-7] INFO  [LogCollector:217] Collecting logs for Pod(s) in Namespace namespace-0
2022-03-28 19:11:54 [ForkJoinPool-1-worker-7] DEBUG [LogCollector:208] Collecting logs for TestCase: testPauseReconciliationInKafkaRebalanceAndTopic, and Pod: my-cluster-959c6ed5-cruise-control-84cff755f9-h7w28
2022-03-28 19:11:54 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-0 describe pod my-cluster-959c6ed5-cruise-control-84cff755f9-h7w28
2022-03-28 19:11:54 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (133429ms till timeout)
2022-03-28 19:11:55 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (132327ms till timeout)
2022-03-28 19:11:57 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (131046ms till timeout)
2022-03-28 19:11:58 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (129763ms till timeout)
2022-03-28 19:11:59 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (128481ms till timeout)
2022-03-28 19:12:00 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-0 describe pod my-cluster-959c6ed5-cruise-control-84cff755f9-h7w28
2022-03-28 19:12:00 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 19:12:00 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-0 describe pod my-cluster-959c6ed5-cruise-control-84cff755f9-h7w28
2022-03-28 19:12:00 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (127200ms till timeout)
2022-03-28 19:12:01 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-0 describe pod my-cluster-959c6ed5-cruise-control-84cff755f9-h7w28
2022-03-28 19:12:01 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 19:12:01 [ForkJoinPool-1-worker-7] DEBUG [LogCollector:208] Collecting logs for TestCase: testPauseReconciliationInKafkaRebalanceAndTopic, and Pod: my-cluster-959c6ed5-entity-operator-5f994dc47d-6m5zr
2022-03-28 19:12:01 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-0 describe pod my-cluster-959c6ed5-entity-operator-5f994dc47d-6m5zr
2022-03-28 19:12:02 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (125917ms till timeout)
2022-03-28 19:12:02 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-0 describe pod my-cluster-959c6ed5-entity-operator-5f994dc47d-6m5zr
2022-03-28 19:12:02 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 19:12:02 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-0 describe pod my-cluster-959c6ed5-entity-operator-5f994dc47d-6m5zr
2022-03-28 19:12:03 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (124813ms till timeout)
2022-03-28 19:12:03 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-0 describe pod my-cluster-959c6ed5-entity-operator-5f994dc47d-6m5zr
2022-03-28 19:12:03 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 19:12:03 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-0 describe pod my-cluster-959c6ed5-entity-operator-5f994dc47d-6m5zr
2022-03-28 19:12:04 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-0 describe pod my-cluster-959c6ed5-entity-operator-5f994dc47d-6m5zr
2022-03-28 19:12:04 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 19:12:04 [ForkJoinPool-1-worker-7] DEBUG [LogCollector:208] Collecting logs for TestCase: testPauseReconciliationInKafkaRebalanceAndTopic, and Pod: my-cluster-959c6ed5-kafka-0
2022-03-28 19:12:04 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (123621ms till timeout)
2022-03-28 19:12:04 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-0 describe pod my-cluster-959c6ed5-kafka-0
2022-03-28 19:12:05 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (122520ms till timeout)
2022-03-28 19:12:05 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-0 describe pod my-cluster-959c6ed5-kafka-0
2022-03-28 19:12:05 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 19:12:05 [ForkJoinPool-1-worker-7] DEBUG [LogCollector:208] Collecting logs for TestCase: testPauseReconciliationInKafkaRebalanceAndTopic, and Pod: my-cluster-959c6ed5-kafka-1
2022-03-28 19:12:05 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-0 describe pod my-cluster-959c6ed5-kafka-1
2022-03-28 19:12:06 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (121419ms till timeout)
2022-03-28 19:12:06 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-0 describe pod my-cluster-959c6ed5-kafka-1
2022-03-28 19:12:06 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 19:12:06 [ForkJoinPool-1-worker-7] DEBUG [LogCollector:208] Collecting logs for TestCase: testPauseReconciliationInKafkaRebalanceAndTopic, and Pod: my-cluster-959c6ed5-kafka-2
2022-03-28 19:12:06 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-0 describe pod my-cluster-959c6ed5-kafka-2
2022-03-28 19:12:07 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (120317ms till timeout)
2022-03-28 19:12:07 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-0 describe pod my-cluster-959c6ed5-kafka-2
2022-03-28 19:12:07 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 19:12:07 [ForkJoinPool-1-worker-7] DEBUG [LogCollector:208] Collecting logs for TestCase: testPauseReconciliationInKafkaRebalanceAndTopic, and Pod: my-cluster-959c6ed5-zookeeper-0
2022-03-28 19:12:08 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-0 describe pod my-cluster-959c6ed5-zookeeper-0
2022-03-28 19:12:08 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (119215ms till timeout)
2022-03-28 19:12:08 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-0 describe pod my-cluster-959c6ed5-zookeeper-0
2022-03-28 19:12:08 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 19:12:08 [ForkJoinPool-1-worker-7] DEBUG [LogCollector:208] Collecting logs for TestCase: testPauseReconciliationInKafkaRebalanceAndTopic, and Pod: my-cluster-959c6ed5-zookeeper-1
2022-03-28 19:12:09 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-0 describe pod my-cluster-959c6ed5-zookeeper-1
2022-03-28 19:12:09 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-0 describe pod my-cluster-959c6ed5-zookeeper-1
2022-03-28 19:12:09 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 19:12:09 [ForkJoinPool-1-worker-7] DEBUG [LogCollector:208] Collecting logs for TestCase: testPauseReconciliationInKafkaRebalanceAndTopic, and Pod: my-cluster-959c6ed5-zookeeper-2
2022-03-28 19:12:09 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (118114ms till timeout)
2022-03-28 19:12:10 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-0 describe pod my-cluster-959c6ed5-zookeeper-2
2022-03-28 19:12:10 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-0 describe pod my-cluster-959c6ed5-zookeeper-2
2022-03-28 19:12:10 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 19:12:10 [ForkJoinPool-1-worker-7] INFO  [LogCollector:266] Collecting Deployments in Namespace namespace-0
2022-03-28 19:12:10 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-0 get Deployment -o yaml
2022-03-28 19:12:11 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (117012ms till timeout)
2022-03-28 19:12:11 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-0 get Deployment -o yaml
2022-03-28 19:12:11 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 19:12:11 [ForkJoinPool-1-worker-7] INFO  [LogCollector:271] Collecting StatefulSets in Namespace namespace-0
2022-03-28 19:12:11 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-0 get StatefulSet -o yaml
2022-03-28 19:12:12 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-0 get StatefulSet -o yaml
2022-03-28 19:12:12 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 19:12:12 [ForkJoinPool-1-worker-7] INFO  [LogCollector:276] Collecting ReplicaSets in Namespace namespace-0
2022-03-28 19:12:12 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-0 get replicaset -o yaml
2022-03-28 19:12:12 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (115822ms till timeout)
2022-03-28 19:12:12 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-0 get replicaset -o yaml
2022-03-28 19:12:12 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 19:12:12 [ForkJoinPool-1-worker-7] INFO  [LogCollector:281] Collecting Strimzi in Namespace namespace-0
2022-03-28 19:12:12 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc get strimzi -o yaml -n namespace-0
2022-03-28 19:12:13 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (114504ms till timeout)
2022-03-28 19:12:14 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc get strimzi -o yaml -n namespace-0
2022-03-28 19:12:14 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 19:12:14 [ForkJoinPool-1-worker-7] INFO  [LogCollector:287] Collecting cluster status
2022-03-28 19:12:14 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc describe nodes
2022-03-28 19:12:14 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (113219ms till timeout)
2022-03-28 19:12:16 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (111937ms till timeout)
2022-03-28 19:12:17 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (110641ms till timeout)
2022-03-28 19:12:18 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc describe nodes
2022-03-28 19:12:18 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 19:12:18 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:674] ============================================================================
2022-03-28 19:12:18 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:675] [operators.ReconciliationST - After Each] - Clean up after test
2022-03-28 19:12:18 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:344] ############################################################################
2022-03-28 19:12:18 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:348] Delete all resources for testPauseReconciliationInKafkaRebalanceAndTopic
2022-03-28 19:12:18 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:241] Delete of KafkaTopic my-topic-634125597-1746493046 in namespace namespace-0
2022-03-28 19:12:18 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:241] Delete of KafkaRebalance my-cluster-959c6ed5 in namespace namespace-0
2022-03-28 19:12:18 [ForkJoinPool-1-worker-33] INFO  [ResourceManager:241] Delete of Kafka my-cluster-959c6ed5 in namespace namespace-0
2022-03-28 19:12:18 [ForkJoinPool-1-worker-33] INFO  [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-0, for cruise control Kafka cluster my-cluster-959c6ed5
2022-03-28 19:12:18 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-634125597-1746493046
2022-03-28 19:12:18 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaRebalance:my-cluster-959c6ed5
2022-03-28 19:12:18 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (109392ms till timeout)
2022-03-28 19:12:18 [ForkJoinPool-1-worker-33] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-959c6ed5
2022-03-28 19:12:19 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:369] ############################################################################
2022-03-28 19:12:19 [ForkJoinPool-1-worker-7] INFO  [TestSuiteNamespaceManager:200] Deleting namespace:namespace-0 for test case:testPauseReconciliationInKafkaRebalanceAndTopic
2022-03-28 19:12:19 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Namespace namespace-0 removal
2022-03-28 19:12:19 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-0 -o yaml
2022-03-28 19:12:19 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-0 -o yaml
2022-03-28 19:12:19 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 19:12:19 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (479520ms till timeout)
2022-03-28 19:12:19 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (108200ms till timeout)
2022-03-28 19:12:20 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-0 -o yaml
2022-03-28 19:12:21 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-0 -o yaml
2022-03-28 19:12:21 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 19:12:21 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (478075ms till timeout)
2022-03-28 19:12:21 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (106917ms till timeout)
2022-03-28 19:12:22 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-0 -o yaml
2022-03-28 19:12:22 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (105634ms till timeout)
2022-03-28 19:12:22 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-0 -o yaml
2022-03-28 19:12:22 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 19:12:22 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (476616ms till timeout)
2022-03-28 19:12:23 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-0 -o yaml
2022-03-28 19:12:23 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (104352ms till timeout)
2022-03-28 19:12:24 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-0 -o yaml
2022-03-28 19:12:24 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 19:12:24 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (475186ms till timeout)
2022-03-28 19:12:25 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (103069ms till timeout)
2022-03-28 19:12:25 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-0 -o yaml
2022-03-28 19:12:25 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-0 -o yaml
2022-03-28 19:12:25 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 19:12:25 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (473725ms till timeout)
2022-03-28 19:12:26 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (101787ms till timeout)
2022-03-28 19:12:26 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-0 -o yaml
2022-03-28 19:12:26 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-0 -o yaml
2022-03-28 19:12:26 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 19:12:26 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (472292ms till timeout)
2022-03-28 19:12:27 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (100505ms till timeout)
2022-03-28 19:12:27 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-0 -o yaml
2022-03-28 19:12:28 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-0 -o yaml
2022-03-28 19:12:28 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 19:12:28 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (470857ms till timeout)
2022-03-28 19:12:28 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (99223ms till timeout)
2022-03-28 19:12:29 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-0 -o yaml
2022-03-28 19:12:29 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-0 -o yaml
2022-03-28 19:12:29 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 19:12:29 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (469413ms till timeout)
2022-03-28 19:12:30 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (97940ms till timeout)
2022-03-28 19:12:30 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-0 -o yaml
2022-03-28 19:12:31 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-0 -o yaml
2022-03-28 19:12:31 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 19:12:31 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (467989ms till timeout)
2022-03-28 19:12:31 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (96657ms till timeout)
2022-03-28 19:12:32 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-0 -o yaml
2022-03-28 19:12:32 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-0 -o yaml
2022-03-28 19:12:32 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 19:12:32 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (466530ms till timeout)
2022-03-28 19:12:32 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (95365ms till timeout)
2022-03-28 19:12:33 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-0 -o yaml
2022-03-28 19:12:34 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (94082ms till timeout)
2022-03-28 19:12:34 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-0 -o yaml
2022-03-28 19:12:34 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 19:12:34 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (465063ms till timeout)
2022-03-28 19:12:35 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-0 -o yaml
2022-03-28 19:12:35 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (92791ms till timeout)
2022-03-28 19:12:35 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-0 -o yaml
2022-03-28 19:12:35 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 19:12:35 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (463598ms till timeout)
2022-03-28 19:12:36 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (91508ms till timeout)
2022-03-28 19:12:36 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-0 -o yaml
2022-03-28 19:12:37 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-0 -o yaml
2022-03-28 19:12:37 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 19:12:37 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (462146ms till timeout)
2022-03-28 19:12:37 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (90223ms till timeout)
2022-03-28 19:12:38 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-0 -o yaml
2022-03-28 19:12:38 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-0 -o yaml
2022-03-28 19:12:38 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 19:12:38 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (460693ms till timeout)
2022-03-28 19:12:39 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (88940ms till timeout)
2022-03-28 19:12:39 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-0 -o yaml
2022-03-28 19:12:39 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-0 -o yaml
2022-03-28 19:12:39 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 19:12:39 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (459274ms till timeout)
2022-03-28 19:12:40 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (87659ms till timeout)
2022-03-28 19:12:40 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-0 -o yaml
2022-03-28 19:12:41 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-0 -o yaml
2022-03-28 19:12:41 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 19:12:41 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (457837ms till timeout)
2022-03-28 19:12:41 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (86375ms till timeout)
2022-03-28 19:12:42 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-0 -o yaml
2022-03-28 19:12:42 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-0 -o yaml
2022-03-28 19:12:42 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 19:12:42 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (456388ms till timeout)
2022-03-28 19:12:43 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (85093ms till timeout)
2022-03-28 19:12:43 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-0 -o yaml
2022-03-28 19:12:44 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (83811ms till timeout)
2022-03-28 19:12:44 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-0 -o yaml
2022-03-28 19:12:44 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 19:12:44 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (454927ms till timeout)
2022-03-28 19:12:45 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-0 -o yaml
2022-03-28 19:12:45 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (82528ms till timeout)
2022-03-28 19:12:45 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-0 -o yaml
2022-03-28 19:12:45 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 19:12:45 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (453437ms till timeout)
2022-03-28 19:12:46 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-0 -o yaml
2022-03-28 19:12:46 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (81245ms till timeout)
2022-03-28 19:12:47 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-0 -o yaml
2022-03-28 19:12:47 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 19:12:47 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (451982ms till timeout)
2022-03-28 19:12:48 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (79962ms till timeout)
2022-03-28 19:12:48 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-0 -o yaml
2022-03-28 19:12:48 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-0 -o yaml
2022-03-28 19:12:48 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 19:12:48 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (450503ms till timeout)
2022-03-28 19:12:49 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (78679ms till timeout)
2022-03-28 19:12:49 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-0 -o yaml
2022-03-28 19:12:50 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-0 -o yaml
2022-03-28 19:12:50 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 19:12:50 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (449048ms till timeout)
2022-03-28 19:12:50 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (77384ms till timeout)
2022-03-28 19:12:51 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-0 -o yaml
2022-03-28 19:12:51 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-0 -o yaml
2022-03-28 19:12:51 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 19:12:51 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (447606ms till timeout)
2022-03-28 19:12:52 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (76101ms till timeout)
2022-03-28 19:12:52 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-0 -o yaml
2022-03-28 19:12:53 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-0 -o yaml
2022-03-28 19:12:53 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 19:12:53 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (446068ms till timeout)
2022-03-28 19:12:53 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (74818ms till timeout)
2022-03-28 19:12:54 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-0 -o yaml
2022-03-28 19:12:54 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (73534ms till timeout)
2022-03-28 19:12:54 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-9 get Namespace namespace-0 -o yaml
2022-03-28 19:12:54 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 1
2022-03-28 19:12:54 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 19:12:54 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Error from server (NotFound): namespaces "namespace-0" not found
2022-03-28 19:12:54 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] ======STDERR END======
2022-03-28 19:12:54 [ForkJoinPool-1-worker-7] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[], io.strimzi.test.logs.CollectorElement@c4c0ea0=[], io.strimzi.test.logs.CollectorElement@aec142ef=[], io.strimzi.test.logs.CollectorElement@24b97ba9=[], io.strimzi.test.logs.CollectorElement@f851b6c3=[], io.strimzi.test.logs.CollectorElement@3881d5f2=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@e3067dd1=[], io.strimzi.test.logs.CollectorElement@5c7379cb=[], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[infra-namespace, reconciliation-st], io.strimzi.test.logs.CollectorElement@f4405b3b=[], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 19:12:54 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:267] testPauseReconciliationInKafkaRebalanceAndTopic - Notifies waiting test cases:[testPauseReconciliationInKafkaRebalanceAndTopic, testPauseReconciliationInKafkaAndKafkaConnectWithConnector, testCapacityFile, testConfigurationFileIsCreated, testConfigurationPerformanceOptions, testConfigurationReflection, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testDeployAndUnDeployCruiseControl, testTlsExternalUserWithQuotas, testCreatingUsersWithSecretPrefix, testThrottlingQuotasCreateTopic, testThrottlingQuotasCreateAlterPartitions, testKafkaAdminTopicOperations, testUserTemplate, testUpdateUser, testTlsExternalUser, testSendSimpleMessageTls, testReceiveSimpleMessageTlsScramSha, testThrottlingQuotasDeleteTopic, testReceiveSimpleMessageTls, testTopicModificationOfReplicationFactor, testSendingMessagesToNonExistingTopic, testMoreReplicasThanAvailableBrokers, testCreateTopicViaKafka] to and randomly select one to start execution
2022-03-28 19:12:54 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:93] [operators.ReconciliationST] - Removing parallel test: testPauseReconciliationInKafkaRebalanceAndTopic
2022-03-28 19:12:54 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:97] [operators.ReconciliationST] - Parallel test count: 1
2022-03-28 19:12:54 [ForkJoinPool-1-worker-7] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.ReconciliationST.testPauseReconciliationInKafkaRebalanceAndTopic-FINISHED
2022-03-28 19:12:54 [ForkJoinPool-1-worker-7] INFO  [TestSeparator:30] ############################################################################
2022-03-28 19:12:54 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:689] ============================================================================
2022-03-28 19:12:54 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:690] [operators.ReconciliationST - After All] - Clean up after test suite
2022-03-28 19:12:54 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:344] ############################################################################
2022-03-28 19:12:54 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:346] In context ReconciliationST is everything deleted.
2022-03-28 19:12:54 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:369] ############################################################################
2022-03-28 19:12:54 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Namespace reconciliation-st removal
2022-03-28 19:12:54 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace reconciliation-st -o yaml
2022-03-28 19:12:55 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (72251ms till timeout)
2022-03-28 19:12:57 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (70969ms till timeout)
2022-03-28 19:12:58 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (69686ms till timeout)
2022-03-28 19:12:59 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (68402ms till timeout)
2022-03-28 19:13:00 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace reconciliation-st -o yaml
2022-03-28 19:13:00 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 19:13:00 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace reconciliation-st removal not ready, will try again in 1000 ms (474471ms till timeout)
2022-03-28 19:13:00 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (67118ms till timeout)
2022-03-28 19:13:01 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace reconciliation-st -o yaml
2022-03-28 19:13:01 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-9 get Namespace reconciliation-st -o yaml
2022-03-28 19:13:01 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 1
2022-03-28 19:13:01 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 19:13:01 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Error from server (NotFound): namespaces "reconciliation-st" not found
2022-03-28 19:13:01 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] ======STDERR END======
2022-03-28 19:13:01 [ForkJoinPool-1-worker-13] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[], io.strimzi.test.logs.CollectorElement@c4c0ea0=[], io.strimzi.test.logs.CollectorElement@aec142ef=[], io.strimzi.test.logs.CollectorElement@24b97ba9=[], io.strimzi.test.logs.CollectorElement@f851b6c3=[], io.strimzi.test.logs.CollectorElement@3881d5f2=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@e3067dd1=[], io.strimzi.test.logs.CollectorElement@5c7379cb=[], io.strimzi.test.logs.CollectorElement@62b9e483=[], io.strimzi.test.logs.CollectorElement@b850ef2a=[infra-namespace, reconciliation-st], io.strimzi.test.logs.CollectorElement@f4405b3b=[], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 19:13:01 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:254] ReconciliationST - Notifies waiting test suites:[TopicST, ThrottlingQuotaST, UserST, HttpBridgeTlsST, HttpBridgeScramShaST, ReconciliationST, CruiseControlConfigurationST] to and randomly select one to start execution
2022-03-28 19:13:01 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:85] [operators.ReconciliationST] - Removing parallel suite: ReconciliationST
2022-03-28 19:13:01 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:89] [operators.ReconciliationST] - Parallel suites count: 2
[ERROR] Tests run: 2, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 1,077.376 s <<< FAILURE! - in io.strimzi.systemtest.operators.ReconciliationST
[ERROR] io.strimzi.systemtest.operators.ReconciliationST.testPauseReconciliationInKafkaRebalanceAndTopic(ExtensionContext)  Time elapsed: 1,009.616 s  <<< ERROR!
io.strimzi.test.WaitException: Timeout after 180000 ms waiting for Resource condition: readiness is fulfilled for resource KafkaRebalance:my-cluster-959c6ed5
	at io.strimzi.test.TestUtils.waitFor(TestUtils.java:170)
	at io.strimzi.test.TestUtils.waitFor(TestUtils.java:121)
	at io.strimzi.systemtest.resources.ResourceManager.waitResourceCondition(ResourceManager.java:264)
	at io.strimzi.systemtest.resources.ResourceManager.createResource(ResourceManager.java:226)
	at io.strimzi.systemtest.resources.ResourceManager.createResource(ResourceManager.java:148)
	at io.strimzi.systemtest.operators.ReconciliationST.testPauseReconciliationInKafkaRebalanceAndTopic(ReconciliationST.java:162)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)

2022-03-28 19:13:02 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (65835ms till timeout)
2022-03-28 19:13:03 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (64551ms till timeout)
2022-03-28 19:13:04 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (63268ms till timeout)
2022-03-28 19:13:06 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (61986ms till timeout)
2022-03-28 19:13:07 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (60701ms till timeout)
2022-03-28 19:13:08 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (59418ms till timeout)
2022-03-28 19:13:09 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (58136ms till timeout)
2022-03-28 19:13:11 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (56850ms till timeout)
2022-03-28 19:13:12 [ForkJoinPool-1-worker-41] INFO  [PodUtils:189] Message org.apache.kafka.common.errors.ThrottlingQuotaExceededException: The throttling quota has been exceeded. found in delete-admin-my-cluster-836b31f8-kafka-clients-bxdsd log
2022-03-28 19:13:12 [ForkJoinPool-1-worker-41] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment delete-admin-my-cluster-836b31f8-kafka-clients deletion
2022-03-28 19:13:12 [ForkJoinPool-1-worker-41] DEBUG [TestUtils:125] Waiting for ReplicaSet delete-admin-my-cluster-836b31f8-kafka-clients to be deleted
2022-03-28 19:13:12 [ForkJoinPool-1-worker-41] DEBUG [JobUtils:40] Job delete-admin-my-cluster-836b31f8-kafka-clients was deleted
2022-03-28 19:13:12 [ForkJoinPool-1-worker-41] INFO  [ThrottlingQuotaST:144] Executing 1/5 iteration for delete-admin-my-cluster-836b31f8-kafka-clients.
2022-03-28 19:13:12 [ForkJoinPool-1-worker-41] INFO  [ResourceManager:155] Create/Update Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st
2022-03-28 19:13:12 [ForkJoinPool-1-worker-41] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:delete-admin-my-cluster-836b31f8-kafka-clients
2022-03-28 19:13:12 [ForkJoinPool-1-worker-41] INFO  [JobUtils:81] Waiting for job: delete-admin-my-cluster-836b31f8-kafka-clients will be in active state
2022-03-28 19:13:12 [ForkJoinPool-1-worker-41] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 19:13:13 [ForkJoinPool-1-worker-41] INFO  [ClientUtils:76] Waiting for producer/consumer:delete-admin-my-cluster-836b31f8-kafka-clients to finished
2022-03-28 19:13:13 [ForkJoinPool-1-worker-41] DEBUG [TestUtils:125] Waiting for job finished
2022-03-28 19:13:13 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:13:08Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:13:13 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (129809ms till timeout)
2022-03-28 19:13:14 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:13:08Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:13:14 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (128615ms till timeout)
2022-03-28 19:13:15 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:13:08Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:13:15 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (127422ms till timeout)
2022-03-28 19:13:16 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:13:08Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:13:16 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (126228ms till timeout)
2022-03-28 19:13:17 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:13:08Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:13:18 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (125024ms till timeout)
2022-03-28 19:13:19 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:13:08Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:13:19 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (123830ms till timeout)
2022-03-28 19:13:20 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:13:08Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:13:20 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (122637ms till timeout)
2022-03-28 19:13:21 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:13:08Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:13:21 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (121443ms till timeout)
2022-03-28 19:13:22 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:13:08Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:13:22 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (120249ms till timeout)
2022-03-28 19:13:23 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:13:08Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:13:23 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (119055ms till timeout)
2022-03-28 19:13:25 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:13:08Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:13:25 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (117861ms till timeout)
2022-03-28 19:13:26 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:13:08Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:13:26 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (116669ms till timeout)
2022-03-28 19:13:27 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:13:08Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:13:27 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (115473ms till timeout)
2022-03-28 19:13:28 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:13:08Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:13:28 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (114277ms till timeout)
2022-03-28 19:13:29 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:13:08Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:13:29 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (113081ms till timeout)
2022-03-28 19:13:31 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:13:08Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:13:31 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (111889ms till timeout)
2022-03-28 19:13:32 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:13:08Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:13:32 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (110664ms till timeout)
2022-03-28 19:13:33 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:13:08Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:13:33 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (109470ms till timeout)
2022-03-28 19:13:34 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:13:08Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:13:34 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (108276ms till timeout)
2022-03-28 19:13:35 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:13:08Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:13:35 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (107081ms till timeout)
2022-03-28 19:13:37 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:13:08Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:13:37 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (105887ms till timeout)
2022-03-28 19:13:38 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:13:08Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:13:38 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (104693ms till timeout)
2022-03-28 19:13:39 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:13:08Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:13:39 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (103500ms till timeout)
2022-03-28 19:13:40 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:13:08Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:13:40 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (102308ms till timeout)
2022-03-28 19:13:41 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:13:08Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:13:41 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (101111ms till timeout)
2022-03-28 19:13:43 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:13:08Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:13:43 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (99917ms till timeout)
2022-03-28 19:13:44 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:13:08Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:13:44 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (98723ms till timeout)
2022-03-28 19:13:45 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:13:08Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:13:45 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (97529ms till timeout)
2022-03-28 19:13:46 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:13:08Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:13:46 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (96331ms till timeout)
2022-03-28 19:13:47 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:13:08Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:13:47 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (95137ms till timeout)
2022-03-28 19:13:48 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:13:08Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:13:49 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (93943ms till timeout)
2022-03-28 19:13:50 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:13:08Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:13:50 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (92749ms till timeout)
2022-03-28 19:13:51 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:13:08Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:13:51 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (91555ms till timeout)
2022-03-28 19:13:52 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:13:08Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:13:52 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (90362ms till timeout)
2022-03-28 19:13:53 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:13:08Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:13:53 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (89167ms till timeout)
2022-03-28 19:13:54 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:13:08Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:13:55 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (87974ms till timeout)
2022-03-28 19:13:56 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:13:08Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:13:56 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (86781ms till timeout)
2022-03-28 19:13:57 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:13:08Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:13:57 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (85588ms till timeout)
2022-03-28 19:13:58 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:13:08Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:13:58 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (84394ms till timeout)
2022-03-28 19:13:59 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:13:08Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:13:59 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (83200ms till timeout)
2022-03-28 19:14:00 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:13:08Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:14:01 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (82007ms till timeout)
2022-03-28 19:14:02 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:13:08Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:14:02 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (80812ms till timeout)
2022-03-28 19:14:03 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:13:08Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:14:03 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (79616ms till timeout)
2022-03-28 19:14:04 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:13:08Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:14:04 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (78420ms till timeout)
2022-03-28 19:14:05 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:13:08Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:14:05 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (77227ms till timeout)
2022-03-28 19:14:06 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:13:08Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:14:07 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (76031ms till timeout)
2022-03-28 19:14:08 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:13:08Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:14:08 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (74836ms till timeout)
2022-03-28 19:14:09 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:13:08Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:14:09 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (73642ms till timeout)
2022-03-28 19:14:10 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:13:08Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:14:10 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (72448ms till timeout)
2022-03-28 19:14:11 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:13:08Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:14:11 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (71255ms till timeout)
2022-03-28 19:14:12 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:13:08Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:14:12 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (70061ms till timeout)
2022-03-28 19:14:14 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:13:08Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:14:14 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (68866ms till timeout)
2022-03-28 19:14:15 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:13:08Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:14:15 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (67671ms till timeout)
2022-03-28 19:14:16 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:13:08Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:14:16 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (66478ms till timeout)
2022-03-28 19:14:17 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:13:08Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:14:17 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (65284ms till timeout)
2022-03-28 19:14:18 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:13:08Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:14:18 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (64091ms till timeout)
2022-03-28 19:14:20 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:13:08Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:14:20 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (62896ms till timeout)
2022-03-28 19:14:21 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:13:08Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:14:21 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (61704ms till timeout)
2022-03-28 19:14:22 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:13:08Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:14:22 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (60510ms till timeout)
2022-03-28 19:14:23 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=null, completedIndexes=null, completionTime=2022-03-28T19:14:18Z, conditions=[JobCondition(lastProbeTime=2022-03-28T19:14:18Z, lastTransitionTime=2022-03-28T19:14:18Z, message=null, reason=null, status=True, type=Complete, additionalProperties={})], failed=null, ready=null, startTime=2022-03-28T19:13:08Z, succeeded=1, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:14:23 [ForkJoinPool-1-worker-41] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment delete-admin-my-cluster-836b31f8-kafka-clients deletion
2022-03-28 19:14:23 [ForkJoinPool-1-worker-41] DEBUG [TestUtils:125] Waiting for ReplicaSet delete-admin-my-cluster-836b31f8-kafka-clients to be deleted
2022-03-28 19:14:23 [ForkJoinPool-1-worker-41] DEBUG [JobUtils:40] Job delete-admin-my-cluster-836b31f8-kafka-clients was deleted
2022-03-28 19:14:23 [ForkJoinPool-1-worker-41] INFO  [ThrottlingQuotaST:144] Executing 2/5 iteration for delete-admin-my-cluster-836b31f8-kafka-clients.
2022-03-28 19:14:23 [ForkJoinPool-1-worker-41] INFO  [ResourceManager:155] Create/Update Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st
2022-03-28 19:14:24 [ForkJoinPool-1-worker-41] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:delete-admin-my-cluster-836b31f8-kafka-clients
2022-03-28 19:14:24 [ForkJoinPool-1-worker-41] INFO  [JobUtils:81] Waiting for job: delete-admin-my-cluster-836b31f8-kafka-clients will be in active state
2022-03-28 19:14:24 [ForkJoinPool-1-worker-41] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 19:14:24 [ForkJoinPool-1-worker-41] INFO  [ClientUtils:76] Waiting for producer/consumer:delete-admin-my-cluster-836b31f8-kafka-clients to finished
2022-03-28 19:14:24 [ForkJoinPool-1-worker-41] DEBUG [TestUtils:125] Waiting for job finished
2022-03-28 19:14:24 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:14:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:14:24 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (129808ms till timeout)
2022-03-28 19:14:25 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:14:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:14:25 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (128610ms till timeout)
2022-03-28 19:14:26 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:14:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:14:26 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (127416ms till timeout)
2022-03-28 19:14:27 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:14:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:14:27 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (126222ms till timeout)
2022-03-28 19:14:29 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:14:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:14:29 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (125029ms till timeout)
2022-03-28 19:14:30 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:14:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:14:30 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (123835ms till timeout)
2022-03-28 19:14:31 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:14:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:14:31 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (122642ms till timeout)
2022-03-28 19:14:32 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:14:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:14:32 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (121446ms till timeout)
2022-03-28 19:14:33 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:14:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:14:33 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (120253ms till timeout)
2022-03-28 19:14:35 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:14:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:14:35 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (119058ms till timeout)
2022-03-28 19:14:36 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:14:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:14:36 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (117863ms till timeout)
2022-03-28 19:14:37 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:14:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:14:37 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (116668ms till timeout)
2022-03-28 19:14:38 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:14:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:14:38 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (115471ms till timeout)
2022-03-28 19:14:39 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:14:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:14:39 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (114277ms till timeout)
2022-03-28 19:14:41 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:14:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:14:41 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (113084ms till timeout)
2022-03-28 19:14:42 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:14:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:14:42 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (111888ms till timeout)
2022-03-28 19:14:43 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:14:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:14:43 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (110691ms till timeout)
2022-03-28 19:14:44 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:14:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:14:44 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (109487ms till timeout)
2022-03-28 19:14:45 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:14:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:14:45 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (108293ms till timeout)
2022-03-28 19:14:47 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:14:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:14:47 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (107097ms till timeout)
2022-03-28 19:14:48 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:14:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:14:48 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (105904ms till timeout)
2022-03-28 19:14:49 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:14:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:14:49 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (104706ms till timeout)
2022-03-28 19:14:50 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:14:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:14:50 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (103513ms till timeout)
2022-03-28 19:14:51 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:14:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:14:51 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (102317ms till timeout)
2022-03-28 19:14:52 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:14:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:14:53 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (101125ms till timeout)
2022-03-28 19:14:54 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:14:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:14:54 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (99929ms till timeout)
2022-03-28 19:14:55 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:14:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:14:55 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (98736ms till timeout)
2022-03-28 19:14:56 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:14:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:14:56 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (97544ms till timeout)
2022-03-28 19:14:57 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:14:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:14:57 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (96349ms till timeout)
2022-03-28 19:14:58 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:14:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:14:59 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (95155ms till timeout)
2022-03-28 19:15:00 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:14:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:15:00 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (93961ms till timeout)
2022-03-28 19:15:01 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:14:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:15:01 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (92769ms till timeout)
2022-03-28 19:15:02 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:14:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:15:02 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (91576ms till timeout)
2022-03-28 19:15:03 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:14:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:15:03 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (90383ms till timeout)
2022-03-28 19:15:04 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:14:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:15:05 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (89186ms till timeout)
2022-03-28 19:15:06 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:14:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:15:06 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (87992ms till timeout)
2022-03-28 19:15:07 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:14:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:15:07 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (86791ms till timeout)
2022-03-28 19:15:08 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:14:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:15:08 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (85595ms till timeout)
2022-03-28 19:15:09 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:14:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:15:09 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (84398ms till timeout)
2022-03-28 19:15:10 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:14:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:15:11 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (83139ms till timeout)
2022-03-28 19:15:12 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:14:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:15:12 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (81945ms till timeout)
2022-03-28 19:15:13 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:14:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:15:13 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (80752ms till timeout)
2022-03-28 19:15:14 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:14:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:15:14 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (79559ms till timeout)
2022-03-28 19:15:15 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:14:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:15:15 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (78366ms till timeout)
2022-03-28 19:15:16 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:14:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:15:17 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (77171ms till timeout)
2022-03-28 19:15:18 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:14:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:15:18 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (75977ms till timeout)
2022-03-28 19:15:19 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:14:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:15:19 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (74779ms till timeout)
2022-03-28 19:15:20 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:14:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:15:20 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (73584ms till timeout)
2022-03-28 19:15:21 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:14:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:15:21 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (72390ms till timeout)
2022-03-28 19:15:22 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:14:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:15:23 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (71195ms till timeout)
2022-03-28 19:15:24 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:14:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:15:24 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (70002ms till timeout)
2022-03-28 19:15:25 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:14:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:15:25 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (68808ms till timeout)
2022-03-28 19:15:26 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:14:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:15:26 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (67609ms till timeout)
2022-03-28 19:15:27 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:14:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:15:27 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (66415ms till timeout)
2022-03-28 19:15:28 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=null, completedIndexes=null, completionTime=2022-03-28T19:15:23Z, conditions=[JobCondition(lastProbeTime=2022-03-28T19:15:23Z, lastTransitionTime=2022-03-28T19:15:23Z, message=null, reason=null, status=True, type=Complete, additionalProperties={})], failed=null, ready=null, startTime=2022-03-28T19:14:19Z, succeeded=1, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:15:29 [ForkJoinPool-1-worker-41] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment delete-admin-my-cluster-836b31f8-kafka-clients deletion
2022-03-28 19:15:29 [ForkJoinPool-1-worker-41] DEBUG [TestUtils:125] Waiting for ReplicaSet delete-admin-my-cluster-836b31f8-kafka-clients to be deleted
2022-03-28 19:15:29 [ForkJoinPool-1-worker-41] DEBUG [JobUtils:40] Job delete-admin-my-cluster-836b31f8-kafka-clients was deleted
2022-03-28 19:15:29 [ForkJoinPool-1-worker-41] INFO  [ThrottlingQuotaST:144] Executing 3/5 iteration for delete-admin-my-cluster-836b31f8-kafka-clients.
2022-03-28 19:15:29 [ForkJoinPool-1-worker-41] INFO  [ResourceManager:155] Create/Update Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st
2022-03-28 19:15:29 [ForkJoinPool-1-worker-41] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:delete-admin-my-cluster-836b31f8-kafka-clients
2022-03-28 19:15:29 [ForkJoinPool-1-worker-41] INFO  [JobUtils:81] Waiting for job: delete-admin-my-cluster-836b31f8-kafka-clients will be in active state
2022-03-28 19:15:29 [ForkJoinPool-1-worker-41] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 19:15:29 [ForkJoinPool-1-worker-41] INFO  [ClientUtils:76] Waiting for producer/consumer:delete-admin-my-cluster-836b31f8-kafka-clients to finished
2022-03-28 19:15:29 [ForkJoinPool-1-worker-41] DEBUG [TestUtils:125] Waiting for job finished
2022-03-28 19:15:29 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:15:24Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:15:29 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (129809ms till timeout)
2022-03-28 19:15:30 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:15:24Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:15:30 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (128609ms till timeout)
2022-03-28 19:15:31 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:15:24Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:15:32 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (127414ms till timeout)
2022-03-28 19:15:33 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:15:24Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:15:33 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (126219ms till timeout)
2022-03-28 19:15:34 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:15:24Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:15:34 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (125025ms till timeout)
2022-03-28 19:15:35 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:15:24Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:15:35 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (123829ms till timeout)
2022-03-28 19:15:36 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:15:24Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:15:36 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (122636ms till timeout)
2022-03-28 19:15:37 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:15:24Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:15:38 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (121442ms till timeout)
2022-03-28 19:15:39 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:15:24Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:15:39 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (120248ms till timeout)
2022-03-28 19:15:40 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:15:24Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:15:40 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (119019ms till timeout)
2022-03-28 19:15:41 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:15:24Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:15:41 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (117825ms till timeout)
2022-03-28 19:15:42 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:15:24Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:15:42 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (116632ms till timeout)
2022-03-28 19:15:43 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:15:24Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:15:44 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (115437ms till timeout)
2022-03-28 19:15:45 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:15:24Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:15:45 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (114243ms till timeout)
2022-03-28 19:15:46 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:15:24Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:15:46 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (113049ms till timeout)
2022-03-28 19:15:47 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:15:24Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:15:47 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (111855ms till timeout)
2022-03-28 19:15:48 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:15:24Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:15:48 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (110662ms till timeout)
2022-03-28 19:15:49 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:15:24Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:15:50 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (109468ms till timeout)
2022-03-28 19:15:51 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:15:24Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:15:51 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (108276ms till timeout)
2022-03-28 19:15:52 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:15:24Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:15:52 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (107082ms till timeout)
2022-03-28 19:15:53 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:15:24Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:15:53 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (105871ms till timeout)
2022-03-28 19:15:54 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:15:24Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:15:54 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (104675ms till timeout)
2022-03-28 19:15:55 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:15:24Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:15:55 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (103480ms till timeout)
2022-03-28 19:15:57 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:15:24Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:15:57 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (102287ms till timeout)
2022-03-28 19:15:58 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:15:24Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:15:58 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (101094ms till timeout)
2022-03-28 19:15:59 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:15:24Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:15:59 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (99897ms till timeout)
2022-03-28 19:16:00 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:15:24Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:16:00 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (98704ms till timeout)
2022-03-28 19:16:01 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:15:24Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:16:01 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (97510ms till timeout)
2022-03-28 19:16:03 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:15:24Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:16:03 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (96317ms till timeout)
2022-03-28 19:16:04 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:15:24Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:16:04 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (95123ms till timeout)
2022-03-28 19:16:05 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:15:24Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:16:05 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (93928ms till timeout)
2022-03-28 19:16:06 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:15:24Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:16:06 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (92735ms till timeout)
2022-03-28 19:16:07 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:15:24Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:16:07 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (91541ms till timeout)
2022-03-28 19:16:09 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:15:24Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:16:09 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (90348ms till timeout)
2022-03-28 19:16:10 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:15:24Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:16:10 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (89155ms till timeout)
2022-03-28 19:16:11 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:15:24Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:16:11 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (87963ms till timeout)
2022-03-28 19:16:12 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:15:24Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:16:12 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (86769ms till timeout)
2022-03-28 19:16:13 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:15:24Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:16:13 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (85574ms till timeout)
2022-03-28 19:16:15 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:15:24Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:16:15 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (84377ms till timeout)
2022-03-28 19:16:16 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:15:24Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:16:16 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (83182ms till timeout)
2022-03-28 19:16:17 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:15:24Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:16:17 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (81988ms till timeout)
2022-03-28 19:16:18 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:15:24Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:16:18 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (80794ms till timeout)
2022-03-28 19:16:19 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:15:24Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:16:19 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (79592ms till timeout)
2022-03-28 19:16:20 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:15:24Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:16:21 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (78398ms till timeout)
2022-03-28 19:16:22 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:15:24Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:16:22 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (77204ms till timeout)
2022-03-28 19:16:23 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:15:24Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:16:23 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (76008ms till timeout)
2022-03-28 19:16:24 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:15:24Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:16:24 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (74816ms till timeout)
2022-03-28 19:16:25 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:15:24Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:16:25 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (73623ms till timeout)
2022-03-28 19:16:26 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:15:24Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:16:27 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (72429ms till timeout)
2022-03-28 19:16:28 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:15:24Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:16:28 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (71233ms till timeout)
2022-03-28 19:16:29 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:15:24Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:16:29 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (70039ms till timeout)
2022-03-28 19:16:30 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:15:24Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:16:30 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (68845ms till timeout)
2022-03-28 19:16:31 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:15:24Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:16:31 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (67651ms till timeout)
2022-03-28 19:16:32 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=null, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:15:24Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[8d807953-fa34-4334-80f5-dac5f3a9544b], additionalProperties={}), additionalProperties={})
2022-03-28 19:16:33 [ForkJoinPool-1-worker-41] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment delete-admin-my-cluster-836b31f8-kafka-clients deletion
2022-03-28 19:16:33 [ForkJoinPool-1-worker-41] DEBUG [TestUtils:125] Waiting for ReplicaSet delete-admin-my-cluster-836b31f8-kafka-clients to be deleted
2022-03-28 19:16:33 [ForkJoinPool-1-worker-41] DEBUG [JobUtils:40] Job delete-admin-my-cluster-836b31f8-kafka-clients was deleted
2022-03-28 19:16:33 [ForkJoinPool-1-worker-41] INFO  [ThrottlingQuotaST:144] Executing 4/5 iteration for delete-admin-my-cluster-836b31f8-kafka-clients.
2022-03-28 19:16:33 [ForkJoinPool-1-worker-41] INFO  [ResourceManager:155] Create/Update Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st
2022-03-28 19:16:33 [ForkJoinPool-1-worker-41] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:delete-admin-my-cluster-836b31f8-kafka-clients
2022-03-28 19:16:33 [ForkJoinPool-1-worker-41] INFO  [JobUtils:81] Waiting for job: delete-admin-my-cluster-836b31f8-kafka-clients will be in active state
2022-03-28 19:16:33 [ForkJoinPool-1-worker-41] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 19:16:33 [ForkJoinPool-1-worker-41] INFO  [ClientUtils:76] Waiting for producer/consumer:delete-admin-my-cluster-836b31f8-kafka-clients to finished
2022-03-28 19:16:33 [ForkJoinPool-1-worker-41] DEBUG [TestUtils:125] Waiting for job finished
2022-03-28 19:16:33 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:16:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:16:33 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (129809ms till timeout)
2022-03-28 19:16:34 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:16:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:16:34 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (128613ms till timeout)
2022-03-28 19:16:36 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:16:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:16:36 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (127419ms till timeout)
2022-03-28 19:16:37 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:16:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:16:37 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (126224ms till timeout)
2022-03-28 19:16:38 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:16:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:16:38 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (125029ms till timeout)
2022-03-28 19:16:39 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:16:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:16:39 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (123836ms till timeout)
2022-03-28 19:16:40 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:16:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:16:40 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (122552ms till timeout)
2022-03-28 19:16:42 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:16:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:16:42 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (121356ms till timeout)
2022-03-28 19:16:43 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:16:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:16:43 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (120162ms till timeout)
2022-03-28 19:16:44 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:16:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:16:44 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (118965ms till timeout)
2022-03-28 19:16:45 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:16:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:16:45 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (117770ms till timeout)
2022-03-28 19:16:46 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:16:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:16:46 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (116575ms till timeout)
2022-03-28 19:16:48 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:16:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:16:48 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (115380ms till timeout)
2022-03-28 19:16:49 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:16:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:16:49 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (114184ms till timeout)
2022-03-28 19:16:50 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:16:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:16:50 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (112987ms till timeout)
2022-03-28 19:16:51 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:16:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:16:51 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (111793ms till timeout)
2022-03-28 19:16:52 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:16:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:16:52 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (110599ms till timeout)
2022-03-28 19:16:54 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:16:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:16:54 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (109405ms till timeout)
2022-03-28 19:16:55 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:16:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:16:55 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (108211ms till timeout)
2022-03-28 19:16:56 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:16:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:16:56 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (107017ms till timeout)
2022-03-28 19:16:57 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:16:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:16:57 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (105822ms till timeout)
2022-03-28 19:16:58 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:16:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:16:58 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (104629ms till timeout)
2022-03-28 19:16:59 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:16:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:17:00 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (103435ms till timeout)
2022-03-28 19:17:01 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:16:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:17:01 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (102242ms till timeout)
2022-03-28 19:17:02 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:16:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:17:02 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (101049ms till timeout)
2022-03-28 19:17:03 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:16:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:17:03 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (99854ms till timeout)
2022-03-28 19:17:04 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:16:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:17:04 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (98660ms till timeout)
2022-03-28 19:17:05 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:16:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:17:06 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (97466ms till timeout)
2022-03-28 19:17:07 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:16:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:17:07 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (96271ms till timeout)
2022-03-28 19:17:08 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:16:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:17:08 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (95076ms till timeout)
2022-03-28 19:17:09 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:16:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:17:09 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (93881ms till timeout)
2022-03-28 19:17:10 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:16:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:17:10 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (92687ms till timeout)
2022-03-28 19:17:11 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:16:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:17:12 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (91493ms till timeout)
2022-03-28 19:17:13 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:16:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:17:13 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (90298ms till timeout)
2022-03-28 19:17:14 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:16:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:17:14 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (89104ms till timeout)
2022-03-28 19:17:15 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:16:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:17:15 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (87907ms till timeout)
2022-03-28 19:17:16 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:16:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:17:16 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (86713ms till timeout)
2022-03-28 19:17:17 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:16:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:17:18 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (85516ms till timeout)
2022-03-28 19:17:19 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:16:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:17:19 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (84321ms till timeout)
2022-03-28 19:17:20 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:16:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:17:20 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (83126ms till timeout)
2022-03-28 19:17:21 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:16:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:17:21 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (81934ms till timeout)
2022-03-28 19:17:22 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:16:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:17:22 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (80740ms till timeout)
2022-03-28 19:17:23 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:16:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:17:23 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (79543ms till timeout)
2022-03-28 19:17:25 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:16:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:17:25 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (78350ms till timeout)
2022-03-28 19:17:26 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:16:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:17:26 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (77158ms till timeout)
2022-03-28 19:17:27 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:16:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:17:27 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (75963ms till timeout)
2022-03-28 19:17:28 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:16:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:17:28 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (74768ms till timeout)
2022-03-28 19:17:29 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:16:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:17:29 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (73574ms till timeout)
2022-03-28 19:17:31 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:16:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:17:31 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (72383ms till timeout)
2022-03-28 19:17:32 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:16:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:17:32 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (71189ms till timeout)
2022-03-28 19:17:33 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:16:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:17:33 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (69995ms till timeout)
2022-03-28 19:17:34 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:16:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:17:34 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (68800ms till timeout)
2022-03-28 19:17:35 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:16:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:17:35 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (67607ms till timeout)
2022-03-28 19:17:37 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:16:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:17:37 [ForkJoinPool-1-worker-41] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment delete-admin-my-cluster-836b31f8-kafka-clients deletion
2022-03-28 19:17:37 [ForkJoinPool-1-worker-41] DEBUG [TestUtils:125] Waiting for ReplicaSet delete-admin-my-cluster-836b31f8-kafka-clients to be deleted
2022-03-28 19:17:37 [ForkJoinPool-1-worker-41] DEBUG [JobUtils:40] Job delete-admin-my-cluster-836b31f8-kafka-clients was deleted
2022-03-28 19:17:37 [ForkJoinPool-1-worker-41] INFO  [ThrottlingQuotaST:144] Executing 5/5 iteration for delete-admin-my-cluster-836b31f8-kafka-clients.
2022-03-28 19:17:37 [ForkJoinPool-1-worker-41] INFO  [ResourceManager:155] Create/Update Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st
2022-03-28 19:17:37 [ForkJoinPool-1-worker-41] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:delete-admin-my-cluster-836b31f8-kafka-clients
2022-03-28 19:17:37 [ForkJoinPool-1-worker-41] INFO  [JobUtils:81] Waiting for job: delete-admin-my-cluster-836b31f8-kafka-clients will be in active state
2022-03-28 19:17:37 [ForkJoinPool-1-worker-41] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 19:17:37 [ForkJoinPool-1-worker-41] INFO  [ClientUtils:76] Waiting for producer/consumer:delete-admin-my-cluster-836b31f8-kafka-clients to finished
2022-03-28 19:17:37 [ForkJoinPool-1-worker-41] DEBUG [TestUtils:125] Waiting for job finished
2022-03-28 19:17:37 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:17:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:17:37 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (129806ms till timeout)
2022-03-28 19:17:39 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:17:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:17:39 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (128611ms till timeout)
2022-03-28 19:17:40 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:17:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:17:40 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (127416ms till timeout)
2022-03-28 19:17:41 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:17:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:17:41 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (126224ms till timeout)
2022-03-28 19:17:42 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:17:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:17:42 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (125031ms till timeout)
2022-03-28 19:17:43 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:17:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:17:43 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (123836ms till timeout)
2022-03-28 19:17:45 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:17:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:17:45 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (122637ms till timeout)
2022-03-28 19:17:46 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:17:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:17:46 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (121443ms till timeout)
2022-03-28 19:17:47 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:17:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:17:47 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (120249ms till timeout)
2022-03-28 19:17:48 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:17:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:17:48 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (119016ms till timeout)
2022-03-28 19:17:49 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:17:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:17:49 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (117822ms till timeout)
2022-03-28 19:17:51 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:17:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:17:51 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (116628ms till timeout)
2022-03-28 19:17:52 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:17:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:17:52 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (115434ms till timeout)
2022-03-28 19:17:53 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:17:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:17:53 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (114241ms till timeout)
2022-03-28 19:17:54 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:17:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:17:54 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (113046ms till timeout)
2022-03-28 19:17:55 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:17:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:17:55 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (111850ms till timeout)
2022-03-28 19:17:57 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:17:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:17:57 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (110656ms till timeout)
2022-03-28 19:17:58 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:17:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:17:58 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (109462ms till timeout)
2022-03-28 19:17:59 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:17:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:17:59 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (108268ms till timeout)
2022-03-28 19:18:00 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:17:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:18:00 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (107074ms till timeout)
2022-03-28 19:18:01 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:17:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:18:01 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (105879ms till timeout)
2022-03-28 19:18:02 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:17:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:18:03 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (104685ms till timeout)
2022-03-28 19:18:04 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:17:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:18:04 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (103492ms till timeout)
2022-03-28 19:18:05 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:17:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:18:05 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (102298ms till timeout)
2022-03-28 19:18:06 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:17:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:18:06 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (101103ms till timeout)
2022-03-28 19:18:07 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:17:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:18:07 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (99908ms till timeout)
2022-03-28 19:18:08 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:17:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:18:09 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (98713ms till timeout)
2022-03-28 19:18:10 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:17:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:18:10 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (97519ms till timeout)
2022-03-28 19:18:11 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:17:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:18:11 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (96323ms till timeout)
2022-03-28 19:18:12 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:17:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:18:12 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (95128ms till timeout)
2022-03-28 19:18:13 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:17:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:18:13 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (93936ms till timeout)
2022-03-28 19:18:14 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:17:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:18:15 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (92742ms till timeout)
2022-03-28 19:18:16 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:17:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:18:16 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (91547ms till timeout)
2022-03-28 19:18:17 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:17:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:18:17 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (90353ms till timeout)
2022-03-28 19:18:18 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:17:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:18:18 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (89153ms till timeout)
2022-03-28 19:18:19 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:17:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:18:19 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (87958ms till timeout)
2022-03-28 19:18:20 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:17:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:18:21 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (86764ms till timeout)
2022-03-28 19:18:22 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:17:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:18:22 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (85569ms till timeout)
2022-03-28 19:18:23 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:17:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:18:23 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (84375ms till timeout)
2022-03-28 19:18:24 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:17:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:18:24 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (83181ms till timeout)
2022-03-28 19:18:25 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:17:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:18:25 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (81982ms till timeout)
2022-03-28 19:18:26 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:17:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:18:27 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (80731ms till timeout)
2022-03-28 19:18:28 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:17:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:18:28 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (79537ms till timeout)
2022-03-28 19:18:29 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:17:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:18:29 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (78343ms till timeout)
2022-03-28 19:18:30 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:17:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:18:30 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (77124ms till timeout)
2022-03-28 19:18:31 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:17:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:18:31 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (75932ms till timeout)
2022-03-28 19:18:32 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:17:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:18:33 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (74734ms till timeout)
2022-03-28 19:18:34 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:17:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:18:34 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (73541ms till timeout)
2022-03-28 19:18:35 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:17:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:18:35 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (72348ms till timeout)
2022-03-28 19:18:36 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:17:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:18:36 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (71154ms till timeout)
2022-03-28 19:18:37 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:17:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:18:37 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (69960ms till timeout)
2022-03-28 19:18:38 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:17:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:18:38 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (68769ms till timeout)
2022-03-28 19:18:40 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:17:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:18:40 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (67575ms till timeout)
2022-03-28 19:18:41 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T19:17:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:18:41 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (66382ms till timeout)
2022-03-28 19:18:42 [ForkJoinPool-1-worker-41] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=null, completedIndexes=null, completionTime=2022-03-28T19:18:37Z, conditions=[JobCondition(lastProbeTime=2022-03-28T19:18:37Z, lastTransitionTime=2022-03-28T19:18:37Z, message=null, reason=null, status=True, type=Complete, additionalProperties={})], failed=null, ready=null, startTime=2022-03-28T19:17:32Z, succeeded=1, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 19:18:42 [ForkJoinPool-1-worker-41] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment delete-admin-my-cluster-836b31f8-kafka-clients deletion
2022-03-28 19:18:42 [ForkJoinPool-1-worker-41] DEBUG [TestUtils:125] Waiting for ReplicaSet delete-admin-my-cluster-836b31f8-kafka-clients to be deleted
2022-03-28 19:18:42 [ForkJoinPool-1-worker-41] TRACE [TestUtils:176] ReplicaSet delete-admin-my-cluster-836b31f8-kafka-clients to be deleted not ready, will try again in 5000 ms (179894ms till timeout)
2022-03-28 19:18:47 [ForkJoinPool-1-worker-41] DEBUG [JobUtils:40] Job delete-admin-my-cluster-836b31f8-kafka-clients was deleted
2022-03-28 19:18:47 [ForkJoinPool-1-worker-41] DEBUG [AbstractST:674] ============================================================================
2022-03-28 19:18:47 [ForkJoinPool-1-worker-41] DEBUG [AbstractST:675] [operators.topic.ThrottlingQuotaST - After Each] - Clean up after test
2022-03-28 19:18:47 [ForkJoinPool-1-worker-41] INFO  [ResourceManager:344] ############################################################################
2022-03-28 19:18:47 [ForkJoinPool-1-worker-41] INFO  [ResourceManager:348] Delete all resources for testThrottlingQuotasDeleteTopic
2022-03-28 19:18:47 [ForkJoinPool-1-worker-41] INFO  [ResourceManager:241] Delete of Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st
2022-03-28 19:18:47 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st
2022-03-28 19:18:47 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:241] Delete of Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st
2022-03-28 19:18:47 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:241] Delete of Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st
2022-03-28 19:18:47 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:241] Delete of Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st
2022-03-28 19:18:47 [ForkJoinPool-1-worker-29] INFO  [ResourceManager:241] Delete of Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st
2022-03-28 19:18:47 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:241] Delete of Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st
2022-03-28 19:18:47 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:241] Delete of KafkaUser my-user-1567019138-1355600812 in namespace throttling-quota-st
2022-03-28 19:18:47 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:241] Delete of Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st
2022-03-28 19:18:47 [ForkJoinPool-1-worker-31] INFO  [ResourceManager:241] Delete of Job delete-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st
2022-03-28 19:18:47 [ForkJoinPool-1-worker-33] INFO  [ResourceManager:241] Delete of Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st
2022-03-28 19:18:47 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:241] Delete of Job create-admin-my-cluster-836b31f8-kafka-clients in namespace throttling-quota-st
2022-03-28 19:18:47 [ForkJoinPool-1-worker-41] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:delete-admin-my-cluster-836b31f8-kafka-clients
2022-03-28 19:18:47 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:create-admin-my-cluster-836b31f8-kafka-clients
2022-03-28 19:18:47 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:delete-admin-my-cluster-836b31f8-kafka-clients
2022-03-28 19:18:47 [ForkJoinPool-1-worker-31] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:delete-admin-my-cluster-836b31f8-kafka-clients
2022-03-28 19:18:47 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:delete-admin-my-cluster-836b31f8-kafka-clients
2022-03-28 19:18:48 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-1567019138-1355600812
2022-03-28 19:18:48 [ForkJoinPool-1-worker-25] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:delete-admin-my-cluster-836b31f8-kafka-clients
2022-03-28 19:18:48 [ForkJoinPool-1-worker-33] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:create-admin-my-cluster-836b31f8-kafka-clients
2022-03-28 19:18:48 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:create-admin-my-cluster-836b31f8-kafka-clients
2022-03-28 19:18:48 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:create-admin-my-cluster-836b31f8-kafka-clients
2022-03-28 19:18:48 [ForkJoinPool-1-worker-29] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:delete-admin-my-cluster-836b31f8-kafka-clients
2022-03-28 19:18:48 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:create-admin-my-cluster-836b31f8-kafka-clients
2022-03-28 19:18:48 [ForkJoinPool-1-worker-41] INFO  [ResourceManager:369] ############################################################################
2022-03-28 19:18:48 [ForkJoinPool-1-worker-41] DEBUG [SuiteThreadController:267] testThrottlingQuotasDeleteTopic - Notifies waiting test cases:[testPauseReconciliationInKafkaRebalanceAndTopic, testPauseReconciliationInKafkaAndKafkaConnectWithConnector, testCapacityFile, testConfigurationFileIsCreated, testConfigurationPerformanceOptions, testConfigurationReflection, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testDeployAndUnDeployCruiseControl, testTlsExternalUserWithQuotas, testCreatingUsersWithSecretPrefix, testThrottlingQuotasCreateTopic, testThrottlingQuotasCreateAlterPartitions, testKafkaAdminTopicOperations, testUserTemplate, testUpdateUser, testTlsExternalUser, testSendSimpleMessageTls, testReceiveSimpleMessageTlsScramSha, testThrottlingQuotasDeleteTopic, testReceiveSimpleMessageTls, testTopicModificationOfReplicationFactor, testSendingMessagesToNonExistingTopic, testMoreReplicasThanAvailableBrokers, testCreateTopicViaKafka] to and randomly select one to start execution
2022-03-28 19:18:48 [ForkJoinPool-1-worker-41] DEBUG [SuiteThreadController:93] [operators.topic.ThrottlingQuotaST] - Removing parallel test: testThrottlingQuotasDeleteTopic
2022-03-28 19:18:48 [ForkJoinPool-1-worker-41] DEBUG [SuiteThreadController:97] [operators.topic.ThrottlingQuotaST] - Parallel test count: 0
2022-03-28 19:18:48 [ForkJoinPool-1-worker-41] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.topic.ThrottlingQuotaST.testThrottlingQuotasDeleteTopic-FINISHED
2022-03-28 19:18:48 [ForkJoinPool-1-worker-41] INFO  [TestSeparator:30] ############################################################################
2022-03-28 19:18:48 [ForkJoinPool-1-worker-9] INFO  [ThrottlingQuotaST:353] Tearing down resources after all test
2022-03-28 19:18:48 [ForkJoinPool-1-worker-27] INFO  [TestSeparator:23] ############################################################################
2022-03-28 19:18:48 [ForkJoinPool-1-worker-27] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.topic.TopicST.testDeleteTopicEnableFalse-STARTED
2022-03-28 19:18:48 [ForkJoinPool-1-worker-27] DEBUG [AbstractST:658] ============================================================================
2022-03-28 19:18:48 [ForkJoinPool-1-worker-27] DEBUG [AbstractST:659] [operators.topic.TopicST - Before Each] - Setup test case environment
2022-03-28 19:18:48 [ForkJoinPool-1-worker-27] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 19:18:48 [ForkJoinPool-1-worker-27] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-abc5d34b, testThrottlingQuotasCreateAlterPartitions=my-cluster-ec1b34f8, testSendSimpleMessageTls=my-cluster-89673e85, testConfigurationReflection=my-cluster-188382d8, testUserWithNameMoreThan64Chars=my-cluster-2d5338bf, testDeployAndUnDeployCruiseControl=my-cluster-5dfd606f, testCapacityFile=my-cluster-855b9105, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-afe273c9, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-e6343cec, testTlsExternalUser=my-cluster-45cf8ce4, testSendSimpleMessageTlsScramSha=my-cluster-561d4fae, testCreateTopicAfterUnsupportedOperation=my-cluster-e8a83fb0, testScramUserWithQuotas=my-cluster-ab753b77, testTopicModificationOfReplicationFactor=my-cluster-15f5f09f, testKafkaAdminTopicOperations=my-cluster-506fda27, testConfigurationFileIsCreated=my-cluster-17fc585b, testCreateTopicViaKafka=my-cluster-75679697, testCreatingUsersWithSecretPrefix=my-cluster-f743fdfd, testUpdateUser=my-cluster-d582eb8c, testThrottlingQuotasDeleteTopic=my-cluster-836b31f8, testDeleteTopicEnableFalse=my-cluster-cfc59519, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-959c6ed5, testTlsExternalUserWithQuotas=my-cluster-100f5461, testTlsUserWithQuotas=my-cluster-9bde73ea, testThrottlingQuotasCreateTopic=my-cluster-e9955717, testSendingMessagesToNonExistingTopic=my-cluster-a3c1c5ed, testConfigurationPerformanceOptions=my-cluster-08515847, testReceiveSimpleMessageTls=my-cluster-634e4d65, testUserTemplate=my-cluster-927d5a1b, testMoreReplicasThanAvailableBrokers=my-cluster-22050d08}
2022-03-28 19:18:48 [ForkJoinPool-1-worker-27] TRACE [AbstractST:607] USERS_NAME_MAP: {testReceiveSimpleMessageTlsScramSha=my-user-330562220-1155646423, testThrottlingQuotasCreateAlterPartitions=my-user-333986137-1152182125, testSendSimpleMessageTls=my-user-1887866274-1013125243, testConfigurationReflection=my-user-1890969183-1754686912, testUserWithNameMoreThan64Chars=my-user-1930755541-784294250, testDeployAndUnDeployCruiseControl=my-user-506268651-1309117986, testCapacityFile=my-user-1460643829-103298436, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-user-8456079-650483351, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-2069267102-581538560, testTlsExternalUser=my-user-399414674-1203409092, testSendSimpleMessageTlsScramSha=my-user-1601209876-518796379, testCreateTopicAfterUnsupportedOperation=my-user-1589831352-1271671319, testScramUserWithQuotas=my-user-941123953-266890403, testTopicModificationOfReplicationFactor=my-user-259690203-415132624, testKafkaAdminTopicOperations=my-user-1780746402-846362125, testConfigurationFileIsCreated=my-user-355815510-1316268151, testCreateTopicViaKafka=my-user-541976280-277947468, testCreatingUsersWithSecretPrefix=my-user-607975993-1210920737, testUpdateUser=my-user-1297536277-778093599, testThrottlingQuotasDeleteTopic=my-user-1567019138-1355600812, testDeleteTopicEnableFalse=my-user-692771166-734949996, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-2104248162-45187051, testTlsExternalUserWithQuotas=my-user-1909583883-1106194424, testTlsUserWithQuotas=my-user-1496409761-1456339678, testThrottlingQuotasCreateTopic=my-user-1876060546-1887905718, testSendingMessagesToNonExistingTopic=my-user-134768902-582007171, testConfigurationPerformanceOptions=my-user-98362545-1476587236, testReceiveSimpleMessageTls=my-user-337002809-1497514615, testUserTemplate=my-user-989633374-940717398, testMoreReplicasThanAvailableBrokers=my-user-713898339-1095128639}
2022-03-28 19:18:48 [ForkJoinPool-1-worker-27] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-topic-1027850608-734754781, testThrottlingQuotasCreateAlterPartitions=my-topic-325891726-169328137, testSendSimpleMessageTls=my-topic-874607937-87907051, testConfigurationReflection=my-topic-1394577332-1936226438, testUserWithNameMoreThan64Chars=my-topic-1542907219-2136330275, testDeployAndUnDeployCruiseControl=my-topic-465377127-800134460, testCapacityFile=my-topic-549312506-649708453, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-topic-1687496771-142091227, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-293775067-432059493, testTlsExternalUser=my-topic-707627084-257659651, testSendSimpleMessageTlsScramSha=my-topic-1586843669-332477842, testCreateTopicAfterUnsupportedOperation=my-topic-1619871970-423361410, testScramUserWithQuotas=my-topic-644624843-1738170872, testTopicModificationOfReplicationFactor=my-topic-1480509436-578706559, testKafkaAdminTopicOperations=my-topic-481231182-953607496, testConfigurationFileIsCreated=my-topic-423850169-855699454, testCreateTopicViaKafka=my-topic-1973235938-1730719386, testCreatingUsersWithSecretPrefix=my-topic-1727421070-668192795, testUpdateUser=my-topic-1325563869-1324403074, testThrottlingQuotasDeleteTopic=my-topic-1674611395-2048879468, testDeleteTopicEnableFalse=my-topic-1545127738-3726149, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-634125597-1746493046, testTlsExternalUserWithQuotas=my-topic-1986059791-167638004, testTlsUserWithQuotas=my-topic-2040479778-983803186, testThrottlingQuotasCreateTopic=my-topic-1246597695-228473879, testSendingMessagesToNonExistingTopic=my-topic-1692964628-1348460540, testConfigurationPerformanceOptions=my-topic-696588651-1198599652, testReceiveSimpleMessageTls=my-topic-829184226-1519275114, testUserTemplate=my-topic-1542379258-2002911578, testMoreReplicasThanAvailableBrokers=my-topic-840747042-63255423}
2022-03-28 19:18:48 [ForkJoinPool-1-worker-27] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-abc5d34b-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-ec1b34f8-kafka-clients, testSendSimpleMessageTls=my-cluster-89673e85-kafka-clients, testConfigurationReflection=my-cluster-188382d8-kafka-clients, testUserWithNameMoreThan64Chars=my-cluster-2d5338bf-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-5dfd606f-kafka-clients, testCapacityFile=my-cluster-855b9105-kafka-clients, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-afe273c9-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-e6343cec-kafka-clients, testTlsExternalUser=my-cluster-45cf8ce4-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-561d4fae-kafka-clients, testCreateTopicAfterUnsupportedOperation=my-cluster-e8a83fb0-kafka-clients, testScramUserWithQuotas=my-cluster-ab753b77-kafka-clients, testTopicModificationOfReplicationFactor=my-cluster-15f5f09f-kafka-clients, testKafkaAdminTopicOperations=my-cluster-506fda27-kafka-clients, testConfigurationFileIsCreated=my-cluster-17fc585b-kafka-clients, testCreateTopicViaKafka=my-cluster-75679697-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-f743fdfd-kafka-clients, testUpdateUser=my-cluster-d582eb8c-kafka-clients, testThrottlingQuotasDeleteTopic=my-cluster-836b31f8-kafka-clients, testDeleteTopicEnableFalse=my-cluster-cfc59519-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-959c6ed5-kafka-clients, testTlsExternalUserWithQuotas=my-cluster-100f5461-kafka-clients, testTlsUserWithQuotas=my-cluster-9bde73ea-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-e9955717-kafka-clients, testSendingMessagesToNonExistingTopic=my-cluster-a3c1c5ed-kafka-clients, testConfigurationPerformanceOptions=my-cluster-08515847-kafka-clients, testReceiveSimpleMessageTls=my-cluster-634e4d65-kafka-clients, testUserTemplate=my-cluster-927d5a1b-kafka-clients, testMoreReplicasThanAvailableBrokers=my-cluster-22050d08-kafka-clients}
2022-03-28 19:18:48 [ForkJoinPool-1-worker-27] INFO  [ResourceManager:155] Create/Update Kafka my-cluster-cfc59519-isolated in namespace topic-st
2022-03-28 19:18:48 [ForkJoinPool-1-worker-27] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-cfc59519-isolated
2022-03-28 19:18:48 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-101
2022-03-28 19:18:48 [ForkJoinPool-1-worker-27] INFO  [ResourceManager:433] Wait for Kafka: my-cluster-cfc59519-isolated will have desired state: Ready
2022-03-28 19:18:48 [ForkJoinPool-1-worker-27] DEBUG [TestUtils:125] Waiting for Kafka: my-cluster-cfc59519-isolated will have desired state: Ready
2022-03-28 19:18:48 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-cfc59519-isolated will have desired state: Ready not ready, will try again in 1000 ms (839903ms till timeout)
2022-03-28 19:18:49 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-101
2022-03-28 19:18:49 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:18:49 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-142
2022-03-28 19:18:49 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-142
2022-03-28 19:18:49 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:18:49 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-147
2022-03-28 19:18:49 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-cfc59519-isolated will have desired state: Ready not ready, will try again in 1000 ms (838807ms till timeout)
2022-03-28 19:18:50 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-147
2022-03-28 19:18:50 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:18:50 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-152
2022-03-28 19:18:50 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-cfc59519-isolated will have desired state: Ready not ready, will try again in 1000 ms (837705ms till timeout)
2022-03-28 19:18:51 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-152
2022-03-28 19:18:51 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:18:51 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-162
2022-03-28 19:18:51 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-162
2022-03-28 19:18:51 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:18:51 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-171
2022-03-28 19:18:52 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-cfc59519-isolated will have desired state: Ready not ready, will try again in 1000 ms (836607ms till timeout)
2022-03-28 19:18:52 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-171
2022-03-28 19:18:52 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:18:52 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-175
2022-03-28 19:18:52 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-175
2022-03-28 19:18:52 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:18:52 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-230
2022-03-28 19:18:53 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-cfc59519-isolated will have desired state: Ready not ready, will try again in 1000 ms (835507ms till timeout)
2022-03-28 19:18:53 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-230
2022-03-28 19:18:53 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:18:53 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-250
2022-03-28 19:18:53 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-250
2022-03-28 19:18:53 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:18:53 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-277
2022-03-28 19:18:54 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-cfc59519-isolated will have desired state: Ready not ready, will try again in 1000 ms (834410ms till timeout)
2022-03-28 19:18:54 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-277
2022-03-28 19:18:54 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:18:54 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-28
2022-03-28 19:18:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-28
2022-03-28 19:18:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:18:55 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-281
2022-03-28 19:18:55 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-cfc59519-isolated will have desired state: Ready not ready, will try again in 1000 ms (833312ms till timeout)
2022-03-28 19:18:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-281
2022-03-28 19:18:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:18:55 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-339
2022-03-28 19:18:56 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-339
2022-03-28 19:18:56 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:18:56 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-34
2022-03-28 19:18:56 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-cfc59519-isolated will have desired state: Ready not ready, will try again in 1000 ms (832215ms till timeout)
2022-03-28 19:18:56 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-34
2022-03-28 19:18:56 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:18:56 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-35
2022-03-28 19:18:57 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-35
2022-03-28 19:18:57 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:18:57 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-355
2022-03-28 19:18:57 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-cfc59519-isolated will have desired state: Ready not ready, will try again in 1000 ms (831118ms till timeout)
2022-03-28 19:18:57 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-355
2022-03-28 19:18:57 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:18:57 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-379
2022-03-28 19:18:58 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-379
2022-03-28 19:18:58 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:18:58 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-435
2022-03-28 19:18:58 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-cfc59519-isolated will have desired state: Ready not ready, will try again in 1000 ms (830020ms till timeout)
2022-03-28 19:18:58 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-435
2022-03-28 19:18:58 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:18:58 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-470
2022-03-28 19:18:59 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-470
2022-03-28 19:18:59 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:18:59 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-102
2022-03-28 19:18:59 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-cfc59519-isolated will have desired state: Ready not ready, will try again in 1000 ms (828922ms till timeout)
2022-03-28 19:19:00 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-102
2022-03-28 19:19:00 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:00 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-103
2022-03-28 19:19:00 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-103
2022-03-28 19:19:00 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:00 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-104
2022-03-28 19:19:00 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-cfc59519-isolated will have desired state: Ready not ready, will try again in 1000 ms (827825ms till timeout)
2022-03-28 19:19:01 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-104
2022-03-28 19:19:01 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:01 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-130
2022-03-28 19:19:01 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-130
2022-03-28 19:19:01 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:01 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-131
2022-03-28 19:19:01 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-cfc59519-isolated will have desired state: Ready not ready, will try again in 1000 ms (826727ms till timeout)
2022-03-28 19:19:02 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-131
2022-03-28 19:19:02 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:02 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-132
2022-03-28 19:19:02 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-132
2022-03-28 19:19:02 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:02 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-133
2022-03-28 19:19:03 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-cfc59519-isolated will have desired state: Ready not ready, will try again in 1000 ms (825630ms till timeout)
2022-03-28 19:19:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-133
2022-03-28 19:19:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:03 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-134
2022-03-28 19:19:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-134
2022-03-28 19:19:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:03 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-135
2022-03-28 19:19:04 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-cfc59519-isolated will have desired state: Ready not ready, will try again in 1000 ms (824530ms till timeout)
2022-03-28 19:19:04 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-135
2022-03-28 19:19:04 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:04 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-136
2022-03-28 19:19:04 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-136
2022-03-28 19:19:04 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:04 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-137
2022-03-28 19:19:05 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-cfc59519-isolated will have desired state: Ready not ready, will try again in 1000 ms (823433ms till timeout)
2022-03-28 19:19:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-137
2022-03-28 19:19:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:05 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-138
2022-03-28 19:19:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-138
2022-03-28 19:19:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:06 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-139
2022-03-28 19:19:06 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-cfc59519-isolated will have desired state: Ready not ready, will try again in 1000 ms (822334ms till timeout)
2022-03-28 19:19:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-139
2022-03-28 19:19:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:06 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-140
2022-03-28 19:19:07 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-140
2022-03-28 19:19:07 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:07 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-141
2022-03-28 19:19:07 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-cfc59519-isolated will have desired state: Ready not ready, will try again in 1000 ms (821236ms till timeout)
2022-03-28 19:19:07 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-141
2022-03-28 19:19:07 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:07 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-142
2022-03-28 19:19:08 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-142
2022-03-28 19:19:08 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:08 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-143
2022-03-28 19:19:08 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-cfc59519-isolated will have desired state: Ready not ready, will try again in 1000 ms (820139ms till timeout)
2022-03-28 19:19:08 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-143
2022-03-28 19:19:08 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:08 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-144
2022-03-28 19:19:09 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-144
2022-03-28 19:19:09 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:09 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-145
2022-03-28 19:19:09 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-cfc59519-isolated will have desired state: Ready not ready, will try again in 1000 ms (819042ms till timeout)
2022-03-28 19:19:10 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-145
2022-03-28 19:19:10 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:10 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-146
2022-03-28 19:19:10 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-146
2022-03-28 19:19:10 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:10 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-147
2022-03-28 19:19:10 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-cfc59519-isolated will have desired state: Ready not ready, will try again in 1000 ms (817944ms till timeout)
2022-03-28 19:19:11 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-147
2022-03-28 19:19:11 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:11 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-148
2022-03-28 19:19:11 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-148
2022-03-28 19:19:11 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:11 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-149
2022-03-28 19:19:11 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-cfc59519-isolated will have desired state: Ready not ready, will try again in 1000 ms (816847ms till timeout)
2022-03-28 19:19:12 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-149
2022-03-28 19:19:12 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:12 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-150
2022-03-28 19:19:12 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-150
2022-03-28 19:19:12 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:12 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-151
2022-03-28 19:19:12 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-cfc59519-isolated will have desired state: Ready not ready, will try again in 1000 ms (815750ms till timeout)
2022-03-28 19:19:13 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-151
2022-03-28 19:19:13 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:13 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-152
2022-03-28 19:19:14 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-152
2022-03-28 19:19:14 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:14 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-153
2022-03-28 19:19:14 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-cfc59519-isolated will have desired state: Ready not ready, will try again in 1000 ms (814652ms till timeout)
2022-03-28 19:19:14 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-153
2022-03-28 19:19:14 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:14 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-154
2022-03-28 19:19:15 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-154
2022-03-28 19:19:15 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:15 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-155
2022-03-28 19:19:15 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-cfc59519-isolated will have desired state: Ready not ready, will try again in 1000 ms (813554ms till timeout)
2022-03-28 19:19:15 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-155
2022-03-28 19:19:15 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:15 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-156
2022-03-28 19:19:16 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-cfc59519-isolated will have desired state: Ready not ready, will try again in 1000 ms (812457ms till timeout)
2022-03-28 19:19:16 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-156
2022-03-28 19:19:16 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:16 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-157
2022-03-28 19:19:16 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-157
2022-03-28 19:19:16 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:16 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-158
2022-03-28 19:19:17 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-cfc59519-isolated will have desired state: Ready not ready, will try again in 1000 ms (811360ms till timeout)
2022-03-28 19:19:17 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-158
2022-03-28 19:19:17 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:17 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-159
2022-03-28 19:19:17 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-159
2022-03-28 19:19:17 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:17 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-160
2022-03-28 19:19:18 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-cfc59519-isolated will have desired state: Ready not ready, will try again in 1000 ms (810262ms till timeout)
2022-03-28 19:19:18 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-160
2022-03-28 19:19:18 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:18 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-161
2022-03-28 19:19:19 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-161
2022-03-28 19:19:19 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:19 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-162
2022-03-28 19:19:19 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-cfc59519-isolated will have desired state: Ready not ready, will try again in 1000 ms (809164ms till timeout)
2022-03-28 19:19:19 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-162
2022-03-28 19:19:19 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:19 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-163
2022-03-28 19:19:20 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-163
2022-03-28 19:19:20 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:20 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-164
2022-03-28 19:19:20 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-cfc59519-isolated will have desired state: Ready not ready, will try again in 1000 ms (808063ms till timeout)
2022-03-28 19:19:20 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-164
2022-03-28 19:19:20 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:20 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-165
2022-03-28 19:19:21 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-165
2022-03-28 19:19:21 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:21 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-166
2022-03-28 19:19:21 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-cfc59519-isolated will have desired state: Ready not ready, will try again in 1000 ms (806966ms till timeout)
2022-03-28 19:19:22 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-166
2022-03-28 19:19:22 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:22 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-167
2022-03-28 19:19:22 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-167
2022-03-28 19:19:22 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:22 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-168
2022-03-28 19:19:22 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-cfc59519-isolated will have desired state: Ready not ready, will try again in 1000 ms (805869ms till timeout)
2022-03-28 19:19:23 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-168
2022-03-28 19:19:23 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:23 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-169
2022-03-28 19:19:23 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-169
2022-03-28 19:19:23 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:23 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-170
2022-03-28 19:19:23 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-cfc59519-isolated will have desired state: Ready not ready, will try again in 1000 ms (804769ms till timeout)
2022-03-28 19:19:24 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-170
2022-03-28 19:19:24 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:24 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-171
2022-03-28 19:19:24 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-171
2022-03-28 19:19:24 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:24 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-172
2022-03-28 19:19:25 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-cfc59519-isolated will have desired state: Ready not ready, will try again in 1000 ms (803673ms till timeout)
2022-03-28 19:19:25 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-172
2022-03-28 19:19:25 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:25 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-173
2022-03-28 19:19:25 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-173
2022-03-28 19:19:25 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:25 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-174
2022-03-28 19:19:26 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-cfc59519-isolated will have desired state: Ready not ready, will try again in 1000 ms (802576ms till timeout)
2022-03-28 19:19:26 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-174
2022-03-28 19:19:26 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:26 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-175
2022-03-28 19:19:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-175
2022-03-28 19:19:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:27 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-176
2022-03-28 19:19:27 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-cfc59519-isolated will have desired state: Ready not ready, will try again in 1000 ms (801474ms till timeout)
2022-03-28 19:19:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-176
2022-03-28 19:19:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:27 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-177
2022-03-28 19:19:28 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-177
2022-03-28 19:19:28 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:28 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-178
2022-03-28 19:19:28 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-cfc59519-isolated will have desired state: Ready not ready, will try again in 1000 ms (800377ms till timeout)
2022-03-28 19:19:28 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-178
2022-03-28 19:19:28 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:28 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-179
2022-03-28 19:19:29 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-179
2022-03-28 19:19:29 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:29 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-180
2022-03-28 19:19:29 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-cfc59519-isolated will have desired state: Ready not ready, will try again in 1000 ms (799265ms till timeout)
2022-03-28 19:19:29 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-180
2022-03-28 19:19:29 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:29 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-181
2022-03-28 19:19:30 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-181
2022-03-28 19:19:30 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:30 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-182
2022-03-28 19:19:30 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-cfc59519-isolated will have desired state: Ready not ready, will try again in 1000 ms (798168ms till timeout)
2022-03-28 19:19:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-182
2022-03-28 19:19:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:31 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-183
2022-03-28 19:19:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-183
2022-03-28 19:19:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:31 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-184
2022-03-28 19:19:31 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-cfc59519-isolated will have desired state: Ready not ready, will try again in 1000 ms (797071ms till timeout)
2022-03-28 19:19:32 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-184
2022-03-28 19:19:32 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:32 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-185
2022-03-28 19:19:32 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-185
2022-03-28 19:19:32 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:32 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-186
2022-03-28 19:19:32 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-cfc59519-isolated will have desired state: Ready not ready, will try again in 1000 ms (795973ms till timeout)
2022-03-28 19:19:33 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-186
2022-03-28 19:19:33 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:33 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-187
2022-03-28 19:19:33 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-cfc59519-isolated will have desired state: Ready not ready, will try again in 1000 ms (794877ms till timeout)
2022-03-28 19:19:33 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-187
2022-03-28 19:19:33 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:33 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-188
2022-03-28 19:19:34 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-188
2022-03-28 19:19:34 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:34 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-189
2022-03-28 19:19:34 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-cfc59519-isolated will have desired state: Ready not ready, will try again in 1000 ms (793777ms till timeout)
2022-03-28 19:19:34 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-189
2022-03-28 19:19:34 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:34 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-190
2022-03-28 19:19:35 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-190
2022-03-28 19:19:35 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:35 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-191
2022-03-28 19:19:36 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-cfc59519-isolated will have desired state: Ready not ready, will try again in 1000 ms (792679ms till timeout)
2022-03-28 19:19:36 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-191
2022-03-28 19:19:36 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:36 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-192
2022-03-28 19:19:36 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-192
2022-03-28 19:19:36 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:36 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-193
2022-03-28 19:19:37 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-cfc59519-isolated will have desired state: Ready not ready, will try again in 1000 ms (791566ms till timeout)
2022-03-28 19:19:37 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-193
2022-03-28 19:19:37 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:37 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-194
2022-03-28 19:19:37 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-194
2022-03-28 19:19:37 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:37 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-195
2022-03-28 19:19:38 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-cfc59519-isolated will have desired state: Ready not ready, will try again in 1000 ms (790469ms till timeout)
2022-03-28 19:19:38 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-195
2022-03-28 19:19:38 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:38 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-196
2022-03-28 19:19:38 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-196
2022-03-28 19:19:38 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:38 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-197
2022-03-28 19:19:39 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-cfc59519-isolated will have desired state: Ready not ready, will try again in 1000 ms (789370ms till timeout)
2022-03-28 19:19:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-197
2022-03-28 19:19:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:39 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-198
2022-03-28 19:19:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-198
2022-03-28 19:19:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:39 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-199
2022-03-28 19:19:40 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-cfc59519-isolated will have desired state: Ready not ready, will try again in 1000 ms (788267ms till timeout)
2022-03-28 19:19:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-199
2022-03-28 19:19:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:40 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-200
2022-03-28 19:19:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-200
2022-03-28 19:19:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:41 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-201
2022-03-28 19:19:41 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-cfc59519-isolated will have desired state: Ready not ready, will try again in 1000 ms (787168ms till timeout)
2022-03-28 19:19:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-201
2022-03-28 19:19:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:41 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-202
2022-03-28 19:19:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-202
2022-03-28 19:19:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:42 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-203
2022-03-28 19:19:42 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-cfc59519-isolated will have desired state: Ready not ready, will try again in 1000 ms (786071ms till timeout)
2022-03-28 19:19:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-203
2022-03-28 19:19:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:42 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-204
2022-03-28 19:19:43 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-204
2022-03-28 19:19:43 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:43 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-205
2022-03-28 19:19:43 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-cfc59519-isolated will have desired state: Ready not ready, will try again in 1000 ms (784973ms till timeout)
2022-03-28 19:19:43 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-205
2022-03-28 19:19:43 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:43 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-206
2022-03-28 19:19:44 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-206
2022-03-28 19:19:44 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:44 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-207
2022-03-28 19:19:44 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-cfc59519-isolated will have desired state: Ready not ready, will try again in 1000 ms (783876ms till timeout)
2022-03-28 19:19:45 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-207
2022-03-28 19:19:45 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:45 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-219
2022-03-28 19:19:45 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-219
2022-03-28 19:19:45 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:45 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-223
2022-03-28 19:19:45 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-cfc59519-isolated will have desired state: Ready not ready, will try again in 1000 ms (782778ms till timeout)
2022-03-28 19:19:46 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-223
2022-03-28 19:19:46 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:46 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-224
2022-03-28 19:19:46 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-224
2022-03-28 19:19:46 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:46 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-225
2022-03-28 19:19:47 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-cfc59519-isolated will have desired state: Ready not ready, will try again in 1000 ms (781680ms till timeout)
2022-03-28 19:19:47 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-225
2022-03-28 19:19:47 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:47 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-250
2022-03-28 19:19:47 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-250
2022-03-28 19:19:47 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:47 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-251
2022-03-28 19:19:48 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-cfc59519-isolated will have desired state: Ready not ready, will try again in 1000 ms (780579ms till timeout)
2022-03-28 19:19:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-251
2022-03-28 19:19:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:48 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-252
2022-03-28 19:19:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-252
2022-03-28 19:19:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:48 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-253
2022-03-28 19:19:49 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-cfc59519-isolated will have desired state: Ready not ready, will try again in 1000 ms (779481ms till timeout)
2022-03-28 19:19:49 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-253
2022-03-28 19:19:49 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:49 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-254
2022-03-28 19:19:50 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-254
2022-03-28 19:19:50 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:50 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-255
2022-03-28 19:19:50 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-cfc59519-isolated will have desired state: Ready not ready, will try again in 1000 ms (778383ms till timeout)
2022-03-28 19:19:50 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-255
2022-03-28 19:19:50 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:50 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-256
2022-03-28 19:19:51 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-256
2022-03-28 19:19:51 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:51 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-257
2022-03-28 19:19:51 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-cfc59519-isolated will have desired state: Ready not ready, will try again in 1000 ms (777283ms till timeout)
2022-03-28 19:19:51 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-257
2022-03-28 19:19:51 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:51 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-258
2022-03-28 19:19:52 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-258
2022-03-28 19:19:52 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:52 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-259
2022-03-28 19:19:52 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-cfc59519-isolated will have desired state: Ready not ready, will try again in 1000 ms (776164ms till timeout)
2022-03-28 19:19:53 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-259
2022-03-28 19:19:53 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:53 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-260
2022-03-28 19:19:53 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-260
2022-03-28 19:19:53 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:53 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-261
2022-03-28 19:19:53 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-cfc59519-isolated will have desired state: Ready not ready, will try again in 1000 ms (775065ms till timeout)
2022-03-28 19:19:54 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-261
2022-03-28 19:19:54 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:54 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-262
2022-03-28 19:19:54 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-262
2022-03-28 19:19:54 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:54 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-263
2022-03-28 19:19:54 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-cfc59519-isolated will have desired state: Ready not ready, will try again in 1000 ms (773969ms till timeout)
2022-03-28 19:19:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-263
2022-03-28 19:19:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:55 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-264
2022-03-28 19:19:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-264
2022-03-28 19:19:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:55 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-265
2022-03-28 19:19:55 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-cfc59519-isolated will have desired state: Ready not ready, will try again in 1000 ms (772873ms till timeout)
2022-03-28 19:19:56 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-265
2022-03-28 19:19:56 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:56 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-266
2022-03-28 19:19:56 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-266
2022-03-28 19:19:56 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:56 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-267
2022-03-28 19:19:56 [ForkJoinPool-1-worker-27] INFO  [ResourceManager:444] Kafka: my-cluster-cfc59519-isolated is in desired state: Ready
2022-03-28 19:19:56 [ForkJoinPool-1-worker-27] INFO  [ResourceManager:155] Create/Update Deployment my-cluster-cfc59519-isolated-kafka-clients in namespace topic-st
2022-03-28 19:19:57 [ForkJoinPool-1-worker-27] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:my-cluster-cfc59519-isolated-kafka-clients
2022-03-28 19:19:57 [ForkJoinPool-1-worker-27] INFO  [DeploymentUtils:161] Wait for Deployment: my-cluster-cfc59519-isolated-kafka-clients will be ready
2022-03-28 19:19:57 [ForkJoinPool-1-worker-27] DEBUG [TestUtils:125] Waiting for Wait for Deployment: my-cluster-cfc59519-isolated-kafka-clients will be ready
2022-03-28 19:19:57 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Wait for Deployment: my-cluster-cfc59519-isolated-kafka-clients will be ready not ready, will try again in 1000 ms (479905ms till timeout)
2022-03-28 19:19:57 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-267
2022-03-28 19:19:57 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:57 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-268
2022-03-28 19:19:58 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-268
2022-03-28 19:19:58 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:58 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-269
2022-03-28 19:19:58 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Wait for Deployment: my-cluster-cfc59519-isolated-kafka-clients will be ready not ready, will try again in 1000 ms (478807ms till timeout)
2022-03-28 19:19:58 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-269
2022-03-28 19:19:58 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:58 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-270
2022-03-28 19:19:59 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-270
2022-03-28 19:19:59 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:59 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-271
2022-03-28 19:19:59 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Wait for Deployment: my-cluster-cfc59519-isolated-kafka-clients will be ready not ready, will try again in 1000 ms (477709ms till timeout)
2022-03-28 19:19:59 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-271
2022-03-28 19:19:59 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:19:59 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-272
2022-03-28 19:20:00 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-272
2022-03-28 19:20:00 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:00 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-273
2022-03-28 19:20:00 [ForkJoinPool-1-worker-27] INFO  [DeploymentUtils:168] Deployment: my-cluster-cfc59519-isolated-kafka-clients is ready
2022-03-28 19:20:00 [ForkJoinPool-1-worker-27] INFO  [ResourceManager:155] Create/Update KafkaTopic my-topic-1545127738-3726149 in namespace topic-st
2022-03-28 19:20:00 [ForkJoinPool-1-worker-27] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-1545127738-3726149
2022-03-28 19:20:00 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-273
2022-03-28 19:20:00 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:00 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-274
2022-03-28 19:20:00 [ForkJoinPool-1-worker-27] INFO  [ResourceManager:433] Wait for KafkaTopic: my-topic-1545127738-3726149 will have desired state: Ready
2022-03-28 19:20:00 [ForkJoinPool-1-worker-27] DEBUG [TestUtils:125] Waiting for KafkaTopic: my-topic-1545127738-3726149 will have desired state: Ready
2022-03-28 19:20:01 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] KafkaTopic: my-topic-1545127738-3726149 will have desired state: Ready not ready, will try again in 1000 ms (179904ms till timeout)
2022-03-28 19:20:01 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-274
2022-03-28 19:20:01 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:01 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-275
2022-03-28 19:20:02 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-275
2022-03-28 19:20:02 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:02 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-276
2022-03-28 19:20:02 [ForkJoinPool-1-worker-27] INFO  [ResourceManager:444] KafkaTopic: my-topic-1545127738-3726149 is in desired state: Ready
2022-03-28 19:20:02 [ForkJoinPool-1-worker-27] INFO  [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-03-28 19:20:02 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-276
2022-03-28 19:20:02 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:02 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-277
2022-03-28 19:20:02 [ForkJoinPool-1-worker-27] DEBUG [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@6edfb52f, which are set.
2022-03-28 19:20:02 [ForkJoinPool-1-worker-27] INFO  [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@349ac210, messages=[], arguments=[--bootstrap-server, my-cluster-cfc59519-isolated-kafka-bootstrap.topic-st.svc:9092, --topic, my-topic-1545127738-3726149, --max-messages, 100], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-cfc59519-isolated-kafka-clients-7dcff6f8fc-lgphv', podNamespace='topic-st', bootstrapServer='my-cluster-cfc59519-isolated-kafka-bootstrap.topic-st.svc:9092', topicName='my-topic-1545127738-3726149', maxMessages=100, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@6edfb52f}
2022-03-28 19:20:02 [ForkJoinPool-1-worker-27] INFO  [InternalKafkaClient:94] Producing 100 messages to my-cluster-cfc59519-isolated-kafka-bootstrap.topic-st.svc:9092:my-topic-1545127738-3726149 from pod my-cluster-cfc59519-isolated-kafka-clients-7dcff6f8fc-lgphv
2022-03-28 19:20:02 [ForkJoinPool-1-worker-27] INFO  [VerifiableClient:192] Client command: oc exec my-cluster-cfc59519-isolated-kafka-clients-7dcff6f8fc-lgphv -n topic-st -- /opt/kafka/producer.sh --bootstrap-server my-cluster-cfc59519-isolated-kafka-bootstrap.topic-st.svc:9092 --topic my-topic-1545127738-3726149 --max-messages 100
2022-03-28 19:20:02 [ForkJoinPool-1-worker-27] TRACE [Exec:248] Running command - oc exec my-cluster-cfc59519-isolated-kafka-clients-7dcff6f8fc-lgphv -n topic-st -- /opt/kafka/producer.sh --bootstrap-server my-cluster-cfc59519-isolated-kafka-bootstrap.topic-st.svc:9092 --topic my-topic-1545127738-3726149 --max-messages 100
2022-03-28 19:20:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-277
2022-03-28 19:20:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:03 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-278
2022-03-28 19:20:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-278
2022-03-28 19:20:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:03 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-279
2022-03-28 19:20:04 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-279
2022-03-28 19:20:04 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:04 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-280
2022-03-28 19:20:04 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-280
2022-03-28 19:20:04 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:04 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-281
2022-03-28 19:20:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-281
2022-03-28 19:20:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:05 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-282
2022-03-28 19:20:05 [ForkJoinPool-1-worker-27] INFO  [InternalKafkaClient:97] Producer finished correctly: true
2022-03-28 19:20:05 [ForkJoinPool-1-worker-27] INFO  [InternalKafkaClient:101] Producer produced 100 messages
2022-03-28 19:20:05 [ForkJoinPool-1-worker-27] INFO  [TopicST:395] Deleting KafkaTopic: my-topic-1545127738-3726149
2022-03-28 19:20:05 [ForkJoinPool-1-worker-27] INFO  [TopicST:397] KafkaTopic my-topic-1545127738-3726149 deleted
2022-03-28 19:20:05 [ForkJoinPool-1-worker-27] DEBUG [TestUtils:125] Waiting for Topic my-topic-1545127738-3726149 has rolled
2022-03-28 19:20:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-282
2022-03-28 19:20:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:05 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-283
2022-03-28 19:20:05 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (299903ms till timeout)
2022-03-28 19:20:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-283
2022-03-28 19:20:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:06 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-284
2022-03-28 19:20:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-284
2022-03-28 19:20:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:06 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-285
2022-03-28 19:20:06 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (298805ms till timeout)
2022-03-28 19:20:07 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-285
2022-03-28 19:20:07 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:07 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-286
2022-03-28 19:20:08 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-286
2022-03-28 19:20:08 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:08 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-287
2022-03-28 19:20:08 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (297706ms till timeout)
2022-03-28 19:20:08 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-287
2022-03-28 19:20:08 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:08 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-288
2022-03-28 19:20:09 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-288
2022-03-28 19:20:09 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:09 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-289
2022-03-28 19:20:09 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (296610ms till timeout)
2022-03-28 19:20:09 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-289
2022-03-28 19:20:09 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:09 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-290
2022-03-28 19:20:10 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-290
2022-03-28 19:20:10 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:10 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-291
2022-03-28 19:20:10 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (295512ms till timeout)
2022-03-28 19:20:10 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-291
2022-03-28 19:20:10 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:10 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-292
2022-03-28 19:20:11 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-292
2022-03-28 19:20:11 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:11 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-293
2022-03-28 19:20:11 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (294415ms till timeout)
2022-03-28 19:20:11 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-293
2022-03-28 19:20:11 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:11 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-294
2022-03-28 19:20:12 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-294
2022-03-28 19:20:12 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:12 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-295
2022-03-28 19:20:12 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (293319ms till timeout)
2022-03-28 19:20:12 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-295
2022-03-28 19:20:12 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:12 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-296
2022-03-28 19:20:13 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-296
2022-03-28 19:20:13 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:13 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-297
2022-03-28 19:20:13 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (292223ms till timeout)
2022-03-28 19:20:14 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-297
2022-03-28 19:20:14 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:14 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-298
2022-03-28 19:20:14 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (291125ms till timeout)
2022-03-28 19:20:14 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-298
2022-03-28 19:20:14 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:14 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-299
2022-03-28 19:20:15 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-299
2022-03-28 19:20:15 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:15 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-300
2022-03-28 19:20:15 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (290029ms till timeout)
2022-03-28 19:20:15 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-300
2022-03-28 19:20:15 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:15 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-301
2022-03-28 19:20:16 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-301
2022-03-28 19:20:16 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:16 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-302
2022-03-28 19:20:16 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (288931ms till timeout)
2022-03-28 19:20:16 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-302
2022-03-28 19:20:16 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:16 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-303
2022-03-28 19:20:17 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-303
2022-03-28 19:20:17 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:17 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-304
2022-03-28 19:20:17 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (287834ms till timeout)
2022-03-28 19:20:18 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-304
2022-03-28 19:20:18 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:18 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-305
2022-03-28 19:20:18 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-305
2022-03-28 19:20:18 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:18 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-306
2022-03-28 19:20:19 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (286737ms till timeout)
2022-03-28 19:20:19 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-306
2022-03-28 19:20:19 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:19 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-307
2022-03-28 19:20:19 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-307
2022-03-28 19:20:19 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:19 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-308
2022-03-28 19:20:20 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (285639ms till timeout)
2022-03-28 19:20:20 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-308
2022-03-28 19:20:20 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:20 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-309
2022-03-28 19:20:20 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-309
2022-03-28 19:20:20 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:20 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-310
2022-03-28 19:20:21 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (284539ms till timeout)
2022-03-28 19:20:21 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-310
2022-03-28 19:20:21 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:21 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-311
2022-03-28 19:20:22 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-311
2022-03-28 19:20:22 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:22 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-312
2022-03-28 19:20:22 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (283443ms till timeout)
2022-03-28 19:20:22 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-312
2022-03-28 19:20:22 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:22 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-313
2022-03-28 19:20:23 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-313
2022-03-28 19:20:23 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:23 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-314
2022-03-28 19:20:23 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (282345ms till timeout)
2022-03-28 19:20:23 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-314
2022-03-28 19:20:23 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:23 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-315
2022-03-28 19:20:24 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-315
2022-03-28 19:20:24 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:24 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-316
2022-03-28 19:20:24 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (281248ms till timeout)
2022-03-28 19:20:24 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-316
2022-03-28 19:20:24 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:24 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-317
2022-03-28 19:20:25 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-317
2022-03-28 19:20:25 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:25 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-318
2022-03-28 19:20:25 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (280151ms till timeout)
2022-03-28 19:20:25 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-318
2022-03-28 19:20:25 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:25 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-319
2022-03-28 19:20:26 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-319
2022-03-28 19:20:26 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:26 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-320
2022-03-28 19:20:26 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (279053ms till timeout)
2022-03-28 19:20:26 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-320
2022-03-28 19:20:26 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:26 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-321
2022-03-28 19:20:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-321
2022-03-28 19:20:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:27 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-322
2022-03-28 19:20:27 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (277957ms till timeout)
2022-03-28 19:20:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-322
2022-03-28 19:20:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:27 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-323
2022-03-28 19:20:28 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-323
2022-03-28 19:20:28 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:28 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-324
2022-03-28 19:20:28 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (276859ms till timeout)
2022-03-28 19:20:29 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-324
2022-03-28 19:20:29 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:29 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-325
2022-03-28 19:20:29 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-325
2022-03-28 19:20:29 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:29 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-326
2022-03-28 19:20:30 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (275762ms till timeout)
2022-03-28 19:20:30 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-326
2022-03-28 19:20:30 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:30 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-327
2022-03-28 19:20:30 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-327
2022-03-28 19:20:30 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:30 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-328
2022-03-28 19:20:31 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (274666ms till timeout)
2022-03-28 19:20:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-328
2022-03-28 19:20:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:31 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-344
2022-03-28 19:20:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-344
2022-03-28 19:20:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:31 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-345
2022-03-28 19:20:32 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (273569ms till timeout)
2022-03-28 19:20:32 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-345
2022-03-28 19:20:32 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:32 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-346
2022-03-28 19:20:32 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-346
2022-03-28 19:20:32 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:32 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-370
2022-03-28 19:20:33 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (272472ms till timeout)
2022-03-28 19:20:33 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-370
2022-03-28 19:20:33 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:33 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-371
2022-03-28 19:20:33 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-371
2022-03-28 19:20:33 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:33 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-372
2022-03-28 19:20:34 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (271376ms till timeout)
2022-03-28 19:20:34 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-372
2022-03-28 19:20:34 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:34 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-373
2022-03-28 19:20:34 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-373
2022-03-28 19:20:34 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:34 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-374
2022-03-28 19:20:35 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (270280ms till timeout)
2022-03-28 19:20:35 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-374
2022-03-28 19:20:35 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:35 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-375
2022-03-28 19:20:36 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-375
2022-03-28 19:20:36 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:36 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-376
2022-03-28 19:20:36 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (269182ms till timeout)
2022-03-28 19:20:36 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-376
2022-03-28 19:20:36 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:36 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-377
2022-03-28 19:20:37 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-377
2022-03-28 19:20:37 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:37 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-378
2022-03-28 19:20:37 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (268084ms till timeout)
2022-03-28 19:20:37 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-378
2022-03-28 19:20:37 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:37 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-379
2022-03-28 19:20:38 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-379
2022-03-28 19:20:38 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:38 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-380
2022-03-28 19:20:38 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (266987ms till timeout)
2022-03-28 19:20:38 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-380
2022-03-28 19:20:38 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:38 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-381
2022-03-28 19:20:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-381
2022-03-28 19:20:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:39 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-382
2022-03-28 19:20:39 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (265889ms till timeout)
2022-03-28 19:20:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-382
2022-03-28 19:20:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:40 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-383
2022-03-28 19:20:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-383
2022-03-28 19:20:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:40 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-384
2022-03-28 19:20:41 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (264792ms till timeout)
2022-03-28 19:20:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-384
2022-03-28 19:20:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:41 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-385
2022-03-28 19:20:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-385
2022-03-28 19:20:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:41 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-386
2022-03-28 19:20:42 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (263696ms till timeout)
2022-03-28 19:20:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-386
2022-03-28 19:20:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:42 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-387
2022-03-28 19:20:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-387
2022-03-28 19:20:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:42 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-388
2022-03-28 19:20:43 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (262598ms till timeout)
2022-03-28 19:20:43 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-388
2022-03-28 19:20:43 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:43 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-389
2022-03-28 19:20:43 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-389
2022-03-28 19:20:43 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:43 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-390
2022-03-28 19:20:44 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (261503ms till timeout)
2022-03-28 19:20:44 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-390
2022-03-28 19:20:44 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:44 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-391
2022-03-28 19:20:45 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-391
2022-03-28 19:20:45 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:45 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-392
2022-03-28 19:20:45 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (260406ms till timeout)
2022-03-28 19:20:45 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-392
2022-03-28 19:20:45 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:45 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-393
2022-03-28 19:20:46 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-393
2022-03-28 19:20:46 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:46 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-394
2022-03-28 19:20:46 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (259310ms till timeout)
2022-03-28 19:20:46 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-394
2022-03-28 19:20:46 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:46 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-395
2022-03-28 19:20:47 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-395
2022-03-28 19:20:47 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:47 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-396
2022-03-28 19:20:47 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (258211ms till timeout)
2022-03-28 19:20:47 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-396
2022-03-28 19:20:47 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:47 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-397
2022-03-28 19:20:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-397
2022-03-28 19:20:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:48 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-398
2022-03-28 19:20:48 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (257112ms till timeout)
2022-03-28 19:20:49 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-398
2022-03-28 19:20:49 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:49 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-399
2022-03-28 19:20:49 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-399
2022-03-28 19:20:49 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:49 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-406
2022-03-28 19:20:49 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (256015ms till timeout)
2022-03-28 19:20:50 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-406
2022-03-28 19:20:50 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:50 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-407
2022-03-28 19:20:50 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-407
2022-03-28 19:20:50 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:50 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-408
2022-03-28 19:20:50 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (254917ms till timeout)
2022-03-28 19:20:51 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-408
2022-03-28 19:20:51 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:51 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-409
2022-03-28 19:20:51 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-409
2022-03-28 19:20:51 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:51 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-410
2022-03-28 19:20:51 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (253821ms till timeout)
2022-03-28 19:20:52 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-410
2022-03-28 19:20:52 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:52 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-411
2022-03-28 19:20:53 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (252725ms till timeout)
2022-03-28 19:20:53 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-411
2022-03-28 19:20:53 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:53 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-412
2022-03-28 19:20:53 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-412
2022-03-28 19:20:53 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:53 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-413
2022-03-28 19:20:54 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (251628ms till timeout)
2022-03-28 19:20:54 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-413
2022-03-28 19:20:54 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:54 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-414
2022-03-28 19:20:54 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-414
2022-03-28 19:20:54 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:54 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-415
2022-03-28 19:20:55 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (250531ms till timeout)
2022-03-28 19:20:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-415
2022-03-28 19:20:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:55 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-416
2022-03-28 19:20:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-416
2022-03-28 19:20:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:55 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-417
2022-03-28 19:20:56 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (249434ms till timeout)
2022-03-28 19:20:56 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-417
2022-03-28 19:20:56 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:56 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-418
2022-03-28 19:20:56 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-418
2022-03-28 19:20:56 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:56 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-419
2022-03-28 19:20:57 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (248336ms till timeout)
2022-03-28 19:20:57 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-419
2022-03-28 19:20:57 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:57 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-420
2022-03-28 19:20:58 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-420
2022-03-28 19:20:58 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:58 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-421
2022-03-28 19:20:58 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (247238ms till timeout)
2022-03-28 19:20:58 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-421
2022-03-28 19:20:58 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:58 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-422
2022-03-28 19:20:59 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-422
2022-03-28 19:20:59 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:59 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-423
2022-03-28 19:20:59 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (246140ms till timeout)
2022-03-28 19:20:59 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-423
2022-03-28 19:20:59 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:20:59 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-424
2022-03-28 19:21:00 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-424
2022-03-28 19:21:00 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:21:00 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-425
2022-03-28 19:21:00 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (245043ms till timeout)
2022-03-28 19:21:00 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-425
2022-03-28 19:21:00 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:21:00 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-426
2022-03-28 19:21:01 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-426
2022-03-28 19:21:01 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:21:01 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-427
2022-03-28 19:21:01 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (243944ms till timeout)
2022-03-28 19:21:01 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-427
2022-03-28 19:21:01 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:21:01 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-428
2022-03-28 19:21:02 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-428
2022-03-28 19:21:02 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:21:02 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-429
2022-03-28 19:21:02 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (242846ms till timeout)
2022-03-28 19:21:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-429
2022-03-28 19:21:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:21:03 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-430
2022-03-28 19:21:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-430
2022-03-28 19:21:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:21:03 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-431
2022-03-28 19:21:04 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (241747ms till timeout)
2022-03-28 19:21:04 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-431
2022-03-28 19:21:04 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:21:04 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-432
2022-03-28 19:21:04 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-432
2022-03-28 19:21:04 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:21:04 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-433
2022-03-28 19:21:05 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (240648ms till timeout)
2022-03-28 19:21:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-433
2022-03-28 19:21:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:21:05 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-434
2022-03-28 19:21:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-434
2022-03-28 19:21:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:21:05 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-435
2022-03-28 19:21:06 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (239550ms till timeout)
2022-03-28 19:21:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-435
2022-03-28 19:21:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:21:06 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-436
2022-03-28 19:21:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-436
2022-03-28 19:21:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:21:06 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-437
2022-03-28 19:21:07 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (238452ms till timeout)
2022-03-28 19:21:07 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-437
2022-03-28 19:21:07 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:21:07 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-438
2022-03-28 19:21:08 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-438
2022-03-28 19:21:08 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:21:08 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-439
2022-03-28 19:21:08 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (237355ms till timeout)
2022-03-28 19:21:08 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-439
2022-03-28 19:21:08 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:21:08 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-440
2022-03-28 19:21:09 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-440
2022-03-28 19:21:09 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:21:09 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-441
2022-03-28 19:21:09 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (236258ms till timeout)
2022-03-28 19:21:09 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-441
2022-03-28 19:21:09 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:21:09 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-442
2022-03-28 19:21:10 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-442
2022-03-28 19:21:10 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:21:10 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-443
2022-03-28 19:21:10 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (235159ms till timeout)
2022-03-28 19:21:10 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-443
2022-03-28 19:21:10 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:21:10 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-444
2022-03-28 19:21:11 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-444
2022-03-28 19:21:11 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:21:11 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-445
2022-03-28 19:21:11 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (234061ms till timeout)
2022-03-28 19:21:11 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-445
2022-03-28 19:21:11 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:21:11 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-446
2022-03-28 19:21:12 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-446
2022-03-28 19:21:12 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:21:12 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-447
2022-03-28 19:21:12 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (232965ms till timeout)
2022-03-28 19:21:13 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-447
2022-03-28 19:21:13 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:21:13 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-448
2022-03-28 19:21:13 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-448
2022-03-28 19:21:13 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:21:13 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-449
2022-03-28 19:21:13 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (231866ms till timeout)
2022-03-28 19:21:14 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-449
2022-03-28 19:21:14 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:21:14 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-465
2022-03-28 19:21:14 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-465
2022-03-28 19:21:14 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:21:14 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-466
2022-03-28 19:21:15 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (230768ms till timeout)
2022-03-28 19:21:15 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-466
2022-03-28 19:21:15 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:21:15 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-467
2022-03-28 19:21:15 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-467
2022-03-28 19:21:15 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:21:15 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-490
2022-03-28 19:21:16 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (229670ms till timeout)
2022-03-28 19:21:16 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-490
2022-03-28 19:21:16 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:21:16 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-491
2022-03-28 19:21:16 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-491
2022-03-28 19:21:16 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:21:16 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-492
2022-03-28 19:21:17 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (228570ms till timeout)
2022-03-28 19:21:17 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-492
2022-03-28 19:21:17 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:21:17 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-493
2022-03-28 19:21:18 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-493
2022-03-28 19:21:18 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:21:18 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-494
2022-03-28 19:21:18 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (227473ms till timeout)
2022-03-28 19:21:18 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-494
2022-03-28 19:21:18 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:21:18 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-495
2022-03-28 19:21:19 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-495
2022-03-28 19:21:19 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:21:19 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-496
2022-03-28 19:21:19 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (226375ms till timeout)
2022-03-28 19:21:19 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-496
2022-03-28 19:21:19 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:21:19 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-497
2022-03-28 19:21:20 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-497
2022-03-28 19:21:20 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:21:20 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-498
2022-03-28 19:21:20 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (225276ms till timeout)
2022-03-28 19:21:20 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-498
2022-03-28 19:21:20 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:21:20 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-499
2022-03-28 19:21:21 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-499
2022-03-28 19:21:21 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:21:21 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-0
2022-03-28 19:21:21 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (224179ms till timeout)
2022-03-28 19:21:22 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-0
2022-03-28 19:21:22 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:21:22 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-1
2022-03-28 19:21:22 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-1
2022-03-28 19:21:22 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:21:22 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-10
2022-03-28 19:21:22 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (223082ms till timeout)
2022-03-28 19:21:23 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-10
2022-03-28 19:21:23 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:21:23 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-11
2022-03-28 19:21:23 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (221985ms till timeout)
2022-03-28 19:21:23 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-11
2022-03-28 19:21:23 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:21:23 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-12
2022-03-28 19:21:24 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-12
2022-03-28 19:21:24 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:21:24 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-13
2022-03-28 19:21:24 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (220887ms till timeout)
2022-03-28 19:21:25 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-13
2022-03-28 19:21:25 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:21:25 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-14
2022-03-28 19:21:25 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-14
2022-03-28 19:21:25 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:21:25 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-15
2022-03-28 19:21:26 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (219789ms till timeout)
2022-03-28 19:21:26 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-15
2022-03-28 19:21:26 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:21:26 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-16
2022-03-28 19:21:26 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-16
2022-03-28 19:21:26 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:21:26 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-17
2022-03-28 19:21:27 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (218689ms till timeout)
2022-03-28 19:21:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-17
2022-03-28 19:21:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:21:27 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-18
2022-03-28 19:21:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-18
2022-03-28 19:21:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:21:27 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-19
2022-03-28 19:21:28 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (217590ms till timeout)
2022-03-28 19:21:28 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-19
2022-03-28 19:21:28 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:21:28 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-2
2022-03-28 19:21:29 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-2
2022-03-28 19:21:29 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:21:29 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-3
2022-03-28 19:21:29 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (216492ms till timeout)
2022-03-28 19:21:29 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-3
2022-03-28 19:21:29 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:21:29 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-31
2022-03-28 19:21:30 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-31
2022-03-28 19:21:30 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:21:30 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-32
2022-03-28 19:21:30 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (215396ms till timeout)
2022-03-28 19:21:30 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-32
2022-03-28 19:21:30 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:21:30 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-4
2022-03-28 19:21:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-4
2022-03-28 19:21:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:21:31 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-5
2022-03-28 19:21:31 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (214288ms till timeout)
2022-03-28 19:21:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-5
2022-03-28 19:21:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:21:31 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-6
2022-03-28 19:21:32 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-6
2022-03-28 19:21:32 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:21:32 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-7
2022-03-28 19:21:32 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (213176ms till timeout)
2022-03-28 19:21:33 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-7
2022-03-28 19:21:33 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:21:33 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-8
2022-03-28 19:21:33 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (212078ms till timeout)
2022-03-28 19:21:34 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-8
2022-03-28 19:21:34 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:21:34 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-9
2022-03-28 19:21:34 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (210981ms till timeout)
2022-03-28 19:21:35 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (209884ms till timeout)
2022-03-28 19:21:37 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (208784ms till timeout)
2022-03-28 19:21:38 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (207673ms till timeout)
2022-03-28 19:21:39 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (206575ms till timeout)
2022-03-28 19:21:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-9
2022-03-28 19:21:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:21:39 [ForkJoinPool-1-worker-9] DEBUG [AbstractST:689] ============================================================================
2022-03-28 19:21:39 [ForkJoinPool-1-worker-9] DEBUG [AbstractST:690] [operators.topic.ThrottlingQuotaST - After All] - Clean up after test suite
2022-03-28 19:21:39 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:344] ############################################################################
2022-03-28 19:21:39 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:348] Delete all resources for ThrottlingQuotaST
2022-03-28 19:21:39 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:241] Delete of Kafka quota-cluster in namespace throttling-quota-st
2022-03-28 19:21:39 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:quota-cluster
2022-03-28 19:21:39 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:quota-cluster not ready, will try again in 10000 ms (839883ms till timeout)
2022-03-28 19:21:40 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (205461ms till timeout)
2022-03-28 19:21:41 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (204363ms till timeout)
2022-03-28 19:21:42 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (203265ms till timeout)
2022-03-28 19:21:43 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (202163ms till timeout)
2022-03-28 19:21:44 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (201065ms till timeout)
2022-03-28 19:21:45 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (199966ms till timeout)
2022-03-28 19:21:46 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (198869ms till timeout)
2022-03-28 19:21:48 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (197771ms till timeout)
2022-03-28 19:21:49 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (196674ms till timeout)
2022-03-28 19:21:50 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:369] ############################################################################
2022-03-28 19:21:50 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Namespace throttling-quota-st removal
2022-03-28 19:21:50 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 19:21:50 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-1545127738-3726149 has rolled not ready, will try again in 1000 ms (195511ms till timeout)
2022-03-28 19:21:51 [ForkJoinPool-1-worker-27] INFO  [TopicST:401] Wait KafkaTopic my-topic-1545127738-3726149 recreation
2022-03-28 19:21:51 [ForkJoinPool-1-worker-27] INFO  [KafkaTopicUtils:78] Waiting for KafkaTopic my-topic-1545127738-3726149 creation 
2022-03-28 19:21:51 [ForkJoinPool-1-worker-27] DEBUG [TestUtils:125] Waiting for KafkaTopic creation my-topic-1545127738-3726149
2022-03-28 19:21:51 [ForkJoinPool-1-worker-27] INFO  [TopicST:403] KafkaTopic my-topic-1545127738-3726149 recreated
2022-03-28 19:21:51 [ForkJoinPool-1-worker-27] DEBUG [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@23e51ffd, which are set.
2022-03-28 19:21:51 [ForkJoinPool-1-worker-27] INFO  [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@410dd3d3, messages=[], arguments=[--bootstrap-server, my-cluster-cfc59519-isolated-kafka-bootstrap.topic-st.svc:9092, --topic, my-topic-1545127738-3726149, --group-instance-id, instance332659797, --group-id, my-consumer-group-1732246174, --max-messages, 100], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-cfc59519-isolated-kafka-clients-7dcff6f8fc-lgphv', podNamespace='topic-st', bootstrapServer='my-cluster-cfc59519-isolated-kafka-bootstrap.topic-st.svc:9092', topicName='my-topic-1545127738-3726149', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-1732246174', consumerInstanceId='instance332659797', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@23e51ffd}
2022-03-28 19:21:51 [ForkJoinPool-1-worker-27] INFO  [InternalKafkaClient:157] Consuming 100 messages from my-cluster-cfc59519-isolated-kafka-bootstrap.topic-st.svc:9092#my-topic-1545127738-3726149 from pod my-cluster-cfc59519-isolated-kafka-clients-7dcff6f8fc-lgphv
2022-03-28 19:21:51 [ForkJoinPool-1-worker-27] INFO  [VerifiableClient:192] Client command: oc exec my-cluster-cfc59519-isolated-kafka-clients-7dcff6f8fc-lgphv -n topic-st -- /opt/kafka/consumer.sh --bootstrap-server my-cluster-cfc59519-isolated-kafka-bootstrap.topic-st.svc:9092 --topic my-topic-1545127738-3726149 --group-instance-id instance332659797 --group-id my-consumer-group-1732246174 --max-messages 100
2022-03-28 19:21:51 [ForkJoinPool-1-worker-27] TRACE [Exec:248] Running command - oc exec my-cluster-cfc59519-isolated-kafka-clients-7dcff6f8fc-lgphv -n topic-st -- /opt/kafka/consumer.sh --bootstrap-server my-cluster-cfc59519-isolated-kafka-bootstrap.topic-st.svc:9092 --topic my-topic-1545127738-3726149 --group-instance-id instance332659797 --group-id my-consumer-group-1732246174 --max-messages 100
2022-03-28 19:21:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 19:21:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:21:55 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Namespace throttling-quota-st removal not ready, will try again in 1000 ms (474470ms till timeout)
2022-03-28 19:21:56 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 19:21:57 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 19:21:57 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:21:57 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Namespace throttling-quota-st removal not ready, will try again in 1000 ms (472930ms till timeout)
2022-03-28 19:21:57 [ForkJoinPool-1-worker-27] INFO  [InternalKafkaClient:160] Consumer finished correctly: true
2022-03-28 19:21:57 [ForkJoinPool-1-worker-27] INFO  [InternalKafkaClient:163] Consumer consumed 100 messages
2022-03-28 19:21:57 [ForkJoinPool-1-worker-27] DEBUG [AbstractST:674] ============================================================================
2022-03-28 19:21:57 [ForkJoinPool-1-worker-27] DEBUG [AbstractST:675] [operators.topic.TopicST - After Each] - Clean up after test
2022-03-28 19:21:57 [ForkJoinPool-1-worker-27] INFO  [ResourceManager:344] ############################################################################
2022-03-28 19:21:57 [ForkJoinPool-1-worker-27] INFO  [ResourceManager:348] Delete all resources for testDeleteTopicEnableFalse
2022-03-28 19:21:57 [ForkJoinPool-1-worker-27] INFO  [ResourceManager:241] Delete of Deployment my-cluster-cfc59519-isolated-kafka-clients in namespace topic-st
2022-03-28 19:21:57 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:241] Delete of KafkaTopic my-topic-1545127738-3726149 in namespace topic-st
2022-03-28 19:21:57 [ForkJoinPool-1-worker-41] INFO  [ResourceManager:241] Delete of Kafka my-cluster-cfc59519-isolated in namespace topic-st
2022-03-28 19:21:58 [ForkJoinPool-1-worker-27] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:my-cluster-cfc59519-isolated-kafka-clients
2022-03-28 19:21:58 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1545127738-3726149
2022-03-28 19:21:58 [ForkJoinPool-1-worker-41] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-cfc59519-isolated
2022-03-28 19:21:58 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 19:21:58 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-cfc59519-isolated-kafka-clients not ready, will try again in 10000 ms (479513ms till timeout)
2022-03-28 19:21:58 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 19:21:58 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 19:21:58 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Namespace throttling-quota-st removal not ready, will try again in 1000 ms (471412ms till timeout)
2022-03-28 19:21:59 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 19:22:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 19:22:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 1
2022-03-28 19:22:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 19:22:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Error from server (NotFound): namespaces "throttling-quota-st" not found
2022-03-28 19:22:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] ======STDERR END======
2022-03-28 19:22:05 [ForkJoinPool-1-worker-9] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@34093228=[], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[], io.strimzi.test.logs.CollectorElement@c4c0ea0=[], io.strimzi.test.logs.CollectorElement@aec142ef=[], io.strimzi.test.logs.CollectorElement@24b97ba9=[], io.strimzi.test.logs.CollectorElement@f851b6c3=[], io.strimzi.test.logs.CollectorElement@3881d5f2=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@e3067dd1=[], io.strimzi.test.logs.CollectorElement@5c7379cb=[], io.strimzi.test.logs.CollectorElement@62b9e483=[], io.strimzi.test.logs.CollectorElement@b850ef2a=[infra-namespace, reconciliation-st], io.strimzi.test.logs.CollectorElement@f4405b3b=[], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 19:22:05 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:254] ThrottlingQuotaST - Notifies waiting test suites:[TopicST, ThrottlingQuotaST, UserST, HttpBridgeTlsST, HttpBridgeScramShaST, ReconciliationST, CruiseControlConfigurationST] to and randomly select one to start execution
2022-03-28 19:22:05 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:85] [operators.topic.ThrottlingQuotaST] - Removing parallel suite: ThrottlingQuotaST
2022-03-28 19:22:05 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:89] [operators.topic.ThrottlingQuotaST] - Parallel suites count: 1
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1,620.851 s - in io.strimzi.systemtest.operators.topic.ThrottlingQuotaST
2022-03-28 19:22:08 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-cfc59519-isolated-kafka-clients not ready, will try again in 10000 ms (469128ms till timeout)
2022-03-28 19:22:19 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-cfc59519-isolated-kafka-clients not ready, will try again in 10000 ms (458743ms till timeout)
2022-03-28 19:22:29 [ForkJoinPool-1-worker-27] INFO  [ResourceManager:369] ############################################################################
2022-03-28 19:22:29 [ForkJoinPool-1-worker-27] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.topic.TopicST.testDeleteTopicEnableFalse-FINISHED
2022-03-28 19:22:29 [ForkJoinPool-1-worker-27] INFO  [TestSeparator:30] ############################################################################
2022-03-28 19:22:29 [ForkJoinPool-1-worker-27] DEBUG [AbstractST:689] ============================================================================
2022-03-28 19:22:29 [ForkJoinPool-1-worker-27] DEBUG [AbstractST:690] [operators.topic.TopicST - After All] - Clean up after test suite
2022-03-28 19:22:29 [ForkJoinPool-1-worker-27] INFO  [ResourceManager:344] ############################################################################
2022-03-28 19:22:29 [ForkJoinPool-1-worker-27] INFO  [ResourceManager:348] Delete all resources for TopicST
2022-03-28 19:22:29 [ForkJoinPool-1-worker-27] INFO  [ResourceManager:241] Delete of Kafka topic-cluster-name in namespace topic-st
2022-03-28 19:22:29 [ForkJoinPool-1-worker-27] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:topic-cluster-name
2022-03-28 19:22:29 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:topic-cluster-name not ready, will try again in 10000 ms (839897ms till timeout)
2022-03-28 19:22:39 [ForkJoinPool-1-worker-27] INFO  [ResourceManager:369] ############################################################################
2022-03-28 19:22:39 [ForkJoinPool-1-worker-27] DEBUG [TestUtils:125] Waiting for Namespace topic-st removal
2022-03-28 19:22:39 [ForkJoinPool-1-worker-27] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 19:22:40 [ForkJoinPool-1-worker-27] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 19:22:40 [ForkJoinPool-1-worker-27] DEBUG [Exec:419] Return code: 0
2022-03-28 19:22:40 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Namespace topic-st removal not ready, will try again in 1000 ms (478994ms till timeout)
2022-03-28 19:22:41 [ForkJoinPool-1-worker-27] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 19:22:42 [ForkJoinPool-1-worker-27] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 19:22:42 [ForkJoinPool-1-worker-27] DEBUG [Exec:419] Return code: 0
2022-03-28 19:22:42 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Namespace topic-st removal not ready, will try again in 1000 ms (477550ms till timeout)
2022-03-28 19:22:43 [ForkJoinPool-1-worker-27] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 19:22:43 [ForkJoinPool-1-worker-27] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 19:22:43 [ForkJoinPool-1-worker-27] DEBUG [Exec:419] Return code: 0
2022-03-28 19:22:43 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Namespace topic-st removal not ready, will try again in 1000 ms (476108ms till timeout)
2022-03-28 19:22:44 [ForkJoinPool-1-worker-27] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 19:22:45 [ForkJoinPool-1-worker-27] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 19:22:45 [ForkJoinPool-1-worker-27] DEBUG [Exec:419] Return code: 0
2022-03-28 19:22:45 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Namespace topic-st removal not ready, will try again in 1000 ms (474621ms till timeout)
2022-03-28 19:22:46 [ForkJoinPool-1-worker-27] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 19:22:46 [ForkJoinPool-1-worker-27] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 19:22:46 [ForkJoinPool-1-worker-27] DEBUG [Exec:419] Return code: 0
2022-03-28 19:22:46 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Namespace topic-st removal not ready, will try again in 1000 ms (473156ms till timeout)
2022-03-28 19:22:47 [ForkJoinPool-1-worker-27] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 19:22:53 [ForkJoinPool-1-worker-27] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 19:22:53 [ForkJoinPool-1-worker-27] DEBUG [Exec:419] Return code: 1
2022-03-28 19:22:53 [ForkJoinPool-1-worker-27] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 19:22:53 [ForkJoinPool-1-worker-27] DEBUG [Exec:419] Error from server (NotFound): namespaces "topic-st" not found
2022-03-28 19:22:53 [ForkJoinPool-1-worker-27] DEBUG [Exec:419] ======STDERR END======
2022-03-28 19:22:53 [ForkJoinPool-1-worker-27] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@34093228=[], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[], io.strimzi.test.logs.CollectorElement@c45aa4b1=[], io.strimzi.test.logs.CollectorElement@c4c0ea0=[], io.strimzi.test.logs.CollectorElement@aec142ef=[], io.strimzi.test.logs.CollectorElement@24b97ba9=[], io.strimzi.test.logs.CollectorElement@f851b6c3=[], io.strimzi.test.logs.CollectorElement@3881d5f2=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@e3067dd1=[], io.strimzi.test.logs.CollectorElement@5c7379cb=[], io.strimzi.test.logs.CollectorElement@62b9e483=[], io.strimzi.test.logs.CollectorElement@b850ef2a=[infra-namespace, reconciliation-st], io.strimzi.test.logs.CollectorElement@f4405b3b=[], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 19:22:53 [ForkJoinPool-1-worker-27] DEBUG [SuiteThreadController:254] TopicST - Notifies waiting test suites:[TopicST, ThrottlingQuotaST, UserST, HttpBridgeTlsST, HttpBridgeScramShaST, ReconciliationST, CruiseControlConfigurationST] to and randomly select one to start execution
2022-03-28 19:22:53 [ForkJoinPool-1-worker-27] DEBUG [SuiteThreadController:85] [operators.topic.TopicST] - Removing parallel suite: TopicST
2022-03-28 19:22:53 [ForkJoinPool-1-worker-27] DEBUG [SuiteThreadController:89] [operators.topic.TopicST] - Parallel suites count: 0
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1,668.9 s - in io.strimzi.systemtest.operators.topic.TopicST
2022-03-28 19:22:53 [ForkJoinPool-1-worker-19] INFO  [SetupClusterOperator:618] ============================================================================
2022-03-28 19:22:53 [ForkJoinPool-1-worker-19] INFO  [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-03-28 19:22:53 [ForkJoinPool-1-worker-19] INFO  [SetupClusterOperator:620] ============================================================================
2022-03-28 19:22:53 [ForkJoinPool-1-worker-19] INFO  [ResourceManager:344] ############################################################################
2022-03-28 19:22:53 [ForkJoinPool-1-worker-19] INFO  [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-03-28 19:22:53 [ForkJoinPool-1-worker-19] INFO  [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-28 19:22:53 [ForkJoinPool-1-worker-31] INFO  [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-28 19:22:53 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-03-28 19:22:53 [ForkJoinPool-1-worker-27] INFO  [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-28 19:22:53 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-28 19:22:53 [ForkJoinPool-1-worker-41] INFO  [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-28 19:22:53 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-28 19:22:53 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-28 19:22:53 [ForkJoinPool-1-worker-33] INFO  [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-28 19:22:53 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-28 19:22:53 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-28 19:22:53 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-28 19:22:53 [ForkJoinPool-1-worker-29] INFO  [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-28 19:22:53 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-28 19:22:53 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-28 19:22:53 [ForkJoinPool-1-worker-19] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-28 19:22:53 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-28 19:22:53 [ForkJoinPool-1-worker-31] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-28 19:22:53 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-entity-operator
2022-03-28 19:22:53 [ForkJoinPool-1-worker-25] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-28 19:22:53 [ForkJoinPool-1-worker-29] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-28 19:22:53 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-namespaced
2022-03-28 19:22:53 [ForkJoinPool-1-worker-33] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-28 19:22:53 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-28 19:22:53 [ForkJoinPool-1-worker-27] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-28 19:22:53 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-28 19:22:53 [ForkJoinPool-1-worker-41] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-28 19:22:53 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-28 19:22:53 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-28 19:22:53 [ForkJoinPool-1-worker-19] INFO  [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-28 19:22:53 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-28 19:22:53 [ForkJoinPool-1-worker-29] INFO  [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-28 19:22:53 [ForkJoinPool-1-worker-31] INFO  [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-28 19:22:53 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-28 19:22:53 [ForkJoinPool-1-worker-27] INFO  [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-28 19:22:53 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-28 19:22:53 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-28 19:22:53 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-28 19:22:53 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-28 19:22:53 [ForkJoinPool-1-worker-19] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-28 19:22:53 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-28 19:22:53 [ForkJoinPool-1-worker-33] INFO  [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-28 19:22:53 [ForkJoinPool-1-worker-29] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-28 19:22:54 [ForkJoinPool-1-worker-27] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-28 19:22:54 [ForkJoinPool-1-worker-25] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-28 19:22:54 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-28 19:22:54 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-28 19:22:54 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-28 19:22:54 [ForkJoinPool-1-worker-33] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-28 19:22:54 [ForkJoinPool-1-worker-31] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-28 19:22:54 [ForkJoinPool-1-worker-29] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io not ready, will try again in 10000 ms (179420ms till timeout)
2022-03-28 19:22:54 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io not ready, will try again in 10000 ms (179608ms till timeout)
2022-03-28 19:23:04 [ForkJoinPool-1-worker-19] INFO  [ResourceManager:369] ############################################################################
2022-03-28 19:23:04 [ForkJoinPool-1-worker-19] DEBUG [KubeClusterResource:216] Deleting Namespace: infra-namespace
2022-03-28 19:23:05 [ForkJoinPool-1-worker-19] DEBUG [DefaultSharedIndexInformer:142] informer: ready to run resync and reflector for class io.fabric8.kubernetes.api.model.Namespace with resync 0
2022-03-28 19:23:05 [ForkJoinPool-1-worker-19] DEBUG [DefaultSharedIndexInformer:212] informer#Controller: resync skipped due to 0 full resync period class io.fabric8.kubernetes.api.model.Namespace
2022-03-28 19:23:05 [ForkJoinPool-1-worker-19] DEBUG [Reflector:95] Listing items (1) for resource class io.fabric8.kubernetes.api.model.Namespace v284582
2022-03-28 19:23:05 [ForkJoinPool-1-worker-19] DEBUG [Reflector:103] Starting watcher for resource class io.fabric8.kubernetes.api.model.Namespace v284582
2022-03-28 19:23:05 [ForkJoinPool-1-worker-19] DEBUG [AbstractWatchManager:222] Watching https://api.morsak-410.strimzi.app-services-dev.net:6443/api/v1/namespaces?fieldSelector=metadata.name%3Dinfra-namespace&resourceVersion=284582&allowWatchBookmarks=true&watch=true...
2022-03-28 19:23:05 [OkHttp https://api.morsak-410.strimzi.app-services-dev.net:6443/...] DEBUG [WatcherWebSocketListener:43] WebSocket successfully opened
2022-03-28 19:23:10 [OkHttp https://api.morsak-410.strimzi.app-services-dev.net:6443/...] DEBUG [Reflector:139] Event received MODIFIED Namespace resourceVersion 284659
2022-03-28 19:23:16 [OkHttp https://api.morsak-410.strimzi.app-services-dev.net:6443/...] DEBUG [Reflector:139] Event received MODIFIED Namespace resourceVersion 284697
2022-03-28 19:23:16 [OkHttp https://api.morsak-410.strimzi.app-services-dev.net:6443/...] DEBUG [Reflector:64] Stopping watcher for resource class io.fabric8.kubernetes.api.model.Namespace v284659 in namespace default
2022-03-28 19:23:16 [OkHttp https://api.morsak-410.strimzi.app-services-dev.net:6443/...] DEBUG [AbstractWatchManager:230] Force closing the watch io.fabric8.kubernetes.client.dsl.internal.WatchConnectionManager@28ce29e0
2022-03-28 19:23:16 [OkHttp https://api.morsak-410.strimzi.app-services-dev.net:6443/...] DEBUG [Reflector:181] Watch gracefully closed
2022-03-28 19:23:16 [ForkJoinPool-1-worker-19] DEBUG [KubeClusterResource:216] Deleting Namespace: reconciliation-st
2022-03-28 19:23:16 [OkHttp https://api.morsak-410.strimzi.app-services-dev.net:6443/...] DEBUG [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@10a038bc
2022-03-28 19:23:16 [ForkJoinPool-1-worker-31] DEBUG [KubeClusterResource:216] Deleting Namespace: infra-namespace
2022-03-28 19:23:16 [OkHttp https://api.morsak-410.strimzi.app-services-dev.net:6443/...] DEBUG [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@10a038bc
2022-03-28 19:23:16 [OkHttp https://api.morsak-410.strimzi.app-services-dev.net:6443/...] DEBUG [WatchConnectionManager:63] Websocket already closed io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@10a038bc
2022-03-28 19:23:16 [OkHttp https://api.morsak-410.strimzi.app-services-dev.net:6443/...] DEBUG [Reflector:139] Event received DELETED Namespace resourceVersion 284698
2022-03-28 19:23:16 [OkHttp https://api.morsak-410.strimzi.app-services-dev.net:6443/...] DEBUG [WatcherWebSocketListener:79] WebSocket close received. code: 1000, reason: 
2022-03-28 19:23:16 [OkHttp https://api.morsak-410.strimzi.app-services-dev.net:6443/...] DEBUG [AbstractWatchManager:140] Ignoring error for already closed/closing connection
2022-03-28 19:23:16 [ForkJoinPool-1-worker-31] DEBUG [DefaultSharedIndexInformer:142] informer: ready to run resync and reflector for class io.fabric8.kubernetes.api.model.Namespace with resync 0
2022-03-28 19:23:16 [ForkJoinPool-1-worker-31] DEBUG [DefaultSharedIndexInformer:212] informer#Controller: resync skipped due to 0 full resync period class io.fabric8.kubernetes.api.model.Namespace
2022-03-28 19:23:16 [ForkJoinPool-1-worker-19] DEBUG [DefaultSharedIndexInformer:142] informer: ready to run resync and reflector for class io.fabric8.kubernetes.api.model.Namespace with resync 0
2022-03-28 19:23:16 [ForkJoinPool-1-worker-19] DEBUG [DefaultSharedIndexInformer:212] informer#Controller: resync skipped due to 0 full resync period class io.fabric8.kubernetes.api.model.Namespace
2022-03-28 19:23:17 [ForkJoinPool-1-worker-31] DEBUG [Reflector:95] Listing items (0) for resource class io.fabric8.kubernetes.api.model.Namespace v284704
2022-03-28 19:23:17 [ForkJoinPool-1-worker-19] DEBUG [Reflector:95] Listing items (0) for resource class io.fabric8.kubernetes.api.model.Namespace v284705
2022-03-28 19:23:17 [main] INFO  [TestExecutionListener:40] =======================================================================
2022-03-28 19:23:17 [main] INFO  [TestExecutionListener:41] =======================================================================
2022-03-28 19:23:17 [main] INFO  [TestExecutionListener:42]                         Test run finished
2022-03-28 19:23:17 [main] INFO  [TestExecutionListener:43] =======================================================================
2022-03-28 19:23:17 [main] INFO  [TestExecutionListener:44] =======================================================================
2022-03-28 19:23:17 [main] INFO  [TestExecutionListener:29] =======================================================================
2022-03-28 19:23:17 [main] INFO  [TestExecutionListener:30] =======================================================================
2022-03-28 19:23:17 [main] INFO  [TestExecutionListener:31]                         Test run started
2022-03-28 19:23:17 [main] INFO  [TestExecutionListener:32] =======================================================================
2022-03-28 19:23:17 [main] INFO  [TestExecutionListener:33] =======================================================================
2022-03-28 19:23:17 [main] INFO  [TestExecutionListener:48] Following testclasses are selected for run:
2022-03-28 19:23:17 [main] INFO  [TestExecutionListener:51] -> io.strimzi.systemtest.operators.ReconciliationST
2022-03-28 19:23:17 [main] INFO  [TestExecutionListener:52] =======================================================================
2022-03-28 19:23:17 [main] INFO  [TestExecutionListener:53] =======================================================================
[INFO] Running io.strimzi.systemtest.operators.ReconciliationST
2022-03-28 19:23:17 [ForkJoinPool-2-worker-19] DEBUG [BeforeAllOnce:51] ============================================================================
2022-03-28 19:23:17 [ForkJoinPool-2-worker-19] DEBUG [BeforeAllOnce:52] [io.strimzi.systemtest.operators.ReconciliationST - Before Suite] - Setup Suite environment
2022-03-28 19:23:17 [ForkJoinPool-2-worker-19] INFO  [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@3d1bad79
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=*
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-03-28 19:23:17 [ForkJoinPool-2-worker-19] DEBUG [SetupClusterOperator:198] Cluster operator installation configuration:
SetupClusterOperator{cluster=io.strimzi.test.k8s.KubeClusterResource@27ab2a7d, extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@3d1bad79, clusterOperatorName='strimzi-cluster-operator', namespaceInstallTo='infra-namespace', namespaceToWatch='*', bindingsNamespaces=[infra-namespace], operationTimeout=300000, reconciliationInterval=30000, extraEnvVars=[], extraLabels={}, clusterOperatorRBACType=CLUSTER, testClassName='null', testMethodName='null'}
2022-03-28 19:23:17 [ForkJoinPool-2-worker-19] INFO  [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-03-28 19:23:17 [ForkJoinPool-2-worker-19] INFO  [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-03-28 19:23:17 [ForkJoinPool-2-worker-19] DEBUG [TestUtils:125] Waiting for Namespace infra-namespace
2022-03-28 19:23:17 [ForkJoinPool-2-worker-19] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace infra-namespace -o json
2022-03-28 19:23:18 [ForkJoinPool-2-worker-19] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace infra-namespace -o json
2022-03-28 19:23:18 [ForkJoinPool-2-worker-19] DEBUG [Exec:419] Return code: 0
2022-03-28 19:23:18 [ForkJoinPool-2-worker-19] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c42,c39",
            "openshift.io/sa.scc.supplemental-groups": "1001800000/10000",
            "openshift.io/sa.scc.uid-range": "1001800000/10000"
        },
        "creationTimestamp": "2022-03-28T19:23:12Z",
        "labels": {
            "kubernetes.io/metadata.name": "infra-namespace"
        },
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T19:23:12Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:labels": {
                            ".": {},
                            "f:kubernetes.io/metadata.name": {}
                        }
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T19:23:12Z"
            }
        ],
        "name": "infra-namespace",
        "resourceVersion": "284713",
        "uid": "993a95d1-f263-466c-9fb8-7540befea96e"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 19:23:18 [ForkJoinPool-2-worker-19] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace]}
2022-03-28 19:23:18 [ForkJoinPool-2-worker-19] INFO  [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-28 19:23:18 [ForkJoinPool-2-worker-19] DEBUG [SetupClusterOperator:478] Installation resource type: ServiceAccount
2022-03-28 19:23:18 [ForkJoinPool-2-worker-19] INFO  [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-28 19:23:18 [ForkJoinPool-2-worker-19] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-28 19:23:18 [ForkJoinPool-2-worker-19] DEBUG [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-28 19:23:18 [ForkJoinPool-2-worker-19] INFO  [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-28 19:23:18 [ForkJoinPool-2-worker-19] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-28 19:23:18 [ForkJoinPool-2-worker-19] DEBUG [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-28 19:23:18 [ForkJoinPool-2-worker-19] INFO  [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-28 19:23:18 [ForkJoinPool-2-worker-19] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-28 19:23:18 [ForkJoinPool-2-worker-19] DEBUG [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-28 19:23:18 [ForkJoinPool-2-worker-19] INFO  [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-28 19:23:18 [ForkJoinPool-2-worker-19] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-28 19:23:18 [ForkJoinPool-2-worker-19] DEBUG [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-28 19:23:18 [ForkJoinPool-2-worker-19] INFO  [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-28 19:23:18 [ForkJoinPool-2-worker-19] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-28 19:23:19 [ForkJoinPool-2-worker-19] DEBUG [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-28 19:23:19 [ForkJoinPool-2-worker-19] INFO  [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-28 19:23:19 [ForkJoinPool-2-worker-19] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-28 19:23:19 [ForkJoinPool-2-worker-19] DEBUG [SetupClusterOperator:478] Installation resource type: Crd
2022-03-28 19:23:19 [ForkJoinPool-2-worker-19] INFO  [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-28 19:23:19 [ForkJoinPool-2-worker-19] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-28 19:23:20 [ForkJoinPool-2-worker-19] DEBUG [SetupClusterOperator:478] Installation resource type: Crd
2022-03-28 19:23:20 [ForkJoinPool-2-worker-19] INFO  [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-28 19:23:20 [ForkJoinPool-2-worker-19] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-28 19:23:20 [ForkJoinPool-2-worker-19] DEBUG [SetupClusterOperator:478] Installation resource type: Crd
2022-03-28 19:23:20 [ForkJoinPool-2-worker-19] INFO  [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-28 19:23:21 [ForkJoinPool-2-worker-19] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-28 19:23:21 [ForkJoinPool-2-worker-19] DEBUG [SetupClusterOperator:478] Installation resource type: Crd
2022-03-28 19:23:21 [ForkJoinPool-2-worker-19] INFO  [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-28 19:23:21 [ForkJoinPool-2-worker-19] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-28 19:23:21 [ForkJoinPool-2-worker-19] DEBUG [SetupClusterOperator:478] Installation resource type: Crd
2022-03-28 19:23:21 [ForkJoinPool-2-worker-19] INFO  [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-28 19:23:21 [ForkJoinPool-2-worker-19] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-28 19:23:21 [ForkJoinPool-2-worker-19] DEBUG [SetupClusterOperator:478] Installation resource type: Crd
2022-03-28 19:23:21 [ForkJoinPool-2-worker-19] INFO  [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-28 19:23:21 [ForkJoinPool-2-worker-19] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-28 19:23:22 [ForkJoinPool-2-worker-19] DEBUG [SetupClusterOperator:478] Installation resource type: Crd
2022-03-28 19:23:22 [ForkJoinPool-2-worker-19] INFO  [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-28 19:23:22 [ForkJoinPool-2-worker-19] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-28 19:23:22 [ForkJoinPool-2-worker-19] DEBUG [SetupClusterOperator:478] Installation resource type: Crd
2022-03-28 19:23:22 [ForkJoinPool-2-worker-19] INFO  [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-28 19:23:22 [ForkJoinPool-2-worker-19] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-28 19:23:22 [ForkJoinPool-2-worker-19] DEBUG [SetupClusterOperator:478] Installation resource type: Crd
2022-03-28 19:23:22 [ForkJoinPool-2-worker-19] INFO  [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-28 19:23:22 [ForkJoinPool-2-worker-19] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-28 19:23:23 [ForkJoinPool-2-worker-19] DEBUG [SetupClusterOperator:478] Installation resource type: Crd
2022-03-28 19:23:23 [ForkJoinPool-2-worker-19] INFO  [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-28 19:23:23 [ForkJoinPool-2-worker-19] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-28 19:23:23 [ForkJoinPool-2-worker-19] DEBUG [SetupClusterOperator:478] Installation resource type: ConfigMap
2022-03-28 19:23:23 [ForkJoinPool-2-worker-19] INFO  [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-28 19:23:23 [ForkJoinPool-2-worker-19] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-28 19:23:23 [ForkJoinPool-2-worker-19] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=infra-namespace, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 19:23:23 [ForkJoinPool-2-worker-19] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-03-28 19:23:23 [ForkJoinPool-2-worker-19] INFO  [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/cloud-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-28 19:23:23 [ForkJoinPool-2-worker-19] INFO  [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-28 19:23:23 [ForkJoinPool-2-worker-19] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-28 19:23:23 [ForkJoinPool-2-worker-19] INFO  [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/cloud-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-03-28 19:23:23 [ForkJoinPool-2-worker-19] INFO  [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-28 19:23:23 [ForkJoinPool-2-worker-19] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-28 19:23:24 [ForkJoinPool-2-worker-19] INFO  [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/cloud-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-03-28 19:23:24 [ForkJoinPool-2-worker-19] INFO  [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-28 19:23:24 [ForkJoinPool-2-worker-19] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-28 19:23:24 [ForkJoinPool-2-worker-19] INFO  [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/cloud-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-28 19:23:24 [ForkJoinPool-2-worker-19] INFO  [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-28 19:23:24 [ForkJoinPool-2-worker-19] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-28 19:23:24 [ForkJoinPool-2-worker-19] INFO  [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/cloud-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-03-28 19:23:24 [ForkJoinPool-2-worker-19] INFO  [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-28 19:23:24 [ForkJoinPool-2-worker-19] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-28 19:23:24 [ForkJoinPool-2-worker-19] INFO  [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-03-28 19:23:24 [ForkJoinPool-2-worker-19] INFO  [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-03-28 19:23:24 [ForkJoinPool-2-worker-19] INFO  [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-28 19:23:24 [ForkJoinPool-2-worker-19] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-namespaced
2022-03-28 19:23:24 [ForkJoinPool-2-worker-19] INFO  [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-03-28 19:23:24 [ForkJoinPool-2-worker-19] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-entity-operator
2022-03-28 19:23:25 [ForkJoinPool-2-worker-19] INFO  [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-28 19:23:25 [ForkJoinPool-2-worker-19] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-28 19:23:25 [ForkJoinPool-2-worker-19] INFO  [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-28 19:23:25 [ForkJoinPool-2-worker-19] DEBUG [TestUtils:125] Waiting for Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-28 19:23:25 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (479904ms till timeout)
2022-03-28 19:23:26 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (478807ms till timeout)
2022-03-28 19:23:27 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (477708ms till timeout)
2022-03-28 19:23:28 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (476610ms till timeout)
2022-03-28 19:23:29 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (475512ms till timeout)
2022-03-28 19:23:30 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (474414ms till timeout)
2022-03-28 19:23:32 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (473314ms till timeout)
2022-03-28 19:23:33 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (472216ms till timeout)
2022-03-28 19:23:34 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (471117ms till timeout)
2022-03-28 19:23:35 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (470019ms till timeout)
2022-03-28 19:23:36 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (468920ms till timeout)
2022-03-28 19:23:37 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (467822ms till timeout)
2022-03-28 19:23:38 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (466721ms till timeout)
2022-03-28 19:23:39 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (465623ms till timeout)
2022-03-28 19:23:40 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (464524ms till timeout)
2022-03-28 19:23:41 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (463424ms till timeout)
2022-03-28 19:23:43 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (462327ms till timeout)
2022-03-28 19:23:44 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (461230ms till timeout)
2022-03-28 19:23:45 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (459651ms till timeout)
2022-03-28 19:23:47 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (458368ms till timeout)
2022-03-28 19:23:48 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (457178ms till timeout)
2022-03-28 19:23:49 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (455987ms till timeout)
2022-03-28 19:23:50 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (454888ms till timeout)
2022-03-28 19:23:51 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (453790ms till timeout)
2022-03-28 19:23:52 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (452690ms till timeout)
2022-03-28 19:23:53 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (451591ms till timeout)
2022-03-28 19:23:54 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (450493ms till timeout)
2022-03-28 19:23:55 [ForkJoinPool-2-worker-19] INFO  [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-03-28 19:23:55 [ForkJoinPool-2-worker-19] INFO  [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-03-28 19:23:56 [ForkJoinPool-2-worker-19] DEBUG [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready
2022-03-28 19:23:56 [ForkJoinPool-2-worker-19] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-5dc4cd9447-thnkc not ready: strimzi-cluster-operator)
2022-03-28 19:23:56 [ForkJoinPool-2-worker-19] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-5dc4cd9447-thnkc are ready
2022-03-28 19:23:56 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599904ms till timeout)
2022-03-28 19:23:57 [ForkJoinPool-2-worker-19] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-5dc4cd9447-thnkc not ready: strimzi-cluster-operator)
2022-03-28 19:23:57 [ForkJoinPool-2-worker-19] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-5dc4cd9447-thnkc are ready
2022-03-28 19:23:57 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598804ms till timeout)
2022-03-28 19:23:58 [ForkJoinPool-2-worker-19] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-5dc4cd9447-thnkc not ready: strimzi-cluster-operator)
2022-03-28 19:23:58 [ForkJoinPool-2-worker-19] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-5dc4cd9447-thnkc are ready
2022-03-28 19:23:58 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597706ms till timeout)
2022-03-28 19:23:59 [ForkJoinPool-2-worker-19] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-5dc4cd9447-thnkc not ready: strimzi-cluster-operator)
2022-03-28 19:23:59 [ForkJoinPool-2-worker-19] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-5dc4cd9447-thnkc are ready
2022-03-28 19:23:59 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596606ms till timeout)
2022-03-28 19:24:00 [ForkJoinPool-2-worker-19] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-5dc4cd9447-thnkc not ready: strimzi-cluster-operator)
2022-03-28 19:24:00 [ForkJoinPool-2-worker-19] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-5dc4cd9447-thnkc are ready
2022-03-28 19:24:00 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595505ms till timeout)
2022-03-28 19:24:01 [ForkJoinPool-2-worker-19] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-5dc4cd9447-thnkc not ready: strimzi-cluster-operator)
2022-03-28 19:24:01 [ForkJoinPool-2-worker-19] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-5dc4cd9447-thnkc are ready
2022-03-28 19:24:01 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594406ms till timeout)
2022-03-28 19:24:02 [ForkJoinPool-2-worker-19] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-5dc4cd9447-thnkc not ready: strimzi-cluster-operator)
2022-03-28 19:24:02 [ForkJoinPool-2-worker-19] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-5dc4cd9447-thnkc are ready
2022-03-28 19:24:02 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593306ms till timeout)
2022-03-28 19:24:03 [ForkJoinPool-2-worker-19] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-5dc4cd9447-thnkc not ready: strimzi-cluster-operator)
2022-03-28 19:24:03 [ForkJoinPool-2-worker-19] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-5dc4cd9447-thnkc are ready
2022-03-28 19:24:03 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592207ms till timeout)
2022-03-28 19:24:04 [ForkJoinPool-2-worker-19] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-5dc4cd9447-thnkc not ready: strimzi-cluster-operator)
2022-03-28 19:24:04 [ForkJoinPool-2-worker-19] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-5dc4cd9447-thnkc are ready
2022-03-28 19:24:04 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591105ms till timeout)
2022-03-28 19:24:06 [ForkJoinPool-2-worker-19] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-5dc4cd9447-thnkc not ready: strimzi-cluster-operator)
2022-03-28 19:24:06 [ForkJoinPool-2-worker-19] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-5dc4cd9447-thnkc are ready
2022-03-28 19:24:06 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590004ms till timeout)
2022-03-28 19:24:07 [ForkJoinPool-2-worker-19] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-5dc4cd9447-thnkc not ready: strimzi-cluster-operator)
2022-03-28 19:24:07 [ForkJoinPool-2-worker-19] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-5dc4cd9447-thnkc are ready
2022-03-28 19:24:07 [ForkJoinPool-2-worker-19] INFO  [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-03-28 19:24:07 [ForkJoinPool-2-worker-19] DEBUG [AbstractST:666] ============================================================================
2022-03-28 19:24:07 [ForkJoinPool-2-worker-19] DEBUG [AbstractST:667] [operators.ReconciliationST - Before All] - Setup test suite environment
2022-03-28 19:24:07 [ForkJoinPool-2-worker-19] DEBUG [SuiteThreadController:69] [operators.ReconciliationST] - Adding parallel suite: ReconciliationST
2022-03-28 19:24:07 [ForkJoinPool-2-worker-19] DEBUG [SuiteThreadController:73] [operators.ReconciliationST] - Parallel suites count: 1
2022-03-28 19:24:07 [ForkJoinPool-2-worker-19] DEBUG [SuiteThreadController:184] ReconciliationST suite now can proceed its execution
2022-03-28 19:24:07 [ForkJoinPool-2-worker-19] DEBUG [TestSuiteNamespaceManager:128] Content of the test suite namespaces map:
{HttpBridgeTlsST=[http-bridge-tls-st], CruiseControlST=[cruise-control-st], AbstractST=[abstract-st], OpaIntegrationST=[opa-integration-st], CruiseControlApiST=[cruise-control-api-st], HttpBridgeScramShaST=[http-bridge-scram-sha-st], ReconciliationST=[reconciliation-st], DynamicConfST=[dynamic-conf-st], AbstractUpgradeST=[abstract-upgrade-st], QuotasST=[quotas-st], DynamicConfSharedST=[dynamic-conf-shared-st], RollingUpdateST=[rolling-update-st], OlmAbstractST=[olm-abstract-st], CustomAuthorizerST=[custom-authorizer-st], AlternativeReconcileTriggersST=[alternative-reconcile-triggers-st], LogSettingST=[log-setting-st], ListenersST=[listeners-st], UserST=[user-st], TopicST=[topic-st], SecurityST=[security-st], TracingST=[tracing-st], AbstractNamespaceST=[abstract-namespace-st], CruiseControlConfigurationST=[cruise-control-configuration-st], HttpBridgeCorsST=[http-bridge-cors-st], ConfigProviderST=[config-provider-st], KafkaST=[kafka-st], LoggingChangeST=[logging-change-st], HttpBridgeKafkaExternalListenersST=[http-bridge-kafka-external-listeners-st], OauthAbstractST=[oauth-abstract-st], ThrottlingQuotaST=[throttling-quota-st], MultipleListenersST=[multiple-listeners-st]}
2022-03-28 19:24:07 [ForkJoinPool-2-worker-19] DEBUG [TestSuiteNamespaceManager:129] Test suite `ReconciliationST` creates these additional namespaces:[reconciliation-st]
2022-03-28 19:24:07 [ForkJoinPool-2-worker-19] INFO  [KubeClusterResource:156] Creating Namespace: reconciliation-st
2022-03-28 19:24:07 [ForkJoinPool-2-worker-19] DEBUG [TestUtils:125] Waiting for Namespace reconciliation-st
2022-03-28 19:24:07 [ForkJoinPool-2-worker-19] TRACE [Exec:248] Running command - oc --namespace infra-namespace get Namespace reconciliation-st -o json
2022-03-28 19:24:07 [ForkJoinPool-2-worker-19] DEBUG [Exec:419] Command: oc --namespace infra-namespace get Namespace reconciliation-st -o json
2022-03-28 19:24:07 [ForkJoinPool-2-worker-19] DEBUG [Exec:419] Return code: 0
2022-03-28 19:24:07 [ForkJoinPool-2-worker-19] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c43,c2",
            "openshift.io/sa.scc.supplemental-groups": "1001810000/10000",
            "openshift.io/sa.scc.uid-range": "1001810000/10000"
        },
        "creationTimestamp": "2022-03-28T19:24:02Z",
        "labels": {
            "kubernetes.io/metadata.name": "reconciliation-st"
        },
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T19:24:02Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:labels": {
                            ".": {},
                            "f:kubernetes.io/metadata.name": {}
                        }
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T19:24:02Z"
            }
        ],
        "name": "reconciliation-st",
        "resourceVersion": "285146",
        "uid": "c6695f6c-9357-4217-9334-b33a4b6bedda"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 19:24:07 [ForkJoinPool-2-worker-19] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st]}
2022-03-28 19:24:07 [ForkJoinPool-2-worker-19] INFO  [KubeClusterResource:82] Client use Namespace: reconciliation-st
2022-03-28 19:24:07 [ForkJoinPool-2-worker-19] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=reconciliation-st, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 19:24:07 [ForkJoinPool-2-worker-19] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: reconciliation-st
2022-03-28 19:24:07 [ForkJoinPool-2-worker-19] INFO  [TestSeparator:23] ############################################################################
2022-03-28 19:24:07 [ForkJoinPool-2-worker-19] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.ReconciliationST.testPauseReconciliationInKafkaRebalanceAndTopic-STARTED
2022-03-28 19:24:07 [ForkJoinPool-2-worker-19] DEBUG [AbstractST:658] ============================================================================
2022-03-28 19:24:07 [ForkJoinPool-2-worker-19] DEBUG [AbstractST:659] [operators.ReconciliationST - Before Each] - Setup test case environment
2022-03-28 19:24:07 [ForkJoinPool-2-worker-19] DEBUG [SuiteThreadController:77] [operators.ReconciliationST] - Adding parallel test: testPauseReconciliationInKafkaRebalanceAndTopic
2022-03-28 19:24:07 [ForkJoinPool-2-worker-19] DEBUG [SuiteThreadController:81] [operators.ReconciliationST] - Parallel test count: 1
2022-03-28 19:24:07 [ForkJoinPool-2-worker-19] DEBUG [SuiteThreadController:230] testPauseReconciliationInKafkaRebalanceAndTopic test now can proceed its execution
2022-03-28 19:24:07 [ForkJoinPool-2-worker-19] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 19:24:07 [ForkJoinPool-2-worker-19] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-abc5d34b, testThrottlingQuotasCreateAlterPartitions=my-cluster-ec1b34f8, testSendSimpleMessageTls=my-cluster-89673e85, testConfigurationReflection=my-cluster-188382d8, testUserWithNameMoreThan64Chars=my-cluster-2d5338bf, testDeployAndUnDeployCruiseControl=my-cluster-5dfd606f, testCapacityFile=my-cluster-855b9105, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-afe273c9, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-e6343cec, testTlsExternalUser=my-cluster-45cf8ce4, testSendSimpleMessageTlsScramSha=my-cluster-561d4fae, testCreateTopicAfterUnsupportedOperation=my-cluster-e8a83fb0, testScramUserWithQuotas=my-cluster-ab753b77, testTopicModificationOfReplicationFactor=my-cluster-15f5f09f, testKafkaAdminTopicOperations=my-cluster-506fda27, testConfigurationFileIsCreated=my-cluster-17fc585b, testCreateTopicViaKafka=my-cluster-75679697, testCreatingUsersWithSecretPrefix=my-cluster-f743fdfd, testUpdateUser=my-cluster-d582eb8c, testThrottlingQuotasDeleteTopic=my-cluster-836b31f8, testDeleteTopicEnableFalse=my-cluster-cfc59519, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-49593d97, testTlsExternalUserWithQuotas=my-cluster-100f5461, testTlsUserWithQuotas=my-cluster-9bde73ea, testThrottlingQuotasCreateTopic=my-cluster-e9955717, testSendingMessagesToNonExistingTopic=my-cluster-a3c1c5ed, testConfigurationPerformanceOptions=my-cluster-08515847, testReceiveSimpleMessageTls=my-cluster-634e4d65, testUserTemplate=my-cluster-927d5a1b, testMoreReplicasThanAvailableBrokers=my-cluster-22050d08}
2022-03-28 19:24:07 [ForkJoinPool-2-worker-19] TRACE [AbstractST:607] USERS_NAME_MAP: {testReceiveSimpleMessageTlsScramSha=my-user-330562220-1155646423, testThrottlingQuotasCreateAlterPartitions=my-user-333986137-1152182125, testSendSimpleMessageTls=my-user-1887866274-1013125243, testConfigurationReflection=my-user-1890969183-1754686912, testUserWithNameMoreThan64Chars=my-user-1930755541-784294250, testDeployAndUnDeployCruiseControl=my-user-506268651-1309117986, testCapacityFile=my-user-1460643829-103298436, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-user-8456079-650483351, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-2069267102-581538560, testTlsExternalUser=my-user-399414674-1203409092, testSendSimpleMessageTlsScramSha=my-user-1601209876-518796379, testCreateTopicAfterUnsupportedOperation=my-user-1589831352-1271671319, testScramUserWithQuotas=my-user-941123953-266890403, testTopicModificationOfReplicationFactor=my-user-259690203-415132624, testKafkaAdminTopicOperations=my-user-1780746402-846362125, testConfigurationFileIsCreated=my-user-355815510-1316268151, testCreateTopicViaKafka=my-user-541976280-277947468, testCreatingUsersWithSecretPrefix=my-user-607975993-1210920737, testUpdateUser=my-user-1297536277-778093599, testThrottlingQuotasDeleteTopic=my-user-1567019138-1355600812, testDeleteTopicEnableFalse=my-user-692771166-734949996, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-1912644109-284566493, testTlsExternalUserWithQuotas=my-user-1909583883-1106194424, testTlsUserWithQuotas=my-user-1496409761-1456339678, testThrottlingQuotasCreateTopic=my-user-1876060546-1887905718, testSendingMessagesToNonExistingTopic=my-user-134768902-582007171, testConfigurationPerformanceOptions=my-user-98362545-1476587236, testReceiveSimpleMessageTls=my-user-337002809-1497514615, testUserTemplate=my-user-989633374-940717398, testMoreReplicasThanAvailableBrokers=my-user-713898339-1095128639}
2022-03-28 19:24:07 [ForkJoinPool-2-worker-19] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-topic-1027850608-734754781, testThrottlingQuotasCreateAlterPartitions=my-topic-325891726-169328137, testSendSimpleMessageTls=my-topic-874607937-87907051, testConfigurationReflection=my-topic-1394577332-1936226438, testUserWithNameMoreThan64Chars=my-topic-1542907219-2136330275, testDeployAndUnDeployCruiseControl=my-topic-465377127-800134460, testCapacityFile=my-topic-549312506-649708453, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-topic-1687496771-142091227, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-293775067-432059493, testTlsExternalUser=my-topic-707627084-257659651, testSendSimpleMessageTlsScramSha=my-topic-1586843669-332477842, testCreateTopicAfterUnsupportedOperation=my-topic-1619871970-423361410, testScramUserWithQuotas=my-topic-644624843-1738170872, testTopicModificationOfReplicationFactor=my-topic-1480509436-578706559, testKafkaAdminTopicOperations=my-topic-481231182-953607496, testConfigurationFileIsCreated=my-topic-423850169-855699454, testCreateTopicViaKafka=my-topic-1973235938-1730719386, testCreatingUsersWithSecretPrefix=my-topic-1727421070-668192795, testUpdateUser=my-topic-1325563869-1324403074, testThrottlingQuotasDeleteTopic=my-topic-1674611395-2048879468, testDeleteTopicEnableFalse=my-topic-1545127738-3726149, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-952089698-663307394, testTlsExternalUserWithQuotas=my-topic-1986059791-167638004, testTlsUserWithQuotas=my-topic-2040479778-983803186, testThrottlingQuotasCreateTopic=my-topic-1246597695-228473879, testSendingMessagesToNonExistingTopic=my-topic-1692964628-1348460540, testConfigurationPerformanceOptions=my-topic-696588651-1198599652, testReceiveSimpleMessageTls=my-topic-829184226-1519275114, testUserTemplate=my-topic-1542379258-2002911578, testMoreReplicasThanAvailableBrokers=my-topic-840747042-63255423}
2022-03-28 19:24:07 [ForkJoinPool-2-worker-19] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-abc5d34b-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-ec1b34f8-kafka-clients, testSendSimpleMessageTls=my-cluster-89673e85-kafka-clients, testConfigurationReflection=my-cluster-188382d8-kafka-clients, testUserWithNameMoreThan64Chars=my-cluster-2d5338bf-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-5dfd606f-kafka-clients, testCapacityFile=my-cluster-855b9105-kafka-clients, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-afe273c9-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-e6343cec-kafka-clients, testTlsExternalUser=my-cluster-45cf8ce4-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-561d4fae-kafka-clients, testCreateTopicAfterUnsupportedOperation=my-cluster-e8a83fb0-kafka-clients, testScramUserWithQuotas=my-cluster-ab753b77-kafka-clients, testTopicModificationOfReplicationFactor=my-cluster-15f5f09f-kafka-clients, testKafkaAdminTopicOperations=my-cluster-506fda27-kafka-clients, testConfigurationFileIsCreated=my-cluster-17fc585b-kafka-clients, testCreateTopicViaKafka=my-cluster-75679697-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-f743fdfd-kafka-clients, testUpdateUser=my-cluster-d582eb8c-kafka-clients, testThrottlingQuotasDeleteTopic=my-cluster-836b31f8-kafka-clients, testDeleteTopicEnableFalse=my-cluster-cfc59519-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-49593d97-kafka-clients, testTlsExternalUserWithQuotas=my-cluster-100f5461-kafka-clients, testTlsUserWithQuotas=my-cluster-9bde73ea-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-e9955717-kafka-clients, testSendingMessagesToNonExistingTopic=my-cluster-a3c1c5ed-kafka-clients, testConfigurationPerformanceOptions=my-cluster-08515847-kafka-clients, testReceiveSimpleMessageTls=my-cluster-634e4d65-kafka-clients, testUserTemplate=my-cluster-927d5a1b-kafka-clients, testMoreReplicasThanAvailableBrokers=my-cluster-22050d08-kafka-clients}
2022-03-28 19:24:07 [ForkJoinPool-2-worker-19] INFO  [TestSuiteNamespaceManager:163] Creating namespace:namespace-10 for test case:testPauseReconciliationInKafkaRebalanceAndTopic
2022-03-28 19:24:08 [ForkJoinPool-2-worker-19] INFO  [KubeClusterResource:156] Creating Namespace: namespace-10
2022-03-28 19:24:08 [ForkJoinPool-2-worker-19] DEBUG [TestUtils:125] Waiting for Namespace namespace-10
2022-03-28 19:24:08 [ForkJoinPool-2-worker-19] TRACE [Exec:248] Running command - oc --namespace reconciliation-st get Namespace namespace-10 -o json
2022-03-28 19:24:08 [ForkJoinPool-2-worker-19] DEBUG [Exec:419] Command: oc --namespace reconciliation-st get Namespace namespace-10 -o json
2022-03-28 19:24:08 [ForkJoinPool-2-worker-19] DEBUG [Exec:419] Return code: 0
2022-03-28 19:24:08 [ForkJoinPool-2-worker-19] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c43,c7",
            "openshift.io/sa.scc.supplemental-groups": "1001820000/10000",
            "openshift.io/sa.scc.uid-range": "1001820000/10000"
        },
        "creationTimestamp": "2022-03-28T19:24:03Z",
        "labels": {
            "kubernetes.io/metadata.name": "namespace-10"
        },
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T19:24:03Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:labels": {
                            ".": {},
                            "f:kubernetes.io/metadata.name": {}
                        }
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T19:24:03Z"
            }
        ],
        "name": "namespace-10",
        "resourceVersion": "285184",
        "uid": "c0086b42-e147-480e-9219-8919cb38afaa"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 19:24:08 [ForkJoinPool-2-worker-19] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[namespace-10]}
2022-03-28 19:24:08 [ForkJoinPool-2-worker-19] INFO  [KubeClusterResource:82] Client use Namespace: namespace-10
2022-03-28 19:24:08 [ForkJoinPool-2-worker-19] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-10, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 19:24:08 [ForkJoinPool-2-worker-19] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-10
2022-03-28 19:24:08 [ForkJoinPool-2-worker-19] INFO  [ResourceManager:155] Create/Update Kafka my-cluster-49593d97 in namespace namespace-10
2022-03-28 19:24:08 [ForkJoinPool-2-worker-19] INFO  [ResourceManager:164] Using Namespace: namespace-10
2022-03-28 19:24:08 [ForkJoinPool-2-worker-19] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-49593d97
2022-03-28 19:24:08 [ForkJoinPool-2-worker-19] INFO  [ResourceManager:433] Wait for Kafka: my-cluster-49593d97 will have desired state: Ready
2022-03-28 19:24:08 [ForkJoinPool-2-worker-19] DEBUG [TestUtils:125] Waiting for Kafka: my-cluster-49593d97 will have desired state: Ready
2022-03-28 19:24:09 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1319902ms till timeout)
2022-03-28 19:24:10 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1318775ms till timeout)
2022-03-28 19:24:11 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1317676ms till timeout)
2022-03-28 19:24:12 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1316577ms till timeout)
2022-03-28 19:24:13 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1315477ms till timeout)
2022-03-28 19:24:14 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1314379ms till timeout)
2022-03-28 19:24:15 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1313280ms till timeout)
2022-03-28 19:24:16 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1312180ms till timeout)
2022-03-28 19:24:17 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1311081ms till timeout)
2022-03-28 19:24:19 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1309982ms till timeout)
2022-03-28 19:24:20 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1308883ms till timeout)
2022-03-28 19:24:21 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1307784ms till timeout)
2022-03-28 19:24:22 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1306685ms till timeout)
2022-03-28 19:24:23 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1305586ms till timeout)
2022-03-28 19:24:24 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1304487ms till timeout)
2022-03-28 19:24:25 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1303388ms till timeout)
2022-03-28 19:24:26 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1302289ms till timeout)
2022-03-28 19:24:27 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1301189ms till timeout)
2022-03-28 19:24:28 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1300090ms till timeout)
2022-03-28 19:24:29 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1298992ms till timeout)
2022-03-28 19:24:31 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1297893ms till timeout)
2022-03-28 19:24:32 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1296794ms till timeout)
2022-03-28 19:24:33 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1295695ms till timeout)
2022-03-28 19:24:34 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1294597ms till timeout)
2022-03-28 19:24:35 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1293498ms till timeout)
2022-03-28 19:24:36 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1292399ms till timeout)
2022-03-28 19:24:37 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1291301ms till timeout)
2022-03-28 19:24:38 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1290202ms till timeout)
2022-03-28 19:24:39 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1289103ms till timeout)
2022-03-28 19:24:40 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1288005ms till timeout)
2022-03-28 19:24:42 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1286907ms till timeout)
2022-03-28 19:24:43 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1285809ms till timeout)
2022-03-28 19:24:44 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1284707ms till timeout)
2022-03-28 19:24:45 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1283608ms till timeout)
2022-03-28 19:24:46 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1282509ms till timeout)
2022-03-28 19:24:47 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1281408ms till timeout)
2022-03-28 19:24:48 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1280305ms till timeout)
2022-03-28 19:24:49 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1279207ms till timeout)
2022-03-28 19:24:50 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1278108ms till timeout)
2022-03-28 19:24:51 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1277009ms till timeout)
2022-03-28 19:24:53 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1275911ms till timeout)
2022-03-28 19:24:54 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1274813ms till timeout)
2022-03-28 19:24:55 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1273713ms till timeout)
2022-03-28 19:24:56 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1272615ms till timeout)
2022-03-28 19:24:57 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1271515ms till timeout)
2022-03-28 19:24:58 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1270412ms till timeout)
2022-03-28 19:24:59 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1269313ms till timeout)
2022-03-28 19:25:00 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1268214ms till timeout)
2022-03-28 19:25:01 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1267115ms till timeout)
2022-03-28 19:25:02 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1266017ms till timeout)
2022-03-28 19:25:04 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1264918ms till timeout)
2022-03-28 19:25:05 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1263820ms till timeout)
2022-03-28 19:25:06 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1262721ms till timeout)
2022-03-28 19:25:07 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1261622ms till timeout)
2022-03-28 19:25:08 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1260524ms till timeout)
2022-03-28 19:25:09 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1259425ms till timeout)
2022-03-28 19:25:10 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1258324ms till timeout)
2022-03-28 19:25:11 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1257217ms till timeout)
2022-03-28 19:25:12 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1256119ms till timeout)
2022-03-28 19:25:13 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1255018ms till timeout)
2022-03-28 19:25:15 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1253921ms till timeout)
2022-03-28 19:25:16 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1252823ms till timeout)
2022-03-28 19:25:17 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1251717ms till timeout)
2022-03-28 19:25:18 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1250618ms till timeout)
2022-03-28 19:25:19 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1249518ms till timeout)
2022-03-28 19:25:20 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1248419ms till timeout)
2022-03-28 19:25:21 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1247321ms till timeout)
2022-03-28 19:25:22 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1246223ms till timeout)
2022-03-28 19:25:23 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1245124ms till timeout)
2022-03-28 19:25:24 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1244026ms till timeout)
2022-03-28 19:25:26 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1242927ms till timeout)
2022-03-28 19:25:27 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1241827ms till timeout)
2022-03-28 19:25:28 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1240728ms till timeout)
2022-03-28 19:25:29 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1239630ms till timeout)
2022-03-28 19:25:30 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1238530ms till timeout)
2022-03-28 19:25:31 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1237432ms till timeout)
2022-03-28 19:25:32 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1236330ms till timeout)
2022-03-28 19:25:33 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1235230ms till timeout)
2022-03-28 19:25:34 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1234132ms till timeout)
2022-03-28 19:25:35 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1233033ms till timeout)
2022-03-28 19:25:37 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1231934ms till timeout)
2022-03-28 19:25:38 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1230838ms till timeout)
2022-03-28 19:25:39 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1229738ms till timeout)
2022-03-28 19:25:40 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1228639ms till timeout)
2022-03-28 19:25:41 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1227541ms till timeout)
2022-03-28 19:25:42 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (1226442ms till timeout)
2022-03-28 19:25:43 [ForkJoinPool-2-worker-19] INFO  [ResourceManager:444] Kafka: my-cluster-49593d97 is in desired state: Ready
2022-03-28 19:25:43 [ForkJoinPool-2-worker-19] INFO  [ResourceManager:155] Create/Update KafkaTopic my-topic-952089698-663307394 in namespace namespace-10
2022-03-28 19:25:43 [ForkJoinPool-2-worker-19] INFO  [ResourceManager:164] Using Namespace: namespace-10
2022-03-28 19:25:43 [ForkJoinPool-2-worker-19] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-952089698-663307394
2022-03-28 19:25:43 [ForkJoinPool-2-worker-19] INFO  [ResourceManager:433] Wait for KafkaTopic: my-topic-952089698-663307394 will have desired state: Ready
2022-03-28 19:25:43 [ForkJoinPool-2-worker-19] DEBUG [TestUtils:125] Waiting for KafkaTopic: my-topic-952089698-663307394 will have desired state: Ready
2022-03-28 19:25:43 [ForkJoinPool-2-worker-19] INFO  [ResourceManager:444] KafkaTopic: my-topic-952089698-663307394 is in desired state: Ready
2022-03-28 19:25:43 [ForkJoinPool-2-worker-19] INFO  [ReconciliationST:147] Adding pause annotation into KafkaTopic resource and changing replication factor
2022-03-28 19:25:44 [ForkJoinPool-2-worker-19] INFO  [ResourceManager:433] Wait for KafkaTopic: my-topic-952089698-663307394 will have desired state: ReconciliationPaused
2022-03-28 19:25:44 [ForkJoinPool-2-worker-19] DEBUG [TestUtils:125] Waiting for KafkaTopic: my-topic-952089698-663307394 will have desired state: ReconciliationPaused
2022-03-28 19:25:44 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaTopic: my-topic-952089698-663307394 will have desired state: ReconciliationPaused not ready, will try again in 1000 ms (179903ms till timeout)
2022-03-28 19:25:45 [ForkJoinPool-2-worker-19] INFO  [ResourceManager:444] KafkaTopic: my-topic-952089698-663307394 is in desired state: ReconciliationPaused
2022-03-28 19:25:45 [ForkJoinPool-2-worker-19] TRACE [Exec:248] Running command - oc --namespace namespace-10 exec my-cluster-49593d97-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-952089698-663307394 --describe --bootstrap-server my-cluster-49593d97-kafka-bootstrap:9092
2022-03-28 19:25:49 [ForkJoinPool-2-worker-19] INFO  [Exec:417] Command: oc --namespace namespace-10 exec my-cluster-49593d97-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-952089698-663307394 --describe --bootstrap-server my-cluster-49593d97-kafka-bootstrap:9092
2022-03-28 19:25:49 [ForkJoinPool-2-worker-19] INFO  [Exec:417] Return code: 0
2022-03-28 19:25:49 [ForkJoinPool-2-worker-19] DEBUG [TestUtils:125] Waiting for KafkaTopic's spec will be stable
2022-03-28 19:25:49 [ForkJoinPool-2-worker-19] TRACE [Exec:248] Running command - oc --namespace namespace-10 exec my-cluster-49593d97-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-952089698-663307394 --describe --bootstrap-server my-cluster-49593d97-kafka-bootstrap:9092
2022-03-28 19:25:52 [ForkJoinPool-2-worker-19] INFO  [Exec:417] Command: oc --namespace namespace-10 exec my-cluster-49593d97-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-952089698-663307394 --describe --bootstrap-server my-cluster-49593d97-kafka-bootstrap:9092
2022-03-28 19:25:52 [ForkJoinPool-2-worker-19] INFO  [Exec:417] Return code: 0
2022-03-28 19:25:52 [ForkJoinPool-2-worker-19] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 19 polls
2022-03-28 19:25:52 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (176200ms till timeout)
2022-03-28 19:25:53 [ForkJoinPool-2-worker-19] TRACE [Exec:248] Running command - oc --namespace namespace-10 exec my-cluster-49593d97-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-952089698-663307394 --describe --bootstrap-server my-cluster-49593d97-kafka-bootstrap:9092
2022-03-28 19:25:57 [ForkJoinPool-2-worker-19] INFO  [Exec:417] Command: oc --namespace namespace-10 exec my-cluster-49593d97-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-952089698-663307394 --describe --bootstrap-server my-cluster-49593d97-kafka-bootstrap:9092
2022-03-28 19:25:57 [ForkJoinPool-2-worker-19] INFO  [Exec:417] Return code: 0
2022-03-28 19:25:57 [ForkJoinPool-2-worker-19] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 18 polls
2022-03-28 19:25:57 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (171694ms till timeout)
2022-03-28 19:25:58 [ForkJoinPool-2-worker-19] TRACE [Exec:248] Running command - oc --namespace namespace-10 exec my-cluster-49593d97-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-952089698-663307394 --describe --bootstrap-server my-cluster-49593d97-kafka-bootstrap:9092
2022-03-28 19:26:06 [ForkJoinPool-2-worker-19] INFO  [Exec:417] Command: oc --namespace namespace-10 exec my-cluster-49593d97-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-952089698-663307394 --describe --bootstrap-server my-cluster-49593d97-kafka-bootstrap:9092
2022-03-28 19:26:06 [ForkJoinPool-2-worker-19] INFO  [Exec:417] Return code: 0
2022-03-28 19:26:06 [ForkJoinPool-2-worker-19] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 17 polls
2022-03-28 19:26:06 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (162193ms till timeout)
2022-03-28 19:26:07 [ForkJoinPool-2-worker-19] TRACE [Exec:248] Running command - oc --namespace namespace-10 exec my-cluster-49593d97-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-952089698-663307394 --describe --bootstrap-server my-cluster-49593d97-kafka-bootstrap:9092
2022-03-28 19:26:11 [ForkJoinPool-2-worker-19] INFO  [Exec:417] Command: oc --namespace namespace-10 exec my-cluster-49593d97-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-952089698-663307394 --describe --bootstrap-server my-cluster-49593d97-kafka-bootstrap:9092
2022-03-28 19:26:11 [ForkJoinPool-2-worker-19] INFO  [Exec:417] Return code: 0
2022-03-28 19:26:11 [ForkJoinPool-2-worker-19] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 16 polls
2022-03-28 19:26:11 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (157635ms till timeout)
2022-03-28 19:26:12 [ForkJoinPool-2-worker-19] TRACE [Exec:248] Running command - oc --namespace namespace-10 exec my-cluster-49593d97-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-952089698-663307394 --describe --bootstrap-server my-cluster-49593d97-kafka-bootstrap:9092
2022-03-28 19:26:15 [ForkJoinPool-2-worker-19] INFO  [Exec:417] Command: oc --namespace namespace-10 exec my-cluster-49593d97-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-952089698-663307394 --describe --bootstrap-server my-cluster-49593d97-kafka-bootstrap:9092
2022-03-28 19:26:15 [ForkJoinPool-2-worker-19] INFO  [Exec:417] Return code: 0
2022-03-28 19:26:15 [ForkJoinPool-2-worker-19] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 15 polls
2022-03-28 19:26:15 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (153304ms till timeout)
2022-03-28 19:26:16 [ForkJoinPool-2-worker-19] TRACE [Exec:248] Running command - oc --namespace namespace-10 exec my-cluster-49593d97-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-952089698-663307394 --describe --bootstrap-server my-cluster-49593d97-kafka-bootstrap:9092
2022-03-28 19:26:20 [ForkJoinPool-2-worker-19] INFO  [Exec:417] Command: oc --namespace namespace-10 exec my-cluster-49593d97-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-952089698-663307394 --describe --bootstrap-server my-cluster-49593d97-kafka-bootstrap:9092
2022-03-28 19:26:20 [ForkJoinPool-2-worker-19] INFO  [Exec:417] Return code: 0
2022-03-28 19:26:20 [ForkJoinPool-2-worker-19] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 14 polls
2022-03-28 19:26:20 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (149055ms till timeout)
2022-03-28 19:26:21 [ForkJoinPool-2-worker-19] TRACE [Exec:248] Running command - oc --namespace namespace-10 exec my-cluster-49593d97-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-952089698-663307394 --describe --bootstrap-server my-cluster-49593d97-kafka-bootstrap:9092
2022-03-28 19:26:24 [ForkJoinPool-2-worker-19] INFO  [Exec:417] Command: oc --namespace namespace-10 exec my-cluster-49593d97-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-952089698-663307394 --describe --bootstrap-server my-cluster-49593d97-kafka-bootstrap:9092
2022-03-28 19:26:24 [ForkJoinPool-2-worker-19] INFO  [Exec:417] Return code: 0
2022-03-28 19:26:24 [ForkJoinPool-2-worker-19] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 13 polls
2022-03-28 19:26:24 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (144633ms till timeout)
2022-03-28 19:26:25 [ForkJoinPool-2-worker-19] TRACE [Exec:248] Running command - oc --namespace namespace-10 exec my-cluster-49593d97-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-952089698-663307394 --describe --bootstrap-server my-cluster-49593d97-kafka-bootstrap:9092
2022-03-28 19:26:28 [ForkJoinPool-2-worker-19] INFO  [Exec:417] Command: oc --namespace namespace-10 exec my-cluster-49593d97-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-952089698-663307394 --describe --bootstrap-server my-cluster-49593d97-kafka-bootstrap:9092
2022-03-28 19:26:28 [ForkJoinPool-2-worker-19] INFO  [Exec:417] Return code: 0
2022-03-28 19:26:28 [ForkJoinPool-2-worker-19] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 12 polls
2022-03-28 19:26:28 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (140332ms till timeout)
2022-03-28 19:26:29 [ForkJoinPool-2-worker-19] TRACE [Exec:248] Running command - oc --namespace namespace-10 exec my-cluster-49593d97-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-952089698-663307394 --describe --bootstrap-server my-cluster-49593d97-kafka-bootstrap:9092
2022-03-28 19:26:32 [ForkJoinPool-2-worker-19] INFO  [Exec:417] Command: oc --namespace namespace-10 exec my-cluster-49593d97-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-952089698-663307394 --describe --bootstrap-server my-cluster-49593d97-kafka-bootstrap:9092
2022-03-28 19:26:32 [ForkJoinPool-2-worker-19] INFO  [Exec:417] Return code: 0
2022-03-28 19:26:32 [ForkJoinPool-2-worker-19] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 11 polls
2022-03-28 19:26:33 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (136073ms till timeout)
2022-03-28 19:26:34 [ForkJoinPool-2-worker-19] TRACE [Exec:248] Running command - oc --namespace namespace-10 exec my-cluster-49593d97-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-952089698-663307394 --describe --bootstrap-server my-cluster-49593d97-kafka-bootstrap:9092
2022-03-28 19:26:37 [ForkJoinPool-2-worker-19] INFO  [Exec:417] Command: oc --namespace namespace-10 exec my-cluster-49593d97-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-952089698-663307394 --describe --bootstrap-server my-cluster-49593d97-kafka-bootstrap:9092
2022-03-28 19:26:37 [ForkJoinPool-2-worker-19] INFO  [Exec:417] Return code: 0
2022-03-28 19:26:37 [ForkJoinPool-2-worker-19] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 10 polls
2022-03-28 19:26:37 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (131781ms till timeout)
2022-03-28 19:26:38 [ForkJoinPool-2-worker-19] TRACE [Exec:248] Running command - oc --namespace namespace-10 exec my-cluster-49593d97-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-952089698-663307394 --describe --bootstrap-server my-cluster-49593d97-kafka-bootstrap:9092
2022-03-28 19:26:41 [ForkJoinPool-2-worker-19] INFO  [Exec:417] Command: oc --namespace namespace-10 exec my-cluster-49593d97-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-952089698-663307394 --describe --bootstrap-server my-cluster-49593d97-kafka-bootstrap:9092
2022-03-28 19:26:41 [ForkJoinPool-2-worker-19] INFO  [Exec:417] Return code: 0
2022-03-28 19:26:41 [ForkJoinPool-2-worker-19] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 9 polls
2022-03-28 19:26:41 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (127204ms till timeout)
2022-03-28 19:26:42 [ForkJoinPool-2-worker-19] TRACE [Exec:248] Running command - oc --namespace namespace-10 exec my-cluster-49593d97-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-952089698-663307394 --describe --bootstrap-server my-cluster-49593d97-kafka-bootstrap:9092
2022-03-28 19:26:51 [ForkJoinPool-2-worker-19] INFO  [Exec:417] Command: oc --namespace namespace-10 exec my-cluster-49593d97-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-952089698-663307394 --describe --bootstrap-server my-cluster-49593d97-kafka-bootstrap:9092
2022-03-28 19:26:51 [ForkJoinPool-2-worker-19] INFO  [Exec:417] Return code: 0
2022-03-28 19:26:51 [ForkJoinPool-2-worker-19] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 8 polls
2022-03-28 19:26:51 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (117609ms till timeout)
2022-03-28 19:26:52 [ForkJoinPool-2-worker-19] TRACE [Exec:248] Running command - oc --namespace namespace-10 exec my-cluster-49593d97-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-952089698-663307394 --describe --bootstrap-server my-cluster-49593d97-kafka-bootstrap:9092
2022-03-28 19:26:55 [ForkJoinPool-2-worker-19] INFO  [Exec:417] Command: oc --namespace namespace-10 exec my-cluster-49593d97-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-952089698-663307394 --describe --bootstrap-server my-cluster-49593d97-kafka-bootstrap:9092
2022-03-28 19:26:55 [ForkJoinPool-2-worker-19] INFO  [Exec:417] Return code: 0
2022-03-28 19:26:55 [ForkJoinPool-2-worker-19] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 7 polls
2022-03-28 19:26:55 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (113443ms till timeout)
2022-03-28 19:26:56 [ForkJoinPool-2-worker-19] TRACE [Exec:248] Running command - oc --namespace namespace-10 exec my-cluster-49593d97-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-952089698-663307394 --describe --bootstrap-server my-cluster-49593d97-kafka-bootstrap:9092
2022-03-28 19:26:59 [ForkJoinPool-2-worker-19] INFO  [Exec:417] Command: oc --namespace namespace-10 exec my-cluster-49593d97-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-952089698-663307394 --describe --bootstrap-server my-cluster-49593d97-kafka-bootstrap:9092
2022-03-28 19:26:59 [ForkJoinPool-2-worker-19] INFO  [Exec:417] Return code: 0
2022-03-28 19:26:59 [ForkJoinPool-2-worker-19] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 6 polls
2022-03-28 19:26:59 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (109172ms till timeout)
2022-03-28 19:27:00 [ForkJoinPool-2-worker-19] TRACE [Exec:248] Running command - oc --namespace namespace-10 exec my-cluster-49593d97-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-952089698-663307394 --describe --bootstrap-server my-cluster-49593d97-kafka-bootstrap:9092
2022-03-28 19:27:04 [ForkJoinPool-2-worker-19] INFO  [Exec:417] Command: oc --namespace namespace-10 exec my-cluster-49593d97-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-952089698-663307394 --describe --bootstrap-server my-cluster-49593d97-kafka-bootstrap:9092
2022-03-28 19:27:04 [ForkJoinPool-2-worker-19] INFO  [Exec:417] Return code: 0
2022-03-28 19:27:04 [ForkJoinPool-2-worker-19] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 5 polls
2022-03-28 19:27:04 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (104865ms till timeout)
2022-03-28 19:27:05 [ForkJoinPool-2-worker-19] TRACE [Exec:248] Running command - oc --namespace namespace-10 exec my-cluster-49593d97-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-952089698-663307394 --describe --bootstrap-server my-cluster-49593d97-kafka-bootstrap:9092
2022-03-28 19:27:08 [ForkJoinPool-2-worker-19] INFO  [Exec:417] Command: oc --namespace namespace-10 exec my-cluster-49593d97-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-952089698-663307394 --describe --bootstrap-server my-cluster-49593d97-kafka-bootstrap:9092
2022-03-28 19:27:08 [ForkJoinPool-2-worker-19] INFO  [Exec:417] Return code: 0
2022-03-28 19:27:08 [ForkJoinPool-2-worker-19] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 4 polls
2022-03-28 19:27:08 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (100547ms till timeout)
2022-03-28 19:27:09 [ForkJoinPool-2-worker-19] TRACE [Exec:248] Running command - oc --namespace namespace-10 exec my-cluster-49593d97-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-952089698-663307394 --describe --bootstrap-server my-cluster-49593d97-kafka-bootstrap:9092
2022-03-28 19:27:17 [ForkJoinPool-2-worker-19] INFO  [Exec:417] Command: oc --namespace namespace-10 exec my-cluster-49593d97-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-952089698-663307394 --describe --bootstrap-server my-cluster-49593d97-kafka-bootstrap:9092
2022-03-28 19:27:17 [ForkJoinPool-2-worker-19] INFO  [Exec:417] Return code: 0
2022-03-28 19:27:17 [ForkJoinPool-2-worker-19] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 3 polls
2022-03-28 19:27:17 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (91152ms till timeout)
2022-03-28 19:27:18 [ForkJoinPool-2-worker-19] TRACE [Exec:248] Running command - oc --namespace namespace-10 exec my-cluster-49593d97-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-952089698-663307394 --describe --bootstrap-server my-cluster-49593d97-kafka-bootstrap:9092
2022-03-28 19:27:22 [ForkJoinPool-2-worker-19] INFO  [Exec:417] Command: oc --namespace namespace-10 exec my-cluster-49593d97-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-952089698-663307394 --describe --bootstrap-server my-cluster-49593d97-kafka-bootstrap:9092
2022-03-28 19:27:22 [ForkJoinPool-2-worker-19] INFO  [Exec:417] Return code: 0
2022-03-28 19:27:22 [ForkJoinPool-2-worker-19] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 2 polls
2022-03-28 19:27:22 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (86813ms till timeout)
2022-03-28 19:27:23 [ForkJoinPool-2-worker-19] TRACE [Exec:248] Running command - oc --namespace namespace-10 exec my-cluster-49593d97-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-952089698-663307394 --describe --bootstrap-server my-cluster-49593d97-kafka-bootstrap:9092
2022-03-28 19:27:26 [ForkJoinPool-2-worker-19] INFO  [Exec:417] Command: oc --namespace namespace-10 exec my-cluster-49593d97-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-952089698-663307394 --describe --bootstrap-server my-cluster-49593d97-kafka-bootstrap:9092
2022-03-28 19:27:26 [ForkJoinPool-2-worker-19] INFO  [Exec:417] Return code: 0
2022-03-28 19:27:26 [ForkJoinPool-2-worker-19] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 1 polls
2022-03-28 19:27:26 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (82549ms till timeout)
2022-03-28 19:27:27 [ForkJoinPool-2-worker-19] TRACE [Exec:248] Running command - oc --namespace namespace-10 exec my-cluster-49593d97-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-952089698-663307394 --describe --bootstrap-server my-cluster-49593d97-kafka-bootstrap:9092
2022-03-28 19:27:30 [ForkJoinPool-2-worker-19] INFO  [Exec:417] Command: oc --namespace namespace-10 exec my-cluster-49593d97-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-952089698-663307394 --describe --bootstrap-server my-cluster-49593d97-kafka-bootstrap:9092
2022-03-28 19:27:30 [ForkJoinPool-2-worker-19] INFO  [Exec:417] Return code: 0
2022-03-28 19:27:30 [ForkJoinPool-2-worker-19] INFO  [KafkaTopicUtils:197] KafkaTopic's spec is stable for 20 polls intervals
2022-03-28 19:27:30 [ForkJoinPool-2-worker-19] INFO  [ReconciliationST:156] Setting annotation to "false", partitions should be scaled to 4
2022-03-28 19:27:31 [ForkJoinPool-2-worker-19] INFO  [KafkaTopicUtils:124] Waiting for KafkaTopic change my-topic-952089698-663307394
2022-03-28 19:27:31 [ForkJoinPool-2-worker-19] DEBUG [TestUtils:125] Waiting for KafkaTopic change my-topic-952089698-663307394
2022-03-28 19:27:31 [ForkJoinPool-2-worker-19] INFO  [ResourceManager:155] Create/Update KafkaRebalance my-cluster-49593d97 in namespace namespace-10
2022-03-28 19:27:31 [ForkJoinPool-2-worker-19] INFO  [ResourceManager:164] Using Namespace: namespace-10
2022-03-28 19:27:31 [ForkJoinPool-2-worker-19] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaRebalance:my-cluster-49593d97
2022-03-28 19:27:31 [ForkJoinPool-2-worker-19] INFO  [ResourceManager:433] Wait for KafkaRebalance: my-cluster-49593d97 will have desired state: PendingProposal
2022-03-28 19:27:31 [ForkJoinPool-2-worker-19] DEBUG [TestUtils:125] Waiting for KafkaRebalance: my-cluster-49593d97 will have desired state: PendingProposal
2022-03-28 19:27:32 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: PendingProposal not ready, will try again in 1000 ms (359903ms till timeout)
2022-03-28 19:27:33 [ForkJoinPool-2-worker-19] INFO  [ResourceManager:444] KafkaRebalance: my-cluster-49593d97 is in desired state: PendingProposal
2022-03-28 19:27:33 [ForkJoinPool-2-worker-19] INFO  [ReconciliationST:164] Waiting for ProposalReady, then add pause and rebalance annotation, rebalancing should not be triggered
2022-03-28 19:27:33 [ForkJoinPool-2-worker-19] INFO  [ResourceManager:433] Wait for KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady
2022-03-28 19:27:33 [ForkJoinPool-2-worker-19] DEBUG [TestUtils:125] Waiting for KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady
2022-03-28 19:27:33 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (599904ms till timeout)
2022-03-28 19:27:34 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (598805ms till timeout)
2022-03-28 19:27:35 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (597706ms till timeout)
2022-03-28 19:27:36 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (596605ms till timeout)
2022-03-28 19:27:37 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (595502ms till timeout)
2022-03-28 19:27:38 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (594404ms till timeout)
2022-03-28 19:27:39 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (593306ms till timeout)
2022-03-28 19:27:41 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (592207ms till timeout)
2022-03-28 19:27:42 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (591108ms till timeout)
2022-03-28 19:27:43 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (590011ms till timeout)
2022-03-28 19:27:44 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (588912ms till timeout)
2022-03-28 19:27:45 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (587813ms till timeout)
2022-03-28 19:27:46 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (586713ms till timeout)
2022-03-28 19:27:47 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (585610ms till timeout)
2022-03-28 19:27:48 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (584511ms till timeout)
2022-03-28 19:27:49 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (583388ms till timeout)
2022-03-28 19:27:50 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (582289ms till timeout)
2022-03-28 19:27:52 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (581191ms till timeout)
2022-03-28 19:27:53 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (580092ms till timeout)
2022-03-28 19:27:54 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (578990ms till timeout)
2022-03-28 19:27:55 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (577892ms till timeout)
2022-03-28 19:27:56 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (576794ms till timeout)
2022-03-28 19:27:57 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (575696ms till timeout)
2022-03-28 19:27:58 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (574597ms till timeout)
2022-03-28 19:27:59 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (573497ms till timeout)
2022-03-28 19:28:00 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (572396ms till timeout)
2022-03-28 19:28:01 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (571297ms till timeout)
2022-03-28 19:28:03 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (570198ms till timeout)
2022-03-28 19:28:04 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (569100ms till timeout)
2022-03-28 19:28:05 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (568002ms till timeout)
2022-03-28 19:28:06 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (566902ms till timeout)
2022-03-28 19:28:07 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (565803ms till timeout)
2022-03-28 19:28:08 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (564704ms till timeout)
2022-03-28 19:28:09 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (563606ms till timeout)
2022-03-28 19:28:10 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (562506ms till timeout)
2022-03-28 19:28:11 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (561406ms till timeout)
2022-03-28 19:28:12 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (560307ms till timeout)
2022-03-28 19:28:14 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (559209ms till timeout)
2022-03-28 19:28:15 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (558109ms till timeout)
2022-03-28 19:28:16 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (557009ms till timeout)
2022-03-28 19:28:17 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (555911ms till timeout)
2022-03-28 19:28:18 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (554813ms till timeout)
2022-03-28 19:28:19 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (553714ms till timeout)
2022-03-28 19:28:20 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (552614ms till timeout)
2022-03-28 19:28:21 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (551512ms till timeout)
2022-03-28 19:28:22 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (550414ms till timeout)
2022-03-28 19:28:23 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (549315ms till timeout)
2022-03-28 19:28:25 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (548217ms till timeout)
2022-03-28 19:28:26 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (547084ms till timeout)
2022-03-28 19:28:27 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (545985ms till timeout)
2022-03-28 19:28:28 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (544886ms till timeout)
2022-03-28 19:28:29 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (543786ms till timeout)
2022-03-28 19:28:30 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (542687ms till timeout)
2022-03-28 19:28:31 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (541588ms till timeout)
2022-03-28 19:28:32 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (540489ms till timeout)
2022-03-28 19:28:33 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (539379ms till timeout)
2022-03-28 19:28:34 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (538280ms till timeout)
2022-03-28 19:28:36 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (537182ms till timeout)
2022-03-28 19:28:37 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (536078ms till timeout)
2022-03-28 19:28:38 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (534979ms till timeout)
2022-03-28 19:28:39 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (533878ms till timeout)
2022-03-28 19:28:40 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (532780ms till timeout)
2022-03-28 19:28:41 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (531682ms till timeout)
2022-03-28 19:28:42 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (530580ms till timeout)
2022-03-28 19:28:43 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (529482ms till timeout)
2022-03-28 19:28:44 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (528383ms till timeout)
2022-03-28 19:28:45 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (527284ms till timeout)
2022-03-28 19:28:47 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (526185ms till timeout)
2022-03-28 19:28:48 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (525085ms till timeout)
2022-03-28 19:28:49 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (523986ms till timeout)
2022-03-28 19:28:50 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (522884ms till timeout)
2022-03-28 19:28:51 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (521784ms till timeout)
2022-03-28 19:28:52 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (520686ms till timeout)
2022-03-28 19:28:53 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (519588ms till timeout)
2022-03-28 19:28:54 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (518490ms till timeout)
2022-03-28 19:28:55 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (517390ms till timeout)
2022-03-28 19:28:56 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (516291ms till timeout)
2022-03-28 19:28:58 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (515192ms till timeout)
2022-03-28 19:28:59 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (514094ms till timeout)
2022-03-28 19:29:00 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (512994ms till timeout)
2022-03-28 19:29:01 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (511829ms till timeout)
2022-03-28 19:29:02 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (510730ms till timeout)
2022-03-28 19:29:03 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (509633ms till timeout)
2022-03-28 19:29:04 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (508534ms till timeout)
2022-03-28 19:29:05 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (507435ms till timeout)
2022-03-28 19:29:06 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (506335ms till timeout)
2022-03-28 19:29:07 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (505234ms till timeout)
2022-03-28 19:29:09 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (504134ms till timeout)
2022-03-28 19:29:10 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (503034ms till timeout)
2022-03-28 19:29:11 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (501935ms till timeout)
2022-03-28 19:29:12 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (500836ms till timeout)
2022-03-28 19:29:13 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (499737ms till timeout)
2022-03-28 19:29:14 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (498638ms till timeout)
2022-03-28 19:29:15 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (497538ms till timeout)
2022-03-28 19:29:16 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (496440ms till timeout)
2022-03-28 19:29:17 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (495341ms till timeout)
2022-03-28 19:29:18 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (494241ms till timeout)
2022-03-28 19:29:20 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (493141ms till timeout)
2022-03-28 19:29:21 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (492042ms till timeout)
2022-03-28 19:29:22 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (490943ms till timeout)
2022-03-28 19:29:23 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (489843ms till timeout)
2022-03-28 19:29:24 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (488743ms till timeout)
2022-03-28 19:29:25 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (487643ms till timeout)
2022-03-28 19:29:26 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (486540ms till timeout)
2022-03-28 19:29:27 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (485441ms till timeout)
2022-03-28 19:29:28 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (484339ms till timeout)
2022-03-28 19:29:29 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (483239ms till timeout)
2022-03-28 19:29:31 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (482139ms till timeout)
2022-03-28 19:29:32 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (481040ms till timeout)
2022-03-28 19:29:33 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (479940ms till timeout)
2022-03-28 19:29:34 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (478842ms till timeout)
2022-03-28 19:29:35 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (477743ms till timeout)
2022-03-28 19:29:36 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (476644ms till timeout)
2022-03-28 19:29:37 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (475544ms till timeout)
2022-03-28 19:29:38 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (474445ms till timeout)
2022-03-28 19:29:39 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (473346ms till timeout)
2022-03-28 19:29:40 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (472249ms till timeout)
2022-03-28 19:29:42 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (471151ms till timeout)
2022-03-28 19:29:43 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (470050ms till timeout)
2022-03-28 19:29:44 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (468952ms till timeout)
2022-03-28 19:29:45 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (467853ms till timeout)
2022-03-28 19:29:46 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (466753ms till timeout)
2022-03-28 19:29:47 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (465656ms till timeout)
2022-03-28 19:29:48 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (464557ms till timeout)
2022-03-28 19:29:49 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (463458ms till timeout)
2022-03-28 19:29:50 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (462359ms till timeout)
2022-03-28 19:29:51 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (461259ms till timeout)
2022-03-28 19:29:53 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (460160ms till timeout)
2022-03-28 19:29:54 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (459061ms till timeout)
2022-03-28 19:29:55 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (457962ms till timeout)
2022-03-28 19:29:56 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (456864ms till timeout)
2022-03-28 19:29:57 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (455762ms till timeout)
2022-03-28 19:29:58 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (454665ms till timeout)
2022-03-28 19:29:59 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (453566ms till timeout)
2022-03-28 19:30:00 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (452466ms till timeout)
2022-03-28 19:30:01 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (451365ms till timeout)
2022-03-28 19:30:02 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (450264ms till timeout)
2022-03-28 19:30:04 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (449166ms till timeout)
2022-03-28 19:30:05 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (448067ms till timeout)
2022-03-28 19:30:06 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (446968ms till timeout)
2022-03-28 19:30:07 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (445869ms till timeout)
2022-03-28 19:30:08 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (444770ms till timeout)
2022-03-28 19:30:09 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (443669ms till timeout)
2022-03-28 19:30:10 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (442567ms till timeout)
2022-03-28 19:30:11 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (441463ms till timeout)
2022-03-28 19:30:12 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (440363ms till timeout)
2022-03-28 19:30:13 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (439251ms till timeout)
2022-03-28 19:30:15 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (438148ms till timeout)
2022-03-28 19:30:16 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (437049ms till timeout)
2022-03-28 19:30:17 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (435949ms till timeout)
2022-03-28 19:30:18 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (434849ms till timeout)
2022-03-28 19:30:19 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (433749ms till timeout)
2022-03-28 19:30:20 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (432648ms till timeout)
2022-03-28 19:30:21 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (431549ms till timeout)
2022-03-28 19:30:22 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (430448ms till timeout)
2022-03-28 19:30:23 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (429351ms till timeout)
2022-03-28 19:30:24 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (428251ms till timeout)
2022-03-28 19:30:26 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (427152ms till timeout)
2022-03-28 19:30:27 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (426007ms till timeout)
2022-03-28 19:30:28 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (424907ms till timeout)
2022-03-28 19:30:29 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (423806ms till timeout)
2022-03-28 19:30:30 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (422689ms till timeout)
2022-03-28 19:30:31 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (421589ms till timeout)
2022-03-28 19:30:32 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (420490ms till timeout)
2022-03-28 19:30:33 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (419392ms till timeout)
2022-03-28 19:30:34 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (418293ms till timeout)
2022-03-28 19:30:36 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (417193ms till timeout)
2022-03-28 19:30:37 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (416092ms till timeout)
2022-03-28 19:30:38 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (414993ms till timeout)
2022-03-28 19:30:39 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (413893ms till timeout)
2022-03-28 19:30:40 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (412795ms till timeout)
2022-03-28 19:30:41 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (411696ms till timeout)
2022-03-28 19:30:42 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (410597ms till timeout)
2022-03-28 19:30:43 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (409500ms till timeout)
2022-03-28 19:30:44 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (408401ms till timeout)
2022-03-28 19:30:45 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (407303ms till timeout)
2022-03-28 19:30:47 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (406204ms till timeout)
2022-03-28 19:30:48 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (405105ms till timeout)
2022-03-28 19:30:49 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (404005ms till timeout)
2022-03-28 19:30:50 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (402902ms till timeout)
2022-03-28 19:30:51 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (401802ms till timeout)
2022-03-28 19:30:52 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (400703ms till timeout)
2022-03-28 19:30:53 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (399604ms till timeout)
2022-03-28 19:30:54 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (398506ms till timeout)
2022-03-28 19:30:55 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (397407ms till timeout)
2022-03-28 19:30:56 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (396309ms till timeout)
2022-03-28 19:30:58 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (395205ms till timeout)
2022-03-28 19:30:59 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (394105ms till timeout)
2022-03-28 19:31:00 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (393006ms till timeout)
2022-03-28 19:31:01 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (391829ms till timeout)
2022-03-28 19:31:02 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (390729ms till timeout)
2022-03-28 19:31:03 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (389631ms till timeout)
2022-03-28 19:31:04 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (388532ms till timeout)
2022-03-28 19:31:05 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (387432ms till timeout)
2022-03-28 19:31:06 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (386333ms till timeout)
2022-03-28 19:31:07 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (385234ms till timeout)
2022-03-28 19:31:09 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (384135ms till timeout)
2022-03-28 19:31:10 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (383035ms till timeout)
2022-03-28 19:31:11 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (381930ms till timeout)
2022-03-28 19:31:12 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (380831ms till timeout)
2022-03-28 19:31:13 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (379732ms till timeout)
2022-03-28 19:31:14 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (378634ms till timeout)
2022-03-28 19:31:15 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (377534ms till timeout)
2022-03-28 19:31:16 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (376435ms till timeout)
2022-03-28 19:31:17 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (375336ms till timeout)
2022-03-28 19:31:18 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (374236ms till timeout)
2022-03-28 19:31:20 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (373136ms till timeout)
2022-03-28 19:31:21 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (372037ms till timeout)
2022-03-28 19:31:22 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (370938ms till timeout)
2022-03-28 19:31:23 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (369838ms till timeout)
2022-03-28 19:31:24 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (368738ms till timeout)
2022-03-28 19:31:25 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (367639ms till timeout)
2022-03-28 19:31:26 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (366537ms till timeout)
2022-03-28 19:31:27 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (365437ms till timeout)
2022-03-28 19:31:28 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (364338ms till timeout)
2022-03-28 19:31:29 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (363240ms till timeout)
2022-03-28 19:31:31 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (362141ms till timeout)
2022-03-28 19:31:32 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (361042ms till timeout)
2022-03-28 19:31:33 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (359942ms till timeout)
2022-03-28 19:31:34 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (358843ms till timeout)
2022-03-28 19:31:35 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (357744ms till timeout)
2022-03-28 19:31:36 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady not ready, will try again in 1000 ms (356642ms till timeout)
2022-03-28 19:31:37 [ForkJoinPool-2-worker-19] INFO  [ResourceManager:444] KafkaRebalance: my-cluster-49593d97 is in desired state: ProposalReady
2022-03-28 19:31:37 [ForkJoinPool-2-worker-19] INFO  [ResourceManager:433] Wait for KafkaRebalance: my-cluster-49593d97 will have desired state: ReconciliationPaused
2022-03-28 19:31:37 [ForkJoinPool-2-worker-19] DEBUG [TestUtils:125] Waiting for KafkaRebalance: my-cluster-49593d97 will have desired state: ReconciliationPaused
2022-03-28 19:31:38 [ForkJoinPool-2-worker-19] INFO  [ResourceManager:444] KafkaRebalance: my-cluster-49593d97 is in desired state: ReconciliationPaused
2022-03-28 19:31:38 [ForkJoinPool-2-worker-19] INFO  [KafkaRebalanceUtils:63] Reconciliation #1(test) KafkaRebalance(namespace-10/my-cluster-49593d97): Annotating KafkaRebalance:my-cluster-49593d97 with annotation approve
2022-03-28 19:31:38 [ForkJoinPool-2-worker-19] TRACE [Exec:248] Running command - oc --namespace namespace-10 annotate kafkarebalance my-cluster-49593d97 strimzi.io/rebalance=approve
2022-03-28 19:31:43 [ForkJoinPool-2-worker-19] DEBUG [Exec:419] Command: oc --namespace namespace-10 annotate kafkarebalance my-cluster-49593d97 strimzi.io/rebalance=approve
2022-03-28 19:31:43 [ForkJoinPool-2-worker-19] DEBUG [Exec:419] Return code: 0
2022-03-28 19:31:44 [ForkJoinPool-2-worker-19] DEBUG [TestUtils:125] Waiting for KafkaRebalance status will be stable
2022-03-28 19:31:44 [ForkJoinPool-2-worker-19] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-10/my-cluster-49593d97): KafkaRebalance status gonna be stable in 19 polls
2022-03-28 19:31:44 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (179901ms till timeout)
2022-03-28 19:31:45 [ForkJoinPool-2-worker-19] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-10/my-cluster-49593d97): KafkaRebalance status gonna be stable in 18 polls
2022-03-28 19:31:45 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (178801ms till timeout)
2022-03-28 19:31:46 [ForkJoinPool-2-worker-19] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-10/my-cluster-49593d97): KafkaRebalance status gonna be stable in 17 polls
2022-03-28 19:31:46 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (177702ms till timeout)
2022-03-28 19:31:47 [ForkJoinPool-2-worker-19] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-10/my-cluster-49593d97): KafkaRebalance status gonna be stable in 16 polls
2022-03-28 19:31:47 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (176603ms till timeout)
2022-03-28 19:31:48 [ForkJoinPool-2-worker-19] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-10/my-cluster-49593d97): KafkaRebalance status gonna be stable in 15 polls
2022-03-28 19:31:48 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (175504ms till timeout)
2022-03-28 19:31:49 [ForkJoinPool-2-worker-19] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-10/my-cluster-49593d97): KafkaRebalance status gonna be stable in 14 polls
2022-03-28 19:31:49 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (174404ms till timeout)
2022-03-28 19:31:50 [ForkJoinPool-2-worker-19] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-10/my-cluster-49593d97): KafkaRebalance status gonna be stable in 13 polls
2022-03-28 19:31:50 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (173305ms till timeout)
2022-03-28 19:31:52 [ForkJoinPool-2-worker-19] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-10/my-cluster-49593d97): KafkaRebalance status gonna be stable in 12 polls
2022-03-28 19:31:52 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (172203ms till timeout)
2022-03-28 19:31:53 [ForkJoinPool-2-worker-19] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-10/my-cluster-49593d97): KafkaRebalance status gonna be stable in 11 polls
2022-03-28 19:31:53 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (171103ms till timeout)
2022-03-28 19:31:54 [ForkJoinPool-2-worker-19] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-10/my-cluster-49593d97): KafkaRebalance status gonna be stable in 10 polls
2022-03-28 19:31:54 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (170004ms till timeout)
2022-03-28 19:31:55 [ForkJoinPool-2-worker-19] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-10/my-cluster-49593d97): KafkaRebalance status gonna be stable in 9 polls
2022-03-28 19:31:55 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (168905ms till timeout)
2022-03-28 19:31:56 [ForkJoinPool-2-worker-19] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-10/my-cluster-49593d97): KafkaRebalance status gonna be stable in 8 polls
2022-03-28 19:31:56 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (167807ms till timeout)
2022-03-28 19:31:57 [ForkJoinPool-2-worker-19] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-10/my-cluster-49593d97): KafkaRebalance status gonna be stable in 7 polls
2022-03-28 19:31:57 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (166707ms till timeout)
2022-03-28 19:31:58 [ForkJoinPool-2-worker-19] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-10/my-cluster-49593d97): KafkaRebalance status gonna be stable in 6 polls
2022-03-28 19:31:58 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (165601ms till timeout)
2022-03-28 19:31:59 [ForkJoinPool-2-worker-19] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-10/my-cluster-49593d97): KafkaRebalance status gonna be stable in 5 polls
2022-03-28 19:31:59 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (164501ms till timeout)
2022-03-28 19:32:00 [ForkJoinPool-2-worker-19] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-10/my-cluster-49593d97): KafkaRebalance status gonna be stable in 4 polls
2022-03-28 19:32:00 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (163401ms till timeout)
2022-03-28 19:32:01 [ForkJoinPool-2-worker-19] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-10/my-cluster-49593d97): KafkaRebalance status gonna be stable in 3 polls
2022-03-28 19:32:01 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (162301ms till timeout)
2022-03-28 19:32:03 [ForkJoinPool-2-worker-19] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-10/my-cluster-49593d97): KafkaRebalance status gonna be stable in 2 polls
2022-03-28 19:32:03 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (161193ms till timeout)
2022-03-28 19:32:04 [ForkJoinPool-2-worker-19] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-10/my-cluster-49593d97): KafkaRebalance status gonna be stable in 1 polls
2022-03-28 19:32:04 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (160095ms till timeout)
2022-03-28 19:32:05 [ForkJoinPool-2-worker-19] INFO  [KafkaRebalanceUtils:118] Reconciliation #2(test) KafkaRebalance(namespace-10/my-cluster-49593d97): KafkaRebalance status is stable for 20 polls intervals
2022-03-28 19:32:05 [ForkJoinPool-2-worker-19] INFO  [ReconciliationST:178] Setting annotation to "false" and waiting for KafkaRebalance to be in Ready state
2022-03-28 19:32:05 [ForkJoinPool-2-worker-19] INFO  [ResourceManager:433] Wait for KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady
2022-03-28 19:32:05 [ForkJoinPool-2-worker-19] DEBUG [TestUtils:125] Waiting for KafkaRebalance: my-cluster-49593d97 will have desired state: ProposalReady
2022-03-28 19:32:05 [ForkJoinPool-2-worker-19] INFO  [ResourceManager:444] KafkaRebalance: my-cluster-49593d97 is in desired state: ProposalReady
2022-03-28 19:32:05 [ForkJoinPool-2-worker-19] INFO  [KafkaRebalanceUtils:63] Reconciliation #3(test) KafkaRebalance(namespace-10/my-cluster-49593d97): Annotating KafkaRebalance:my-cluster-49593d97 with annotation approve
2022-03-28 19:32:05 [ForkJoinPool-2-worker-19] TRACE [Exec:248] Running command - oc --namespace namespace-10 annotate kafkarebalance my-cluster-49593d97 strimzi.io/rebalance=approve
2022-03-28 19:32:06 [ForkJoinPool-2-worker-19] DEBUG [Exec:419] Command: oc --namespace namespace-10 annotate kafkarebalance my-cluster-49593d97 strimzi.io/rebalance=approve
2022-03-28 19:32:06 [ForkJoinPool-2-worker-19] DEBUG [Exec:419] Return code: 0
2022-03-28 19:32:06 [ForkJoinPool-2-worker-19] INFO  [ResourceManager:433] Wait for KafkaRebalance: my-cluster-49593d97 will have desired state: Ready
2022-03-28 19:32:06 [ForkJoinPool-2-worker-19] DEBUG [TestUtils:125] Waiting for KafkaRebalance: my-cluster-49593d97 will have desired state: Ready
2022-03-28 19:32:06 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (599902ms till timeout)
2022-03-28 19:32:07 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (598802ms till timeout)
2022-03-28 19:32:08 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (597699ms till timeout)
2022-03-28 19:32:09 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (596599ms till timeout)
2022-03-28 19:32:10 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (595498ms till timeout)
2022-03-28 19:32:11 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (594399ms till timeout)
2022-03-28 19:32:13 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (593300ms till timeout)
2022-03-28 19:32:14 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (592203ms till timeout)
2022-03-28 19:32:15 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (591103ms till timeout)
2022-03-28 19:32:16 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (590004ms till timeout)
2022-03-28 19:32:17 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (588904ms till timeout)
2022-03-28 19:32:18 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (587805ms till timeout)
2022-03-28 19:32:19 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (586705ms till timeout)
2022-03-28 19:32:20 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (585606ms till timeout)
2022-03-28 19:32:21 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (584508ms till timeout)
2022-03-28 19:32:22 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (583406ms till timeout)
2022-03-28 19:32:24 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (582308ms till timeout)
2022-03-28 19:32:25 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (581209ms till timeout)
2022-03-28 19:32:26 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (580112ms till timeout)
2022-03-28 19:32:27 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (578981ms till timeout)
2022-03-28 19:32:28 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (577881ms till timeout)
2022-03-28 19:32:29 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (576782ms till timeout)
2022-03-28 19:32:30 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (575683ms till timeout)
2022-03-28 19:32:31 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (574583ms till timeout)
2022-03-28 19:32:32 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (573478ms till timeout)
2022-03-28 19:32:33 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (572381ms till timeout)
2022-03-28 19:32:35 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (571282ms till timeout)
2022-03-28 19:32:36 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (570183ms till timeout)
2022-03-28 19:32:37 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (569084ms till timeout)
2022-03-28 19:32:38 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (567986ms till timeout)
2022-03-28 19:32:39 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (566886ms till timeout)
2022-03-28 19:32:40 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (565787ms till timeout)
2022-03-28 19:32:41 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (564688ms till timeout)
2022-03-28 19:32:42 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (563590ms till timeout)
2022-03-28 19:32:43 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (562491ms till timeout)
2022-03-28 19:32:44 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (561391ms till timeout)
2022-03-28 19:32:46 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (560292ms till timeout)
2022-03-28 19:32:47 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (559193ms till timeout)
2022-03-28 19:32:48 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (558095ms till timeout)
2022-03-28 19:32:49 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (556996ms till timeout)
2022-03-28 19:32:50 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (555898ms till timeout)
2022-03-28 19:32:51 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (554800ms till timeout)
2022-03-28 19:32:52 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (553701ms till timeout)
2022-03-28 19:32:53 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (552601ms till timeout)
2022-03-28 19:32:54 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (551502ms till timeout)
2022-03-28 19:32:55 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (550404ms till timeout)
2022-03-28 19:32:57 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (549306ms till timeout)
2022-03-28 19:32:58 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (548205ms till timeout)
2022-03-28 19:32:59 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (547107ms till timeout)
2022-03-28 19:33:00 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (546009ms till timeout)
2022-03-28 19:33:01 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] KafkaRebalance: my-cluster-49593d97 will have desired state: Ready not ready, will try again in 1000 ms (544908ms till timeout)
2022-03-28 19:33:02 [ForkJoinPool-2-worker-19] INFO  [ResourceManager:444] KafkaRebalance: my-cluster-49593d97 is in desired state: Ready
2022-03-28 19:33:02 [ForkJoinPool-2-worker-19] DEBUG [AbstractST:674] ============================================================================
2022-03-28 19:33:02 [ForkJoinPool-2-worker-19] DEBUG [AbstractST:675] [operators.ReconciliationST - After Each] - Clean up after test
2022-03-28 19:33:02 [ForkJoinPool-2-worker-19] INFO  [ResourceManager:344] ############################################################################
2022-03-28 19:33:02 [ForkJoinPool-2-worker-19] INFO  [ResourceManager:348] Delete all resources for testPauseReconciliationInKafkaRebalanceAndTopic
2022-03-28 19:33:02 [ForkJoinPool-2-worker-19] INFO  [ResourceManager:241] Delete of KafkaTopic my-topic-952089698-663307394 in namespace namespace-10
2022-03-28 19:33:02 [ForkJoinPool-2-worker-23] INFO  [ResourceManager:241] Delete of KafkaRebalance my-cluster-49593d97 in namespace namespace-10
2022-03-28 19:33:02 [ForkJoinPool-2-worker-5] INFO  [ResourceManager:241] Delete of Kafka my-cluster-49593d97 in namespace namespace-10
2022-03-28 19:33:02 [ForkJoinPool-2-worker-5] INFO  [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-10, for cruise control Kafka cluster my-cluster-49593d97
2022-03-28 19:33:02 [ForkJoinPool-2-worker-19] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-952089698-663307394
2022-03-28 19:33:02 [ForkJoinPool-2-worker-23] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaRebalance:my-cluster-49593d97
2022-03-28 19:33:03 [ForkJoinPool-2-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-49593d97
2022-03-28 19:33:03 [ForkJoinPool-2-worker-5] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster-49593d97 not ready, will try again in 10000 ms (839893ms till timeout)
2022-03-28 19:33:13 [ForkJoinPool-2-worker-19] INFO  [ResourceManager:369] ############################################################################
2022-03-28 19:33:13 [ForkJoinPool-2-worker-19] INFO  [TestSuiteNamespaceManager:200] Deleting namespace:namespace-10 for test case:testPauseReconciliationInKafkaRebalanceAndTopic
2022-03-28 19:33:13 [ForkJoinPool-2-worker-19] DEBUG [TestUtils:125] Waiting for Namespace namespace-10 removal
2022-03-28 19:33:13 [ForkJoinPool-2-worker-19] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 19:33:14 [ForkJoinPool-2-worker-19] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 19:33:14 [ForkJoinPool-2-worker-19] DEBUG [Exec:419] Return code: 0
2022-03-28 19:33:14 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Namespace namespace-10 removal not ready, will try again in 1000 ms (478802ms till timeout)
2022-03-28 19:33:15 [ForkJoinPool-2-worker-19] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 19:33:16 [ForkJoinPool-2-worker-19] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 19:33:16 [ForkJoinPool-2-worker-19] DEBUG [Exec:419] Return code: 0
2022-03-28 19:33:16 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Namespace namespace-10 removal not ready, will try again in 1000 ms (477339ms till timeout)
2022-03-28 19:33:17 [ForkJoinPool-2-worker-19] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 19:33:17 [ForkJoinPool-2-worker-19] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 19:33:17 [ForkJoinPool-2-worker-19] DEBUG [Exec:419] Return code: 0
2022-03-28 19:33:17 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Namespace namespace-10 removal not ready, will try again in 1000 ms (475850ms till timeout)
2022-03-28 19:33:18 [ForkJoinPool-2-worker-19] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 19:33:19 [ForkJoinPool-2-worker-19] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 19:33:19 [ForkJoinPool-2-worker-19] DEBUG [Exec:419] Return code: 0
2022-03-28 19:33:19 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Namespace namespace-10 removal not ready, will try again in 1000 ms (474378ms till timeout)
2022-03-28 19:33:20 [ForkJoinPool-2-worker-19] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 19:33:20 [ForkJoinPool-2-worker-19] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 19:33:20 [ForkJoinPool-2-worker-19] DEBUG [Exec:419] Return code: 0
2022-03-28 19:33:20 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Namespace namespace-10 removal not ready, will try again in 1000 ms (472917ms till timeout)
2022-03-28 19:33:21 [ForkJoinPool-2-worker-19] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 19:33:22 [ForkJoinPool-2-worker-19] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 19:33:22 [ForkJoinPool-2-worker-19] DEBUG [Exec:419] Return code: 0
2022-03-28 19:33:22 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Namespace namespace-10 removal not ready, will try again in 1000 ms (471432ms till timeout)
2022-03-28 19:33:23 [ForkJoinPool-2-worker-19] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 19:33:23 [ForkJoinPool-2-worker-19] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 19:33:23 [ForkJoinPool-2-worker-19] DEBUG [Exec:419] Return code: 0
2022-03-28 19:33:23 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Namespace namespace-10 removal not ready, will try again in 1000 ms (469979ms till timeout)
2022-03-28 19:33:24 [ForkJoinPool-2-worker-19] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 19:33:25 [ForkJoinPool-2-worker-19] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 19:33:25 [ForkJoinPool-2-worker-19] DEBUG [Exec:419] Return code: 0
2022-03-28 19:33:25 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Namespace namespace-10 removal not ready, will try again in 1000 ms (468527ms till timeout)
2022-03-28 19:33:26 [ForkJoinPool-2-worker-19] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 19:33:26 [ForkJoinPool-2-worker-19] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 19:33:26 [ForkJoinPool-2-worker-19] DEBUG [Exec:419] Return code: 0
2022-03-28 19:33:26 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Namespace namespace-10 removal not ready, will try again in 1000 ms (467069ms till timeout)
2022-03-28 19:33:27 [ForkJoinPool-2-worker-19] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 19:33:28 [ForkJoinPool-2-worker-19] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 19:33:28 [ForkJoinPool-2-worker-19] DEBUG [Exec:419] Return code: 0
2022-03-28 19:33:28 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Namespace namespace-10 removal not ready, will try again in 1000 ms (465627ms till timeout)
2022-03-28 19:33:29 [ForkJoinPool-2-worker-19] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 19:33:29 [ForkJoinPool-2-worker-19] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 19:33:29 [ForkJoinPool-2-worker-19] DEBUG [Exec:419] Return code: 0
2022-03-28 19:33:29 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Namespace namespace-10 removal not ready, will try again in 1000 ms (464174ms till timeout)
2022-03-28 19:33:30 [ForkJoinPool-2-worker-19] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 19:33:30 [ForkJoinPool-2-worker-19] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 19:33:30 [ForkJoinPool-2-worker-19] DEBUG [Exec:419] Return code: 0
2022-03-28 19:33:30 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Namespace namespace-10 removal not ready, will try again in 1000 ms (462734ms till timeout)
2022-03-28 19:33:31 [ForkJoinPool-2-worker-19] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 19:33:32 [ForkJoinPool-2-worker-19] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 19:33:32 [ForkJoinPool-2-worker-19] DEBUG [Exec:419] Return code: 0
2022-03-28 19:33:32 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Namespace namespace-10 removal not ready, will try again in 1000 ms (461236ms till timeout)
2022-03-28 19:33:33 [ForkJoinPool-2-worker-19] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 19:33:33 [ForkJoinPool-2-worker-19] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 19:33:33 [ForkJoinPool-2-worker-19] DEBUG [Exec:419] Return code: 0
2022-03-28 19:33:33 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Namespace namespace-10 removal not ready, will try again in 1000 ms (459758ms till timeout)
2022-03-28 19:33:34 [ForkJoinPool-2-worker-19] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 19:33:35 [ForkJoinPool-2-worker-19] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 19:33:35 [ForkJoinPool-2-worker-19] DEBUG [Exec:419] Return code: 0
2022-03-28 19:33:35 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Namespace namespace-10 removal not ready, will try again in 1000 ms (458266ms till timeout)
2022-03-28 19:33:36 [ForkJoinPool-2-worker-19] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 19:33:36 [ForkJoinPool-2-worker-19] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 19:33:36 [ForkJoinPool-2-worker-19] DEBUG [Exec:419] Return code: 0
2022-03-28 19:33:36 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Namespace namespace-10 removal not ready, will try again in 1000 ms (456817ms till timeout)
2022-03-28 19:33:37 [ForkJoinPool-2-worker-19] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 19:33:38 [ForkJoinPool-2-worker-19] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 19:33:38 [ForkJoinPool-2-worker-19] DEBUG [Exec:419] Return code: 0
2022-03-28 19:33:38 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Namespace namespace-10 removal not ready, will try again in 1000 ms (455376ms till timeout)
2022-03-28 19:33:39 [ForkJoinPool-2-worker-19] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 19:33:39 [ForkJoinPool-2-worker-19] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 19:33:39 [ForkJoinPool-2-worker-19] DEBUG [Exec:419] Return code: 0
2022-03-28 19:33:39 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Namespace namespace-10 removal not ready, will try again in 1000 ms (453929ms till timeout)
2022-03-28 19:33:40 [ForkJoinPool-2-worker-19] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 19:33:41 [ForkJoinPool-2-worker-19] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 19:33:41 [ForkJoinPool-2-worker-19] DEBUG [Exec:419] Return code: 0
2022-03-28 19:33:41 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Namespace namespace-10 removal not ready, will try again in 1000 ms (452452ms till timeout)
2022-03-28 19:33:42 [ForkJoinPool-2-worker-19] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 19:33:42 [ForkJoinPool-2-worker-19] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 19:33:42 [ForkJoinPool-2-worker-19] DEBUG [Exec:419] Return code: 0
2022-03-28 19:33:42 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Namespace namespace-10 removal not ready, will try again in 1000 ms (450998ms till timeout)
2022-03-28 19:33:43 [ForkJoinPool-2-worker-19] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 19:33:44 [ForkJoinPool-2-worker-19] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 19:33:44 [ForkJoinPool-2-worker-19] DEBUG [Exec:419] Return code: 1
2022-03-28 19:33:44 [ForkJoinPool-2-worker-19] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 19:33:44 [ForkJoinPool-2-worker-19] DEBUG [Exec:419] Error from server (NotFound): namespaces "namespace-10" not found
2022-03-28 19:33:44 [ForkJoinPool-2-worker-19] DEBUG [Exec:419] ======STDERR END======
2022-03-28 19:33:44 [ForkJoinPool-2-worker-19] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[]}
2022-03-28 19:33:44 [ForkJoinPool-2-worker-19] DEBUG [SuiteThreadController:267] testPauseReconciliationInKafkaRebalanceAndTopic - Notifies waiting test cases:[testPauseReconciliationInKafkaRebalanceAndTopic, testPauseReconciliationInKafkaAndKafkaConnectWithConnector, testCapacityFile, testConfigurationFileIsCreated, testConfigurationPerformanceOptions, testConfigurationReflection, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testDeployAndUnDeployCruiseControl, testTlsExternalUserWithQuotas, testCreatingUsersWithSecretPrefix, testThrottlingQuotasCreateTopic, testThrottlingQuotasCreateAlterPartitions, testKafkaAdminTopicOperations, testUserTemplate, testUpdateUser, testTlsExternalUser, testSendSimpleMessageTls, testReceiveSimpleMessageTlsScramSha, testThrottlingQuotasDeleteTopic, testReceiveSimpleMessageTls, testTopicModificationOfReplicationFactor, testSendingMessagesToNonExistingTopic, testMoreReplicasThanAvailableBrokers, testCreateTopicViaKafka, testPauseReconciliationInKafkaRebalanceAndTopic] to and randomly select one to start execution
2022-03-28 19:33:44 [ForkJoinPool-2-worker-19] DEBUG [SuiteThreadController:93] [operators.ReconciliationST] - Removing parallel test: testPauseReconciliationInKafkaRebalanceAndTopic
2022-03-28 19:33:44 [ForkJoinPool-2-worker-19] DEBUG [SuiteThreadController:97] [operators.ReconciliationST] - Parallel test count: 0
2022-03-28 19:33:44 [ForkJoinPool-2-worker-19] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.ReconciliationST.testPauseReconciliationInKafkaRebalanceAndTopic-FINISHED
2022-03-28 19:33:44 [ForkJoinPool-2-worker-19] INFO  [TestSeparator:30] ############################################################################
2022-03-28 19:33:44 [ForkJoinPool-2-worker-19] DEBUG [AbstractST:689] ============================================================================
2022-03-28 19:33:44 [ForkJoinPool-2-worker-19] DEBUG [AbstractST:690] [operators.ReconciliationST - After All] - Clean up after test suite
2022-03-28 19:33:44 [ForkJoinPool-2-worker-19] INFO  [ResourceManager:344] ############################################################################
2022-03-28 19:33:44 [ForkJoinPool-2-worker-19] INFO  [ResourceManager:346] In context ReconciliationST is everything deleted.
2022-03-28 19:33:44 [ForkJoinPool-2-worker-19] INFO  [ResourceManager:369] ############################################################################
2022-03-28 19:33:44 [ForkJoinPool-2-worker-19] DEBUG [TestUtils:125] Waiting for Namespace reconciliation-st removal
2022-03-28 19:33:44 [ForkJoinPool-2-worker-19] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace reconciliation-st -o yaml
2022-03-28 19:33:44 [ForkJoinPool-2-worker-19] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace reconciliation-st -o yaml
2022-03-28 19:33:44 [ForkJoinPool-2-worker-19] DEBUG [Exec:419] Return code: 0
2022-03-28 19:33:44 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Namespace reconciliation-st removal not ready, will try again in 1000 ms (479538ms till timeout)
2022-03-28 19:33:45 [ForkJoinPool-2-worker-19] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace reconciliation-st -o yaml
2022-03-28 19:33:46 [ForkJoinPool-2-worker-19] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace reconciliation-st -o yaml
2022-03-28 19:33:46 [ForkJoinPool-2-worker-19] DEBUG [Exec:419] Return code: 0
2022-03-28 19:33:46 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Namespace reconciliation-st removal not ready, will try again in 1000 ms (478062ms till timeout)
2022-03-28 19:33:47 [ForkJoinPool-2-worker-19] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace reconciliation-st -o yaml
2022-03-28 19:33:47 [ForkJoinPool-2-worker-19] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace reconciliation-st -o yaml
2022-03-28 19:33:47 [ForkJoinPool-2-worker-19] DEBUG [Exec:419] Return code: 0
2022-03-28 19:33:47 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Namespace reconciliation-st removal not ready, will try again in 1000 ms (476542ms till timeout)
2022-03-28 19:33:48 [ForkJoinPool-2-worker-19] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace reconciliation-st -o yaml
2022-03-28 19:33:49 [ForkJoinPool-2-worker-19] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace reconciliation-st -o yaml
2022-03-28 19:33:49 [ForkJoinPool-2-worker-19] DEBUG [Exec:419] Return code: 0
2022-03-28 19:33:49 [ForkJoinPool-2-worker-19] TRACE [TestUtils:176] Namespace reconciliation-st removal not ready, will try again in 1000 ms (475015ms till timeout)
2022-03-28 19:33:50 [ForkJoinPool-2-worker-19] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace reconciliation-st -o yaml
2022-03-28 19:33:50 [ForkJoinPool-2-worker-19] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-10 get Namespace reconciliation-st -o yaml
2022-03-28 19:33:50 [ForkJoinPool-2-worker-19] DEBUG [Exec:419] Return code: 1
2022-03-28 19:33:50 [ForkJoinPool-2-worker-19] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 19:33:50 [ForkJoinPool-2-worker-19] DEBUG [Exec:419] Error from server (NotFound): namespaces "reconciliation-st" not found
2022-03-28 19:33:50 [ForkJoinPool-2-worker-19] DEBUG [Exec:419] ======STDERR END======
2022-03-28 19:33:50 [ForkJoinPool-2-worker-19] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@62b9e483=[], io.strimzi.test.logs.CollectorElement@b850ef2a=[]}
2022-03-28 19:33:50 [ForkJoinPool-2-worker-19] DEBUG [SuiteThreadController:254] ReconciliationST - Notifies waiting test suites:[TopicST, ThrottlingQuotaST, UserST, HttpBridgeTlsST, HttpBridgeScramShaST, ReconciliationST, CruiseControlConfigurationST, ReconciliationST] to and randomly select one to start execution
2022-03-28 19:33:50 [ForkJoinPool-2-worker-19] DEBUG [SuiteThreadController:85] [operators.ReconciliationST] - Removing parallel suite: ReconciliationST
2022-03-28 19:33:50 [ForkJoinPool-2-worker-19] DEBUG [SuiteThreadController:89] [operators.ReconciliationST] - Parallel suites count: 0
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 633.387 s - in io.strimzi.systemtest.operators.ReconciliationST
2022-03-28 19:33:50 [ForkJoinPool-2-worker-19] INFO  [SetupClusterOperator:618] ============================================================================
2022-03-28 19:33:50 [ForkJoinPool-2-worker-19] INFO  [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-03-28 19:33:50 [ForkJoinPool-2-worker-19] INFO  [SetupClusterOperator:620] ============================================================================
2022-03-28 19:33:50 [ForkJoinPool-2-worker-19] INFO  [ResourceManager:344] ############################################################################
2022-03-28 19:33:50 [ForkJoinPool-2-worker-19] INFO  [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-03-28 19:33:50 [ForkJoinPool-2-worker-19] INFO  [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-28 19:33:50 [ForkJoinPool-2-worker-5] INFO  [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-28 19:33:50 [ForkJoinPool-2-worker-31] INFO  [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-28 19:33:50 [ForkJoinPool-2-worker-13] INFO  [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-28 19:33:50 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-28 19:33:50 [ForkJoinPool-2-worker-11] INFO  [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-28 19:33:50 [ForkJoinPool-2-worker-23] INFO  [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-28 19:33:50 [ForkJoinPool-2-worker-25] INFO  [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-28 19:33:50 [ForkJoinPool-2-worker-9] INFO  [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-28 19:33:50 [ForkJoinPool-2-worker-7] INFO  [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-28 19:33:50 [ForkJoinPool-2-worker-21] INFO  [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-28 19:33:50 [ForkJoinPool-2-worker-27] INFO  [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-28 19:33:50 [ForkJoinPool-2-worker-29] INFO  [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-28 19:33:50 [ForkJoinPool-2-worker-17] INFO  [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-28 19:33:50 [ForkJoinPool-2-worker-15] INFO  [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-28 19:33:50 [ForkJoinPool-2-worker-31] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-28 19:33:50 [ForkJoinPool-2-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-namespaced
2022-03-28 19:33:50 [ForkJoinPool-2-worker-23] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-28 19:33:50 [ForkJoinPool-2-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-28 19:33:50 [ForkJoinPool-2-worker-21] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-28 19:33:50 [ForkJoinPool-2-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-28 19:33:50 [ForkJoinPool-2-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-28 19:33:51 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-28 19:33:51 [ForkJoinPool-2-worker-17] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-28 19:33:51 [ForkJoinPool-2-worker-25] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-28 19:33:51 [ForkJoinPool-2-worker-27] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-28 19:33:51 [ForkJoinPool-2-worker-15] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-28 19:33:51 [ForkJoinPool-2-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-28 19:33:51 [ForkJoinPool-2-worker-29] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-28 19:33:51 [ForkJoinPool-2-worker-19] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-28 19:33:51 [ForkJoinPool-2-worker-7] INFO  [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-28 19:33:51 [ForkJoinPool-2-worker-11] INFO  [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-28 19:33:51 [ForkJoinPool-2-worker-17] INFO  [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-28 19:33:51 [ForkJoinPool-2-worker-31] INFO  [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-28 19:33:51 [ForkJoinPool-2-worker-23] INFO  [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-03-28 19:33:51 [ForkJoinPool-2-worker-21] INFO  [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-28 19:33:51 [ForkJoinPool-2-worker-5] INFO  [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-28 19:33:51 [ForkJoinPool-2-worker-25] INFO  [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-28 19:33:51 [ForkJoinPool-2-worker-9] INFO  [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-28 19:33:51 [ForkJoinPool-2-worker-13] INFO  [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-28 19:33:51 [ForkJoinPool-2-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-28 19:33:51 [ForkJoinPool-2-worker-23] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-entity-operator
2022-03-28 19:33:51 [ForkJoinPool-2-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-28 19:33:52 [ForkJoinPool-2-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-28 19:33:52 [ForkJoinPool-2-worker-21] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-28 19:33:52 [ForkJoinPool-2-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-28 19:33:52 [ForkJoinPool-2-worker-17] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-28 19:33:52 [ForkJoinPool-2-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-28 19:33:52 [ForkJoinPool-2-worker-25] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-28 19:33:52 [ForkJoinPool-2-worker-31] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-28 19:33:52 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io not ready, will try again in 10000 ms (178854ms till timeout)
2022-03-28 19:34:02 [ForkJoinPool-2-worker-19] INFO  [ResourceManager:369] ############################################################################
2022-03-28 19:34:02 [ForkJoinPool-2-worker-19] DEBUG [KubeClusterResource:216] Deleting Namespace: infra-namespace
2022-03-28 19:34:02 [ForkJoinPool-2-worker-19] DEBUG [DefaultSharedIndexInformer:142] informer: ready to run resync and reflector for class io.fabric8.kubernetes.api.model.Namespace with resync 0
2022-03-28 19:34:02 [ForkJoinPool-2-worker-19] DEBUG [DefaultSharedIndexInformer:212] informer#Controller: resync skipped due to 0 full resync period class io.fabric8.kubernetes.api.model.Namespace
2022-03-28 19:34:02 [ForkJoinPool-2-worker-19] DEBUG [Reflector:95] Listing items (1) for resource class io.fabric8.kubernetes.api.model.Namespace v289664
2022-03-28 19:34:02 [ForkJoinPool-2-worker-19] DEBUG [Reflector:103] Starting watcher for resource class io.fabric8.kubernetes.api.model.Namespace v289664
2022-03-28 19:34:02 [ForkJoinPool-2-worker-19] DEBUG [AbstractWatchManager:222] Watching https://api.morsak-410.strimzi.app-services-dev.net:6443/api/v1/namespaces?fieldSelector=metadata.name%3Dinfra-namespace&resourceVersion=289664&allowWatchBookmarks=true&watch=true...
2022-03-28 19:34:03 [OkHttp https://api.morsak-410.strimzi.app-services-dev.net:6443/...] DEBUG [WatcherWebSocketListener:43] WebSocket successfully opened
2022-03-28 19:34:08 [OkHttp https://api.morsak-410.strimzi.app-services-dev.net:6443/...] DEBUG [Reflector:139] Event received MODIFIED Namespace resourceVersion 289746
2022-03-28 19:34:14 [OkHttp https://api.morsak-410.strimzi.app-services-dev.net:6443/...] DEBUG [Reflector:139] Event received MODIFIED Namespace resourceVersion 289788
2022-03-28 19:34:14 [OkHttp https://api.morsak-410.strimzi.app-services-dev.net:6443/...] DEBUG [Reflector:64] Stopping watcher for resource class io.fabric8.kubernetes.api.model.Namespace v289746 in namespace default
2022-03-28 19:34:14 [OkHttp https://api.morsak-410.strimzi.app-services-dev.net:6443/...] DEBUG [AbstractWatchManager:230] Force closing the watch io.fabric8.kubernetes.client.dsl.internal.WatchConnectionManager@3aa7c04c
2022-03-28 19:34:14 [main] INFO  [TestExecutionListener:40] =======================================================================
2022-03-28 19:34:14 [main] INFO  [TestExecutionListener:41] =======================================================================
2022-03-28 19:34:14 [OkHttp https://api.morsak-410.strimzi.app-services-dev.net:6443/...] DEBUG [Reflector:181] Watch gracefully closed
2022-03-28 19:34:14 [main] INFO  [TestExecutionListener:42]                         Test run finished
2022-03-28 19:34:14 [main] INFO  [TestExecutionListener:43] =======================================================================
2022-03-28 19:34:14 [OkHttp https://api.morsak-410.strimzi.app-services-dev.net:6443/...] DEBUG [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@3e9853c0
2022-03-28 19:34:14 [main] INFO  [TestExecutionListener:44] =======================================================================
2022-03-28 19:34:14 [OkHttp https://api.morsak-410.strimzi.app-services-dev.net:6443/...] DEBUG [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@3e9853c0
2022-03-28 19:34:14 [OkHttp https://api.morsak-410.strimzi.app-services-dev.net:6443/...] DEBUG [WatchConnectionManager:63] Websocket already closed io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@3e9853c0
2022-03-28 19:34:14 [OkHttp https://api.morsak-410.strimzi.app-services-dev.net:6443/...] DEBUG [Reflector:139] Event received DELETED Namespace resourceVersion 289789
[INFO] 
[INFO] Results:
[INFO] 
[WARNING] Flakes: 
[WARNING] io.strimzi.systemtest.operators.ReconciliationST.testPauseReconciliationInKafkaRebalanceAndTopic(ExtensionContext)
[ERROR]   Run 1: ReconciliationST.testPauseReconciliationInKafkaRebalanceAndTopic:162 ? Wait Ti...
[INFO]   Run 2: PASS
[INFO] 
[INFO] 
[WARNING] Tests run: 30, Failures: 0, Errors: 0, Skipped: 0, Flakes: 1
[INFO] 
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:verify (default) @ systemtest ---
[INFO] 
[INFO] --- maven-dependency-plugin:3.1.1:analyze-only (analyze) @ systemtest ---
2022-03-28 19:34:14 [OkHttp https://api.morsak-410.strimzi.app-services-dev.net:6443/...] DEBUG [WatcherWebSocketListener:79] WebSocket close received. code: 1000, reason: 
2022-03-28 19:34:14 [OkHttp https://api.morsak-410.strimzi.app-services-dev.net:6443/...] DEBUG [AbstractWatchManager:140] Ignoring error for already closed/closing connection
[INFO] No dependency problems found
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary for Strimzi - Apache Kafka on Kubernetes and OpenShift 0.29.0-SNAPSHOT:
[INFO] 
[INFO] Strimzi - Apache Kafka on Kubernetes and OpenShift . SUCCESS [  3.602 s]
[INFO] test ............................................... SUCCESS [  5.072 s]
[INFO] crd-annotations .................................... SUCCESS [  3.808 s]
[INFO] crd-generator ...................................... SUCCESS [  6.726 s]
[INFO] api ................................................ SUCCESS [ 29.039 s]
[INFO] mockkube ........................................... SUCCESS [  4.061 s]
[INFO] config-model ....................................... SUCCESS [  3.003 s]
[INFO] certificate-manager ................................ SUCCESS [  3.764 s]
[INFO] operator-common .................................... SUCCESS [  7.584 s]
[INFO] systemtest ......................................... SUCCESS [39:21 min]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  40:28 min
[INFO] Finished at: 2022-03-28T15:34:14-04:00
[INFO] ------------------------------------------------------------------------
