[INFO] Scanning for projects...
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Build Order:
[INFO] 
[INFO] Strimzi - Apache Kafka on Kubernetes and OpenShift                 [pom]
[INFO] test                                                               [jar]
[INFO] crd-annotations                                                    [jar]
[INFO] crd-generator                                                      [jar]
[INFO] api                                                                [jar]
[INFO] mockkube                                                           [jar]
[INFO] config-model                                                       [jar]
[INFO] certificate-manager                                                [jar]
[INFO] operator-common                                                    [jar]
[INFO] systemtest                                                         [jar]
[INFO] 
[INFO] -------------------------< io.strimzi:strimzi >-------------------------
[INFO] Building Strimzi - Apache Kafka on Kubernetes and OpenShift 0.29.0-SNAPSHOT [1/10]
[INFO] --------------------------------[ pom ]---------------------------------
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ strimzi ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ strimzi ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ strimzi ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/target/jacoco.exec
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:report (report) @ strimzi ---
[INFO] Skipping JaCoCo execution due to missing execution data file.
[INFO] 
[INFO] >>> maven-source-plugin:3.0.1:jar (attach-sources) > generate-sources @ strimzi >>>
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ strimzi ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ strimzi ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ strimzi ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/target/jacoco.exec
[INFO] 
[INFO] <<< maven-source-plugin:3.0.1:jar (attach-sources) < generate-sources @ strimzi <<<
[INFO] 
[INFO] 
[INFO] --- maven-source-plugin:3.0.1:jar (attach-sources) @ strimzi ---
[INFO] 
[INFO] --- maven-javadoc-plugin:3.1.0:jar (attach-javadocs) @ strimzi ---
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:integration-test (default) @ strimzi ---
[INFO] No tests to run.
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:verify (default) @ strimzi ---
[INFO] 
[INFO] --- maven-dependency-plugin:3.1.1:analyze-only (analyze) @ strimzi ---
[INFO] Skipping pom project
[INFO] 
[INFO] --------------------------< io.strimzi:test >---------------------------
[INFO] Building test 0.29.0-SNAPSHOT                                     [2/10]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ test ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ test ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ test ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/test/target/jacoco.exec
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ test ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/cloud-user/strimzi-kafka-operator/test/src/main/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:compile (default-compile) @ test ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ test ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/cloud-user/strimzi-kafka-operator/test/src/test/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:testCompile (default-testCompile) @ test ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:3.0.0-M5:test (default-test) @ test ---
[INFO] No tests to run.
[INFO] 
[INFO] --- maven-jar-plugin:3.1.0:jar (default-jar) @ test ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:report (report) @ test ---
[INFO] Skipping JaCoCo execution due to missing execution data file.
[INFO] 
[INFO] >>> maven-source-plugin:3.0.1:jar (attach-sources) > generate-sources @ test >>>
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ test ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ test ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ test ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/test/target/jacoco.exec
[INFO] 
[INFO] <<< maven-source-plugin:3.0.1:jar (attach-sources) < generate-sources @ test <<<
[INFO] 
[INFO] 
[INFO] --- maven-source-plugin:3.0.1:jar (attach-sources) @ test ---
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/test/target/test-0.29.0-SNAPSHOT-sources.jar
[INFO] 
[INFO] --- maven-javadoc-plugin:3.1.0:jar (attach-javadocs) @ test ---
[INFO] 
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/EmbeddedZooKeeper.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/WaitException.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/executor/ExecResult.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/executor/Exec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/k8s/cluster/OpenShift.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/k8s/cluster/KubeCluster.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/k8s/cluster/Kubernetes.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/k8s/cluster/Minikube.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/k8s/cmdClient/Kubectl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/k8s/cmdClient/Oc.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/k8s/cmdClient/BaseCmdKubeClient.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/k8s/cmdClient/KubeCmdClient.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/k8s/exceptions/KubeClusterException.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/k8s/exceptions/NoClusterException.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/k8s/HelmClient.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/k8s/KubeClient.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/k8s/KubeClusterResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/annotations/IsolatedTest.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/annotations/ParallelSuite.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/annotations/ParallelTest.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/annotations/IsolatedSuite.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/interfaces/ExtensionContextParameterResolver.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/interfaces/TestSeparator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/TestUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/logs/CollectorElement.java...
Constructing Javadoc information...
Standard Doclet version 11.0.11
Building tree for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/annotations/IsolatedSuite.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/annotations/IsolatedTest.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/annotations/ParallelSuite.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/annotations/ParallelTest.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cluster/KubeCluster.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cluster/Kubernetes.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cluster/Minikube.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cluster/OpenShift.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cmdClient/BaseCmdKubeClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cmdClient/KubeCmdClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cmdClient/Kubectl.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cmdClient/Oc.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/exceptions/KubeClusterException.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/exceptions/KubeClusterException.AlreadyExists.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/exceptions/KubeClusterException.InvalidResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/exceptions/KubeClusterException.NotFound.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/exceptions/NoClusterException.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/executor/Exec.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/executor/ExecResult.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/interfaces/ExtensionContextParameterResolver.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/interfaces/TestSeparator.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/HelmClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/KubeClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/KubeClusterResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/logs/CollectorElement.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/EmbeddedZooKeeper.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/TestUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/WaitException.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/annotations/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/annotations/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/executor/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/executor/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/interfaces/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/interfaces/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cluster/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cluster/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cmdClient/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cmdClient/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/exceptions/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/exceptions/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/logs/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/logs/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/constant-values.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/serialized-form.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/class-use/EmbeddedZooKeeper.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/class-use/WaitException.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/executor/class-use/ExecResult.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/executor/class-use/Exec.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cluster/class-use/OpenShift.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cluster/class-use/KubeCluster.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cluster/class-use/Kubernetes.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cluster/class-use/Minikube.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cmdClient/class-use/Kubectl.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cmdClient/class-use/Oc.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cmdClient/class-use/BaseCmdKubeClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cmdClient/class-use/KubeCmdClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/exceptions/class-use/KubeClusterException.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/exceptions/class-use/KubeClusterException.InvalidResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/exceptions/class-use/KubeClusterException.AlreadyExists.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/exceptions/class-use/KubeClusterException.NotFound.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/exceptions/class-use/NoClusterException.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/class-use/HelmClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/class-use/KubeClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/class-use/KubeClusterResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/annotations/class-use/IsolatedTest.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/annotations/class-use/ParallelSuite.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/annotations/class-use/ParallelTest.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/annotations/class-use/IsolatedSuite.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/interfaces/class-use/ExtensionContextParameterResolver.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/interfaces/class-use/TestSeparator.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/class-use/TestUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/logs/class-use/CollectorElement.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/annotations/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/executor/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/interfaces/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cluster/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cmdClient/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/exceptions/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/logs/package-use.html...
Building index for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/overview-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/index-all.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/allclasses-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/allpackages-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/deprecated-list.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/index.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/overview-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/help-doc.html...
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/test/target/test-0.29.0-SNAPSHOT-javadoc.jar
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:integration-test (default) @ test ---
[INFO] No tests to run.
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:verify (default) @ test ---
[INFO] 
[INFO] --- maven-dependency-plugin:3.1.1:analyze-only (analyze) @ test ---
[INFO] No dependency problems found
[INFO] 
[INFO] ---------------------< io.strimzi:crd-annotations >---------------------
[INFO] Building crd-annotations 0.29.0-SNAPSHOT                          [3/10]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ crd-annotations ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ crd-annotations ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ crd-annotations ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/crd-annotations/target/jacoco.exec
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ crd-annotations ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/cloud-user/strimzi-kafka-operator/crd-annotations/src/main/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:compile (default-compile) @ crd-annotations ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ crd-annotations ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/cloud-user/strimzi-kafka-operator/crd-annotations/src/test/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:testCompile (default-testCompile) @ crd-annotations ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:3.0.0-M5:test (default-test) @ crd-annotations ---
[INFO] 
[INFO] -------------------------------------------------------
[INFO]  T E S T S
[INFO] -------------------------------------------------------
[INFO] 
[INFO] Results:
[INFO] 
[INFO] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[INFO] 
[INFO] 
[INFO] --- maven-jar-plugin:3.1.0:jar (default-jar) @ crd-annotations ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:report (report) @ crd-annotations ---
[INFO] Skipping JaCoCo execution due to missing execution data file.
[INFO] 
[INFO] >>> maven-source-plugin:3.0.1:jar (attach-sources) > generate-sources @ crd-annotations >>>
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ crd-annotations ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ crd-annotations ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ crd-annotations ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/crd-annotations/target/jacoco.exec
[INFO] 
[INFO] <<< maven-source-plugin:3.0.1:jar (attach-sources) < generate-sources @ crd-annotations <<<
[INFO] 
[INFO] 
[INFO] --- maven-source-plugin:3.0.1:jar (attach-sources) @ crd-annotations ---
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/crd-annotations-0.29.0-SNAPSHOT-sources.jar
[INFO] 
[INFO] --- maven-javadoc-plugin:3.1.0:jar (attach-javadocs) @ crd-annotations ---
[INFO] 
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-annotations/src/main/java/io/strimzi/api/annotations/DeprecatedProperty.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-annotations/src/main/java/io/strimzi/api/annotations/ApiVersion.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-annotations/src/main/java/io/strimzi/api/annotations/DeprecatedType.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-annotations/src/main/java/io/strimzi/api/annotations/KubeVersion.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-annotations/src/main/java/io/strimzi/api/annotations/VersionRange.java...
Constructing Javadoc information...
Standard Doclet version 11.0.11
Building tree for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/io/strimzi/api/annotations/ApiVersion.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/io/strimzi/api/annotations/DeprecatedProperty.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/io/strimzi/api/annotations/DeprecatedType.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/io/strimzi/api/annotations/KubeVersion.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/io/strimzi/api/annotations/VersionRange.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/io/strimzi/api/annotations/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/io/strimzi/api/annotations/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/constant-values.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/io/strimzi/api/annotations/class-use/DeprecatedProperty.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/io/strimzi/api/annotations/class-use/ApiVersion.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/io/strimzi/api/annotations/class-use/DeprecatedType.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/io/strimzi/api/annotations/class-use/KubeVersion.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/io/strimzi/api/annotations/class-use/VersionRange.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/io/strimzi/api/annotations/package-use.html...
Building index for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/overview-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/index-all.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/allclasses-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/allpackages-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/deprecated-list.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/index.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/help-doc.html...
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/crd-annotations-0.29.0-SNAPSHOT-javadoc.jar
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:integration-test (default) @ crd-annotations ---
[WARNING] useSystemClassLoader setting has no effect when not forking
[WARNING] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:verify (default) @ crd-annotations ---
[INFO] 
[INFO] --- maven-dependency-plugin:3.1.1:analyze-only (analyze) @ crd-annotations ---
[INFO] No dependency problems found
[INFO] 
[INFO] ----------------------< io.strimzi:crd-generator >----------------------
[INFO] Building crd-generator 0.29.0-SNAPSHOT                            [4/10]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ crd-generator ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ crd-generator ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ crd-generator ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/crd-generator/target/jacoco.exec
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ crd-generator ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:compile (default-compile) @ crd-generator ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ crd-generator ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 7 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:testCompile (default-testCompile) @ crd-generator ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:3.0.0-M5:test (default-test) @ crd-generator ---
[INFO] 
[INFO] -------------------------------------------------------
[INFO]  T E S T S
[INFO] -------------------------------------------------------
[INFO] 
[INFO] Results:
[INFO] 
[INFO] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[INFO] 
[INFO] 
[INFO] --- maven-jar-plugin:3.1.0:jar (default-jar) @ crd-generator ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:report (report) @ crd-generator ---
[INFO] Skipping JaCoCo execution due to missing execution data file.
[INFO] 
[INFO] >>> maven-source-plugin:3.0.1:jar (attach-sources) > generate-sources @ crd-generator >>>
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ crd-generator ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ crd-generator ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ crd-generator ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/crd-generator/target/jacoco.exec
[INFO] 
[INFO] <<< maven-source-plugin:3.0.1:jar (attach-sources) < generate-sources @ crd-generator <<<
[INFO] 
[INFO] 
[INFO] --- maven-source-plugin:3.0.1:jar (attach-sources) @ crd-generator ---
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/crd-generator/target/crd-generator-0.29.0-SNAPSHOT-sources.jar
[INFO] 
[INFO] --- maven-javadoc-plugin:3.1.0:jar (attach-javadocs) @ crd-generator ---
[INFO] 
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/KubeLinker.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/Linker.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/OpenShiftOriginLinker.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/PropertyType.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/Schema.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/annotations/Alternation.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/annotations/DescriptionFile.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/annotations/Example.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/annotations/KubeLink.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/annotations/OneOf.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/annotations/Type.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/annotations/Alternative.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/annotations/Description.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/annotations/Maximum.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/annotations/Minimum.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/annotations/MinimumItems.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/annotations/PresentInVersions.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/annotations/Crd.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/annotations/Pattern.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/Property.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/CrdGenerator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/DocGenerator.java...
Constructing Javadoc information...
Standard Doclet version 11.0.11
Building tree for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Alternation.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Alternative.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Crd.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Crd.Spec.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Crd.Spec.AdditionalPrinterColumn.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Crd.Spec.Names.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Crd.Spec.Subresources.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Crd.Spec.Subresources.Scale.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Crd.Spec.Subresources.Status.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Crd.Spec.Version.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Description.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Description.List.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/DescriptionFile.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Example.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/KubeLink.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Maximum.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Maximum.List.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Minimum.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Minimum.List.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/MinimumItems.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/MinimumItems.List.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/OneOf.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/OneOf.Alternative.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/OneOf.Alternative.Property.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Pattern.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Pattern.List.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/PresentInVersions.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Type.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/CrdGenerator.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/CrdGenerator.ConversionStrategy.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/CrdGenerator.DefaultReporter.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/CrdGenerator.NoneConversionStrategy.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/CrdGenerator.Reporter.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/CrdGenerator.WebhookConversionStrategy.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/DocGenerator.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/KubeLinker.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/Linker.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/OpenShiftOriginLinker.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/PropertyType.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/constant-values.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/class-use/KubeLinker.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/class-use/Linker.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/class-use/OpenShiftOriginLinker.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/class-use/PropertyType.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Alternation.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/DescriptionFile.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Example.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/KubeLink.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/OneOf.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/OneOf.Alternative.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/OneOf.Alternative.Property.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Type.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Alternative.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Description.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Description.List.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Maximum.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Maximum.List.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Minimum.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Minimum.List.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/MinimumItems.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/MinimumItems.List.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/PresentInVersions.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Crd.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Crd.Spec.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Crd.Spec.AdditionalPrinterColumn.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Crd.Spec.Subresources.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Crd.Spec.Subresources.Scale.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Crd.Spec.Subresources.Status.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Crd.Spec.Version.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Crd.Spec.Names.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Pattern.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Pattern.List.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/class-use/CrdGenerator.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/class-use/CrdGenerator.WebhookConversionStrategy.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/class-use/CrdGenerator.NoneConversionStrategy.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/class-use/CrdGenerator.ConversionStrategy.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/class-use/CrdGenerator.DefaultReporter.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/class-use/CrdGenerator.Reporter.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/class-use/DocGenerator.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/package-use.html...
Building index for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/overview-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/index-all.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/allclasses-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/allpackages-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/deprecated-list.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/index.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/overview-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/help-doc.html...
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/crd-generator/target/crd-generator-0.29.0-SNAPSHOT-javadoc.jar
[INFO] 
[INFO] --- maven-shade-plugin:3.1.0:shade (default) @ crd-generator ---
[INFO] Including io.strimzi:crd-annotations:jar:0.29.0-SNAPSHOT in the shaded jar.
[INFO] Including com.fasterxml.jackson.core:jackson-core:jar:2.12.6 in the shaded jar.
[INFO] Including com.fasterxml.jackson.core:jackson-databind:jar:2.12.6 in the shaded jar.
[INFO] Including com.fasterxml.jackson.dataformat:jackson-dataformat-yaml:jar:2.12.6 in the shaded jar.
[INFO] Including org.yaml:snakeyaml:jar:1.27 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-client:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-rbac:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-admissionregistration:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-apps:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-autoscaling:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-apiextensions:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-batch:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-certificates:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-coordination:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-discovery:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-events:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-extensions:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-flowcontrol:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-networking:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-metrics:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-policy:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-scheduling:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-storageclass:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-node:jar:5.12.0 in the shaded jar.
[INFO] Including com.squareup.okhttp3:okhttp:jar:3.12.12 in the shaded jar.
[INFO] Including com.squareup.okio:okio:jar:1.15.0 in the shaded jar.
[INFO] Including com.squareup.okhttp3:logging-interceptor:jar:3.12.12 in the shaded jar.
[INFO] Including org.slf4j:slf4j-api:jar:1.7.36 in the shaded jar.
[INFO] Including com.fasterxml.jackson.datatype:jackson-datatype-jsr310:jar:2.13.1 in the shaded jar.
[INFO] Including io.fabric8:zjsonpatch:jar:0.3.0 in the shaded jar.
[INFO] Including com.github.mifmif:generex:jar:1.0.2 in the shaded jar.
[INFO] Including dk.brics.automaton:automaton:jar:1.11-8 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-core:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-common:jar:5.12.0 in the shaded jar.
[INFO] Including com.fasterxml.jackson.core:jackson-annotations:jar:2.12.6 in the shaded jar.
[WARNING] Discovered module-info.class. Shading will break its strong encapsulation.
[WARNING] Discovered module-info.class. Shading will break its strong encapsulation.
[WARNING] Discovered module-info.class. Shading will break its strong encapsulation.
[WARNING] Discovered module-info.class. Shading will break its strong encapsulation.
[WARNING] kubernetes-model-coordination-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 18 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseListFluentImpl$ItemsNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseFluentImpl$SpecNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseSpec
[WARNING]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseFluent$SpecNested
[WARNING]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseListFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseSpecBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseSpecFluent
[WARNING]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseFluentImpl$MetadataNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseFluent
[WARNING]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseListFluent
[WARNING]   - 8 more...
[WARNING] logging-interceptor-3.12.12.jar, crd-generator-0.29.0-SNAPSHOT.jar define 8 overlapping classes: 
[WARNING]   - okhttp3.logging.HttpLoggingInterceptor$Logger$1
[WARNING]   - okhttp3.logging.LoggingEventListener$Factory
[WARNING]   - okhttp3.logging.HttpLoggingInterceptor$Level
[WARNING]   - okhttp3.logging.HttpLoggingInterceptor
[WARNING]   - okhttp3.logging.package-info
[WARNING]   - okhttp3.logging.LoggingEventListener
[WARNING]   - okhttp3.logging.LoggingEventListener$1
[WARNING]   - okhttp3.logging.HttpLoggingInterceptor$Logger
[WARNING] kubernetes-client-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 536 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.client.internal.CertUtils
[WARNING]   - io.fabric8.kubernetes.client.CustomResource
[WARNING]   - io.fabric8.kubernetes.client.osgi.ManagedKubernetesClient
[WARNING]   - io.fabric8.kubernetes.client.V1beta1ApiextensionAPIGroupDSL
[WARNING]   - io.fabric8.kubernetes.client.internal.PatchUtils$SingletonHolder
[WARNING]   - io.fabric8.kubernetes.client.VersionInfo$1
[WARNING]   - io.fabric8.kubernetes.client.utils.ReplaceValueStream
[WARNING]   - io.fabric8.kubernetes.client.dsl.CreateFromServerGettable
[WARNING]   - io.fabric8.kubernetes.client.dsl.ApiextensionsAPIGroupDSL
[WARNING]   - io.fabric8.kubernetes.client.dsl.Containerable
[WARNING]   - 526 more...
[WARNING] kubernetes-model-events-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 44 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventFluent$MetadataNested
[WARNING]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventFluent$SeriesNested
[WARNING]   - io.fabric8.kubernetes.api.model.events.v1.EventList
[WARNING]   - io.fabric8.kubernetes.api.model.events.v1.EventListBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventFluent$RegardingNested
[WARNING]   - io.fabric8.kubernetes.api.model.events.v1.EventSeriesFluent
[WARNING]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventFluent
[WARNING]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventListFluent$ItemsNested
[WARNING]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventListFluentImpl$ItemsNestedImpl
[WARNING]   - 34 more...
[WARNING] crd-generator-0.29.0-SNAPSHOT.jar, automaton-1.11-8.jar define 25 overlapping classes: 
[WARNING]   - dk.brics.automaton.AutomatonMatcher
[WARNING]   - dk.brics.automaton.ShuffleOperations$ShuffleConfiguration
[WARNING]   - dk.brics.automaton.RegExp$Kind
[WARNING]   - dk.brics.automaton.RunAutomaton
[WARNING]   - dk.brics.automaton.Automaton
[WARNING]   - dk.brics.automaton.RegExp
[WARNING]   - dk.brics.automaton.AutomatonProvider
[WARNING]   - dk.brics.automaton.RegExp$1
[WARNING]   - dk.brics.automaton.MinimizationOperations$StateListNode
[WARNING]   - dk.brics.automaton.State
[WARNING]   - 15 more...
[WARNING] kubernetes-model-metrics-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 30 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.PodMetricsFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.NodeMetricsListFluentImpl$ItemsNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.ContainerMetricsFluent
[WARNING]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.NodeMetrics
[WARNING]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.NodeMetricsListFluent
[WARNING]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.NodeMetricsListFluent$ItemsNested
[WARNING]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.NodeMetricsFluent$MetadataNested
[WARNING]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.PodMetricsFluentImpl$ContainersNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.PodMetricsListFluentImpl$ItemsNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.PodMetricsListBuilder
[WARNING]   - 20 more...
[WARNING] kubernetes-model-networking-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 234 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.networking.v1beta1.IngressClassListFluent$ItemsNested
[WARNING]   - io.fabric8.kubernetes.api.model.networking.v1beta1.IngressClassListFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.networking.v1.IngressServiceBackend
[WARNING]   - io.fabric8.kubernetes.api.model.networking.v1.NetworkPolicyPort
[WARNING]   - io.fabric8.kubernetes.api.model.networking.v1beta1.IngressFluentImpl$StatusNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.networking.v1beta1.IngressClassSpec
[WARNING]   - io.fabric8.kubernetes.api.model.networking.v1.IngressStatus
[WARNING]   - io.fabric8.kubernetes.api.model.networking.v1.IngressClassFluent$SpecNested
[WARNING]   - io.fabric8.kubernetes.api.model.networking.v1.NetworkPolicyBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.networking.v1.IngressRuleFluentImpl$HttpNestedImpl
[WARNING]   - 224 more...
[WARNING] kubernetes-model-apiextensions-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 350 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.CustomResourceDefinitionVersionBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.JSONSchemaPropsOrBoolBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.CustomResourceDefinitionSpecFluent$ValidationNested
[WARNING]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.CustomResourceDefinitionVersionFluentImpl$SchemaNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.JSONSchemaPropsOrStringArraySerDe$Deserializer$1
[WARNING]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.CustomResourceValidationFluentImpl$OpenAPIV3SchemaNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.JSONSchemaPropsFluentImpl$NotNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.WebhookClientConfigFluentImpl$ServiceNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.JSONSchemaPropsOrArrayFluent$SchemaNested
[WARNING]   - io.fabric8.kubernetes.api.model.apiextensions.v1.JSONSchemaPropsOrBoolSerDe
[WARNING]   - 340 more...
[WARNING] generex-1.0.2.jar, crd-generator-0.29.0-SNAPSHOT.jar define 7 overlapping classes: 
[WARNING]   - com.mifmif.common.regex.GenerexIterator
[WARNING]   - com.mifmif.common.regex.Generex
[WARNING]   - com.mifmif.common.regex.GenerexIterator$Step
[WARNING]   - com.mifmif.common.regex.Node
[WARNING]   - com.mifmif.common.regex.Main
[WARNING]   - com.mifmif.common.regex.util.Iterable
[WARNING]   - com.mifmif.common.regex.util.Iterator
[WARNING] kubernetes-model-autoscaling-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 350 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta1.MetricSpecFluentImpl$ObjectNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta2.HorizontalPodAutoscalerSpec
[WARNING]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta1.CrossVersionObjectReferenceBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta1.HorizontalPodAutoscalerStatusFluent
[WARNING]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta2.HorizontalPodAutoscalerFluent$SpecNested
[WARNING]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta2.ContainerResourceMetricStatusFluent
[WARNING]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta1.HorizontalPodAutoscalerStatus
[WARNING]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta1.MetricStatusFluent$ObjectNested
[WARNING]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta2.HorizontalPodAutoscalerSpecFluent$ScaleTargetRefNested
[WARNING]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta1.HorizontalPodAutoscalerFluent$SpecNested
[WARNING]   - 340 more...
[WARNING] crd-generator-0.29.0-SNAPSHOT.jar, zjsonpatch-0.3.0.jar define 24 overlapping classes: 
[WARNING]   - io.fabric8.zjsonpatch.internal.collections4.sequence.InsertCommand
[WARNING]   - io.fabric8.zjsonpatch.Operation
[WARNING]   - io.fabric8.zjsonpatch.internal.collections4.sequence.CommandVisitor
[WARNING]   - io.fabric8.zjsonpatch.internal.guava.Strings
[WARNING]   - io.fabric8.zjsonpatch.internal.collections4.sequence.EditCommand
[WARNING]   - io.fabric8.zjsonpatch.JsonDiff$EncodePathFunction
[WARNING]   - io.fabric8.zjsonpatch.internal.collections4.sequence.SequencesComparator
[WARNING]   - io.fabric8.zjsonpatch.Diff
[WARNING]   - io.fabric8.zjsonpatch.internal.collections4.ListUtils
[WARNING]   - io.fabric8.zjsonpatch.JsonPatch
[WARNING]   - 14 more...
[WARNING] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-admissionregistration-5.12.0.jar define 362 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.admissionregistration.v1beta1.MutatingWebhookFluent$ObjectSelectorNested
[WARNING]   - io.fabric8.kubernetes.api.model.authorization.v1.SubjectAccessReviewSpecFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.authorization.v1beta1.SubjectRulesReviewStatusBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.admissionregistration.v1.ValidatingWebhookConfigurationBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.authentication.TokenReviewFluentImpl$MetadataNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.authorization.v1beta1.SelfSubjectRulesReviewSpec
[WARNING]   - io.fabric8.kubernetes.api.model.admissionregistration.v1beta1.MutatingWebhookFluentImpl$NamespaceSelectorNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.admissionregistration.v1beta1.MutatingWebhookConfigurationFluentImpl$WebhooksNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.authorization.v1beta1.SelfSubjectAccessReviewFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.admissionregistration.v1.MutatingWebhookFluent$ClientConfigNested
[WARNING]   - 352 more...
[WARNING] kubernetes-model-rbac-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 80 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.rbac.SubjectBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.rbac.ClusterRoleBindingFluent
[WARNING]   - io.fabric8.kubernetes.api.model.rbac.RoleListFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.rbac.ClusterRoleBindingFluent$MetadataNested
[WARNING]   - io.fabric8.kubernetes.api.model.rbac.RoleBindingBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.rbac.ClusterRoleBindingBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.rbac.AggregationRuleFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.rbac.SubjectFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.rbac.ClusterRoleListFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.rbac.PolicyRuleFluent
[WARNING]   - 70 more...
[WARNING] kubernetes-model-certificates-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 60 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.certificates.v1.CertificateSigningRequestStatusFluent$ConditionsNested
[WARNING]   - io.fabric8.kubernetes.api.model.certificates.v1.CertificateSigningRequestFluent$SpecNested
[WARNING]   - io.fabric8.kubernetes.api.model.certificates.v1.CertificateSigningRequestFluentImpl$StatusNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.certificates.v1beta1.CertificateSigningRequestListFluent$ItemsNested
[WARNING]   - io.fabric8.kubernetes.api.model.certificates.v1beta1.CertificateSigningRequestStatusFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.certificates.v1beta1.CertificateSigningRequestFluent$MetadataNested
[WARNING]   - io.fabric8.kubernetes.api.model.certificates.v1beta1.CertificateSigningRequestConditionFluent
[WARNING]   - io.fabric8.kubernetes.api.model.certificates.v1.CertificateSigningRequestStatusBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.certificates.v1beta1.CertificateSigningRequestStatusFluent$ConditionsNested
[WARNING]   - io.fabric8.kubernetes.api.model.certificates.v1.CertificateSigningRequestFluentImpl
[WARNING]   - 50 more...
[WARNING] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-policy-5.12.0.jar define 162 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.policy.v1.PodDisruptionBudgetList
[WARNING]   - io.fabric8.kubernetes.api.model.policy.v1beta1.HostPortRangeBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.policy.v1beta1.EvictionFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.policy.v1beta1.PodSecurityPolicyFluent
[WARNING]   - io.fabric8.kubernetes.api.model.policy.v1beta1.PodSecurityPolicySpecFluentImpl$AllowedCSIDriversNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.policy.v1beta1.PodSecurityPolicyListFluentImpl$ItemsNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.policy.v1beta1.AllowedFlexVolumeBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.policy.v1beta1.IDRangeFluent
[WARNING]   - io.fabric8.kubernetes.api.model.policy.v1beta1.SELinuxStrategyOptionsFluent
[WARNING]   - io.fabric8.kubernetes.api.model.policy.v1beta1.PodSecurityPolicySpecFluentImpl$FsGroupNestedImpl
[WARNING]   - 152 more...
[WARNING] jackson-datatype-jsr310-2.13.1.jar, crd-generator-0.29.0-SNAPSHOT.jar define 59 overlapping classes: 
[WARNING]   - com.fasterxml.jackson.datatype.jsr310.deser.LocalDateDeserializer
[WARNING]   - com.fasterxml.jackson.datatype.jsr310.deser.key.Jsr310KeyDeserializer
[WARNING]   - com.fasterxml.jackson.datatype.jsr310.PackageVersion
[WARNING]   - com.fasterxml.jackson.datatype.jsr310.deser.YearDeserializer
[WARNING]   - com.fasterxml.jackson.datatype.jsr310.ser.key.Jsr310NullKeySerializer
[WARNING]   - com.fasterxml.jackson.datatype.jsr310.deser.key.LocalDateTimeKeyDeserializer
[WARNING]   - com.fasterxml.jackson.datatype.jsr310.util.DurationUnitConverter
[WARNING]   - com.fasterxml.jackson.datatype.jsr310.ser.InstantSerializerBase
[WARNING]   - com.fasterxml.jackson.datatype.jsr310.ser.LocalDateTimeSerializer
[WARNING]   - com.fasterxml.jackson.datatype.jsr310.ser.OffsetDateTimeSerializer
[WARNING]   - 49 more...
[WARNING] kubernetes-model-flowcontrol-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 132 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.PriorityLevelConfigurationFluentImpl$SpecNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.FlowSchemaConditionFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.FlowDistinguisherMethodBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.PriorityLevelConfigurationReferenceBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.QueuingConfiguration
[WARNING]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.QueuingConfigurationFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.PriorityLevelConfiguration
[WARNING]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.PriorityLevelConfigurationReference
[WARNING]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.PolicyRulesWithSubjects
[WARNING]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.PriorityLevelConfigurationListFluent$ItemsNested
[WARNING]   - 122 more...
[WARNING] crd-generator-0.29.0-SNAPSHOT.jar, crd-annotations-0.29.0-SNAPSHOT.jar define 8 overlapping classes: 
[WARNING]   - io.strimzi.api.annotations.VersionRange
[WARNING]   - io.strimzi.api.annotations.ApiVersion
[WARNING]   - io.strimzi.api.annotations.ApiVersion$Stability
[WARNING]   - io.strimzi.api.annotations.ApiVersion$1
[WARNING]   - io.strimzi.api.annotations.DeprecatedType
[WARNING]   - io.strimzi.api.annotations.DeprecatedProperty
[WARNING]   - io.strimzi.api.annotations.VersionRange$VersionParser
[WARNING]   - io.strimzi.api.annotations.KubeVersion
[WARNING] jackson-core-2.12.6.jar, crd-generator-0.29.0-SNAPSHOT.jar define 124 overlapping classes: 
[WARNING]   - com.fasterxml.jackson.core.JsonGenerator$Feature
[WARNING]   - com.fasterxml.jackson.core.json.JsonReadFeature
[WARNING]   - com.fasterxml.jackson.core.util.ThreadLocalBufferManager$ThreadLocalBufferManagerHolder
[WARNING]   - com.fasterxml.jackson.core.util.Separators
[WARNING]   - com.fasterxml.jackson.core.io.SegmentedStringWriter
[WARNING]   - com.fasterxml.jackson.core.TreeNode
[WARNING]   - com.fasterxml.jackson.core.sym.Name
[WARNING]   - com.fasterxml.jackson.core.util.RequestPayload
[WARNING]   - com.fasterxml.jackson.core.util.JsonGeneratorDelegate
[WARNING]   - com.fasterxml.jackson.core.async.NonBlockingInputFeeder
[WARNING]   - 114 more...
[WARNING] crd-generator-0.29.0-SNAPSHOT.jar, okio-1.15.0.jar define 44 overlapping classes: 
[WARNING]   - okio.ByteString
[WARNING]   - okio.Source
[WARNING]   - okio.ForwardingSink
[WARNING]   - okio.BufferedSource
[WARNING]   - okio.Util
[WARNING]   - okio.AsyncTimeout$1
[WARNING]   - okio.HashingSource
[WARNING]   - okio.GzipSink
[WARNING]   - okio.Okio$1
[WARNING]   - okio.Pipe$PipeSink
[WARNING]   - 34 more...
[WARNING] crd-generator-0.29.0-SNAPSHOT.jar, jackson-databind-2.12.6.jar define 700 overlapping classes: 
[WARNING]   - com.fasterxml.jackson.databind.introspect.AnnotationCollector$NoAnnotations
[WARNING]   - com.fasterxml.jackson.databind.jsontype.BasicPolymorphicTypeValidator$Builder
[WARNING]   - com.fasterxml.jackson.databind.BeanDescription
[WARNING]   - com.fasterxml.jackson.databind.deser.impl.BeanAsArrayBuilderDeserializer
[WARNING]   - com.fasterxml.jackson.databind.introspect.AnnotatedMethodMap
[WARNING]   - com.fasterxml.jackson.databind.SerializerProvider
[WARNING]   - com.fasterxml.jackson.databind.introspect.AnnotationCollector$OneAnnotation
[WARNING]   - com.fasterxml.jackson.databind.ser.std.StaticListSerializerBase
[WARNING]   - com.fasterxml.jackson.databind.ser.std.NumberSerializers$ShortSerializer
[WARNING]   - com.fasterxml.jackson.databind.ser.BeanSerializerFactory
[WARNING]   - 690 more...
[WARNING] kubernetes-model-discovery-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 88 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.EndpointSliceFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.EndpointFluent
[WARNING]   - io.fabric8.kubernetes.api.model.discovery.v1.EndpointSliceBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.EndpointPort
[WARNING]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.ForZoneBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.discovery.v1.EndpointFluent$TargetRefNested
[WARNING]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.EndpointFluentImpl$ConditionsNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.discovery.v1.EndpointSliceListFluentImpl$ItemsNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.EndpointConditionsFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.EndpointSliceListFluentImpl$ItemsNestedImpl
[WARNING]   - 78 more...
[WARNING] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-extensions-5.12.0.jar define 264 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.extensions.DaemonSetConditionBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.extensions.NetworkPolicyListFluent$ItemsNested
[WARNING]   - io.fabric8.kubernetes.api.model.extensions.DeploymentStrategyFluent
[WARNING]   - io.fabric8.kubernetes.api.model.extensions.IngressListFluentImpl$ItemsNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.extensions.NetworkPolicySpecFluent$IngressNested
[WARNING]   - io.fabric8.kubernetes.api.model.extensions.IngressStatus
[WARNING]   - io.fabric8.kubernetes.api.model.extensions.DaemonSetFluentImpl$SpecNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.extensions.IngressSpecFluent$RulesNested
[WARNING]   - io.fabric8.kubernetes.api.model.extensions.DaemonSetSpecFluent$UpdateStrategyNested
[WARNING]   - io.fabric8.kubernetes.api.model.extensions.NetworkPolicyPeerBuilder
[WARNING]   - 254 more...
[WARNING] snakeyaml-1.27.jar, crd-generator-0.29.0-SNAPSHOT.jar define 216 overlapping classes: 
[WARNING]   - org.yaml.snakeyaml.emitter.Emitter$ExpectBlockMappingValue
[WARNING]   - org.yaml.snakeyaml.parser.ParserImpl$ParseBlockNode
[WARNING]   - org.yaml.snakeyaml.emitter.Emitter$ExpectBlockMappingSimpleValue
[WARNING]   - org.yaml.snakeyaml.emitter.Emitter$ExpectDocumentEnd
[WARNING]   - org.yaml.snakeyaml.Yaml$3
[WARNING]   - org.yaml.snakeyaml.emitter.Emitter$ExpectBlockSequenceItem
[WARNING]   - org.yaml.snakeyaml.parser.ParserImpl$ParseBlockSequenceEntry
[WARNING]   - org.yaml.snakeyaml.util.ArrayUtils
[WARNING]   - org.yaml.snakeyaml.tokens.Token$ID
[WARNING]   - org.yaml.snakeyaml.reader.StreamReader
[WARNING]   - 206 more...
[WARNING] kubernetes-model-node-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 78 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.node.v1beta1.OverheadBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.node.v1.RuntimeClassListFluent$ItemsNested
[WARNING]   - io.fabric8.kubernetes.api.model.node.v1alpha1.Scheduling
[WARNING]   - io.fabric8.kubernetes.api.model.node.v1beta1.RuntimeClassFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.node.v1beta1.RuntimeClassListBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.node.v1.RuntimeClassFluent
[WARNING]   - io.fabric8.kubernetes.api.model.node.v1alpha1.SchedulingFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.node.v1alpha1.RuntimeClassSpecFluent$OverheadNested
[WARNING]   - io.fabric8.kubernetes.api.model.node.v1.RuntimeClassBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.node.v1.RuntimeClassFluentImpl
[WARNING]   - 68 more...
[WARNING] kubernetes-model-core-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 2394 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.BaseKubernetesListFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.StatusBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.KubeSchemaFluentImpl$APIResourceNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.NodeListBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.ResourceQuotaListFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.WatchEventFluentImpl$APIServiceStatusObjectNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.WatchEventFluent$VsphereVirtualDiskVolumeSourceObjectNested
[WARNING]   - io.fabric8.kubernetes.api.model.ProbeFluentImpl$HttpGetNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.PatchOptionsFluent
[WARNING]   - io.fabric8.kubernetes.api.model.ServerAddressByClientCIDRFluentImpl
[WARNING]   - 2384 more...
[WARNING] crd-generator-0.29.0-SNAPSHOT.jar, jackson-annotations-2.12.6.jar define 71 overlapping classes: 
[WARNING]   - com.fasterxml.jackson.annotation.JsonAutoDetect
[WARNING]   - com.fasterxml.jackson.annotation.JsonInclude
[WARNING]   - com.fasterxml.jackson.annotation.ObjectIdGenerators
[WARNING]   - com.fasterxml.jackson.annotation.JsonFormat$Features
[WARNING]   - com.fasterxml.jackson.annotation.JsonFormat$Feature
[WARNING]   - com.fasterxml.jackson.annotation.JsonIgnore
[WARNING]   - com.fasterxml.jackson.annotation.JsonSetter
[WARNING]   - com.fasterxml.jackson.annotation.JsonTypeInfo$None
[WARNING]   - com.fasterxml.jackson.annotation.JsonFormat$Shape
[WARNING]   - com.fasterxml.jackson.annotation.JsonSubTypes
[WARNING]   - 61 more...
[WARNING] crd-generator-0.29.0-SNAPSHOT.jar, slf4j-api-1.7.36.jar define 34 overlapping classes: 
[WARNING]   - org.slf4j.helpers.SubstituteLogger
[WARNING]   - org.slf4j.helpers.NamedLoggerBase
[WARNING]   - org.slf4j.helpers.NOPMDCAdapter
[WARNING]   - org.slf4j.MarkerFactory
[WARNING]   - org.slf4j.helpers.BasicMarker
[WARNING]   - org.slf4j.spi.LoggerFactoryBinder
[WARNING]   - org.slf4j.MDC$MDCCloseable
[WARNING]   - org.slf4j.spi.LocationAwareLogger
[WARNING]   - org.slf4j.helpers.MessageFormatter
[WARNING]   - org.slf4j.helpers.Util$ClassContextSecurityManager
[WARNING]   - 24 more...
[WARNING] jackson-dataformat-yaml-2.12.6.jar, crd-generator-0.29.0-SNAPSHOT.jar define 17 overlapping classes: 
[WARNING]   - com.fasterxml.jackson.dataformat.yaml.YAMLMapper$Builder
[WARNING]   - com.fasterxml.jackson.dataformat.yaml.snakeyaml.error.Mark
[WARNING]   - com.fasterxml.jackson.dataformat.yaml.YAMLGenerator
[WARNING]   - com.fasterxml.jackson.dataformat.yaml.UTF8Reader
[WARNING]   - com.fasterxml.jackson.dataformat.yaml.YAMLParser
[WARNING]   - com.fasterxml.jackson.dataformat.yaml.util.StringQuotingChecker
[WARNING]   - com.fasterxml.jackson.dataformat.yaml.YAMLGenerator$Feature
[WARNING]   - com.fasterxml.jackson.dataformat.yaml.JacksonYAMLParseException
[WARNING]   - com.fasterxml.jackson.dataformat.yaml.YAMLParser$Feature
[WARNING]   - com.fasterxml.jackson.dataformat.yaml.util.StringQuotingChecker$Default
[WARNING]   - 7 more...
[WARNING] kubernetes-model-apps-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 212 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.apps.ReplicaSetSpec
[WARNING]   - io.fabric8.kubernetes.api.model.apps.StatefulSetConditionFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.apps.DeploymentStrategyFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.apps.ControllerRevisionFluent$DeploymentDataNested
[WARNING]   - io.fabric8.kubernetes.api.model.apps.ReplicaSetSpecFluent
[WARNING]   - io.fabric8.kubernetes.api.model.apps.DeploymentFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.apps.ReplicaSetStatusFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.apps.ControllerRevisionFluentImpl$PersistentVolumeClaimDataNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.apps.ReplicaSetCondition
[WARNING]   - io.fabric8.kubernetes.api.model.apps.StatefulSetSpecFluent$UpdateStrategyNested
[WARNING]   - 202 more...
[WARNING] kubernetes-model-scheduling-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 24 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClassFluent
[WARNING]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClassFluentImpl$MetadataNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClassListFluent$ItemsNested
[WARNING]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClassList
[WARNING]   - io.fabric8.kubernetes.api.model.scheduling.v1beta1.PriorityClassListFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClassListFluent
[WARNING]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClass
[WARNING]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClassListFluentImpl$ItemsNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.scheduling.v1beta1.PriorityClassBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.scheduling.v1beta1.PriorityClassFluent$MetadataNested
[WARNING]   - 14 more...
[WARNING] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-common-5.12.0.jar define 16 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.model.annotation.Plural
[WARNING]   - io.fabric8.kubernetes.model.annotation.Group
[WARNING]   - io.fabric8.kubernetes.model.jackson.JsonUnwrappedDeserializer$CancelUnwrapped
[WARNING]   - io.fabric8.kubernetes.model.jackson.JsonUnwrappedDeserializer
[WARNING]   - io.fabric8.kubernetes.model.annotation.PrinterColumn
[WARNING]   - io.fabric8.kubernetes.model.jackson.UnwrappedTypeResolverBuilder
[WARNING]   - io.fabric8.kubernetes.model.annotation.Singular
[WARNING]   - io.fabric8.kubernetes.model.annotation.StatusReplicas
[WARNING]   - io.fabric8.kubernetes.model.annotation.SpecReplicas
[WARNING]   - io.fabric8.kubernetes.model.annotation.Version
[WARNING]   - 6 more...
[WARNING] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-batch-5.12.0.jar define 112 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.batch.v1.JobFluentImpl$StatusNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.batch.v1.CronJobStatusFluentImpl$ActiveNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.batch.v1.JobSpecFluent$TemplateNested
[WARNING]   - io.fabric8.kubernetes.api.model.batch.v1beta1.CronJobListFluentImpl$ItemsNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.batch.v1beta1.CronJobSpecFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.batch.v1.JobSpecFluentImpl$TemplateNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.batch.v1.Job
[WARNING]   - io.fabric8.kubernetes.api.model.batch.v1beta1.CronJobFluent
[WARNING]   - io.fabric8.kubernetes.api.model.batch.v1beta1.CronJobListFluent
[WARNING]   - io.fabric8.kubernetes.api.model.batch.v1.CronJobListFluent
[WARNING]   - 102 more...
[WARNING] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-storageclass-5.12.0.jar define 172 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.storage.v1beta1.CSIStorageCapacityListFluentImpl$ItemsNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.storage.CSINodeFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.storage.CSINodeDriverFluentImpl$AllocatableNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.storage.StorageClass
[WARNING]   - io.fabric8.kubernetes.api.model.storage.VolumeAttachmentSourceBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.storage.VolumeAttachmentSourceFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.storage.TokenRequestFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.storage.v1beta1.CSINodeDriverFluent$AllocatableNested
[WARNING]   - io.fabric8.kubernetes.api.model.storage.v1beta1.CSIDriverSpecBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.storage.VolumeAttachmentSpecFluent
[WARNING]   - 162 more...
[WARNING] okhttp-3.12.12.jar, crd-generator-0.29.0-SNAPSHOT.jar define 208 overlapping classes: 
[WARNING]   - okhttp3.WebSocket
[WARNING]   - okhttp3.Cookie$Builder
[WARNING]   - okhttp3.internal.http.HttpHeaders
[WARNING]   - okhttp3.internal.http2.Http2Connection$ReaderRunnable
[WARNING]   - okhttp3.internal.http2.Http2Reader$ContinuationSource
[WARNING]   - okhttp3.internal.tls.OkHostnameVerifier
[WARNING]   - okhttp3.Cache$Entry
[WARNING]   - okhttp3.internal.http2.Http2Connection$3
[WARNING]   - okhttp3.internal.ws.RealWebSocket$Streams
[WARNING]   - okhttp3.CacheControl$Builder
[WARNING]   - 198 more...
[WARNING] maven-shade-plugin has detected that some class files are
[WARNING] present in two or more JARs. When this happens, only one
[WARNING] single version of the class is copied to the uber jar.
[WARNING] Usually this is not harmful and you can skip these warnings,
[WARNING] otherwise try to manually exclude artifacts based on
[WARNING] mvn dependency:tree -Ddetail=true and the above output.
[WARNING] See http://maven.apache.org/plugins/maven-shade-plugin/
[INFO] Replacing original artifact with shaded artifact.
[INFO] Replacing /home/cloud-user/strimzi-kafka-operator/crd-generator/target/crd-generator-0.29.0-SNAPSHOT.jar with /home/cloud-user/strimzi-kafka-operator/crd-generator/target/crd-generator-0.29.0-SNAPSHOT-shaded.jar
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:integration-test (default) @ crd-generator ---
[WARNING] useSystemClassLoader setting has no effect when not forking
[WARNING] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:verify (default) @ crd-generator ---
[INFO] 
[INFO] --- maven-dependency-plugin:3.1.1:analyze-only (analyze) @ crd-generator ---
[INFO] No dependency problems found
[INFO] 
[INFO] ---------------------------< io.strimzi:api >---------------------------
[INFO] Building api 0.29.0-SNAPSHOT                                      [5/10]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ api ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ api ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ api ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/api/target/jacoco.exec
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ api ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/cloud-user/strimzi-kafka-operator/api/src/main/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:compile (default-compile) @ api ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- exec-maven-plugin:1.6.0:exec (generate-crd-co-install-v1) @ api ---
[INFO] 
[INFO] --- exec-maven-plugin:1.6.0:exec (generate-crd-co-install-v1-eo) @ api ---
[INFO] 
[INFO] --- exec-maven-plugin:1.6.0:exec (generate-doc) @ api ---
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ api ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 99 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:testCompile (default-testCompile) @ api ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:testCompile (default-test-compile) @ api ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:3.0.0-M5:test (default-test) @ api ---
[INFO] 
[INFO] -------------------------------------------------------
[INFO]  T E S T S
[INFO] -------------------------------------------------------
[INFO] 
[INFO] Results:
[INFO] 
[INFO] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[INFO] 
[INFO] 
[INFO] --- maven-jar-plugin:3.1.0:jar (default-jar) @ api ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:report (report) @ api ---
[INFO] Skipping JaCoCo execution due to missing execution data file.
[INFO] 
[INFO] >>> maven-source-plugin:3.0.1:jar (attach-sources) > generate-sources @ api >>>
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ api ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ api ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ api ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/api/target/jacoco.exec
[INFO] 
[INFO] <<< maven-source-plugin:3.0.1:jar (attach-sources) < generate-sources @ api <<<
[INFO] 
[INFO] 
[INFO] --- maven-source-plugin:3.0.1:jar (attach-sources) @ api ---
[INFO] 
[INFO] --- maven-javadoc-plugin:3.1.0:jar (attach-javadocs) @ api ---
[INFO] 
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/KafkaBridgeList.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/KafkaConnectList.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/KafkaConnectorList.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/KafkaList.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/KafkaMirrorMaker2List.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/KafkaMirrorMakerList.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/KafkaRebalanceList.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/KafkaTopicList.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/KafkaUserList.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/AclRuleClusterResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/CruiseControlResources.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/AclOperation.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/AclResourcePatternType.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/AclRuleResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/AclRuleGroupResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/AclRuleTopicResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/AclRuleTransactionalIdResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/ExternalLogging.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/AclRuleType.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/CertAndKeySecretSource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/CertSecretSource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/CruiseControlSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/CertificateExpirationPolicy.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaConnect.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/ContainerEnvVar.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/EntityOperatorSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/EntityTopicOperatorSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/EntityUserOperatorSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/HasConfigurableMetrics.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/InlineLogging.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/JvmOptions.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/GenericSecretSource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/ClientTls.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/JmxTransResources.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/JmxTransSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaAuthorization.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaAuthorizationKeycloak.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaAuthorizationOpa.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaAuthorizationSimple.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaBridge.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/Probe.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaBridgeClientSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaBridgeConsumerSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaBridgeHttpConfig.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaBridgeHttpCors.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaBridgeProducerSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaBridgeResources.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaClusterSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaConnector.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/Constants.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaConnectResources.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/Spec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/AbstractKafkaConnectSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/AclRule.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/CertificateAuthority.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaConnectSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaExporterSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaJmxAuthentication.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaConnectorSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaExporterResources.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaJmxAuthenticationPassword.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaMirrorMaker.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaJmxOptions.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaMirrorMaker2.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaMirrorMaker2Spec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaMirrorMakerSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaMirrorMaker2ConnectorSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaMirrorMaker2MirrorSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaMirrorMaker2Resources.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaRebalance.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/HasSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaMirrorMakerConsumerSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaMirrorMakerProducerSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaMirrorMakerResources.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaResources.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaBridgeSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaRebalanceSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaTopic.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaUser.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaTopicSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaUserAuthentication.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaUserAuthorization.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaUserAuthorizationSimple.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaUserQuotas.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaUserScramSha512ClientAuthentication.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaUserSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaUserTlsClientAuthentication.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/MetricsConfig.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/Password.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/Logging.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/PasswordSecretSource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/Rack.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/Sidecar.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/SystemProperty.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/TlsClientAuthentication.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/TlsSidecar.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/TlsSidecarLogLevel.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/Kafka.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/UnknownPropertyPreserving.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/ZookeeperClusterSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/authentication/KafkaClientAuthentication.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationOAuth.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationPlain.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScram.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha256.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha512.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationTls.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/balancing/KafkaRebalanceAnnotation.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/balancing/KafkaRebalanceState.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/balancing/BrokerCapacity.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/ConnectorPlugin.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnv.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/ExternalConfigurationVolumeSource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnvVarSource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/build/Build.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/build/Plugin.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/build/Artifact.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/build/DockerOutput.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/build/DownloadableArtifact.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/build/ImageStreamOutput.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/build/JarArtifact.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/build/MavenArtifact.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/build/OtherArtifact.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/build/Output.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/build/TgzArtifact.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/build/ZipArtifact.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/ExternalConfiguration.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/listener/NodeAddressType.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/listener/arraylistener/KafkaListenerType.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListener.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBootstrap.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBroker.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfiguration.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/listener/KafkaListenerAuthentication.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationCustom.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationOAuth.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationScramSha512.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationTls.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/status/Condition.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/status/KafkaBridgeStatus.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/status/KafkaConnectStatus.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/status/KafkaMirrorMaker2Status.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/status/KafkaMirrorMakerStatus.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/status/KafkaRebalanceStatus.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/status/KafkaUserStatus.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/status/ListenerAddress.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/status/HasStatus.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/status/KafkaConnectorStatus.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/status/KafkaStatus.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/status/KafkaTopicStatus.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/status/Status.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/status/ListenerStatus.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/status/StrimziPodSetStatus.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/storage/PersistentClaimStorageOverride.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/storage/PersistentClaimStorage.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/storage/EphemeralStorage.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/storage/JbodStorage.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/storage/SingleVolumeStorage.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/storage/Storage.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/ContainerTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/ExternalTrafficPolicy.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/JmxTransOutputDefinitionTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/KafkaUserTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/PodManagementPolicy.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/StatefulSetTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/DeploymentStrategy.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/DeploymentTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/InternalServiceTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/IpFamily.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/IpFamilyPolicy.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/MetadataTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/PodDisruptionBudgetTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/PodTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/ResourceTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/BuildConfigTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/CruiseControlTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/EntityOperatorTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/JmxTransQueryTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/JmxTransTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/KafkaBridgeTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/KafkaClusterTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/KafkaConnectTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/KafkaExporterTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2Template.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/tracing/JaegerTracing.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/tracing/Tracing.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/AbstractConnectorSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/ExternalConfigurationReference.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/JmxPrometheusExporterMetrics.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaAuthorizationCustom.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaBridgeAdminClientSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaUserTlsExternalClientAuthentication.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/PasswordSource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/StrimziPodSet.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/StrimziPodSetSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/Crds.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/StrimziPodSetList.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaStatusFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaStatusFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaStatusBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/ListenerStatusFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/ListenerStatusFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/ListenerStatusBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/ConditionFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/ConditionFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/ConditionBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaBridgeStatusFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaBridgeStatusFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaBridgeStatusBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaMirrorMaker2StatusFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaMirrorMaker2StatusFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaMirrorMaker2StatusBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaRebalanceStatusFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaRebalanceStatusFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaRebalanceStatusBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaUserStatusFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaUserStatusFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaUserStatusBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaConnectorStatusFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaConnectorStatusFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaConnectorStatusBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/StatusFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/StatusFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaTopicStatusFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaTopicStatusFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaTopicStatusBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaMirrorMakerStatusFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaMirrorMakerStatusFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaMirrorMakerStatusBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/ListenerAddressFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/ListenerAddressFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/ListenerAddressBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaConnectStatusFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaConnectStatusFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaConnectStatusBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/StrimziPodSetStatusFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/StrimziPodSetStatusFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/StrimziPodSetStatusBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ProbeFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ProbeFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ProbeBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMaker2SpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMaker2SpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMaker2SpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/TgzArtifactFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/TgzArtifactFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/TgzArtifactBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/BuildFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/BuildFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/BuildBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/DockerOutputFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/DockerOutputFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/DockerOutputBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/MavenArtifactFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/MavenArtifactFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/MavenArtifactBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/DownloadableArtifactFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/DownloadableArtifactFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/JarArtifactFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/JarArtifactFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/JarArtifactBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/OtherArtifactFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/OtherArtifactFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/OtherArtifactBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/ZipArtifactFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/ZipArtifactFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/ZipArtifactBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/ImageStreamOutputFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/ImageStreamOutputFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/ImageStreamOutputBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/PluginFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/PluginFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/PluginBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/ExternalConfigurationVolumeSourceFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/ExternalConfigurationVolumeSourceFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/ExternalConfigurationVolumeSourceBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/ConnectorPluginFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/ConnectorPluginFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/ConnectorPluginBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnvVarSourceFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnvVarSourceFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnvVarSourceBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/ExternalConfigurationFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/ExternalConfigurationFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/ExternalConfigurationBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnvFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnvFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnvBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationScramSha512Fluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationScramSha512FluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationScramSha512Builder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationOAuthFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationOAuthFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationOAuthBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBootstrapFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBootstrapFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBootstrapBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBrokerFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBrokerFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBrokerBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationCustomFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationCustomFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationCustomBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationTlsFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationTlsFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationTlsBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserAuthorizationSimpleFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserAuthorizationSimpleFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserAuthorizationSimpleBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/StrimziPodSetSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/StrimziPodSetSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/StrimziPodSetSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/EntityOperatorTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/EntityOperatorTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/EntityOperatorTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/PodTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/PodTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/PodTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/PodDisruptionBudgetTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/PodDisruptionBudgetTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/PodDisruptionBudgetTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/JmxTransOutputDefinitionTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/JmxTransOutputDefinitionTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/JmxTransOutputDefinitionTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/StatefulSetTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/StatefulSetTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/StatefulSetTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/CruiseControlTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/MetadataTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/MetadataTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/MetadataTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaExporterTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaExporterTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaExporterTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaClusterTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/ContainerTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/ContainerTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/ContainerTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaConnectTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaUserTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaUserTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaUserTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/JmxTransQueryTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/JmxTransQueryTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/JmxTransQueryTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/InternalServiceTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/InternalServiceTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/InternalServiceTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/ResourceTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/ResourceTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/ResourceTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/JmxTransTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/JmxTransTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/JmxTransTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/DeploymentTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/DeploymentTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/DeploymentTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/BuildConfigTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/BuildConfigTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/BuildConfigTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaJmxOptionsFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaJmxOptionsFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaJmxOptionsBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/storage/JbodStorageFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/storage/JbodStorageFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/storage/JbodStorageBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/storage/PersistentClaimStorageOverrideFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/storage/PersistentClaimStorageOverrideFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/storage/PersistentClaimStorageOverrideBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/storage/EphemeralStorageFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/storage/EphemeralStorageFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/storage/EphemeralStorageBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/storage/PersistentClaimStorageFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/storage/PersistentClaimStorageFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/storage/PersistentClaimStorageBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMakerFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMakerFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMakerBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/RackFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/RackFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/RackBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/TlsClientAuthenticationFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/TlsClientAuthenticationFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/TlsClientAuthenticationBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AclRuleFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AclRuleFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AclRuleBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMaker2Fluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMaker2FluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMaker2Builder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ClientTlsFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ClientTlsFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ClientTlsBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaConnectorSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaConnectorSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaConnectorSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/EntityUserOperatorSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/EntityUserOperatorSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/EntityUserOperatorSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaTopicFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaTopicFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaTopicBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserTlsClientAuthenticationFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserTlsClientAuthenticationFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserTlsClientAuthenticationBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeHttpConfigFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeHttpConfigFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeHttpConfigBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/JmxPrometheusExporterMetricsFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/JmxPrometheusExporterMetricsFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/JmxPrometheusExporterMetricsBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/SystemPropertyFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/SystemPropertyFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/SystemPropertyBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaTopicSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaTopicSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaTopicSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/CruiseControlSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/CruiseControlSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/CruiseControlSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserTlsExternalClientAuthenticationFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserTlsExternalClientAuthenticationFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserTlsExternalClientAuthenticationBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ZookeeperClusterSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/StrimziPodSetFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/StrimziPodSetFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/StrimziPodSetBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMaker2ConnectorSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMaker2ConnectorSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMaker2ConnectorSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/balancing/BrokerCapacityFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/balancing/BrokerCapacityFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/balancing/BrokerCapacityBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/PasswordSourceFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/tracing/JaegerTracingFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/tracing/JaegerTracingFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/tracing/JaegerTracingBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/PasswordSourceFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/PasswordSourceBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaRebalanceSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaRebalanceSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaRebalanceSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaJmxAuthenticationPasswordFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaJmxAuthenticationPasswordFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaJmxAuthenticationPasswordBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMaker2MirrorSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMaker2MirrorSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMaker2MirrorSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha256Fluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha256FluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha256Builder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationOAuthFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationOAuthFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationOAuthBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha512Fluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha512FluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha512Builder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationPlainFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationPlainFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationPlainBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationTlsFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationTlsFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationTlsBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/TlsSidecarFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/TlsSidecarFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/TlsSidecarBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/CertificateAuthorityFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/CertificateAuthorityFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/CertificateAuthorityBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeConsumerSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeConsumerSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeConsumerSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaConnectSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaConnectSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaConnectSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AclRuleTopicResourceFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AclRuleTopicResourceFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AclRuleTopicResourceBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AclRuleGroupResourceFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AclRuleGroupResourceFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AclRuleGroupResourceBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AbstractConnectorSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AbstractConnectorSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/EntityOperatorSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/EntityOperatorSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/EntityOperatorSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeHttpCorsFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeHttpCorsFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeHttpCorsBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/EntityTopicOperatorSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/EntityTopicOperatorSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/EntityTopicOperatorSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/SidecarFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/SidecarFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/SidecarBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/JvmOptionsFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/JvmOptionsFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/JvmOptionsBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaRebalanceFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaRebalanceFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaRebalanceBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaAuthorizationOpaFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaAuthorizationOpaFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaAuthorizationOpaBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeProducerSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeProducerSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeProducerSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ExternalLoggingFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ExternalLoggingFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ExternalLoggingBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/PasswordSecretSourceFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/PasswordSecretSourceFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/PasswordSecretSourceBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaAuthorizationCustomFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaAuthorizationCustomFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaAuthorizationCustomBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/PasswordFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/PasswordFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/PasswordBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserScramSha512ClientAuthenticationFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserScramSha512ClientAuthenticationFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserScramSha512ClientAuthenticationBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/CertAndKeySecretSourceFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/CertAndKeySecretSourceFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/CertAndKeySecretSourceBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMakerConsumerSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMakerConsumerSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMakerConsumerSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/CertSecretSourceFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/CertSecretSourceFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/CertSecretSourceBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/InlineLoggingFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/InlineLoggingFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/InlineLoggingBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserQuotasFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserQuotasFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserQuotasBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ExternalConfigurationReferenceFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ExternalConfigurationReferenceFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ExternalConfigurationReferenceBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaAuthorizationSimpleFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaAuthorizationSimpleFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaAuthorizationSimpleBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AclRuleTransactionalIdResourceFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AclRuleTransactionalIdResourceFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AclRuleTransactionalIdResourceBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMakerProducerSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMakerProducerSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMakerProducerSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/GenericSecretSourceFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/GenericSecretSourceFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/GenericSecretSourceBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/JmxTransSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/JmxTransSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/JmxTransSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaClusterSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaConnectorFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaConnectorFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaConnectorBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/SpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/SpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeClientSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeClientSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ContainerEnvVarFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ContainerEnvVarFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ContainerEnvVarBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaAuthorizationKeycloakFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaAuthorizationKeycloakFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaAuthorizationKeycloakBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeAdminClientSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeAdminClientSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeAdminClientSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaExporterSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaExporterSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaExporterSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AclRuleClusterResourceFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AclRuleClusterResourceFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AclRuleClusterResourceBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaConnectFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaConnectFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaConnectBuilder.java...
Constructing Javadoc information...
Standard Doclet version 11.0.11
Building tree for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListener.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfiguration.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBootstrap.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBootstrapBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBootstrapFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBootstrapFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBroker.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBrokerBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBrokerFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBrokerFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationFluent.BootstrapNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationFluent.BrokerCertChainAndKeyNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationFluent.BrokersNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationFluentImpl.BootstrapNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationFluentImpl.BrokerCertChainAndKeyNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationFluentImpl.BrokersNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerFluent.ConfigurationNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerFluent.KafkaListenerAuthenticationCustomAuthNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerFluent.KafkaListenerAuthenticationOAuthNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerFluent.KafkaListenerAuthenticationScramSha512AuthNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerFluent.KafkaListenerAuthenticationTlsAuthNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerFluentImpl.ConfigurationNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerFluentImpl.KafkaListenerAuthenticationCustomAuthNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerFluentImpl.KafkaListenerAuthenticationOAuthNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerFluentImpl.KafkaListenerAuthenticationScramSha512AuthNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerFluentImpl.KafkaListenerAuthenticationTlsAuthNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/KafkaListenerType.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthentication.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationOAuth.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationOAuthBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationOAuthFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationOAuthFluent.AccessTokenNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationOAuthFluent.ClientSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationOAuthFluent.RefreshTokenNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationOAuthFluent.TlsTrustedCertificatesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationOAuthFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationOAuthFluentImpl.AccessTokenNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationOAuthFluentImpl.ClientSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationOAuthFluentImpl.RefreshTokenNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationOAuthFluentImpl.TlsTrustedCertificatesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationPlain.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationPlainBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationPlainFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationPlainFluent.PasswordSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationPlainFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationPlainFluentImpl.PasswordSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScram.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha256.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha256Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha256Fluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha256Fluent.PasswordSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha256FluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha256FluentImpl.PasswordSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha512.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha512Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha512Fluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha512Fluent.PasswordSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha512FluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha512FluentImpl.PasswordSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationTls.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationTlsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationTlsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationTlsFluent.CertificateAndKeyNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationTlsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationTlsFluentImpl.CertificateAndKeyNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/balancing/BrokerCapacity.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/balancing/BrokerCapacityBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/balancing/BrokerCapacityFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/balancing/BrokerCapacityFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/balancing/KafkaRebalanceAnnotation.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/balancing/KafkaRebalanceState.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/Artifact.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/Build.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/BuildBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/BuildFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/BuildFluent.DockerOutputNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/BuildFluent.ImageStreamOutputNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/BuildFluent.PluginsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/BuildFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/BuildFluentImpl.DockerOutputNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/BuildFluentImpl.ImageStreamOutputNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/BuildFluentImpl.PluginsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/DockerOutput.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/DockerOutputBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/DockerOutputFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/DockerOutputFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/DownloadableArtifact.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/DownloadableArtifactFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/DownloadableArtifactFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/ImageStreamOutput.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/ImageStreamOutputBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/ImageStreamOutputFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/ImageStreamOutputFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/JarArtifact.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/JarArtifactBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/JarArtifactFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/JarArtifactFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/MavenArtifact.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/MavenArtifactBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/MavenArtifactFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/MavenArtifactFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/OtherArtifact.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/OtherArtifactBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/OtherArtifactFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/OtherArtifactFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/Output.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/Plugin.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/PluginBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/PluginFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/PluginFluent.JarArtifactArtifactsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/PluginFluent.MavenArtifactArtifactsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/PluginFluent.OtherArtifactArtifactsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/PluginFluent.TgzArtifactArtifactsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/PluginFluent.ZipArtifactArtifactsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/PluginFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/PluginFluentImpl.JarArtifactArtifactsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/PluginFluentImpl.MavenArtifactArtifactsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/PluginFluentImpl.OtherArtifactArtifactsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/PluginFluentImpl.TgzArtifactArtifactsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/PluginFluentImpl.ZipArtifactArtifactsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/TgzArtifact.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/TgzArtifactBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/TgzArtifactFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/TgzArtifactFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/ZipArtifact.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/ZipArtifactBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/ZipArtifactFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/ZipArtifactFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ConnectorPlugin.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ConnectorPluginBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ConnectorPluginFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ConnectorPluginFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfiguration.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnv.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnvBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnvFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnvFluent.ValueFromNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnvFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnvFluentImpl.ValueFromNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnvVarSource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnvVarSourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnvVarSourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnvVarSourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationFluent.EnvNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationFluent.VolumesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationFluentImpl.EnvNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationFluentImpl.VolumesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationVolumeSource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationVolumeSourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationVolumeSourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationVolumeSourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/Crds.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/KafkaBridgeList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/KafkaConnectList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/KafkaConnectorList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/KafkaList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/KafkaMirrorMaker2List.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/KafkaMirrorMakerList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/KafkaRebalanceList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/KafkaTopicList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/KafkaUserList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/StrimziPodSetList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthentication.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationCustom.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationCustomBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationCustomFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationCustomFluent.SecretsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationCustomFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationCustomFluentImpl.SecretsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationOAuth.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationOAuthBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationOAuthFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationOAuthFluent.ClientSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationOAuthFluent.TlsTrustedCertificatesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationOAuthFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationOAuthFluentImpl.ClientSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationOAuthFluentImpl.TlsTrustedCertificatesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationScramSha512.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationScramSha512Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationScramSha512Fluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationScramSha512FluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationTls.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationTlsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationTlsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationTlsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/NodeAddressType.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractConnectorSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractConnectorSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractConnectorSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluent.ExternalConfigurationNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluent.ExternalLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluent.InlineLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluent.JaegerTracingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluent.JmxOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluent.JmxPrometheusExporterMetricsConfigNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluent.JvmOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluentImpl.ExternalConfigurationNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluentImpl.ExternalLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluentImpl.InlineLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluentImpl.JaegerTracingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluentImpl.JmxOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluentImpl.JmxPrometheusExporterMetricsConfigNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluentImpl.JvmOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclOperation.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclResourcePatternType.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRule.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleClusterResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleClusterResourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleClusterResourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleClusterResourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleFluent.AclRuleClusterResourceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleFluent.AclRuleGroupResourceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleFluent.AclRuleTopicResourceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleFluent.AclRuleTransactionalIdResourceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleFluentImpl.AclRuleClusterResourceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleFluentImpl.AclRuleGroupResourceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleFluentImpl.AclRuleTopicResourceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleFluentImpl.AclRuleTransactionalIdResourceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleGroupResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleGroupResourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleGroupResourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleGroupResourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleTopicResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleTopicResourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleTopicResourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleTopicResourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleTransactionalIdResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleTransactionalIdResourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleTransactionalIdResourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleTransactionalIdResourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleType.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CertAndKeySecretSource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CertAndKeySecretSourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CertAndKeySecretSourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CertAndKeySecretSourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CertificateAuthority.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CertificateAuthorityBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CertificateAuthorityFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CertificateAuthorityFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CertificateExpirationPolicy.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CertSecretSource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CertSecretSourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CertSecretSourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CertSecretSourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ClientTls.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ClientTlsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ClientTlsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ClientTlsFluent.TrustedCertificatesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ClientTlsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ClientTlsFluentImpl.TrustedCertificatesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/Constants.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ContainerEnvVar.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ContainerEnvVarBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ContainerEnvVarFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ContainerEnvVarFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlResources.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluent.BrokerCapacityNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluent.ExternalLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluent.InlineLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluent.JmxPrometheusExporterMetricsConfigNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluent.JvmOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluent.TlsSidecarNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluentImpl.BrokerCapacityNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluentImpl.ExternalLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluentImpl.InlineLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluentImpl.JmxPrometheusExporterMetricsConfigNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluentImpl.JvmOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluentImpl.TlsSidecarNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityOperatorSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityOperatorSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityOperatorSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityOperatorSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityOperatorSpecFluent.TlsSidecarNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityOperatorSpecFluent.TopicOperatorNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityOperatorSpecFluent.UserOperatorNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityOperatorSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityOperatorSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityOperatorSpecFluentImpl.TlsSidecarNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityOperatorSpecFluentImpl.TopicOperatorNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityOperatorSpecFluentImpl.UserOperatorNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityTopicOperatorSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityTopicOperatorSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityTopicOperatorSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityTopicOperatorSpecFluent.ExternalLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityTopicOperatorSpecFluent.InlineLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityTopicOperatorSpecFluent.JvmOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityTopicOperatorSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityTopicOperatorSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityTopicOperatorSpecFluent.StartupProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityTopicOperatorSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityTopicOperatorSpecFluentImpl.ExternalLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityTopicOperatorSpecFluentImpl.InlineLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityTopicOperatorSpecFluentImpl.JvmOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityTopicOperatorSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityTopicOperatorSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityTopicOperatorSpecFluentImpl.StartupProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityUserOperatorSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityUserOperatorSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityUserOperatorSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityUserOperatorSpecFluent.ExternalLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityUserOperatorSpecFluent.InlineLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityUserOperatorSpecFluent.JvmOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityUserOperatorSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityUserOperatorSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityUserOperatorSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityUserOperatorSpecFluentImpl.ExternalLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityUserOperatorSpecFluentImpl.InlineLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityUserOperatorSpecFluentImpl.JvmOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityUserOperatorSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityUserOperatorSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ExternalConfigurationReference.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ExternalConfigurationReferenceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ExternalConfigurationReferenceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ExternalConfigurationReferenceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ExternalLogging.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ExternalLoggingBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ExternalLoggingFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ExternalLoggingFluent.ValueFromNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ExternalLoggingFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ExternalLoggingFluentImpl.ValueFromNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/GenericSecretSource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/GenericSecretSourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/GenericSecretSourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/GenericSecretSourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/HasConfigurableMetrics.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/HasSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/InlineLogging.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/InlineLoggingBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/InlineLoggingFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/InlineLoggingFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxPrometheusExporterMetrics.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxPrometheusExporterMetricsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxPrometheusExporterMetricsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxPrometheusExporterMetricsFluent.ValueFromNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxPrometheusExporterMetricsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxPrometheusExporterMetricsFluentImpl.ValueFromNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxTransResources.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxTransSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxTransSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxTransSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxTransSpecFluent.KafkaQueriesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxTransSpecFluent.OutputDefinitionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxTransSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxTransSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxTransSpecFluentImpl.KafkaQueriesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxTransSpecFluentImpl.OutputDefinitionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxTransSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JvmOptions.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JvmOptionsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JvmOptionsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JvmOptionsFluent.JavaSystemPropertiesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JvmOptionsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JvmOptionsFluentImpl.JavaSystemPropertiesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/Kafka.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorization.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationCustom.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationCustomBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationCustomFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationCustomFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationKeycloak.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationKeycloakBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationKeycloakFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationKeycloakFluent.TlsTrustedCertificatesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationKeycloakFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationKeycloakFluentImpl.TlsTrustedCertificatesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationOpa.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationOpaBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationOpaFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationOpaFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationSimple.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationSimpleBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationSimpleFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationSimpleFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridge.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeAdminClientSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeAdminClientSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeAdminClientSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeAdminClientSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeClientSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeClientSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeClientSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeConsumerSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeConsumerSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeConsumerSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeConsumerSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeHttpConfig.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeHttpConfigBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeHttpConfigFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeHttpConfigFluent.CorsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeHttpConfigFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeHttpConfigFluentImpl.CorsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeHttpCors.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeHttpCorsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeHttpCorsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeHttpCorsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeProducerSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeProducerSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeProducerSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeProducerSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeResources.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.AdminClientNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.ConsumerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.ExternalLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.HttpNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.InlineLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.JaegerTracingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.JvmOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.KafkaClientAuthenticationOAuthNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.KafkaClientAuthenticationPlainNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.KafkaClientAuthenticationScramSha256Nested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.KafkaClientAuthenticationScramSha512Nested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.KafkaClientAuthenticationTlsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.ProducerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.TlsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.AdminClientNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.ConsumerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.ExternalLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.HttpNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.InlineLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.JaegerTracingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.JvmOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.KafkaClientAuthenticationOAuthNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.KafkaClientAuthenticationPlainNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.KafkaClientAuthenticationScramSha256NestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.KafkaClientAuthenticationScramSha512NestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.KafkaClientAuthenticationTlsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.ProducerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.TlsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.EphemeralStorageNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.ExternalLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.InlineLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.JbodStorageNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.JmxOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.JmxPrometheusExporterMetricsConfigNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.JvmOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.KafkaAuthorizationCustomNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.KafkaAuthorizationKeycloakNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.KafkaAuthorizationOpaNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.KafkaAuthorizationSimpleNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.ListenersNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.PersistentClaimStorageNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.RackNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.EphemeralStorageNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.ExternalLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.InlineLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.JbodStorageNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.JmxOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.JmxPrometheusExporterMetricsConfigNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.JvmOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.KafkaAuthorizationCustomNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.KafkaAuthorizationKeycloakNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.KafkaAuthorizationOpaNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.KafkaAuthorizationSimpleNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.ListenersNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.PersistentClaimStorageNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.RackNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnect.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnector.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectorBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectorFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectorFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectorFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectorFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectorFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectorFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectorFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectorFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectorSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectorSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectorSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectorSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectResources.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluent.BuildNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluent.KafkaClientAuthenticationOAuthNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluent.KafkaClientAuthenticationPlainNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluent.KafkaClientAuthenticationScramSha256Nested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluent.KafkaClientAuthenticationScramSha512Nested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluent.KafkaClientAuthenticationTlsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluent.RackNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluent.TlsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluentImpl.BuildNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluentImpl.KafkaClientAuthenticationOAuthNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluentImpl.KafkaClientAuthenticationPlainNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluentImpl.KafkaClientAuthenticationScramSha256NestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluentImpl.KafkaClientAuthenticationScramSha512NestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluentImpl.KafkaClientAuthenticationTlsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluentImpl.RackNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluentImpl.TlsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaExporterResources.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaExporterSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaExporterSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaExporterSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaExporterSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaExporterSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaExporterSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaExporterSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaExporterSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaExporterSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaExporterSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaJmxAuthentication.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaJmxAuthenticationPassword.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaJmxAuthenticationPasswordBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaJmxAuthenticationPasswordFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaJmxAuthenticationPasswordFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaJmxOptions.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaJmxOptionsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaJmxOptionsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaJmxOptionsFluent.KafkaJmxAuthenticationPasswordNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaJmxOptionsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaJmxOptionsFluentImpl.KafkaJmxAuthenticationPasswordNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecFluent.KafkaClientAuthenticationOAuthNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecFluent.KafkaClientAuthenticationPlainNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecFluent.KafkaClientAuthenticationScramSha256Nested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecFluent.KafkaClientAuthenticationScramSha512Nested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecFluent.KafkaClientAuthenticationTlsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecFluent.TlsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecFluentImpl.KafkaClientAuthenticationOAuthNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecFluentImpl.KafkaClientAuthenticationPlainNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecFluentImpl.KafkaClientAuthenticationScramSha256NestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecFluentImpl.KafkaClientAuthenticationScramSha512NestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecFluentImpl.KafkaClientAuthenticationTlsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecFluentImpl.TlsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ConnectorSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ConnectorSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ConnectorSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ConnectorSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2Fluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2Fluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2Fluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2Fluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2FluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2FluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2FluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2FluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2MirrorSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2MirrorSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2MirrorSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2MirrorSpecFluent.CheckpointConnectorNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2MirrorSpecFluent.HeartbeatConnectorNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2MirrorSpecFluent.SourceConnectorNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2MirrorSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2MirrorSpecFluentImpl.CheckpointConnectorNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2MirrorSpecFluentImpl.HeartbeatConnectorNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2MirrorSpecFluentImpl.SourceConnectorNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2Resources.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2Spec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2SpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2SpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2SpecFluent.ClustersNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2SpecFluent.MirrorsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2SpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2SpecFluentImpl.ClustersNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2SpecFluentImpl.MirrorsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecFluent.KafkaClientAuthenticationOAuthNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecFluent.KafkaClientAuthenticationPlainNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecFluent.KafkaClientAuthenticationScramSha256Nested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecFluent.KafkaClientAuthenticationScramSha512Nested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecFluent.KafkaClientAuthenticationTlsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecFluent.TlsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecFluentImpl.KafkaClientAuthenticationOAuthNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecFluentImpl.KafkaClientAuthenticationPlainNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecFluentImpl.KafkaClientAuthenticationScramSha256NestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecFluentImpl.KafkaClientAuthenticationScramSha512NestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecFluentImpl.KafkaClientAuthenticationTlsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecFluentImpl.TlsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerConsumerSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerConsumerSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerConsumerSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerConsumerSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerProducerSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerProducerSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerProducerSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerProducerSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerResources.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluent.ConsumerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluent.ExternalLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluent.InlineLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluent.JaegerTracingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluent.JmxPrometheusExporterMetricsConfigNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluent.JvmOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluent.ProducerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluentImpl.ConsumerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluentImpl.ExternalLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluentImpl.InlineLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluentImpl.JaegerTracingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluentImpl.JmxPrometheusExporterMetricsConfigNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluentImpl.JvmOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluentImpl.ProducerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaRebalance.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaRebalanceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaRebalanceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaRebalanceFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaRebalanceFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaRebalanceFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaRebalanceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaRebalanceFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaRebalanceFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaRebalanceFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaRebalanceSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaRebalanceSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaRebalanceSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaRebalanceSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaResources.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluent.ClientsCaNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluent.ClusterCaNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluent.CruiseControlNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluent.EntityOperatorNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluent.JmxTransNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluent.KafkaExporterNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluent.KafkaNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluent.ZookeeperNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluentImpl.ClientsCaNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluentImpl.ClusterCaNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluentImpl.CruiseControlNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluentImpl.EntityOperatorNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluentImpl.JmxTransNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluentImpl.KafkaExporterNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluentImpl.KafkaNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluentImpl.ZookeeperNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaTopic.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaTopicBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaTopicFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaTopicFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaTopicFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaTopicFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaTopicFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaTopicFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaTopicFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaTopicFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaTopicSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaTopicSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaTopicSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaTopicSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUser.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserAuthentication.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserAuthorization.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserAuthorizationSimple.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserAuthorizationSimpleBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserAuthorizationSimpleFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserAuthorizationSimpleFluent.AclsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserAuthorizationSimpleFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserAuthorizationSimpleFluentImpl.AclsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserQuotas.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserQuotasBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserQuotasFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserQuotasFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserScramSha512ClientAuthentication.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserScramSha512ClientAuthenticationBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserScramSha512ClientAuthenticationFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserScramSha512ClientAuthenticationFluent.PasswordNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserScramSha512ClientAuthenticationFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserScramSha512ClientAuthenticationFluentImpl.PasswordNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserSpecFluent.KafkaUserAuthorizationSimpleNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserSpecFluent.KafkaUserScramSha512ClientAuthenticationNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserSpecFluent.KafkaUserTlsClientAuthenticationNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserSpecFluent.KafkaUserTlsExternalClientAuthenticationNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserSpecFluent.QuotasNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserSpecFluentImpl.KafkaUserAuthorizationSimpleNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserSpecFluentImpl.KafkaUserScramSha512ClientAuthenticationNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserSpecFluentImpl.KafkaUserTlsClientAuthenticationNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserSpecFluentImpl.KafkaUserTlsExternalClientAuthenticationNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserSpecFluentImpl.QuotasNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserTlsClientAuthentication.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserTlsClientAuthenticationBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserTlsClientAuthenticationFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserTlsClientAuthenticationFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserTlsExternalClientAuthentication.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserTlsExternalClientAuthenticationBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserTlsExternalClientAuthenticationFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserTlsExternalClientAuthenticationFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/Logging.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/MetricsConfig.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/Password.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/PasswordBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/PasswordFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/PasswordFluent.ValueFromNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/PasswordFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/PasswordFluentImpl.ValueFromNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/PasswordSecretSource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/PasswordSecretSourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/PasswordSecretSourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/PasswordSecretSourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/PasswordSource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/PasswordSourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/PasswordSourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/PasswordSourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/Probe.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ProbeBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ProbeFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ProbeFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/Rack.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/RackBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/RackFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/RackFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/Sidecar.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/SidecarBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/SidecarFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/SidecarFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/Spec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/SpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/SpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/StrimziPodSet.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/StrimziPodSetBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/StrimziPodSetFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/StrimziPodSetFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/StrimziPodSetFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/StrimziPodSetFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/StrimziPodSetFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/StrimziPodSetFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/StrimziPodSetFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/StrimziPodSetFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/StrimziPodSetSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/StrimziPodSetSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/StrimziPodSetSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/StrimziPodSetSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/SystemProperty.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/SystemPropertyBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/SystemPropertyFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/SystemPropertyFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/TlsClientAuthentication.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/TlsClientAuthenticationBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/TlsClientAuthenticationFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/TlsClientAuthenticationFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/TlsSidecar.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/TlsSidecarBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/TlsSidecarFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/TlsSidecarFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/TlsSidecarFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/TlsSidecarFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/TlsSidecarFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/TlsSidecarFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/TlsSidecarLogLevel.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/UnknownPropertyPreserving.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluent.EphemeralStorageNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluent.ExternalLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluent.InlineLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluent.JmxOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluent.JmxPrometheusExporterMetricsConfigNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluent.JvmOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluent.PersistentClaimStorageNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluentImpl.EphemeralStorageNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluentImpl.ExternalLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluentImpl.InlineLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluentImpl.JmxOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluentImpl.JmxPrometheusExporterMetricsConfigNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluentImpl.JvmOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluentImpl.PersistentClaimStorageNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/Condition.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/ConditionBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/ConditionFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/ConditionFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/HasStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaBridgeStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaBridgeStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaBridgeStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaBridgeStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaConnectorStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaConnectorStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaConnectorStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaConnectorStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaConnectStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaConnectStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaConnectStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaConnectStatusFluent.ConnectorPluginsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaConnectStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaConnectStatusFluentImpl.ConnectorPluginsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaMirrorMaker2Status.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaMirrorMaker2StatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaMirrorMaker2StatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaMirrorMaker2StatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaMirrorMakerStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaMirrorMakerStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaMirrorMakerStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaMirrorMakerStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaRebalanceStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaRebalanceStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaRebalanceStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaRebalanceStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaStatusFluent.ListenersNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaStatusFluentImpl.ListenersNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaTopicStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaTopicStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaTopicStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaTopicStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaUserStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaUserStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaUserStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaUserStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/ListenerAddress.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/ListenerAddressBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/ListenerAddressFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/ListenerAddressFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/ListenerStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/ListenerStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/ListenerStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/ListenerStatusFluent.AddressesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/ListenerStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/ListenerStatusFluentImpl.AddressesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/Status.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/StatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/StatusFluent.ConditionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/StatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/StatusFluentImpl.ConditionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/StrimziPodSetStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/StrimziPodSetStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/StrimziPodSetStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/StrimziPodSetStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/EphemeralStorage.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/EphemeralStorageBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/EphemeralStorageFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/EphemeralStorageFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/JbodStorage.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/JbodStorageBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/JbodStorageFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/JbodStorageFluent.EphemeralStorageVolumesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/JbodStorageFluent.PersistentClaimStorageVolumesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/JbodStorageFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/JbodStorageFluentImpl.EphemeralStorageVolumesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/JbodStorageFluentImpl.PersistentClaimStorageVolumesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/PersistentClaimStorage.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/PersistentClaimStorageBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/PersistentClaimStorageFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/PersistentClaimStorageFluent.OverridesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/PersistentClaimStorageFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/PersistentClaimStorageFluentImpl.OverridesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/PersistentClaimStorageOverride.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/PersistentClaimStorageOverrideBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/PersistentClaimStorageOverrideFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/PersistentClaimStorageOverrideFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/SingleVolumeStorage.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/Storage.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/BuildConfigTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/BuildConfigTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/BuildConfigTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/BuildConfigTemplateFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/BuildConfigTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/BuildConfigTemplateFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ContainerTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ContainerTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ContainerTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ContainerTemplateFluent.EnvNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ContainerTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ContainerTemplateFluentImpl.EnvNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluent.ApiServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluent.CruiseControlContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluent.DeploymentNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluent.PodDisruptionBudgetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluent.TlsSidecarContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluentImpl.ApiServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluentImpl.CruiseControlContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluentImpl.DeploymentNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluentImpl.PodDisruptionBudgetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluentImpl.TlsSidecarContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/DeploymentStrategy.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/DeploymentTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/DeploymentTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/DeploymentTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/DeploymentTemplateFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/DeploymentTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/DeploymentTemplateFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/EntityOperatorTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/EntityOperatorTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/EntityOperatorTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/EntityOperatorTemplateFluent.DeploymentNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/EntityOperatorTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/EntityOperatorTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/EntityOperatorTemplateFluent.TlsSidecarContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/EntityOperatorTemplateFluent.TopicOperatorContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/EntityOperatorTemplateFluent.UserOperatorContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/EntityOperatorTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/EntityOperatorTemplateFluentImpl.DeploymentNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/EntityOperatorTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/EntityOperatorTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/EntityOperatorTemplateFluentImpl.TlsSidecarContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/EntityOperatorTemplateFluentImpl.TopicOperatorContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/EntityOperatorTemplateFluentImpl.UserOperatorContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ExternalTrafficPolicy.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/InternalServiceTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/InternalServiceTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/InternalServiceTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/InternalServiceTemplateFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/InternalServiceTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/InternalServiceTemplateFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/IpFamily.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/IpFamilyPolicy.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransOutputDefinitionTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransOutputDefinitionTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransOutputDefinitionTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransOutputDefinitionTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransQueryTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransQueryTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransQueryTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransQueryTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransTemplateFluent.ContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransTemplateFluent.DeploymentNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransTemplateFluentImpl.ContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransTemplateFluentImpl.DeploymentNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaBridgeTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateFluent.ApiServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateFluent.BridgeContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateFluent.DeploymentNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateFluent.PodDisruptionBudgetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateFluentImpl.ApiServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateFluentImpl.BridgeContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateFluentImpl.DeploymentNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateFluentImpl.PodDisruptionBudgetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.BootstrapServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.BrokersServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.ClusterCaCertNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.ClusterRoleBindingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.ExternalBootstrapIngressNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.ExternalBootstrapRouteNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.ExternalBootstrapServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.InitContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.JmxSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.KafkaContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.PerPodIngressNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.PerPodRouteNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.PerPodServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.PersistentVolumeClaimNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.PodDisruptionBudgetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.PodSetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.StatefulsetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.BootstrapServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.BrokersServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.ClusterCaCertNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.ClusterRoleBindingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.ExternalBootstrapIngressNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.ExternalBootstrapRouteNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.ExternalBootstrapServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.InitContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.JmxSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.KafkaContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.PerPodIngressNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.PerPodRouteNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.PerPodServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.PersistentVolumeClaimNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.PodDisruptionBudgetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.PodSetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.StatefulsetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluent.ApiServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluent.BuildConfigNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluent.BuildContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluent.BuildPodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluent.BuildServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluent.ClusterRoleBindingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluent.ConnectContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluent.DeploymentNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluent.InitContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluent.JmxSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluent.PodDisruptionBudgetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluentImpl.ApiServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluentImpl.BuildConfigNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluentImpl.BuildContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluentImpl.BuildPodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluentImpl.BuildServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluentImpl.ClusterRoleBindingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluentImpl.ConnectContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluentImpl.DeploymentNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluentImpl.InitContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluentImpl.JmxSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluentImpl.PodDisruptionBudgetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaExporterTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaExporterTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaExporterTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaExporterTemplateFluent.ContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaExporterTemplateFluent.DeploymentNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaExporterTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaExporterTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaExporterTemplateFluent.ServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaExporterTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaExporterTemplateFluentImpl.ContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaExporterTemplateFluentImpl.DeploymentNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaExporterTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaExporterTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaExporterTemplateFluentImpl.ServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2Template.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluent.ApiServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluent.DeploymentNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluent.JmxSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluent.MirrorMaker2ContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluent.PodDisruptionBudgetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluentImpl.ApiServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluentImpl.DeploymentNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluentImpl.JmxSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluentImpl.MirrorMaker2ContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluentImpl.PodDisruptionBudgetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplateFluent.DeploymentNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplateFluent.MirrorMakerContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplateFluent.PodDisruptionBudgetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplateFluentImpl.DeploymentNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplateFluentImpl.MirrorMakerContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplateFluentImpl.PodDisruptionBudgetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaUserTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaUserTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaUserTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaUserTemplateFluent.SecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaUserTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaUserTemplateFluentImpl.SecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/MetadataTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/MetadataTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/MetadataTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/MetadataTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/PodDisruptionBudgetTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/PodDisruptionBudgetTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/PodDisruptionBudgetTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/PodDisruptionBudgetTemplateFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/PodDisruptionBudgetTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/PodDisruptionBudgetTemplateFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/PodManagementPolicy.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/PodTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/PodTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/PodTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/PodTemplateFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/PodTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/PodTemplateFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ResourceTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ResourceTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ResourceTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ResourceTemplateFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ResourceTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ResourceTemplateFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/StatefulSetTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/StatefulSetTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/StatefulSetTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/StatefulSetTemplateFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/StatefulSetTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/StatefulSetTemplateFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluent.ClientServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluent.JmxSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluent.NodesServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluent.PersistentVolumeClaimNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluent.PodDisruptionBudgetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluent.PodSetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluent.StatefulsetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluent.ZookeeperContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluentImpl.ClientServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluentImpl.JmxSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluentImpl.NodesServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluentImpl.PersistentVolumeClaimNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluentImpl.PodDisruptionBudgetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluentImpl.PodSetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluentImpl.StatefulsetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluentImpl.ZookeeperContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/tracing/JaegerTracing.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/tracing/JaegerTracingBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/tracing/JaegerTracingFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/tracing/JaegerTracingFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/tracing/Tracing.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/balancing/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/balancing/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/tracing/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/tracing/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/constant-values.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/serialized-form.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/class-use/KafkaBridgeList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/class-use/KafkaConnectList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/class-use/KafkaConnectorList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/class-use/KafkaList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/class-use/KafkaMirrorMaker2List.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/class-use/KafkaMirrorMakerList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/class-use/KafkaRebalanceList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/class-use/KafkaTopicList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/class-use/KafkaUserList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleClusterResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlResources.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclOperation.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclResourcePatternType.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleGroupResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleTopicResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleTransactionalIdResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ExternalLogging.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleType.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CertAndKeySecretSource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CertSecretSource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CertificateExpirationPolicy.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnect.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ContainerEnvVar.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityOperatorSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityTopicOperatorSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityUserOperatorSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/HasConfigurableMetrics.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/InlineLogging.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JvmOptions.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/GenericSecretSource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ClientTls.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxTransResources.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxTransSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorization.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationKeycloak.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationOpa.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationSimple.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridge.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/Probe.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeClientSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeConsumerSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeHttpConfig.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeHttpCors.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeProducerSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeResources.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnector.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/Constants.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectResources.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/Spec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRule.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CertificateAuthority.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaExporterSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaJmxAuthentication.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectorSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaExporterResources.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaJmxAuthenticationPassword.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaJmxOptions.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ClusterSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2Spec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ConnectorSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2MirrorSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2Resources.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaRebalance.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/HasSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerClientSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerConsumerSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerProducerSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerResources.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaResources.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaRebalanceSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaTopic.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUser.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaTopicSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserAuthentication.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserAuthorization.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserAuthorizationSimple.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserQuotas.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserScramSha512ClientAuthentication.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserTlsClientAuthentication.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/MetricsConfig.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/Password.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/Logging.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/PasswordSecretSource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/Rack.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/Sidecar.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/SystemProperty.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/TlsClientAuthentication.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/TlsSidecar.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/TlsSidecarLogLevel.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/Kafka.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/UnknownPropertyPreserving.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthentication.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationOAuth.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationPlain.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationScram.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationScramSha256.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationScramSha512.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationTls.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/balancing/class-use/KafkaRebalanceAnnotation.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/balancing/class-use/KafkaRebalanceState.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/balancing/class-use/BrokerCapacity.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ConnectorPlugin.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationEnv.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationVolumeSource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationEnvVarSource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/Build.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/Plugin.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/Artifact.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/DockerOutput.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/DownloadableArtifact.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/ImageStreamOutput.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/JarArtifact.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/MavenArtifact.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/OtherArtifact.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/Output.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/TgzArtifact.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/ZipArtifact.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfiguration.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/NodeAddressType.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/KafkaListenerType.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListener.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationBootstrap.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationBroker.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfiguration.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthentication.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationCustom.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationOAuth.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationScramSha512.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationTls.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/Condition.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaBridgeStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaConnectStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaMirrorMaker2Status.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaMirrorMakerStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaRebalanceStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaUserStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/ListenerAddress.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/HasStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaConnectorStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaTopicStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/Status.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/ListenerStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/StrimziPodSetStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/PersistentClaimStorageOverride.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/PersistentClaimStorage.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/EphemeralStorage.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/JbodStorage.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/SingleVolumeStorage.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/Storage.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ContainerTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ExternalTrafficPolicy.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransOutputDefinitionTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaUserTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/PodManagementPolicy.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/StatefulSetTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/DeploymentStrategy.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/DeploymentTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/InternalServiceTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/IpFamily.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/IpFamilyPolicy.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMakerTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/MetadataTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/PodDisruptionBudgetTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/PodTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ResourceTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/BuildConfigTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/EntityOperatorTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransQueryTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaBridgeTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaExporterTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2Template.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/tracing/class-use/JaegerTracing.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/tracing/class-use/Tracing.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractConnectorSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ExternalConfigurationReference.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxPrometheusExporterMetrics.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationCustom.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeAdminClientSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserTlsExternalClientAuthentication.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/PasswordSource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/StrimziPodSet.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/StrimziPodSetSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/class-use/Crds.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/class-use/StrimziPodSetList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaStatusFluent.ListenersNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaStatusFluentImpl.ListenersNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/ListenerStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/ListenerStatusFluent.AddressesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/ListenerStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/ListenerStatusFluentImpl.AddressesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/ListenerStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/ConditionFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/ConditionFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/ConditionBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaBridgeStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaBridgeStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaBridgeStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaMirrorMaker2StatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaMirrorMaker2StatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaMirrorMaker2StatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaRebalanceStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaRebalanceStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaRebalanceStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaUserStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaUserStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaUserStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaConnectorStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaConnectorStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaConnectorStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/StatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/StatusFluent.ConditionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/StatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/StatusFluentImpl.ConditionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaTopicStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaTopicStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaTopicStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaMirrorMakerStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaMirrorMakerStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaMirrorMakerStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/ListenerAddressFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/ListenerAddressFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/ListenerAddressBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaConnectStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaConnectStatusFluent.ConnectorPluginsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaConnectStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaConnectStatusFluentImpl.ConnectorPluginsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaConnectStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/StrimziPodSetStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/StrimziPodSetStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/StrimziPodSetStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ProbeFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ProbeFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ProbeBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2SpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2SpecFluent.MirrorsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2SpecFluent.ClustersNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2SpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2SpecFluentImpl.MirrorsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2SpecFluentImpl.ClustersNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2SpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/TgzArtifactFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/TgzArtifactFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/TgzArtifactBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/BuildFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/BuildFluent.PluginsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/BuildFluent.DockerOutputNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/BuildFluent.ImageStreamOutputNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/BuildFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/BuildFluentImpl.PluginsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/BuildFluentImpl.DockerOutputNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/BuildFluentImpl.ImageStreamOutputNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/BuildBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/DockerOutputFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/DockerOutputFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/DockerOutputBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/MavenArtifactFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/MavenArtifactFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/MavenArtifactBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/DownloadableArtifactFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/DownloadableArtifactFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/JarArtifactFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/JarArtifactFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/JarArtifactBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/OtherArtifactFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/OtherArtifactFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/OtherArtifactBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/ZipArtifactFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/ZipArtifactFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/ZipArtifactBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/ImageStreamOutputFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/ImageStreamOutputFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/ImageStreamOutputBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/PluginFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/PluginFluent.TgzArtifactArtifactsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/PluginFluent.JarArtifactArtifactsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/PluginFluent.OtherArtifactArtifactsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/PluginFluent.ZipArtifactArtifactsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/PluginFluent.MavenArtifactArtifactsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/PluginFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/PluginFluentImpl.TgzArtifactArtifactsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/PluginFluentImpl.JarArtifactArtifactsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/PluginFluentImpl.OtherArtifactArtifactsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/PluginFluentImpl.ZipArtifactArtifactsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/PluginFluentImpl.MavenArtifactArtifactsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/PluginBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationVolumeSourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationVolumeSourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationVolumeSourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ConnectorPluginFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ConnectorPluginFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ConnectorPluginBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationEnvVarSourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationEnvVarSourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationEnvVarSourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationFluent.VolumesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationFluent.EnvNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationFluentImpl.VolumesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationFluentImpl.EnvNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationEnvFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationEnvFluent.ValueFromNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationEnvFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationEnvFluentImpl.ValueFromNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationEnvBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationScramSha512Fluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationScramSha512FluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationScramSha512Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationOAuthFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationOAuthFluent.TlsTrustedCertificatesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationOAuthFluent.ClientSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationOAuthFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationOAuthFluentImpl.TlsTrustedCertificatesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationOAuthFluentImpl.ClientSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationOAuthBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationFluent.BrokersNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationFluent.BootstrapNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationFluent.BrokerCertChainAndKeyNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationFluentImpl.BrokersNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationFluentImpl.BootstrapNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationFluentImpl.BrokerCertChainAndKeyNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationBootstrapFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationBootstrapFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationBootstrapBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationBrokerFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationBrokerFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationBrokerBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerFluent.ConfigurationNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerFluent.KafkaListenerAuthenticationCustomAuthNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerFluent.KafkaListenerAuthenticationScramSha512AuthNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerFluent.KafkaListenerAuthenticationTlsAuthNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerFluent.KafkaListenerAuthenticationOAuthNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerFluentImpl.ConfigurationNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerFluentImpl.KafkaListenerAuthenticationCustomAuthNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerFluentImpl.KafkaListenerAuthenticationScramSha512AuthNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerFluentImpl.KafkaListenerAuthenticationTlsAuthNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerFluentImpl.KafkaListenerAuthenticationOAuthNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationCustomFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationCustomFluent.SecretsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationCustomFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationCustomFluentImpl.SecretsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationCustomBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationTlsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationTlsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationTlsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.JaegerTracingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.InlineLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.ExternalLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.JvmOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.AdminClientNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.ProducerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.ConsumerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.KafkaClientAuthenticationTlsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.KafkaClientAuthenticationOAuthNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.KafkaClientAuthenticationPlainNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.KafkaClientAuthenticationScramSha256Nested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.KafkaClientAuthenticationScramSha512Nested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.TlsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.HttpNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.JaegerTracingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.InlineLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.ExternalLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.JvmOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.AdminClientNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.ProducerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.ConsumerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.KafkaClientAuthenticationTlsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.KafkaClientAuthenticationOAuthNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.KafkaClientAuthenticationPlainNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.KafkaClientAuthenticationScramSha256NestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.KafkaClientAuthenticationScramSha512NestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.TlsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.HttpNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserAuthorizationSimpleFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserAuthorizationSimpleFluent.AclsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserAuthorizationSimpleFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserAuthorizationSimpleFluentImpl.AclsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserAuthorizationSimpleBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/StrimziPodSetSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/StrimziPodSetSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/StrimziPodSetSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/EntityOperatorTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/EntityOperatorTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/EntityOperatorTemplateFluent.TlsSidecarContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/EntityOperatorTemplateFluent.UserOperatorContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/EntityOperatorTemplateFluent.TopicOperatorContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/EntityOperatorTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/EntityOperatorTemplateFluent.DeploymentNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/EntityOperatorTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/EntityOperatorTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/EntityOperatorTemplateFluentImpl.TlsSidecarContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/EntityOperatorTemplateFluentImpl.UserOperatorContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/EntityOperatorTemplateFluentImpl.TopicOperatorContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/EntityOperatorTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/EntityOperatorTemplateFluentImpl.DeploymentNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/EntityOperatorTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/PodTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/PodTemplateFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/PodTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/PodTemplateFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/PodTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/PodDisruptionBudgetTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/PodDisruptionBudgetTemplateFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/PodDisruptionBudgetTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/PodDisruptionBudgetTemplateFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/PodDisruptionBudgetTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransOutputDefinitionTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransOutputDefinitionTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransOutputDefinitionTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/StatefulSetTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/StatefulSetTemplateFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/StatefulSetTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/StatefulSetTemplateFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/StatefulSetTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateFluent.TlsSidecarContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateFluent.CruiseControlContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateFluent.PodDisruptionBudgetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateFluent.ApiServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateFluent.DeploymentNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateFluentImpl.TlsSidecarContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateFluentImpl.CruiseControlContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateFluentImpl.PodDisruptionBudgetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateFluentImpl.ApiServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateFluentImpl.DeploymentNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/MetadataTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/MetadataTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/MetadataTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaExporterTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaExporterTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaExporterTemplateFluent.ContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaExporterTemplateFluent.ServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaExporterTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaExporterTemplateFluent.DeploymentNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaExporterTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaExporterTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaExporterTemplateFluentImpl.ContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaExporterTemplateFluentImpl.ServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaExporterTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaExporterTemplateFluentImpl.DeploymentNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaExporterTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.JmxSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.ClusterRoleBindingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.InitContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.KafkaContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.PodDisruptionBudgetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.ClusterCaCertNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.PersistentVolumeClaimNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.PerPodIngressNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.ExternalBootstrapIngressNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.PerPodRouteNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.ExternalBootstrapRouteNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.PerPodServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.ExternalBootstrapServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.BrokersServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.BootstrapServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.PodSetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.StatefulsetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.JmxSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.ClusterRoleBindingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.InitContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.KafkaContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.PodDisruptionBudgetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.ClusterCaCertNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.PersistentVolumeClaimNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.PerPodIngressNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.ExternalBootstrapIngressNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.PerPodRouteNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.ExternalBootstrapRouteNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.PerPodServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.ExternalBootstrapServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.BrokersServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.BootstrapServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.PodSetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.StatefulsetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMakerTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMakerTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMakerTemplateFluent.MirrorMakerContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMakerTemplateFluent.PodDisruptionBudgetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMakerTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMakerTemplateFluent.DeploymentNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMakerTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMakerTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMakerTemplateFluentImpl.MirrorMakerContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMakerTemplateFluentImpl.PodDisruptionBudgetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMakerTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMakerTemplateFluentImpl.DeploymentNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMakerTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ContainerTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ContainerTemplateFluent.EnvNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ContainerTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ContainerTemplateFluentImpl.EnvNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ContainerTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluent.JmxSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluent.BuildServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluent.ClusterRoleBindingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluent.BuildConfigNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluent.BuildContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluent.InitContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluent.ConnectContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluent.PodDisruptionBudgetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluent.ApiServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluent.BuildPodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluent.DeploymentNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluentImpl.JmxSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluentImpl.BuildServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluentImpl.ClusterRoleBindingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluentImpl.BuildConfigNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluentImpl.BuildContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluentImpl.InitContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluentImpl.ConnectContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluentImpl.PodDisruptionBudgetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluentImpl.ApiServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluentImpl.BuildPodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluentImpl.DeploymentNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaUserTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaUserTemplateFluent.SecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaUserTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaUserTemplateFluentImpl.SecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaUserTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransQueryTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransQueryTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransQueryTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaBridgeTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaBridgeTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaBridgeTemplateFluent.BridgeContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaBridgeTemplateFluent.PodDisruptionBudgetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaBridgeTemplateFluent.ApiServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaBridgeTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaBridgeTemplateFluent.DeploymentNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaBridgeTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaBridgeTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaBridgeTemplateFluentImpl.BridgeContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaBridgeTemplateFluentImpl.PodDisruptionBudgetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaBridgeTemplateFluentImpl.ApiServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaBridgeTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaBridgeTemplateFluentImpl.DeploymentNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaBridgeTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/InternalServiceTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/InternalServiceTemplateFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/InternalServiceTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/InternalServiceTemplateFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/InternalServiceTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ResourceTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ResourceTemplateFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ResourceTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ResourceTemplateFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ResourceTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransTemplateFluent.ContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransTemplateFluent.DeploymentNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransTemplateFluentImpl.ContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransTemplateFluentImpl.DeploymentNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluent.JmxSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluent.ZookeeperContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluent.PodDisruptionBudgetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluent.PersistentVolumeClaimNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluent.NodesServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluent.ClientServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluent.PodSetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluent.StatefulsetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluentImpl.JmxSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluentImpl.ZookeeperContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluentImpl.PodDisruptionBudgetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluentImpl.PersistentVolumeClaimNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluentImpl.NodesServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluentImpl.ClientServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluentImpl.PodSetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluentImpl.StatefulsetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/DeploymentTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/DeploymentTemplateFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/DeploymentTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/DeploymentTemplateFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/DeploymentTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/BuildConfigTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/BuildConfigTemplateFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/BuildConfigTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/BuildConfigTemplateFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/BuildConfigTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateFluent.JmxSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateFluent.MirrorMaker2ContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateFluent.PodDisruptionBudgetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateFluent.ApiServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateFluent.DeploymentNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateFluentImpl.JmxSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateFluentImpl.MirrorMaker2ContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateFluentImpl.PodDisruptionBudgetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateFluentImpl.ApiServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateFluentImpl.DeploymentNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserSpecFluent.QuotasNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserSpecFluent.KafkaUserAuthorizationSimpleNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserSpecFluent.KafkaUserScramSha512ClientAuthenticationNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserSpecFluent.KafkaUserTlsExternalClientAuthenticationNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserSpecFluent.KafkaUserTlsClientAuthenticationNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserSpecFluentImpl.QuotasNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserSpecFluentImpl.KafkaUserAuthorizationSimpleNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserSpecFluentImpl.KafkaUserScramSha512ClientAuthenticationNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserSpecFluentImpl.KafkaUserTlsExternalClientAuthenticationNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserSpecFluentImpl.KafkaUserTlsClientAuthenticationNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaJmxOptionsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaJmxOptionsFluent.KafkaJmxAuthenticationPasswordNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaJmxOptionsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaJmxOptionsFluentImpl.KafkaJmxAuthenticationPasswordNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaJmxOptionsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/JbodStorageFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/JbodStorageFluent.EphemeralStorageVolumesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/JbodStorageFluent.PersistentClaimStorageVolumesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/JbodStorageFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/JbodStorageFluentImpl.EphemeralStorageVolumesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/JbodStorageFluentImpl.PersistentClaimStorageVolumesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/JbodStorageBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/PersistentClaimStorageOverrideFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/PersistentClaimStorageOverrideFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/PersistentClaimStorageOverrideBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/EphemeralStorageFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/EphemeralStorageFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/EphemeralStorageBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/PersistentClaimStorageFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/PersistentClaimStorageFluent.OverridesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/PersistentClaimStorageFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/PersistentClaimStorageFluentImpl.OverridesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/PersistentClaimStorageBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/RackFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/RackFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/RackBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/TlsClientAuthenticationFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/TlsClientAuthenticationFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/TlsClientAuthenticationBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleFluent.AclRuleClusterResourceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleFluent.AclRuleTransactionalIdResourceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleFluent.AclRuleTopicResourceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleFluent.AclRuleGroupResourceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleFluentImpl.AclRuleClusterResourceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleFluentImpl.AclRuleTransactionalIdResourceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleFluentImpl.AclRuleTopicResourceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleFluentImpl.AclRuleGroupResourceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2Fluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2Fluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2Fluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2Fluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2FluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2FluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2FluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2FluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ClientTlsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ClientTlsFluent.TrustedCertificatesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ClientTlsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ClientTlsFluentImpl.TrustedCertificatesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ClientTlsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectorSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectorSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectorSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityUserOperatorSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityUserOperatorSpecFluent.JvmOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityUserOperatorSpecFluent.InlineLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityUserOperatorSpecFluent.ExternalLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityUserOperatorSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityUserOperatorSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityUserOperatorSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityUserOperatorSpecFluentImpl.JvmOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityUserOperatorSpecFluentImpl.InlineLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityUserOperatorSpecFluentImpl.ExternalLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityUserOperatorSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityUserOperatorSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityUserOperatorSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaTopicFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaTopicFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaTopicFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaTopicFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaTopicFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaTopicFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaTopicFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaTopicFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaTopicBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserTlsClientAuthenticationFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserTlsClientAuthenticationFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserTlsClientAuthenticationBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeHttpConfigFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeHttpConfigFluent.CorsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeHttpConfigFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeHttpConfigFluentImpl.CorsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeHttpConfigBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxPrometheusExporterMetricsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxPrometheusExporterMetricsFluent.ValueFromNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxPrometheusExporterMetricsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxPrometheusExporterMetricsFluentImpl.ValueFromNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxPrometheusExporterMetricsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/SystemPropertyFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/SystemPropertyFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/SystemPropertyBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaTopicSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaTopicSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaTopicSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluent.JmxPrometheusExporterMetricsConfigNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluent.BrokerCapacityNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluent.InlineLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluent.ExternalLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluent.JvmOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluent.TlsSidecarNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluentImpl.JmxPrometheusExporterMetricsConfigNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluentImpl.BrokerCapacityNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluentImpl.InlineLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluentImpl.ExternalLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluentImpl.JvmOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluentImpl.TlsSidecarNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserTlsExternalClientAuthenticationFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserTlsExternalClientAuthenticationFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserTlsExternalClientAuthenticationBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluent.JmxPrometheusExporterMetricsConfigNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluent.JmxOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluent.JvmOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluent.InlineLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluent.ExternalLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluent.EphemeralStorageNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluent.PersistentClaimStorageNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluentImpl.JmxPrometheusExporterMetricsConfigNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluentImpl.JmxOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluentImpl.JvmOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluentImpl.InlineLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluentImpl.ExternalLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluentImpl.EphemeralStorageNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluentImpl.PersistentClaimStorageNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/StrimziPodSetFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/StrimziPodSetFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/StrimziPodSetFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/StrimziPodSetFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/StrimziPodSetFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/StrimziPodSetFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/StrimziPodSetFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/StrimziPodSetFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/StrimziPodSetBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ConnectorSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ConnectorSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ConnectorSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/balancing/class-use/BrokerCapacityFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/balancing/class-use/BrokerCapacityFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/balancing/class-use/BrokerCapacityBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/PasswordSourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/tracing/class-use/JaegerTracingFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/tracing/class-use/JaegerTracingFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/tracing/class-use/JaegerTracingBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/PasswordSourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/PasswordSourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaRebalanceSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaRebalanceSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaRebalanceSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluent.ExternalConfigurationNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluent.JaegerTracingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluent.JmxPrometheusExporterMetricsConfigNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluent.JvmOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluent.JmxOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluent.InlineLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluent.ExternalLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluentImpl.ExternalConfigurationNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluentImpl.JaegerTracingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluentImpl.JmxPrometheusExporterMetricsConfigNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluentImpl.JvmOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluentImpl.JmxOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluentImpl.InlineLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluentImpl.ExternalLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaJmxAuthenticationPasswordFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaJmxAuthenticationPasswordFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaJmxAuthenticationPasswordBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2MirrorSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2MirrorSpecFluent.HeartbeatConnectorNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2MirrorSpecFluent.CheckpointConnectorNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2MirrorSpecFluent.SourceConnectorNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2MirrorSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2MirrorSpecFluentImpl.HeartbeatConnectorNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2MirrorSpecFluentImpl.CheckpointConnectorNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2MirrorSpecFluentImpl.SourceConnectorNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2MirrorSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationScramSha256Fluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationScramSha256Fluent.PasswordSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationScramSha256FluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationScramSha256FluentImpl.PasswordSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationScramSha256Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationOAuthFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationOAuthFluent.TlsTrustedCertificatesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationOAuthFluent.RefreshTokenNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationOAuthFluent.AccessTokenNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationOAuthFluent.ClientSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationOAuthFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationOAuthFluentImpl.TlsTrustedCertificatesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationOAuthFluentImpl.RefreshTokenNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationOAuthFluentImpl.AccessTokenNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationOAuthFluentImpl.ClientSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationOAuthBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationScramSha512Fluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationScramSha512Fluent.PasswordSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationScramSha512FluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationScramSha512FluentImpl.PasswordSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationScramSha512Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationPlainFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationPlainFluent.PasswordSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationPlainFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationPlainFluentImpl.PasswordSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationPlainBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationTlsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationTlsFluent.CertificateAndKeyNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationTlsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationTlsFluentImpl.CertificateAndKeyNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationTlsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/TlsSidecarFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/TlsSidecarFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/TlsSidecarFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/TlsSidecarFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/TlsSidecarFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/TlsSidecarFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/TlsSidecarBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CertificateAuthorityFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CertificateAuthorityFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CertificateAuthorityBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeConsumerSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeConsumerSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeConsumerSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluent.BuildNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluent.KafkaClientAuthenticationTlsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluent.KafkaClientAuthenticationOAuthNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluent.KafkaClientAuthenticationPlainNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluent.KafkaClientAuthenticationScramSha256Nested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluent.KafkaClientAuthenticationScramSha512Nested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluent.TlsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluent.RackNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluentImpl.BuildNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluentImpl.KafkaClientAuthenticationTlsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluentImpl.KafkaClientAuthenticationOAuthNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluentImpl.KafkaClientAuthenticationPlainNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluentImpl.KafkaClientAuthenticationScramSha256NestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluentImpl.KafkaClientAuthenticationScramSha512NestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluentImpl.TlsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluentImpl.RackNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleTopicResourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleTopicResourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleTopicResourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleGroupResourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleGroupResourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleGroupResourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractConnectorSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractConnectorSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityOperatorSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityOperatorSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityOperatorSpecFluent.TlsSidecarNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityOperatorSpecFluent.UserOperatorNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityOperatorSpecFluent.TopicOperatorNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityOperatorSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityOperatorSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityOperatorSpecFluentImpl.TlsSidecarNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityOperatorSpecFluentImpl.UserOperatorNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityOperatorSpecFluentImpl.TopicOperatorNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityOperatorSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeHttpCorsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeHttpCorsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeHttpCorsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityTopicOperatorSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityTopicOperatorSpecFluent.JvmOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityTopicOperatorSpecFluent.InlineLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityTopicOperatorSpecFluent.ExternalLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityTopicOperatorSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityTopicOperatorSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityTopicOperatorSpecFluent.StartupProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityTopicOperatorSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityTopicOperatorSpecFluentImpl.JvmOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityTopicOperatorSpecFluentImpl.InlineLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityTopicOperatorSpecFluentImpl.ExternalLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityTopicOperatorSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityTopicOperatorSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityTopicOperatorSpecFluentImpl.StartupProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityTopicOperatorSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/SidecarFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/SidecarFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/SidecarBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JvmOptionsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JvmOptionsFluent.JavaSystemPropertiesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JvmOptionsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JvmOptionsFluentImpl.JavaSystemPropertiesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JvmOptionsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaRebalanceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaRebalanceFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaRebalanceFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaRebalanceFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaRebalanceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaRebalanceFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaRebalanceFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaRebalanceFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaRebalanceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ClusterSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ClusterSpecFluent.KafkaClientAuthenticationTlsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ClusterSpecFluent.KafkaClientAuthenticationOAuthNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ClusterSpecFluent.KafkaClientAuthenticationPlainNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ClusterSpecFluent.KafkaClientAuthenticationScramSha256Nested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ClusterSpecFluent.KafkaClientAuthenticationScramSha512Nested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ClusterSpecFluent.TlsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ClusterSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ClusterSpecFluentImpl.KafkaClientAuthenticationTlsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ClusterSpecFluentImpl.KafkaClientAuthenticationOAuthNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ClusterSpecFluentImpl.KafkaClientAuthenticationPlainNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ClusterSpecFluentImpl.KafkaClientAuthenticationScramSha256NestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ClusterSpecFluentImpl.KafkaClientAuthenticationScramSha512NestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ClusterSpecFluentImpl.TlsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ClusterSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationOpaFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationOpaFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationOpaBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeProducerSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeProducerSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeProducerSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ExternalLoggingFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ExternalLoggingFluent.ValueFromNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ExternalLoggingFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ExternalLoggingFluentImpl.ValueFromNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ExternalLoggingBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/PasswordSecretSourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/PasswordSecretSourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/PasswordSecretSourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationCustomFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationCustomFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationCustomBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/PasswordFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/PasswordFluent.ValueFromNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/PasswordFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/PasswordFluentImpl.ValueFromNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/PasswordBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerClientSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerClientSpecFluent.KafkaClientAuthenticationTlsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerClientSpecFluent.KafkaClientAuthenticationOAuthNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerClientSpecFluent.KafkaClientAuthenticationPlainNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerClientSpecFluent.KafkaClientAuthenticationScramSha256Nested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerClientSpecFluent.KafkaClientAuthenticationScramSha512Nested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerClientSpecFluent.TlsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerClientSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerClientSpecFluentImpl.KafkaClientAuthenticationTlsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerClientSpecFluentImpl.KafkaClientAuthenticationOAuthNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerClientSpecFluentImpl.KafkaClientAuthenticationPlainNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerClientSpecFluentImpl.KafkaClientAuthenticationScramSha256NestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerClientSpecFluentImpl.KafkaClientAuthenticationScramSha512NestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerClientSpecFluentImpl.TlsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerClientSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserScramSha512ClientAuthenticationFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserScramSha512ClientAuthenticationFluent.PasswordNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserScramSha512ClientAuthenticationFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserScramSha512ClientAuthenticationFluentImpl.PasswordNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserScramSha512ClientAuthenticationBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CertAndKeySecretSourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CertAndKeySecretSourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CertAndKeySecretSourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerConsumerSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluent.ClientsCaNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluent.CruiseControlNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluent.KafkaExporterNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluent.JmxTransNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluent.ClusterCaNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluent.EntityOperatorNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluent.ZookeeperNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluent.KafkaNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerConsumerSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerConsumerSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CertSecretSourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CertSecretSourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CertSecretSourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluentImpl.ClientsCaNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluentImpl.CruiseControlNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluentImpl.KafkaExporterNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluentImpl.JmxTransNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluentImpl.ClusterCaNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluentImpl.EntityOperatorNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluentImpl.ZookeeperNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluentImpl.KafkaNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/InlineLoggingFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/InlineLoggingFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/InlineLoggingBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserQuotasFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserQuotasFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserQuotasBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ExternalConfigurationReferenceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ExternalConfigurationReferenceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ExternalConfigurationReferenceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationSimpleFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationSimpleFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationSimpleBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleTransactionalIdResourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleTransactionalIdResourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleTransactionalIdResourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerProducerSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerProducerSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerProducerSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/GenericSecretSourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/GenericSecretSourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/GenericSecretSourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxTransSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxTransSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxTransSpecFluent.KafkaQueriesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxTransSpecFluent.OutputDefinitionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxTransSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxTransSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxTransSpecFluentImpl.KafkaQueriesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxTransSpecFluentImpl.OutputDefinitionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxTransSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.KafkaAuthorizationSimpleNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.KafkaAuthorizationKeycloakNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.KafkaAuthorizationCustomNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.KafkaAuthorizationOpaNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.ListenersNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.JmxPrometheusExporterMetricsConfigNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.JmxOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.JvmOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.InlineLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.ExternalLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.RackNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.JbodStorageNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.EphemeralStorageNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.PersistentClaimStorageNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.KafkaAuthorizationSimpleNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.KafkaAuthorizationKeycloakNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.KafkaAuthorizationCustomNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.KafkaAuthorizationOpaNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.ListenersNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.JmxPrometheusExporterMetricsConfigNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.JmxOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.JvmOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.InlineLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.ExternalLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.RackNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.JbodStorageNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.EphemeralStorageNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.PersistentClaimStorageNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectorFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectorFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectorFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectorFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectorFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectorFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectorFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectorFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectorBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/SpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/SpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeClientSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeClientSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ContainerEnvVarFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ContainerEnvVarFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ContainerEnvVarBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationKeycloakFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationKeycloakFluent.TlsTrustedCertificatesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationKeycloakFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationKeycloakFluentImpl.TlsTrustedCertificatesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationKeycloakBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeAdminClientSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeAdminClientSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeAdminClientSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaExporterSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaExporterSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaExporterSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaExporterSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaExporterSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaExporterSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaExporterSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaExporterSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaExporterSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleClusterResourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleClusterResourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleClusterResourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluent.JaegerTracingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluent.JmxPrometheusExporterMetricsConfigNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluent.InlineLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluent.ExternalLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluent.JvmOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluent.ProducerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluent.ConsumerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluentImpl.JaegerTracingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluentImpl.JmxPrometheusExporterMetricsConfigNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluentImpl.InlineLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluentImpl.ExternalLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluentImpl.JvmOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluentImpl.ProducerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluentImpl.ConsumerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/balancing/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/tracing/package-use.html...
Building index for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/overview-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/index-all.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/allclasses-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/allpackages-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/deprecated-list.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/index.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/overview-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/help-doc.html...
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/api/target/api-0.29.0-SNAPSHOT-javadoc.jar
[INFO] 
[INFO] --- maven-jar-plugin:3.1.0:test-jar (default) @ api ---
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:integration-test (default) @ api ---
[WARNING] useSystemClassLoader setting has no effect when not forking
[WARNING] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:verify (default) @ api ---
[INFO] 
[INFO] --- maven-dependency-plugin:3.1.1:analyze-only (analyze) @ api ---
[INFO] No dependency problems found
[INFO] 
[INFO] ------------------------< io.strimzi:mockkube >-------------------------
[INFO] Building mockkube 0.29.0-SNAPSHOT                                 [6/10]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ mockkube ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ mockkube ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ mockkube ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/mockkube/target/jacoco.exec
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ mockkube ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/cloud-user/strimzi-kafka-operator/mockkube/src/main/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:compile (default-compile) @ mockkube ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ mockkube ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 1 resource
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:testCompile (default-testCompile) @ mockkube ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:3.0.0-M5:test (default-test) @ mockkube ---
[INFO] 
[INFO] -------------------------------------------------------
[INFO]  T E S T S
[INFO] -------------------------------------------------------
[INFO] 
[INFO] Results:
[INFO] 
[INFO] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[INFO] 
[INFO] 
[INFO] --- maven-jar-plugin:3.1.0:jar (default-jar) @ mockkube ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:report (report) @ mockkube ---
[INFO] Skipping JaCoCo execution due to missing execution data file.
[INFO] 
[INFO] >>> maven-source-plugin:3.0.1:jar (attach-sources) > generate-sources @ mockkube >>>
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ mockkube ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ mockkube ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ mockkube ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/mockkube/target/jacoco.exec
[INFO] 
[INFO] <<< maven-source-plugin:3.0.1:jar (attach-sources) < generate-sources @ mockkube <<<
[INFO] 
[INFO] 
[INFO] --- maven-source-plugin:3.0.1:jar (attach-sources) @ mockkube ---
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/mockkube/target/mockkube-0.29.0-SNAPSHOT-sources.jar
[INFO] 
[INFO] --- maven-javadoc-plugin:3.1.0:jar (attach-javadocs) @ mockkube ---
[INFO] 
Loading source file /home/cloud-user/strimzi-kafka-operator/mockkube/src/main/java/io/strimzi/test/mockkube/Observer.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/mockkube/src/main/java/io/strimzi/test/mockkube/PredicatedWatcher.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/mockkube/src/main/java/io/strimzi/test/mockkube/CustomResourceMockBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/mockkube/src/main/java/io/strimzi/test/mockkube/ServiceMockBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/mockkube/src/main/java/io/strimzi/test/mockkube/DeploymentMockBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/mockkube/src/main/java/io/strimzi/test/mockkube/MockBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/mockkube/src/main/java/io/strimzi/test/mockkube/MockKube.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/mockkube/src/main/java/io/strimzi/test/mockkube/StatefulSetMockBuilder.java...
Constructing Javadoc information...
Standard Doclet version 11.0.11
Building tree for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/io/strimzi/test/mockkube/MockKube.html...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/io/strimzi/test/mockkube/MockKube.MockedCrd.html...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/io/strimzi/test/mockkube/Observer.html...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/io/strimzi/test/mockkube/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/io/strimzi/test/mockkube/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/constant-values.html...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/io/strimzi/test/mockkube/class-use/Observer.html...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/io/strimzi/test/mockkube/class-use/MockKube.html...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/io/strimzi/test/mockkube/class-use/MockKube.MockedCrd.html...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/io/strimzi/test/mockkube/package-use.html...
Building index for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/overview-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/index-all.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/allclasses-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/allpackages-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/deprecated-list.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/index.html...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/help-doc.html...
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/mockkube/target/mockkube-0.29.0-SNAPSHOT-javadoc.jar
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:integration-test (default) @ mockkube ---
[WARNING] useSystemClassLoader setting has no effect when not forking
[WARNING] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:verify (default) @ mockkube ---
[INFO] 
[INFO] --- maven-dependency-plugin:3.1.1:analyze-only (analyze) @ mockkube ---
[INFO] No dependency problems found
[INFO] 
[INFO] ----------------------< io.strimzi:config-model >-----------------------
[INFO] Building config-model 0.29.0-SNAPSHOT                             [7/10]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ config-model ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ config-model ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ config-model ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/config-model/target/jacoco.exec
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ config-model ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/cloud-user/strimzi-kafka-operator/config-model/src/main/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:compile (default-compile) @ config-model ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ config-model ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/cloud-user/strimzi-kafka-operator/config-model/src/test/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:testCompile (default-testCompile) @ config-model ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:3.0.0-M5:test (default-test) @ config-model ---
[INFO] 
[INFO] -------------------------------------------------------
[INFO]  T E S T S
[INFO] -------------------------------------------------------
[INFO] 
[INFO] Results:
[INFO] 
[INFO] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[INFO] 
[INFO] 
[INFO] --- maven-jar-plugin:3.1.0:jar (default-jar) @ config-model ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:report (report) @ config-model ---
[INFO] Skipping JaCoCo execution due to missing execution data file.
[INFO] 
[INFO] >>> maven-source-plugin:3.0.1:jar (attach-sources) > generate-sources @ config-model >>>
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ config-model ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ config-model ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ config-model ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/config-model/target/jacoco.exec
[INFO] 
[INFO] <<< maven-source-plugin:3.0.1:jar (attach-sources) < generate-sources @ config-model <<<
[INFO] 
[INFO] 
[INFO] --- maven-source-plugin:3.0.1:jar (attach-sources) @ config-model ---
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/config-model/target/config-model-0.29.0-SNAPSHOT-sources.jar
[INFO] 
[INFO] --- maven-javadoc-plugin:3.1.0:jar (attach-javadocs) @ config-model ---
[INFO] 
Loading source file /home/cloud-user/strimzi-kafka-operator/config-model/src/main/java/io/strimzi/kafka/config/model/ConfigModel.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/config-model/src/main/java/io/strimzi/kafka/config/model/ConfigModels.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/config-model/src/main/java/io/strimzi/kafka/config/model/Scope.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/config-model/src/main/java/io/strimzi/kafka/config/model/Type.java...
Constructing Javadoc information...
Standard Doclet version 11.0.11
Building tree for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/io/strimzi/kafka/config/model/ConfigModel.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/io/strimzi/kafka/config/model/ConfigModels.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/io/strimzi/kafka/config/model/Scope.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/io/strimzi/kafka/config/model/Type.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/io/strimzi/kafka/config/model/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/io/strimzi/kafka/config/model/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/constant-values.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/io/strimzi/kafka/config/model/class-use/ConfigModel.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/io/strimzi/kafka/config/model/class-use/ConfigModels.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/io/strimzi/kafka/config/model/class-use/Scope.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/io/strimzi/kafka/config/model/class-use/Type.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/io/strimzi/kafka/config/model/package-use.html...
Building index for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/overview-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/index-all.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/allclasses-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/allpackages-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/deprecated-list.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/index.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/help-doc.html...
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/config-model/target/config-model-0.29.0-SNAPSHOT-javadoc.jar
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:integration-test (default) @ config-model ---
[WARNING] useSystemClassLoader setting has no effect when not forking
[WARNING] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:verify (default) @ config-model ---
[INFO] 
[INFO] --- maven-dependency-plugin:3.1.1:analyze-only (analyze) @ config-model ---
[INFO] No dependency problems found
[INFO] 
[INFO] -------------------< io.strimzi:certificate-manager >-------------------
[INFO] Building certificate-manager 0.29.0-SNAPSHOT                      [8/10]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ certificate-manager ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ certificate-manager ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ certificate-manager ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/certificate-manager/target/jacoco.exec
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ certificate-manager ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 1 resource
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:compile (default-compile) @ certificate-manager ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ certificate-manager ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 1 resource
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:testCompile (default-testCompile) @ certificate-manager ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:3.0.0-M5:test (default-test) @ certificate-manager ---
[INFO] 
[INFO] -------------------------------------------------------
[INFO]  T E S T S
[INFO] -------------------------------------------------------
[INFO] 
[INFO] Results:
[INFO] 
[INFO] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[INFO] 
[INFO] 
[INFO] --- maven-jar-plugin:3.1.0:jar (default-jar) @ certificate-manager ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:report (report) @ certificate-manager ---
[INFO] Skipping JaCoCo execution due to missing execution data file.
[INFO] 
[INFO] >>> maven-source-plugin:3.0.1:jar (attach-sources) > generate-sources @ certificate-manager >>>
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ certificate-manager ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ certificate-manager ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ certificate-manager ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/certificate-manager/target/jacoco.exec
[INFO] 
[INFO] <<< maven-source-plugin:3.0.1:jar (attach-sources) < generate-sources @ certificate-manager <<<
[INFO] 
[INFO] 
[INFO] --- maven-source-plugin:3.0.1:jar (attach-sources) @ certificate-manager ---
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/certificate-manager-0.29.0-SNAPSHOT-sources.jar
[INFO] 
[INFO] --- maven-javadoc-plugin:3.1.0:jar (attach-javadocs) @ certificate-manager ---
[INFO] 
Loading source file /home/cloud-user/strimzi-kafka-operator/certificate-manager/src/main/java/io/strimzi/certs/CertAndKey.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/certificate-manager/src/main/java/io/strimzi/certs/CertManager.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/certificate-manager/src/main/java/io/strimzi/certs/OpenSslCertManager.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/certificate-manager/src/main/java/io/strimzi/certs/SecretCertProvider.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/certificate-manager/src/main/java/io/strimzi/certs/Subject.java...
Constructing Javadoc information...
Standard Doclet version 11.0.11
Building tree for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/io/strimzi/certs/CertAndKey.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/io/strimzi/certs/CertManager.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/io/strimzi/certs/OpenSslCertManager.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/io/strimzi/certs/SecretCertProvider.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/io/strimzi/certs/Subject.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/io/strimzi/certs/Subject.Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/io/strimzi/certs/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/io/strimzi/certs/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/constant-values.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/io/strimzi/certs/class-use/CertAndKey.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/io/strimzi/certs/class-use/CertManager.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/io/strimzi/certs/class-use/OpenSslCertManager.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/io/strimzi/certs/class-use/SecretCertProvider.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/io/strimzi/certs/class-use/Subject.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/io/strimzi/certs/class-use/Subject.Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/io/strimzi/certs/package-use.html...
Building index for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/overview-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/index-all.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/allclasses-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/allpackages-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/deprecated-list.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/index.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/help-doc.html...
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/certificate-manager-0.29.0-SNAPSHOT-javadoc.jar
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:integration-test (default) @ certificate-manager ---
[WARNING] useSystemClassLoader setting has no effect when not forking
[WARNING] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:verify (default) @ certificate-manager ---
[INFO] 
[INFO] --- maven-dependency-plugin:3.1.1:analyze-only (analyze) @ certificate-manager ---
[INFO] No dependency problems found
[INFO] 
[INFO] ---------------------< io.strimzi:operator-common >---------------------
[INFO] Building operator-common 0.29.0-SNAPSHOT                          [9/10]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ operator-common ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ operator-common ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ operator-common ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/operator-common/target/jacoco.exec
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ operator-common ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:compile (default-compile) @ operator-common ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ operator-common ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 9 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:testCompile (default-testCompile) @ operator-common ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:3.0.0-M5:test (default-test) @ operator-common ---
[INFO] 
[INFO] -------------------------------------------------------
[INFO]  T E S T S
[INFO] -------------------------------------------------------
[INFO] 
[INFO] Results:
[INFO] 
[INFO] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[INFO] 
[INFO] 
[INFO] --- maven-jar-plugin:3.1.0:jar (default-jar) @ operator-common ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:report (report) @ operator-common ---
[INFO] Skipping JaCoCo execution due to missing execution data file.
[INFO] 
[INFO] >>> maven-source-plugin:3.0.1:jar (attach-sources) > generate-sources @ operator-common >>>
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ operator-common ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ operator-common ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ operator-common ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/operator-common/target/jacoco.exec
[INFO] 
[INFO] <<< maven-source-plugin:3.0.1:jar (attach-sources) < generate-sources @ operator-common <<<
[INFO] 
[INFO] 
[INFO] --- maven-source-plugin:3.0.1:jar (attach-sources) @ operator-common ---
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/operator-common/target/operator-common-0.29.0-SNAPSHOT-sources.jar
[INFO] 
[INFO] --- maven-javadoc-plugin:3.1.0:jar (attach-javadocs) @ operator-common ---
[INFO] 
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/cluster/model/InvalidResourceException.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/cluster/model/NoSuchResourceException.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/cluster/model/Ca.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/cluster/model/ClientsCa.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/cluster/model/NodeUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/cluster/model/StatusDiff.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/cluster/operator/resource/cruisecontrol/CruiseControlEndpoints.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/cluster/operator/resource/cruisecontrol/CruiseControlUserTaskStatus.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/cluster/operator/resource/cruisecontrol/CruiseControlGoals.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/cluster/operator/resource/cruisecontrol/CruiseControlConfigurationParameters.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/cluster/operator/resource/cruisecontrol/CruiseControlLoadParameters.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/cluster/operator/resource/cruisecontrol/CruiseControlParameters.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/cluster/operator/resource/cruisecontrol/CruiseControlRebalanceKeys.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/AdminClientProvider.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/BackOff.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/InvalidConfigurationException.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/MaxAttemptsExceededException.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/MetricsProvider.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/PasswordGenerator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/model/NamespaceAndName.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/model/Labels.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/model/OrderedProperties.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/model/ResourceVisitor.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/model/ValidationVisitor.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/DeploymentOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/IngressV1Beta1Operator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/PodDisruptionBudgetOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/PvcOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/ResourceSupport.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/ServiceAccountOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/StatusUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/AbstractNonNamespacedResourceOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/AbstractScalableResourceOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/ClusterRoleOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/ReconcileResult.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/CrdOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/TimeoutException.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/AbstractJsonDiff.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/AbstractResourceOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/AbstractWatchableStatusedResourceOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/BuildOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/EndpointOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/ImageStreamOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/NetworkPolicyOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/NoStackTraceTimeoutException.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/RoleBindingOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/RoleOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/SecretOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/BuildConfigOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/ClusterRoleBindingOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/ConfigMapOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/DeploymentConfigOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/IngressOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/NodeOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/PodOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/ResourceDiff.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/RouteOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/ServiceOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/StorageClassOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/AbstractReadyResourceOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/AbstractWatchableResourceOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/PodDisruptionBudgetV1Beta1Operator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/process/ProcessHelper.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/MetricsAndLogging.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/ReconciliationException.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/AbstractOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/Annotations.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/DefaultAdminClientProvider.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/InvalidConfigParameterException.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/MicrometerMetricsProvider.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/Operator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/OperatorWatcher.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/Reconciliation.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/ReconciliationLogger.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/Util.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/KubernetesVersion.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/PlatformFeaturesAvailability.java...
Constructing Javadoc information...
Standard Doclet version 11.0.11
Building tree for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/AbstractOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/AdminClientProvider.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/Annotations.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/BackOff.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/DefaultAdminClientProvider.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/InvalidConfigParameterException.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/InvalidConfigurationException.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/MaxAttemptsExceededException.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/MetricsAndLogging.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/MetricsProvider.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/MicrometerMetricsProvider.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/Operator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/PasswordGenerator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/Reconciliation.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/ReconciliationException.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/ReconciliationLogger.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/Util.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/CruiseControlConfigurationParameters.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/CruiseControlEndpoints.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/CruiseControlGoals.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/CruiseControlLoadParameters.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/CruiseControlParameters.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/CruiseControlRebalanceKeys.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/CruiseControlUserTaskStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/model/Ca.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/model/ClientsCa.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/model/InvalidResourceException.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/model/NodeUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/model/NoSuchResourceException.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/model/StatusDiff.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/Labels.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/NamespaceAndName.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/OrderedProperties.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/ResourceVisitor.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/ResourceVisitor.Property.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/ResourceVisitor.Visitor.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/ValidationVisitor.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/KubernetesVersion.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/PlatformFeaturesAvailability.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/process/ProcessHelper.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/process/ProcessHelper.ProcessResult.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/AbstractJsonDiff.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/AbstractNonNamespacedResourceOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/AbstractReadyResourceOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/AbstractResourceOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/AbstractScalableResourceOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/AbstractWatchableResourceOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/AbstractWatchableStatusedResourceOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/BuildConfigOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/BuildOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/ClusterRoleBindingOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/ClusterRoleOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/ConfigMapOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/CrdOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/DeploymentConfigOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/DeploymentOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/EndpointOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/ImageStreamOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/IngressOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/IngressV1Beta1Operator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/NetworkPolicyOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/NodeOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/NoStackTraceTimeoutException.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/PodDisruptionBudgetOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/PodDisruptionBudgetV1Beta1Operator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/PodOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/PvcOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/ReconcileResult.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/ReconcileResult.Created.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/ReconcileResult.Noop.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/ReconcileResult.Patched.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/ResourceSupport.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/RoleBindingOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/RoleOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/RouteOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/SecretOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/ServiceAccountOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/ServiceOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/StatusUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/StorageClassOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/TimeoutException.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/model/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/model/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/process/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/process/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/constant-values.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/serialized-form.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/model/class-use/InvalidResourceException.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/model/class-use/NoSuchResourceException.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/model/class-use/Ca.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/model/class-use/ClientsCa.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/model/class-use/NodeUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/model/class-use/StatusDiff.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/class-use/CruiseControlEndpoints.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/class-use/CruiseControlUserTaskStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/class-use/CruiseControlGoals.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/class-use/CruiseControlConfigurationParameters.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/class-use/CruiseControlLoadParameters.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/class-use/CruiseControlParameters.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/class-use/CruiseControlRebalanceKeys.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/AdminClientProvider.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/BackOff.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/InvalidConfigurationException.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/MaxAttemptsExceededException.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/MetricsProvider.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/PasswordGenerator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/class-use/NamespaceAndName.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/class-use/Labels.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/class-use/OrderedProperties.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/class-use/ResourceVisitor.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/class-use/ResourceVisitor.Property.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/class-use/ResourceVisitor.Visitor.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/class-use/ValidationVisitor.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/DeploymentOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/IngressV1Beta1Operator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/PodDisruptionBudgetOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/PvcOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/ResourceSupport.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/ServiceAccountOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/StatusUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/AbstractNonNamespacedResourceOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/AbstractScalableResourceOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/ClusterRoleOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/ReconcileResult.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/ReconcileResult.Patched.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/ReconcileResult.Created.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/ReconcileResult.Noop.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/CrdOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/TimeoutException.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/AbstractJsonDiff.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/AbstractResourceOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/AbstractWatchableStatusedResourceOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/BuildOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/EndpointOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/ImageStreamOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/NetworkPolicyOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/NoStackTraceTimeoutException.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/RoleBindingOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/RoleOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/SecretOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/BuildConfigOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/ClusterRoleBindingOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/ConfigMapOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/DeploymentConfigOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/IngressOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/NodeOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/PodOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/RouteOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/ServiceOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/StorageClassOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/AbstractReadyResourceOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/AbstractWatchableResourceOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/PodDisruptionBudgetV1Beta1Operator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/process/class-use/ProcessHelper.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/process/class-use/ProcessHelper.ProcessResult.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/MetricsAndLogging.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/ReconciliationException.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/AbstractOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/Annotations.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/DefaultAdminClientProvider.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/InvalidConfigParameterException.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/MicrometerMetricsProvider.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/Operator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/Reconciliation.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/ReconciliationLogger.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/Util.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/class-use/KubernetesVersion.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/class-use/PlatformFeaturesAvailability.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/model/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/process/package-use.html...
Building index for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/overview-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/index-all.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/allclasses-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/allpackages-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/deprecated-list.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/index.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/overview-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/help-doc.html...
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/operator-common/target/operator-common-0.29.0-SNAPSHOT-javadoc.jar
[INFO] 
[INFO] --- maven-jar-plugin:3.1.0:test-jar (default) @ operator-common ---
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:integration-test (default) @ operator-common ---
[WARNING] useSystemClassLoader setting has no effect when not forking
[WARNING] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:verify (default) @ operator-common ---
[INFO] 
[INFO] --- maven-dependency-plugin:3.1.1:analyze-only (analyze) @ operator-common ---
[INFO] No dependency problems found
[INFO] 
[INFO] -----------------------< io.strimzi:systemtest >------------------------
[INFO] Building systemtest 0.29.0-SNAPSHOT                              [10/10]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ systemtest ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ systemtest ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ systemtest ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/systemtest/target/jacoco.exec
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ systemtest ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 3 resources
[INFO] Copying 2 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:compile (default-compile) @ systemtest ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 147 source files to /home/cloud-user/strimzi-kafka-operator/systemtest/target/classes
[INFO] /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/BaseClientsFluentImpl.java: Some input files use unchecked or unsafe operations.
[INFO] /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/BaseClientsFluentImpl.java: Recompile with -Xlint:unchecked for details.
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ systemtest ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 32 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:testCompile (default-testCompile) @ systemtest ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 67 source files to /home/cloud-user/strimzi-kafka-operator/systemtest/target/test-classes
[INFO] /home/cloud-user/strimzi-kafka-operator/systemtest/src/test/java/io/strimzi/systemtest/security/oauth/OauthPlainIsolatedST.java: Some input files use or override a deprecated API.
[INFO] /home/cloud-user/strimzi-kafka-operator/systemtest/src/test/java/io/strimzi/systemtest/security/oauth/OauthPlainIsolatedST.java: Recompile with -Xlint:deprecation for details.
[INFO] /home/cloud-user/strimzi-kafka-operator/systemtest/src/test/java/io/strimzi/systemtest/operators/CustomResourceStatusIsolatedST.java: /home/cloud-user/strimzi-kafka-operator/systemtest/src/test/java/io/strimzi/systemtest/operators/CustomResourceStatusIsolatedST.java uses unchecked or unsafe operations.
[INFO] /home/cloud-user/strimzi-kafka-operator/systemtest/src/test/java/io/strimzi/systemtest/operators/CustomResourceStatusIsolatedST.java: Recompile with -Xlint:unchecked for details.
[INFO] 
[INFO] --- maven-surefire-plugin:3.0.0-M5:test (default-test) @ systemtest ---
[INFO] 
[INFO] -------------------------------------------------------
[INFO]  T E S T S
[INFO] -------------------------------------------------------
[INFO] 
[INFO] Results:
[INFO] 
[INFO] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[INFO] 
[WARNING] Corrupted STDOUT by directly writing to native stream in forked JVM 1. See FAQ web page and the dump file /home/cloud-user/strimzi-kafka-operator/systemtest/target/surefire-reports/2022-03-28T09-47-21_217-jvmRun1.dumpstream
[INFO] 
[INFO] --- maven-jar-plugin:3.1.0:jar (default-jar) @ systemtest ---
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/systemtest/target/systemtest-0.29.0-SNAPSHOT.jar
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:report (report) @ systemtest ---
[INFO] Skipping JaCoCo execution due to missing execution data file.
[INFO] 
[INFO] >>> maven-source-plugin:3.0.1:jar (attach-sources) > generate-sources @ systemtest >>>
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ systemtest ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ systemtest ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ systemtest ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/systemtest/target/jacoco.exec
[INFO] 
[INFO] <<< maven-source-plugin:3.0.1:jar (attach-sources) < generate-sources @ systemtest <<<
[INFO] 
[INFO] 
[INFO] --- maven-source-plugin:3.0.1:jar (attach-sources) @ systemtest ---
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/systemtest/target/systemtest-0.29.0-SNAPSHOT-sources.jar
[INFO] 
[INFO] --- maven-javadoc-plugin:3.1.0:jar (attach-javadocs) @ systemtest ---
[INFO] 
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/annotations/MultiNodeClusterOnly.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/annotations/MultiNodeClusterOnlyCondition.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/annotations/OpenShiftOnly.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/annotations/RequiredMinKubeApiVersion.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/annotations/RequiredMinKubeApiVersionCondition.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/annotations/IsolatedTest.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/annotations/ParallelNamespaceTest.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/annotations/IsolatedSuite.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/annotations/OpenShiftOnlyCondition.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/annotations/ParallelSuite.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/annotations/ParallelTest.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/annotations/StatefulSetTest.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/annotations/StatefulSetTestCondition.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/annotations/StrimziPodSetTest.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/annotations/StrimziPodSetTestCondition.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/cli/KafkaCmdClient.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/enums/DefaultNetworkPolicy.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/enums/ClusterOperatorInstallType.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/enums/CustomResourceStatus.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/enums/OlmInstallationStrategy.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/enums/ClusterOperatorRBACType.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/enums/DeploymentTypes.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/k8s/Events.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/clientproperties/ProducerProperties.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/clientproperties/AbstractKafkaClientProperties.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/clientproperties/ConsumerProperties.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/AbstractKafkaClient.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/KafkaClientOperations.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/clients/ClientArgument.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/clients/ClientArgumentMap.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/clients/ClientType.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/clients/InternalKafkaClient.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/clients/VerifiableClient.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/externalClients/ExternalKafkaClient.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/internalClients/AdminClientOperations.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/internalClients/BaseClients.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/internalClients/BridgeClients.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/internalClients/KafkaAdminClients.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/internalClients/KafkaClients.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/internalClients/KafkaOauthClients.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/internalClients/KafkaTracingClients.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/keycloak/KeycloakInstance.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/listeners/ExecutionListener.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/listeners/OrderTestSuites.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/logs/LogCollector.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/logs/TestExecutionWatcher.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/matchers/HasAllOfReasons.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/matchers/HasAnyOfReasons.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/matchers/HasNoneOfReasons.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/matchers/Matchers.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/matchers/LogHasNoUnexpectedErrors.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/crd/KafkaBridgeResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/crd/KafkaConnectResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/crd/KafkaConnectorResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/crd/KafkaMirrorMaker2Resource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/crd/KafkaTopicResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/crd/KafkaClientsResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/crd/KafkaMirrorMakerResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/crd/KafkaRebalanceResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/crd/KafkaResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/crd/KafkaUserResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/crd/StrimziPodSetResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/operator/BundleResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/operator/specific/HelmResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/operator/specific/SpecificResourceType.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/operator/specific/OlmResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/operator/SetupClusterOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/ResourceCondition.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/ResourceItem.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/ResourceType.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/ThrowableRunner.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/kubernetes/ServiceResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/kubernetes/ClusterOperatorCustomResourceDefinition.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/kubernetes/ClusterRoleBindingResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/kubernetes/ClusterRoleResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/kubernetes/ConfigMapResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/kubernetes/JobResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/kubernetes/NetworkPolicyResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/kubernetes/RoleBindingResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/kubernetes/RoleResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/kubernetes/SecretResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/kubernetes/ServiceAccountResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/kubernetes/ValidatingWebhookConfigurationResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/kubernetes/DeploymentResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/ComponentType.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/ResourceManager.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/draincleaner/DrainCleanerResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/draincleaner/SetupDrainCleaner.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/ResourceOperation.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/security/CertAndKeyFiles.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/security/SystemTestCertAndKey.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/security/SystemTestCertAndKeyBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/security/SystemTestCertManager.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaBridgeUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaConnectUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaConnectorUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaMirrorMaker2Utils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaMirrorMakerUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaRebalanceUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUserUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaTopicUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kubeUtils/controllers/ConfigMapUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kubeUtils/controllers/DeploymentUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kubeUtils/controllers/JobUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kubeUtils/controllers/StatefulSetUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kubeUtils/controllers/StrimziPodSetUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kubeUtils/objects/NamespaceUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kubeUtils/objects/NodeUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kubeUtils/objects/PersistentVolumeClaimUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kubeUtils/objects/PodUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kubeUtils/objects/SecretUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kubeUtils/objects/ServiceUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/specific/BridgeUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/specific/CruiseControlUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/specific/JmxUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/specific/OlmUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/specific/TracingUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/specific/KeycloakUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/ClientUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/FileUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/HttpUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/RollingUpdateUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/StUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/TestKafkaVersion.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/interfaces/IndicativeSentences.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/templates/crd/KafkaBridgeTemplates.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/templates/crd/KafkaClientsTemplates.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/templates/crd/KafkaConnectTemplates.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/templates/crd/KafkaConnectorTemplates.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/templates/crd/KafkaMirrorMaker2Templates.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/templates/crd/KafkaMirrorMakerTemplates.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/templates/crd/KafkaRebalanceTemplates.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/templates/crd/KafkaTemplates.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/templates/crd/KafkaTopicTemplates.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/templates/crd/KafkaUserTemplates.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/templates/kubernetes/ServiceTemplates.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/templates/kubernetes/ClusterRoleBindingTemplates.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/templates/kubernetes/NetworkPolicyTemplates.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/templates/specific/ScraperTemplates.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/BeforeAllOnce.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/metrics/MetricsCollector.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/parallel/SuiteThreadController.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/parallel/TestSuiteNamespaceManager.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/storage/TestStorage.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/Constants.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/Environment.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/KafkaOauthClientsFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/KafkaOauthClientsFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/KafkaOauthClientsBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/BridgeClientsFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/BridgeClientsFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/BridgeClientsBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/KafkaTracingClientsFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/KafkaTracingClientsFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/KafkaTracingClientsBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/BaseClientsFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/BaseClientsFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/KafkaClientsFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/KafkaClientsFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/KafkaClientsBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/KafkaAdminClientsFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/KafkaAdminClientsFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/KafkaAdminClientsBuilder.java...
Constructing Javadoc information...
Standard Doclet version 11.0.11
Building tree for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/IsolatedSuite.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/IsolatedTest.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/MultiNodeClusterOnly.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/MultiNodeClusterOnlyCondition.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/OpenShiftOnly.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/OpenShiftOnlyCondition.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/ParallelNamespaceTest.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/ParallelSuite.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/ParallelTest.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/RequiredMinKubeApiVersion.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/RequiredMinKubeApiVersionCondition.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/StatefulSetTest.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/StatefulSetTestCondition.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/StrimziPodSetTest.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/StrimziPodSetTestCondition.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/cli/KafkaCmdClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clientproperties/AbstractKafkaClientProperties.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clientproperties/AbstractKafkaClientProperties.KafkaClientPropertiesBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clientproperties/ConsumerProperties.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clientproperties/ConsumerProperties.ConsumerPropertiesBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clientproperties/ProducerProperties.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clientproperties/ProducerProperties.ProducerPropertiesBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/ClientArgument.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/ClientArgumentMap.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/ClientType.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/InternalKafkaClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/InternalKafkaClient.Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/VerifiableClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/VerifiableClient.VerifiableClientBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/controllers/ConfigMapUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/controllers/DeploymentUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/controllers/JobUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/controllers/StatefulSetUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/controllers/StrimziPodSetUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/KafkaBridgeResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/KafkaClientsResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/KafkaConnectorResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/KafkaConnectResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/KafkaMirrorMaker2Resource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/KafkaMirrorMakerResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/KafkaRebalanceResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/KafkaResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/KafkaTopicResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/KafkaUserResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/StrimziPodSetResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/KafkaBridgeTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/KafkaClientsTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/KafkaConnectorTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/KafkaConnectTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/KafkaMirrorMaker2Templates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/KafkaMirrorMakerTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/KafkaRebalanceTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/KafkaTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/KafkaTopicTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/KafkaUserTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/draincleaner/DrainCleanerResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/draincleaner/SetupDrainCleaner.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/enums/ClusterOperatorInstallType.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/enums/ClusterOperatorRBACType.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/enums/CustomResourceStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/enums/DefaultNetworkPolicy.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/enums/DeploymentTypes.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/enums/OlmInstallationStrategy.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/externalClients/ExternalKafkaClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/externalClients/ExternalKafkaClient.Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/interfaces/IndicativeSentences.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/AdminClientOperations.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/BaseClients.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/BaseClientsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/BaseClientsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/BridgeClients.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/BridgeClientsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/BridgeClientsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/BridgeClientsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/KafkaAdminClients.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/KafkaAdminClientsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/KafkaAdminClientsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/KafkaAdminClientsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/KafkaClients.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/KafkaClientsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/KafkaClientsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/KafkaClientsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/KafkaOauthClients.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/KafkaOauthClientsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/KafkaOauthClientsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/KafkaOauthClientsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/KafkaTracingClients.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/KafkaTracingClientsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/KafkaTracingClientsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/KafkaTracingClientsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/k8s/Events.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/AbstractKafkaClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/AbstractKafkaClient.Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/KafkaClientOperations.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/KafkaBridgeUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/KafkaConnectorUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/KafkaConnectUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/KafkaMirrorMaker2Utils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/KafkaMirrorMakerUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/KafkaRebalanceUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/KafkaTopicUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/KafkaUserUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/keycloak/KeycloakInstance.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/ClusterOperatorCustomResourceDefinition.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/ClusterRoleBindingResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/ClusterRoleResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/ConfigMapResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/DeploymentResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/JobResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/NetworkPolicyResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/RoleBindingResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/RoleResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/SecretResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/ServiceAccountResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/ServiceResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/ValidatingWebhookConfigurationResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/kubernetes/ClusterRoleBindingTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/kubernetes/NetworkPolicyTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/kubernetes/ServiceTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/listeners/ExecutionListener.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/listeners/OrderTestSuites.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/logs/LogCollector.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/logs/TestExecutionWatcher.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/matchers/HasAllOfReasons.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/matchers/HasAnyOfReasons.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/matchers/HasNoneOfReasons.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/matchers/LogHasNoUnexpectedErrors.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/matchers/Matchers.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/metrics/MetricsCollector.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/metrics/MetricsCollector.Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/objects/NamespaceUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/objects/NodeUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/objects/PersistentVolumeClaimUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/objects/PodUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/objects/SecretUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/objects/ServiceUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/BundleResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/BundleResource.BundleResourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/SetupClusterOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/SetupClusterOperator.SetupClusterOperatorBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/parallel/SuiteThreadController.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/parallel/TestSuiteNamespaceManager.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/ComponentType.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/ResourceCondition.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/ResourceItem.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/ResourceManager.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/ResourceOperation.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/ResourceType.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/ThrowableRunner.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/security/CertAndKeyFiles.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/security/SystemTestCertAndKey.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/security/SystemTestCertAndKeyBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/security/SystemTestCertManager.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/specific/HelmResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/specific/OlmResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/specific/SpecificResourceType.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/specific/ScraperTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/BridgeUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/CruiseControlUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/CruiseControlUtils.SupportedHttpMethods.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/CruiseControlUtils.SupportedSchemes.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/JmxUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/KeycloakUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/OlmUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/TracingUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/storage/TestStorage.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/BeforeAllOnce.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/Constants.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/Environment.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/ClientUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/FileUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/HttpUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/RollingUpdateUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/StUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/TestKafkaVersion.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/cli/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/cli/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/enums/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/enums/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/interfaces/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/interfaces/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/k8s/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/k8s/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clientproperties/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clientproperties/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/externalClients/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/externalClients/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/keycloak/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/keycloak/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/listeners/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/listeners/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/logs/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/logs/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/matchers/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/matchers/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/metrics/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/metrics/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/parallel/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/parallel/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/draincleaner/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/draincleaner/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/specific/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/specific/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/security/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/security/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/storage/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/storage/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/kubernetes/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/kubernetes/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/specific/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/specific/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/controllers/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/controllers/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/objects/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/objects/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/constant-values.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/class-use/MultiNodeClusterOnly.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/class-use/MultiNodeClusterOnlyCondition.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/class-use/OpenShiftOnly.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/class-use/RequiredMinKubeApiVersion.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/class-use/RequiredMinKubeApiVersionCondition.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/class-use/IsolatedTest.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/class-use/ParallelNamespaceTest.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/class-use/IsolatedSuite.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/class-use/OpenShiftOnlyCondition.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/class-use/ParallelSuite.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/class-use/ParallelTest.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/class-use/StatefulSetTest.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/class-use/StatefulSetTestCondition.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/class-use/StrimziPodSetTest.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/class-use/StrimziPodSetTestCondition.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/cli/class-use/KafkaCmdClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/enums/class-use/DefaultNetworkPolicy.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/enums/class-use/ClusterOperatorInstallType.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/enums/class-use/CustomResourceStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/enums/class-use/OlmInstallationStrategy.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/enums/class-use/ClusterOperatorRBACType.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/enums/class-use/DeploymentTypes.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/k8s/class-use/Events.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clientproperties/class-use/ProducerProperties.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clientproperties/class-use/ProducerProperties.ProducerPropertiesBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clientproperties/class-use/AbstractKafkaClientProperties.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clientproperties/class-use/AbstractKafkaClientProperties.KafkaClientPropertiesBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clientproperties/class-use/ConsumerProperties.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clientproperties/class-use/ConsumerProperties.ConsumerPropertiesBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/class-use/AbstractKafkaClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/class-use/AbstractKafkaClient.Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/class-use/KafkaClientOperations.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/class-use/ClientArgument.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/class-use/ClientArgumentMap.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/class-use/ClientType.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/class-use/InternalKafkaClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/class-use/InternalKafkaClient.Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/class-use/VerifiableClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/class-use/VerifiableClient.VerifiableClientBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/externalClients/class-use/ExternalKafkaClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/externalClients/class-use/ExternalKafkaClient.Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/AdminClientOperations.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/BaseClients.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/BridgeClients.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/KafkaAdminClients.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/KafkaClients.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/KafkaOauthClients.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/KafkaTracingClients.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/keycloak/class-use/KeycloakInstance.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/listeners/class-use/ExecutionListener.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/listeners/class-use/OrderTestSuites.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/logs/class-use/LogCollector.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/logs/class-use/TestExecutionWatcher.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/matchers/class-use/HasAllOfReasons.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/matchers/class-use/HasAnyOfReasons.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/matchers/class-use/HasNoneOfReasons.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/matchers/class-use/Matchers.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/matchers/class-use/LogHasNoUnexpectedErrors.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/class-use/KafkaBridgeResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/class-use/KafkaConnectResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/class-use/KafkaConnectorResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/class-use/KafkaMirrorMaker2Resource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/class-use/KafkaTopicResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/class-use/KafkaClientsResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/class-use/KafkaMirrorMakerResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/class-use/KafkaRebalanceResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/class-use/KafkaResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/class-use/KafkaUserResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/class-use/StrimziPodSetResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/class-use/BundleResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/class-use/BundleResource.BundleResourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/specific/class-use/HelmResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/specific/class-use/SpecificResourceType.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/specific/class-use/OlmResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/class-use/SetupClusterOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/class-use/SetupClusterOperator.SetupClusterOperatorBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/class-use/ResourceCondition.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/class-use/ResourceItem.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/class-use/ResourceType.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/class-use/ThrowableRunner.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/class-use/ServiceResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/class-use/ClusterOperatorCustomResourceDefinition.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/class-use/ClusterRoleBindingResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/class-use/ClusterRoleResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/class-use/ConfigMapResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/class-use/JobResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/class-use/NetworkPolicyResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/class-use/RoleBindingResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/class-use/RoleResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/class-use/SecretResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/class-use/ServiceAccountResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/class-use/ValidatingWebhookConfigurationResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/class-use/DeploymentResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/class-use/ComponentType.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/class-use/ResourceManager.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/draincleaner/class-use/DrainCleanerResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/draincleaner/class-use/SetupDrainCleaner.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/class-use/ResourceOperation.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/security/class-use/CertAndKeyFiles.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/security/class-use/SystemTestCertAndKey.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/security/class-use/SystemTestCertAndKeyBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/security/class-use/SystemTestCertManager.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/class-use/KafkaBridgeUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/class-use/KafkaConnectUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/class-use/KafkaConnectorUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/class-use/KafkaMirrorMaker2Utils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/class-use/KafkaMirrorMakerUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/class-use/KafkaRebalanceUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/class-use/KafkaUserUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/class-use/KafkaUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/class-use/KafkaTopicUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/controllers/class-use/ConfigMapUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/controllers/class-use/DeploymentUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/controllers/class-use/JobUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/controllers/class-use/StatefulSetUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/controllers/class-use/StrimziPodSetUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/objects/class-use/NamespaceUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/objects/class-use/NodeUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/objects/class-use/PersistentVolumeClaimUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/objects/class-use/PodUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/objects/class-use/SecretUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/objects/class-use/ServiceUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/class-use/BridgeUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/class-use/CruiseControlUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/class-use/CruiseControlUtils.SupportedSchemes.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/class-use/CruiseControlUtils.SupportedHttpMethods.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/class-use/JmxUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/class-use/OlmUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/class-use/TracingUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/class-use/KeycloakUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/class-use/ClientUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/class-use/FileUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/class-use/HttpUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/class-use/RollingUpdateUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/class-use/StUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/class-use/TestKafkaVersion.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/interfaces/class-use/IndicativeSentences.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/class-use/KafkaBridgeTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/class-use/KafkaClientsTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/class-use/KafkaConnectTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/class-use/KafkaConnectorTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/class-use/KafkaMirrorMaker2Templates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/class-use/KafkaMirrorMakerTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/class-use/KafkaRebalanceTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/class-use/KafkaTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/class-use/KafkaTopicTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/class-use/KafkaUserTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/kubernetes/class-use/ServiceTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/kubernetes/class-use/ClusterRoleBindingTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/kubernetes/class-use/NetworkPolicyTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/specific/class-use/ScraperTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/class-use/BeforeAllOnce.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/metrics/class-use/MetricsCollector.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/metrics/class-use/MetricsCollector.Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/parallel/class-use/SuiteThreadController.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/parallel/class-use/TestSuiteNamespaceManager.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/storage/class-use/TestStorage.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/class-use/Constants.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/class-use/Environment.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/KafkaOauthClientsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/KafkaOauthClientsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/KafkaOauthClientsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/BridgeClientsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/BridgeClientsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/BridgeClientsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/KafkaTracingClientsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/KafkaTracingClientsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/KafkaTracingClientsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/BaseClientsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/BaseClientsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/KafkaClientsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/KafkaClientsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/KafkaClientsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/KafkaAdminClientsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/KafkaAdminClientsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/KafkaAdminClientsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/cli/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/enums/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/interfaces/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/k8s/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clientproperties/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/externalClients/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/keycloak/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/listeners/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/logs/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/matchers/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/metrics/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/parallel/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/draincleaner/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/specific/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/security/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/storage/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/kubernetes/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/specific/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/controllers/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/objects/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/package-use.html...
Building index for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/overview-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/index-all.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/allclasses-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/allpackages-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/deprecated-list.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/index.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/overview-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/help-doc.html...
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/systemtest/target/systemtest-0.29.0-SNAPSHOT-javadoc.jar
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:integration-test (default) @ systemtest ---
[WARNING] useSystemClassLoader setting has no effect when not forking
[WARNING] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
2022-03-28 13:48:49 [main] INFO  [TestExecutionListener:29] =======================================================================
2022-03-28 13:48:49 [main] INFO  [TestExecutionListener:30] =======================================================================
2022-03-28 13:48:49 [main] INFO  [TestExecutionListener:31]                         Test run started
2022-03-28 13:48:49 [main] INFO  [TestExecutionListener:32] =======================================================================
2022-03-28 13:48:49 [main] INFO  [TestExecutionListener:33] =======================================================================
2022-03-28 13:48:49 [main] INFO  [TestExecutionListener:48] Following testclasses are selected for run:
2022-03-28 13:48:49 [main] INFO  [TestExecutionListener:51] -> io.strimzi.systemtest.bridge.HttpBridgeScramShaST
2022-03-28 13:48:49 [main] INFO  [TestExecutionListener:51] -> io.strimzi.systemtest.bridge.HttpBridgeTlsST
2022-03-28 13:48:49 [main] INFO  [TestExecutionListener:51] -> io.strimzi.systemtest.operators.topic.ThrottlingQuotaST
2022-03-28 13:48:49 [main] INFO  [TestExecutionListener:51] -> io.strimzi.systemtest.operators.topic.TopicST
2022-03-28 13:48:49 [main] INFO  [TestExecutionListener:51] -> io.strimzi.systemtest.operators.user.UserST
2022-03-28 13:48:49 [main] INFO  [TestExecutionListener:51] -> io.strimzi.systemtest.operators.ReconciliationST
2022-03-28 13:48:49 [main] INFO  [TestExecutionListener:51] -> io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST
2022-03-28 13:48:49 [main] INFO  [TestExecutionListener:52] =======================================================================
2022-03-28 13:48:49 [main] INFO  [TestExecutionListener:53] =======================================================================
[INFO] Running io.strimzi.systemtest.bridge.HttpBridgeTlsST
[INFO] Running io.strimzi.systemtest.bridge.HttpBridgeScramShaST
[INFO] Running io.strimzi.systemtest.operators.topic.TopicST
[INFO] Running io.strimzi.systemtest.operators.user.UserST
[INFO] Running io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST
[INFO] Running io.strimzi.systemtest.operators.topic.ThrottlingQuotaST
2022-03-28 13:48:50 [ForkJoinPool-1-worker-3] DEBUG [Environment:271] Json configuration is not provided or cannot be processed!
2022-03-28 13:48:50 [ForkJoinPool-1-worker-3] INFO  [Environment:219] Used environment variables:
2022-03-28 13:48:50 [ForkJoinPool-1-worker-3] INFO  [Environment:220] CONFIG: /home/cloud-user/strimzi-kafka-operator/systemtest/config.json
2022-03-28 13:48:50 [ForkJoinPool-1-worker-3] INFO  [Environment:221] STRIMZI_RBAC_SCOPE: CLUSTER
2022-03-28 13:48:50 [ForkJoinPool-1-worker-3] INFO  [Environment:221] OLM_APP_BUNDLE_PREFIX: strimzi-cluster-operator
2022-03-28 13:48:50 [ForkJoinPool-1-worker-3] INFO  [Environment:221] TEST_CLIENTS_VERSION: 0.2.0
2022-03-28 13:48:50 [ForkJoinPool-1-worker-3] INFO  [Environment:221] OLM_SOURCE_NAMESPACE: openshift-marketplace
2022-03-28 13:48:50 [ForkJoinPool-1-worker-3] INFO  [Environment:221] CLUSTER_OPERATOR_INSTALL_TYPE: BUNDLE
2022-03-28 13:48:50 [ForkJoinPool-1-worker-3] INFO  [Environment:221] STRIMZI_COMPONENTS_LOG_LEVEL: INFO
2022-03-28 13:48:50 [ForkJoinPool-1-worker-3] INFO  [Environment:221] SKIP_TEARDOWN: false
2022-03-28 13:48:50 [ForkJoinPool-1-worker-3] INFO  [Environment:221] LB_FINALIZERS: false
2022-03-28 13:48:50 [ForkJoinPool-1-worker-3] INFO  [Environment:221] OLM_OPERATOR_DEPLOYMENT_NAME: strimzi-cluster-operator
2022-03-28 13:48:50 [ForkJoinPool-1-worker-3] INFO  [Environment:221] DOCKER_ORG: strimzi
2022-03-28 13:48:50 [ForkJoinPool-1-worker-3] INFO  [Environment:221] TEST_LOG_DIR: /home/cloud-user/strimzi-kafka-operator/systemtest/../systemtest/target/logs/
2022-03-28 13:48:50 [ForkJoinPool-1-worker-3] INFO  [Environment:221] COMPONENTS_IMAGE_PULL_POLICY: IfNotPresent
2022-03-28 13:48:50 [ForkJoinPool-1-worker-3] INFO  [Environment:221] DOCKER_REGISTRY: quay.io
2022-03-28 13:48:50 [ForkJoinPool-1-worker-3] INFO  [Environment:221] TEST_CLIENT_IMAGE: quay.io/strimzi/test-client:latest-kafka-3.1.0
2022-03-28 13:48:50 [ForkJoinPool-1-worker-3] INFO  [Environment:221] SYSTEM_TEST_STRIMZI_IMAGE_PULL_SECRET: 
2022-03-28 13:48:50 [ForkJoinPool-1-worker-3] INFO  [Environment:221] TEST_ADMIN_IMAGE: quay.io/strimzi-test-clients/test-client-kafka-admin:0.2.0-kafka-3.1.0
2022-03-28 13:48:50 [ForkJoinPool-1-worker-3] INFO  [Environment:221] TEST_HTTP_PRODUCER_IMAGE: quay.io/strimzi-test-clients/test-client-http-producer:0.2.0
2022-03-28 13:48:50 [ForkJoinPool-1-worker-3] INFO  [Environment:221] OLM_OPERATOR_NAME: strimzi-kafka-operator
2022-03-28 13:48:50 [ForkJoinPool-1-worker-3] INFO  [Environment:221] DOCKER_TAG: latest
2022-03-28 13:48:50 [ForkJoinPool-1-worker-3] INFO  [Environment:221] OLM_SOURCE_NAME: community-operators
2022-03-28 13:48:50 [ForkJoinPool-1-worker-3] INFO  [Environment:221] STRIMZI_FEATURE_GATES: 
2022-03-28 13:48:50 [ForkJoinPool-1-worker-3] INFO  [Environment:221] CLIENTS_KAFKA_VERSION: 3.1.0
2022-03-28 13:48:50 [ForkJoinPool-1-worker-3] INFO  [Environment:221] TEST_HTTP_CONSUMER_IMAGE: quay.io/strimzi-test-clients/test-client-http-consumer:0.2.0
2022-03-28 13:48:50 [ForkJoinPool-1-worker-3] INFO  [Environment:221] STRIMZI_LOG_LEVEL: DEBUG
2022-03-28 13:48:50 [ForkJoinPool-1-worker-3] INFO  [Environment:221] ST_KAFKA_VERSION: 3.1.0
2022-03-28 13:48:50 [ForkJoinPool-1-worker-3] INFO  [Environment:221] OPERATOR_IMAGE_PULL_POLICY: Always
2022-03-28 13:48:50 [ForkJoinPool-1-worker-3] INFO  [Environment:221] DEFAULT_TO_DENY_NETWORK_POLICIES: true
2022-03-28 13:48:50 [ForkJoinPool-1-worker-3] INFO  [Environment:221] TEST_PRODUCER_IMAGE: quay.io/strimzi-test-clients/test-client-kafka-producer:0.2.0-kafka-3.1.0
2022-03-28 13:48:50 [ForkJoinPool-1-worker-3] INFO  [Environment:221] BRIDGE_IMAGE: latest-released
2022-03-28 13:48:50 [ForkJoinPool-1-worker-3] INFO  [Environment:221] TEST_STREAMS_IMAGE: quay.io/strimzi-test-clients/test-client-kafka-streams:0.2.0-kafka-3.1.0
2022-03-28 13:48:50 [ForkJoinPool-1-worker-3] INFO  [Environment:221] TEST_CONSUMER_IMAGE: quay.io/strimzi-test-clients/test-client-kafka-consumer:0.2.0-kafka-3.1.0
2022-03-28 13:48:50 [ForkJoinPool-1-worker-3] INFO  [Environment:221] OLM_OPERATOR_VERSION: 
2022-03-28 13:48:50 [ForkJoinPool-1-worker-7] DEBUG [BeforeAllOnce:51] ============================================================================
2022-03-28 13:48:50 [ForkJoinPool-1-worker-7] DEBUG [BeforeAllOnce:52] [io.strimzi.systemtest.bridge.HttpBridgeTlsST - Before Suite] - Setup Suite environment
2022-03-28 13:48:50 [ForkJoinPool-1-worker-7] DEBUG [Config:540] Trying to configure client from Kubernetes config...
2022-03-28 13:48:50 [ForkJoinPool-1-worker-7] DEBUG [Config:549] Found for Kubernetes config at: [/home/cloud-user/.kube/config].
2022-03-28 13:48:50 [ForkJoinPool-1-worker-7] DEBUG [Config:540] Trying to configure client from Kubernetes config...
2022-03-28 13:48:50 [ForkJoinPool-1-worker-7] DEBUG [Config:549] Found for Kubernetes config at: [/home/cloud-user/.kube/config].
2022-03-28 13:48:50 [ForkJoinPool-1-worker-7] DEBUG [KubeCluster:80] Cluster minikube is not installed!
2022-03-28 13:48:50 [ForkJoinPool-1-worker-7] DEBUG [KubeCluster:71] Cluster kubectl is installed
2022-03-28 13:48:50 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - kubectl cluster-info
2022-03-28 13:48:51 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: kubectl cluster-info
2022-03-28 13:48:51 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 13:48:51 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - kubectl api-resources
2022-03-28 13:48:52 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: kubectl api-resources
2022-03-28 13:48:52 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 13:48:52 [ForkJoinPool-1-worker-7] DEBUG [KubeCluster:77] Cluster kubectl is not running!
2022-03-28 13:48:52 [ForkJoinPool-1-worker-7] DEBUG [KubeCluster:71] Cluster oc is installed
[INFO] Running io.strimzi.systemtest.operators.ReconciliationST
2022-03-28 13:48:52 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc status -n default
2022-03-28 13:48:53 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc status -n default
2022-03-28 13:48:53 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 13:48:53 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc api-resources
2022-03-28 13:48:54 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc api-resources
2022-03-28 13:48:54 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 13:48:54 [ForkJoinPool-1-worker-7] DEBUG [KubeCluster:73] Cluster oc is running
2022-03-28 13:48:54 [ForkJoinPool-1-worker-7] INFO  [KubeCluster:87] Using cluster: oc
2022-03-28 13:48:54 [ForkJoinPool-1-worker-7] INFO  [KubeClusterResource:60] Cluster default namespace is 'default'
2022-03-28 13:48:54 [ForkJoinPool-1-worker-7] INFO  [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@39e06965
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=*
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-03-28 13:48:54 [ForkJoinPool-1-worker-7] DEBUG [SetupClusterOperator:198] Cluster operator installation configuration:
SetupClusterOperator{cluster=io.strimzi.test.k8s.KubeClusterResource@68a13edb, extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@39e06965, clusterOperatorName='strimzi-cluster-operator', namespaceInstallTo='infra-namespace', namespaceToWatch='*', bindingsNamespaces=[infra-namespace], operationTimeout=300000, reconciliationInterval=30000, extraEnvVars=[], extraLabels={}, clusterOperatorRBACType=CLUSTER, testClassName='null', testMethodName='null'}
2022-03-28 13:48:54 [ForkJoinPool-1-worker-7] INFO  [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-03-28 13:48:56 [ForkJoinPool-1-worker-7] INFO  [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-03-28 13:48:56 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Namespace infra-namespace
2022-03-28 13:48:56 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace default get Namespace infra-namespace -o json
2022-03-28 13:48:56 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace default get Namespace infra-namespace -o json
2022-03-28 13:48:56 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 13:48:56 [ForkJoinPool-1-worker-7] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c29,c24",
            "openshift.io/sa.scc.supplemental-groups": "1000860000/10000",
            "openshift.io/sa.scc.uid-range": "1000860000/10000"
        },
        "creationTimestamp": "2022-03-28T13:48:51Z",
        "labels": {
            "kubernetes.io/metadata.name": "infra-namespace"
        },
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T13:48:51Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:labels": {
                            ".": {},
                            "f:kubernetes.io/metadata.name": {}
                        }
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T13:48:51Z"
            }
        ],
        "name": "infra-namespace",
        "resourceVersion": "82198",
        "uid": "7a872ca4-1d30-40cc-b837-49971fbeeaef"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 13:48:56 [ForkJoinPool-1-worker-7] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace]}
2022-03-28 13:48:56 [ForkJoinPool-1-worker-7] INFO  [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-28 13:48:56 [ForkJoinPool-1-worker-7] DEBUG [SetupClusterOperator:478] Installation resource type: ServiceAccount
2022-03-28 13:48:56 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-28 13:48:57 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-28 13:48:57 [ForkJoinPool-1-worker-7] DEBUG [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-28 13:48:57 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-28 13:48:57 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-28 13:48:57 [ForkJoinPool-1-worker-7] DEBUG [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-28 13:48:57 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-28 13:48:57 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-28 13:48:57 [ForkJoinPool-1-worker-7] DEBUG [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-28 13:48:57 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-28 13:48:57 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-28 13:48:57 [ForkJoinPool-1-worker-7] DEBUG [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-28 13:48:57 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-28 13:48:57 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-28 13:48:58 [ForkJoinPool-1-worker-7] DEBUG [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-28 13:48:58 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-28 13:48:58 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-28 13:48:58 [ForkJoinPool-1-worker-7] DEBUG [SetupClusterOperator:478] Installation resource type: Crd
2022-03-28 13:48:58 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-28 13:48:59 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-28 13:48:59 [ForkJoinPool-1-worker-7] DEBUG [SetupClusterOperator:478] Installation resource type: Crd
2022-03-28 13:48:59 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-28 13:48:59 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-28 13:48:59 [ForkJoinPool-1-worker-7] DEBUG [SetupClusterOperator:478] Installation resource type: Crd
2022-03-28 13:48:59 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-28 13:49:00 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-28 13:49:00 [ForkJoinPool-1-worker-7] DEBUG [SetupClusterOperator:478] Installation resource type: Crd
2022-03-28 13:49:00 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-28 13:49:00 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-28 13:49:00 [ForkJoinPool-1-worker-7] DEBUG [SetupClusterOperator:478] Installation resource type: Crd
2022-03-28 13:49:00 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-28 13:49:00 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-28 13:49:00 [ForkJoinPool-1-worker-7] DEBUG [SetupClusterOperator:478] Installation resource type: Crd
2022-03-28 13:49:00 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-28 13:49:01 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-28 13:49:01 [ForkJoinPool-1-worker-7] DEBUG [SetupClusterOperator:478] Installation resource type: Crd
2022-03-28 13:49:01 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-28 13:49:01 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-28 13:49:01 [ForkJoinPool-1-worker-7] DEBUG [SetupClusterOperator:478] Installation resource type: Crd
2022-03-28 13:49:01 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-28 13:49:01 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-28 13:49:01 [ForkJoinPool-1-worker-7] DEBUG [SetupClusterOperator:478] Installation resource type: Crd
2022-03-28 13:49:01 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-28 13:49:02 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-28 13:49:02 [ForkJoinPool-1-worker-7] DEBUG [SetupClusterOperator:478] Installation resource type: Crd
2022-03-28 13:49:02 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-28 13:49:02 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-28 13:49:02 [ForkJoinPool-1-worker-7] DEBUG [SetupClusterOperator:478] Installation resource type: ConfigMap
2022-03-28 13:49:02 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-28 13:49:02 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-28 13:49:02 [ForkJoinPool-1-worker-7] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=infra-namespace, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 13:49:02 [ForkJoinPool-1-worker-7] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-03-28 13:49:02 [ForkJoinPool-1-worker-7] INFO  [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/cloud-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-28 13:49:02 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-28 13:49:02 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-28 13:49:03 [ForkJoinPool-1-worker-7] INFO  [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/cloud-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-03-28 13:49:03 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-28 13:49:03 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-28 13:49:03 [ForkJoinPool-1-worker-7] INFO  [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/cloud-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-03-28 13:49:03 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-28 13:49:03 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-28 13:49:03 [ForkJoinPool-1-worker-7] INFO  [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/cloud-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-28 13:49:03 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-28 13:49:03 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-28 13:49:03 [ForkJoinPool-1-worker-7] INFO  [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/cloud-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-03-28 13:49:03 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-28 13:49:03 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-28 13:49:03 [ForkJoinPool-1-worker-7] INFO  [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-03-28 13:49:03 [ForkJoinPool-1-worker-7] INFO  [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-03-28 13:49:03 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-28 13:49:03 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-namespaced
2022-03-28 13:49:04 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-03-28 13:49:04 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-entity-operator
2022-03-28 13:49:04 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-28 13:49:04 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-28 13:49:04 [ForkJoinPool-1-worker-7] INFO  [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-28 13:49:04 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-28 13:49:05 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (479899ms till timeout)
2022-03-28 13:49:06 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (478798ms till timeout)
2022-03-28 13:49:07 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (477692ms till timeout)
2022-03-28 13:49:08 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (476590ms till timeout)
2022-03-28 13:49:09 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (475482ms till timeout)
2022-03-28 13:49:10 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (474382ms till timeout)
2022-03-28 13:49:11 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (473277ms till timeout)
2022-03-28 13:49:12 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (472176ms till timeout)
2022-03-28 13:49:13 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (471075ms till timeout)
2022-03-28 13:49:15 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (469971ms till timeout)
2022-03-28 13:49:16 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (468857ms till timeout)
2022-03-28 13:49:17 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (467754ms till timeout)
2022-03-28 13:49:18 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (466648ms till timeout)
2022-03-28 13:49:19 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (465544ms till timeout)
2022-03-28 13:49:20 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (464442ms till timeout)
2022-03-28 13:49:21 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (463341ms till timeout)
2022-03-28 13:49:22 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (462237ms till timeout)
2022-03-28 13:49:23 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (461137ms till timeout)
2022-03-28 13:49:24 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (460035ms till timeout)
2022-03-28 13:49:26 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (458935ms till timeout)
2022-03-28 13:49:27 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (457834ms till timeout)
2022-03-28 13:49:28 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (456732ms till timeout)
2022-03-28 13:49:29 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (455630ms till timeout)
2022-03-28 13:49:30 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (454525ms till timeout)
2022-03-28 13:49:31 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (453423ms till timeout)
2022-03-28 13:49:32 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (452321ms till timeout)
2022-03-28 13:49:33 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (451220ms till timeout)
2022-03-28 13:49:34 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (450120ms till timeout)
2022-03-28 13:49:35 [ForkJoinPool-1-worker-7] INFO  [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-03-28 13:49:35 [ForkJoinPool-1-worker-7] INFO  [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-03-28 13:49:36 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready
2022-03-28 13:49:36 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-5dc4cd9447-8cpvs not ready: strimzi-cluster-operator)
2022-03-28 13:49:36 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-5dc4cd9447-8cpvs are ready
2022-03-28 13:49:36 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599881ms till timeout)
2022-03-28 13:49:37 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-5dc4cd9447-8cpvs not ready: strimzi-cluster-operator)
2022-03-28 13:49:37 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-5dc4cd9447-8cpvs are ready
2022-03-28 13:49:37 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598777ms till timeout)
2022-03-28 13:49:38 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-5dc4cd9447-8cpvs not ready: strimzi-cluster-operator)
2022-03-28 13:49:38 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-5dc4cd9447-8cpvs are ready
2022-03-28 13:49:38 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597674ms till timeout)
2022-03-28 13:49:39 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-5dc4cd9447-8cpvs not ready: strimzi-cluster-operator)
2022-03-28 13:49:39 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-5dc4cd9447-8cpvs are ready
2022-03-28 13:49:39 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596570ms till timeout)
2022-03-28 13:49:40 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-5dc4cd9447-8cpvs not ready: strimzi-cluster-operator)
2022-03-28 13:49:40 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-5dc4cd9447-8cpvs are ready
2022-03-28 13:49:40 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595468ms till timeout)
2022-03-28 13:49:41 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-5dc4cd9447-8cpvs not ready: strimzi-cluster-operator)
2022-03-28 13:49:41 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-5dc4cd9447-8cpvs are ready
2022-03-28 13:49:41 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594364ms till timeout)
2022-03-28 13:49:42 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-5dc4cd9447-8cpvs not ready: strimzi-cluster-operator)
2022-03-28 13:49:42 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-5dc4cd9447-8cpvs are ready
2022-03-28 13:49:42 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593260ms till timeout)
2022-03-28 13:49:43 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-5dc4cd9447-8cpvs not ready: strimzi-cluster-operator)
2022-03-28 13:49:43 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-5dc4cd9447-8cpvs are ready
2022-03-28 13:49:43 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592155ms till timeout)
2022-03-28 13:49:45 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-5dc4cd9447-8cpvs not ready: strimzi-cluster-operator)
2022-03-28 13:49:45 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-5dc4cd9447-8cpvs are ready
2022-03-28 13:49:45 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591048ms till timeout)
2022-03-28 13:49:46 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-5dc4cd9447-8cpvs not ready: strimzi-cluster-operator)
2022-03-28 13:49:46 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-5dc4cd9447-8cpvs are ready
2022-03-28 13:49:46 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (589944ms till timeout)
2022-03-28 13:49:47 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-5dc4cd9447-8cpvs not ready: strimzi-cluster-operator)
2022-03-28 13:49:47 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-5dc4cd9447-8cpvs are ready
2022-03-28 13:49:47 [ForkJoinPool-1-worker-7] INFO  [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-03-28 13:49:47 [ForkJoinPool-1-worker-9] DEBUG [AbstractST:666] ============================================================================
2022-03-28 13:49:47 [ForkJoinPool-1-worker-5] DEBUG [AbstractST:666] ============================================================================
2022-03-28 13:49:47 [ForkJoinPool-1-worker-3] DEBUG [AbstractST:666] ============================================================================
2022-03-28 13:49:47 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:666] ============================================================================
2022-03-28 13:49:47 [ForkJoinPool-1-worker-11] DEBUG [AbstractST:666] ============================================================================
2022-03-28 13:49:47 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:666] ============================================================================
2022-03-28 13:49:47 [ForkJoinPool-1-worker-9] DEBUG [AbstractST:667] [operators.topic.ThrottlingQuotaST - Before All] - Setup test suite environment
2022-03-28 13:49:47 [ForkJoinPool-1-worker-5] DEBUG [AbstractST:667] [bridge.HttpBridgeScramShaST - Before All] - Setup test suite environment
2022-03-28 13:49:47 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:667] [operators.user.UserST - Before All] - Setup test suite environment
2022-03-28 13:49:47 [ForkJoinPool-1-worker-11] DEBUG [AbstractST:667] [operators.topic.TopicST - Before All] - Setup test suite environment
2022-03-28 13:49:47 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:667] [bridge.HttpBridgeTlsST - Before All] - Setup test suite environment
2022-03-28 13:49:47 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:69] [operators.topic.ThrottlingQuotaST] - Adding parallel suite: ThrottlingQuotaST
2022-03-28 13:49:47 [ForkJoinPool-1-worker-3] DEBUG [AbstractST:667] [cruisecontrol.CruiseControlConfigurationST - Before All] - Setup test suite environment
2022-03-28 13:49:47 [ForkJoinPool-1-worker-11] DEBUG [SuiteThreadController:69] [operators.topic.TopicST] - Adding parallel suite: TopicST
2022-03-28 13:49:47 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:69] [operators.user.UserST] - Adding parallel suite: UserST
2022-03-28 13:49:47 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:69] [bridge.HttpBridgeScramShaST] - Adding parallel suite: HttpBridgeScramShaST
2022-03-28 13:49:47 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:69] [cruisecontrol.CruiseControlConfigurationST] - Adding parallel suite: CruiseControlConfigurationST
2022-03-28 13:49:47 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:73] [operators.user.UserST] - Parallel suites count: 2
2022-03-28 13:49:47 [ForkJoinPool-1-worker-15] DEBUG [AbstractST:666] ============================================================================
2022-03-28 13:49:47 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:73] [operators.topic.ThrottlingQuotaST] - Parallel suites count: 1
2022-03-28 13:49:47 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:69] [bridge.HttpBridgeTlsST] - Adding parallel suite: HttpBridgeTlsST
2022-03-28 13:49:47 [ForkJoinPool-1-worker-11] DEBUG [SuiteThreadController:73] [operators.topic.TopicST] - Parallel suites count: 5
2022-03-28 13:49:47 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:73] [cruisecontrol.CruiseControlConfigurationST] - Parallel suites count: 4
2022-03-28 13:49:47 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:73] [bridge.HttpBridgeScramShaST] - Parallel suites count: 3
2022-03-28 13:49:47 [ForkJoinPool-1-worker-11] DEBUG [SuiteThreadController:184] TopicST suite now can proceed its execution
2022-03-28 13:49:47 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:184] CruiseControlConfigurationST suite now can proceed its execution
2022-03-28 13:49:47 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:73] [bridge.HttpBridgeTlsST] - Parallel suites count: 6
2022-03-28 13:49:47 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:184] ThrottlingQuotaST suite now can proceed its execution
2022-03-28 13:49:47 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:184] HttpBridgeTlsST suite now can proceed its execution
2022-03-28 13:49:47 [ForkJoinPool-1-worker-15] DEBUG [AbstractST:667] [operators.ReconciliationST - Before All] - Setup test suite environment
2022-03-28 13:49:47 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:184] UserST suite now can proceed its execution
2022-03-28 13:49:47 [ForkJoinPool-1-worker-7] DEBUG [TestSuiteNamespaceManager:128] Content of the test suite namespaces map:
{HttpBridgeTlsST=[http-bridge-tls-st], CruiseControlST=[cruise-control-st], AbstractST=[abstract-st], OpaIntegrationST=[opa-integration-st], CruiseControlApiST=[cruise-control-api-st], HttpBridgeScramShaST=[http-bridge-scram-sha-st], ReconciliationST=[reconciliation-st], DynamicConfST=[dynamic-conf-st], AbstractUpgradeST=[abstract-upgrade-st], QuotasST=[quotas-st], DynamicConfSharedST=[dynamic-conf-shared-st], RollingUpdateST=[rolling-update-st], OlmAbstractST=[olm-abstract-st], CustomAuthorizerST=[custom-authorizer-st], AlternativeReconcileTriggersST=[alternative-reconcile-triggers-st], LogSettingST=[log-setting-st], ListenersST=[listeners-st], UserST=[user-st], TopicST=[topic-st], SecurityST=[security-st], TracingST=[tracing-st], AbstractNamespaceST=[abstract-namespace-st], CruiseControlConfigurationST=[cruise-control-configuration-st], HttpBridgeCorsST=[http-bridge-cors-st], ConfigProviderST=[config-provider-st], KafkaST=[kafka-st], LoggingChangeST=[logging-change-st], HttpBridgeKafkaExternalListenersST=[http-bridge-kafka-external-listeners-st], OauthAbstractST=[oauth-abstract-st], ThrottlingQuotaST=[throttling-quota-st], MultipleListenersST=[multiple-listeners-st]}
2022-03-28 13:49:47 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:184] HttpBridgeScramShaST suite now can proceed its execution
2022-03-28 13:49:47 [ForkJoinPool-1-worker-7] DEBUG [TestSuiteNamespaceManager:129] Test suite `HttpBridgeTlsST` creates these additional namespaces:[http-bridge-tls-st]
2022-03-28 13:49:47 [ForkJoinPool-1-worker-15] DEBUG [SuiteThreadController:69] [operators.ReconciliationST] - Adding parallel suite: ReconciliationST
2022-03-28 13:49:47 [ForkJoinPool-1-worker-9] DEBUG [TestSuiteNamespaceManager:128] Content of the test suite namespaces map:
{HttpBridgeTlsST=[http-bridge-tls-st], CruiseControlST=[cruise-control-st], AbstractST=[abstract-st], OpaIntegrationST=[opa-integration-st], CruiseControlApiST=[cruise-control-api-st], HttpBridgeScramShaST=[http-bridge-scram-sha-st], ReconciliationST=[reconciliation-st], DynamicConfST=[dynamic-conf-st], AbstractUpgradeST=[abstract-upgrade-st], QuotasST=[quotas-st], DynamicConfSharedST=[dynamic-conf-shared-st], RollingUpdateST=[rolling-update-st], OlmAbstractST=[olm-abstract-st], CustomAuthorizerST=[custom-authorizer-st], AlternativeReconcileTriggersST=[alternative-reconcile-triggers-st], LogSettingST=[log-setting-st], ListenersST=[listeners-st], UserST=[user-st], TopicST=[topic-st], SecurityST=[security-st], TracingST=[tracing-st], AbstractNamespaceST=[abstract-namespace-st], CruiseControlConfigurationST=[cruise-control-configuration-st], HttpBridgeCorsST=[http-bridge-cors-st], ConfigProviderST=[config-provider-st], KafkaST=[kafka-st], LoggingChangeST=[logging-change-st], HttpBridgeKafkaExternalListenersST=[http-bridge-kafka-external-listeners-st], OauthAbstractST=[oauth-abstract-st], ThrottlingQuotaST=[throttling-quota-st], MultipleListenersST=[multiple-listeners-st]}
2022-03-28 13:49:47 [ForkJoinPool-1-worker-11] DEBUG [TestSuiteNamespaceManager:128] Content of the test suite namespaces map:
{HttpBridgeTlsST=[http-bridge-tls-st], CruiseControlST=[cruise-control-st], AbstractST=[abstract-st], OpaIntegrationST=[opa-integration-st], CruiseControlApiST=[cruise-control-api-st], HttpBridgeScramShaST=[http-bridge-scram-sha-st], ReconciliationST=[reconciliation-st], DynamicConfST=[dynamic-conf-st], AbstractUpgradeST=[abstract-upgrade-st], QuotasST=[quotas-st], DynamicConfSharedST=[dynamic-conf-shared-st], RollingUpdateST=[rolling-update-st], OlmAbstractST=[olm-abstract-st], CustomAuthorizerST=[custom-authorizer-st], AlternativeReconcileTriggersST=[alternative-reconcile-triggers-st], LogSettingST=[log-setting-st], ListenersST=[listeners-st], UserST=[user-st], TopicST=[topic-st], SecurityST=[security-st], TracingST=[tracing-st], AbstractNamespaceST=[abstract-namespace-st], CruiseControlConfigurationST=[cruise-control-configuration-st], HttpBridgeCorsST=[http-bridge-cors-st], ConfigProviderST=[config-provider-st], KafkaST=[kafka-st], LoggingChangeST=[logging-change-st], HttpBridgeKafkaExternalListenersST=[http-bridge-kafka-external-listeners-st], OauthAbstractST=[oauth-abstract-st], ThrottlingQuotaST=[throttling-quota-st], MultipleListenersST=[multiple-listeners-st]}
2022-03-28 13:49:47 [ForkJoinPool-1-worker-3] DEBUG [TestSuiteNamespaceManager:128] Content of the test suite namespaces map:
{HttpBridgeTlsST=[http-bridge-tls-st], CruiseControlST=[cruise-control-st], AbstractST=[abstract-st], OpaIntegrationST=[opa-integration-st], CruiseControlApiST=[cruise-control-api-st], HttpBridgeScramShaST=[http-bridge-scram-sha-st], ReconciliationST=[reconciliation-st], DynamicConfST=[dynamic-conf-st], AbstractUpgradeST=[abstract-upgrade-st], QuotasST=[quotas-st], DynamicConfSharedST=[dynamic-conf-shared-st], RollingUpdateST=[rolling-update-st], OlmAbstractST=[olm-abstract-st], CustomAuthorizerST=[custom-authorizer-st], AlternativeReconcileTriggersST=[alternative-reconcile-triggers-st], LogSettingST=[log-setting-st], ListenersST=[listeners-st], UserST=[user-st], TopicST=[topic-st], SecurityST=[security-st], TracingST=[tracing-st], AbstractNamespaceST=[abstract-namespace-st], CruiseControlConfigurationST=[cruise-control-configuration-st], HttpBridgeCorsST=[http-bridge-cors-st], ConfigProviderST=[config-provider-st], KafkaST=[kafka-st], LoggingChangeST=[logging-change-st], HttpBridgeKafkaExternalListenersST=[http-bridge-kafka-external-listeners-st], OauthAbstractST=[oauth-abstract-st], ThrottlingQuotaST=[throttling-quota-st], MultipleListenersST=[multiple-listeners-st]}
2022-03-28 13:49:47 [ForkJoinPool-1-worker-5] DEBUG [TestSuiteNamespaceManager:128] Content of the test suite namespaces map:
{HttpBridgeTlsST=[http-bridge-tls-st], CruiseControlST=[cruise-control-st], AbstractST=[abstract-st], OpaIntegrationST=[opa-integration-st], CruiseControlApiST=[cruise-control-api-st], HttpBridgeScramShaST=[http-bridge-scram-sha-st], ReconciliationST=[reconciliation-st], DynamicConfST=[dynamic-conf-st], AbstractUpgradeST=[abstract-upgrade-st], QuotasST=[quotas-st], DynamicConfSharedST=[dynamic-conf-shared-st], RollingUpdateST=[rolling-update-st], OlmAbstractST=[olm-abstract-st], CustomAuthorizerST=[custom-authorizer-st], AlternativeReconcileTriggersST=[alternative-reconcile-triggers-st], LogSettingST=[log-setting-st], ListenersST=[listeners-st], UserST=[user-st], TopicST=[topic-st], SecurityST=[security-st], TracingST=[tracing-st], AbstractNamespaceST=[abstract-namespace-st], CruiseControlConfigurationST=[cruise-control-configuration-st], HttpBridgeCorsST=[http-bridge-cors-st], ConfigProviderST=[config-provider-st], KafkaST=[kafka-st], LoggingChangeST=[logging-change-st], HttpBridgeKafkaExternalListenersST=[http-bridge-kafka-external-listeners-st], OauthAbstractST=[oauth-abstract-st], ThrottlingQuotaST=[throttling-quota-st], MultipleListenersST=[multiple-listeners-st]}
2022-03-28 13:49:47 [ForkJoinPool-1-worker-9] DEBUG [TestSuiteNamespaceManager:129] Test suite `ThrottlingQuotaST` creates these additional namespaces:[throttling-quota-st]
2022-03-28 13:49:47 [ForkJoinPool-1-worker-5] DEBUG [TestSuiteNamespaceManager:129] Test suite `HttpBridgeScramShaST` creates these additional namespaces:[http-bridge-scram-sha-st]
2022-03-28 13:49:47 [ForkJoinPool-1-worker-15] DEBUG [SuiteThreadController:73] [operators.ReconciliationST] - Parallel suites count: 7
2022-03-28 13:49:47 [ForkJoinPool-1-worker-13] DEBUG [TestSuiteNamespaceManager:128] Content of the test suite namespaces map:
{HttpBridgeTlsST=[http-bridge-tls-st], CruiseControlST=[cruise-control-st], AbstractST=[abstract-st], OpaIntegrationST=[opa-integration-st], CruiseControlApiST=[cruise-control-api-st], HttpBridgeScramShaST=[http-bridge-scram-sha-st], ReconciliationST=[reconciliation-st], DynamicConfST=[dynamic-conf-st], AbstractUpgradeST=[abstract-upgrade-st], QuotasST=[quotas-st], DynamicConfSharedST=[dynamic-conf-shared-st], RollingUpdateST=[rolling-update-st], OlmAbstractST=[olm-abstract-st], CustomAuthorizerST=[custom-authorizer-st], AlternativeReconcileTriggersST=[alternative-reconcile-triggers-st], LogSettingST=[log-setting-st], ListenersST=[listeners-st], UserST=[user-st], TopicST=[topic-st], SecurityST=[security-st], TracingST=[tracing-st], AbstractNamespaceST=[abstract-namespace-st], CruiseControlConfigurationST=[cruise-control-configuration-st], HttpBridgeCorsST=[http-bridge-cors-st], ConfigProviderST=[config-provider-st], KafkaST=[kafka-st], LoggingChangeST=[logging-change-st], HttpBridgeKafkaExternalListenersST=[http-bridge-kafka-external-listeners-st], OauthAbstractST=[oauth-abstract-st], ThrottlingQuotaST=[throttling-quota-st], MultipleListenersST=[multiple-listeners-st]}
2022-03-28 13:49:47 [ForkJoinPool-1-worker-3] DEBUG [TestSuiteNamespaceManager:129] Test suite `CruiseControlConfigurationST` creates these additional namespaces:[cruise-control-configuration-st]
2022-03-28 13:49:47 [ForkJoinPool-1-worker-11] DEBUG [TestSuiteNamespaceManager:129] Test suite `TopicST` creates these additional namespaces:[topic-st]
2022-03-28 13:49:47 [ForkJoinPool-1-worker-13] DEBUG [TestSuiteNamespaceManager:129] Test suite `UserST` creates these additional namespaces:[user-st]
2022-03-28 13:49:47 [ForkJoinPool-1-worker-15] DEBUG [SuiteThreadController:159] [ReconciliationST] moved to the WaitZone, because current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 13:49:47 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 13:49:47 [ForkJoinPool-1-worker-7] INFO  [KubeClusterResource:156] Creating Namespace: http-bridge-tls-st
2022-03-28 13:49:47 [ForkJoinPool-1-worker-3] INFO  [KubeClusterResource:156] Creating Namespace: cruise-control-configuration-st
2022-03-28 13:49:47 [ForkJoinPool-1-worker-11] INFO  [KubeClusterResource:156] Creating Namespace: topic-st
2022-03-28 13:49:47 [ForkJoinPool-1-worker-13] INFO  [KubeClusterResource:156] Creating Namespace: user-st
2022-03-28 13:49:47 [ForkJoinPool-1-worker-9] INFO  [KubeClusterResource:156] Creating Namespace: throttling-quota-st
2022-03-28 13:49:47 [ForkJoinPool-1-worker-5] INFO  [KubeClusterResource:156] Creating Namespace: http-bridge-scram-sha-st
2022-03-28 13:49:47 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Namespace http-bridge-tls-st
2022-03-28 13:49:47 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace infra-namespace get Namespace http-bridge-tls-st -o json
2022-03-28 13:49:47 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Namespace throttling-quota-st
2022-03-28 13:49:47 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace infra-namespace get Namespace throttling-quota-st -o json
2022-03-28 13:49:47 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Namespace cruise-control-configuration-st
2022-03-28 13:49:47 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace infra-namespace get Namespace cruise-control-configuration-st -o json
2022-03-28 13:49:47 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Namespace topic-st
2022-03-28 13:49:47 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace infra-namespace get Namespace topic-st -o json
2022-03-28 13:49:47 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Namespace http-bridge-scram-sha-st
2022-03-28 13:49:47 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace infra-namespace get Namespace http-bridge-scram-sha-st -o json
2022-03-28 13:49:47 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Namespace user-st
2022-03-28 13:49:47 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace infra-namespace get Namespace user-st -o json
2022-03-28 13:49:48 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace infra-namespace get Namespace http-bridge-tls-st -o json
2022-03-28 13:49:48 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 13:49:48 [ForkJoinPool-1-worker-7] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c30,c0",
            "openshift.io/sa.scc.supplemental-groups": "1000870000/10000",
            "openshift.io/sa.scc.uid-range": "1000870000/10000"
        },
        "creationTimestamp": "2022-03-28T13:49:43Z",
        "labels": {
            "kubernetes.io/metadata.name": "http-bridge-tls-st"
        },
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T13:49:43Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:labels": {
                            ".": {},
                            "f:kubernetes.io/metadata.name": {}
                        }
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T13:49:43Z"
            }
        ],
        "name": "http-bridge-tls-st",
        "resourceVersion": "82676",
        "uid": "944339ef-1597-421a-bf2e-f83d8bd0fb31"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 13:49:48 [ForkJoinPool-1-worker-7] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st]}
2022-03-28 13:49:48 [ForkJoinPool-1-worker-7] INFO  [KubeClusterResource:82] Client use Namespace: http-bridge-tls-st
2022-03-28 13:49:48 [ForkJoinPool-1-worker-7] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=http-bridge-tls-st, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 13:49:48 [ForkJoinPool-1-worker-7] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: http-bridge-tls-st
2022-03-28 13:49:48 [ForkJoinPool-1-worker-7] INFO  [HttpBridgeTlsST:129] Deploy Kafka and KafkaBridge before tests
2022-03-28 13:49:48 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace infra-namespace get Namespace http-bridge-scram-sha-st -o json
2022-03-28 13:49:48 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 13:49:48 [ForkJoinPool-1-worker-5] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c30,c20",
            "openshift.io/sa.scc.supplemental-groups": "1000910000/10000",
            "openshift.io/sa.scc.uid-range": "1000910000/10000"
        },
        "creationTimestamp": "2022-03-28T13:49:43Z",
        "labels": {
            "kubernetes.io/metadata.name": "http-bridge-scram-sha-st"
        },
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T13:49:43Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:labels": {
                            ".": {},
                            "f:kubernetes.io/metadata.name": {}
                        }
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T13:49:43Z"
            }
        ],
        "name": "http-bridge-scram-sha-st",
        "resourceVersion": "82778",
        "uid": "700d0526-a227-4776-ae19-87dbbded0792"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 13:49:48 [ForkJoinPool-1-worker-5] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[http-bridge-scram-sha-st], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st]}
2022-03-28 13:49:48 [ForkJoinPool-1-worker-5] INFO  [KubeClusterResource:82] Client use Namespace: http-bridge-scram-sha-st
2022-03-28 13:49:48 [ForkJoinPool-1-worker-5] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=http-bridge-scram-sha-st, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 13:49:48 [ForkJoinPool-1-worker-5] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: http-bridge-scram-sha-st
2022-03-28 13:49:48 [ForkJoinPool-1-worker-5] INFO  [HttpBridgeScramShaST:123] Deploy Kafka and KafkaBridge before tests
2022-03-28 13:49:48 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace infra-namespace get Namespace user-st -o json
2022-03-28 13:49:48 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 13:49:48 [ForkJoinPool-1-worker-13] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c30,c25",
            "openshift.io/sa.scc.supplemental-groups": "1000920000/10000",
            "openshift.io/sa.scc.uid-range": "1000920000/10000"
        },
        "creationTimestamp": "2022-03-28T13:49:43Z",
        "labels": {
            "kubernetes.io/metadata.name": "user-st"
        },
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T13:49:43Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:labels": {
                            ".": {},
                            "f:kubernetes.io/metadata.name": {}
                        }
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T13:49:43Z"
            }
        ],
        "name": "user-st",
        "resourceVersion": "82793",
        "uid": "fdd4ad2d-e222-4950-84a9-46e119f2238c"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 13:49:48 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace infra-namespace get Namespace cruise-control-configuration-st -o json
2022-03-28 13:49:48 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 13:49:48 [ForkJoinPool-1-worker-3] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c30,c10",
            "openshift.io/sa.scc.supplemental-groups": "1000890000/10000",
            "openshift.io/sa.scc.uid-range": "1000890000/10000"
        },
        "creationTimestamp": "2022-03-28T13:49:43Z",
        "labels": {
            "kubernetes.io/metadata.name": "cruise-control-configuration-st"
        },
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T13:49:43Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:labels": {
                            ".": {},
                            "f:kubernetes.io/metadata.name": {}
                        }
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T13:49:43Z"
            }
        ],
        "name": "cruise-control-configuration-st",
        "resourceVersion": "82747",
        "uid": "828e484a-b462-46b5-99c9-e8db10c1398b"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 13:49:48 [ForkJoinPool-1-worker-13] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[http-bridge-scram-sha-st], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st]}
2022-03-28 13:49:48 [ForkJoinPool-1-worker-3] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[http-bridge-scram-sha-st], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st]}
2022-03-28 13:49:48 [ForkJoinPool-1-worker-13] INFO  [KubeClusterResource:82] Client use Namespace: user-st
2022-03-28 13:49:48 [ForkJoinPool-1-worker-3] INFO  [KubeClusterResource:82] Client use Namespace: cruise-control-configuration-st
2022-03-28 13:49:48 [ForkJoinPool-1-worker-13] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=user-st, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 13:49:48 [ForkJoinPool-1-worker-13] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: user-st
2022-03-28 13:49:48 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace infra-namespace get Namespace topic-st -o json
2022-03-28 13:49:48 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:49:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace infra-namespace get Namespace throttling-quota-st -o json
2022-03-28 13:49:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:49:48 [ForkJoinPool-1-worker-9] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c30,c5",
            "openshift.io/sa.scc.supplemental-groups": "1000880000/10000",
            "openshift.io/sa.scc.uid-range": "1000880000/10000"
        },
        "creationTimestamp": "2022-03-28T13:49:43Z",
        "labels": {
            "kubernetes.io/metadata.name": "throttling-quota-st"
        },
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T13:49:43Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:labels": {
                            ".": {},
                            "f:kubernetes.io/metadata.name": {}
                        }
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T13:49:43Z"
            }
        ],
        "name": "throttling-quota-st",
        "resourceVersion": "82717",
        "uid": "8d6f168d-3b96-4d1b-adb3-ae91d6b9dcb9"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 13:49:48 [ForkJoinPool-1-worker-9] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[http-bridge-scram-sha-st], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st]}
2022-03-28 13:49:48 [ForkJoinPool-1-worker-9] INFO  [KubeClusterResource:82] Client use Namespace: throttling-quota-st
2022-03-28 13:49:48 [ForkJoinPool-1-worker-9] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=throttling-quota-st, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 13:49:48 [ForkJoinPool-1-worker-9] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: throttling-quota-st
2022-03-28 13:49:48 [ForkJoinPool-1-worker-9] INFO  [ThrottlingQuotaST:304] Deploying shared Kafka across all test cases in throttling-quota-st namespace
2022-03-28 13:49:48 [ForkJoinPool-1-worker-3] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=cruise-control-configuration-st, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 13:49:48 [ForkJoinPool-1-worker-3] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: cruise-control-configuration-st
2022-03-28 13:49:48 [ForkJoinPool-1-worker-11] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c30,c15",
            "openshift.io/sa.scc.supplemental-groups": "1000900000/10000",
            "openshift.io/sa.scc.uid-range": "1000900000/10000"
        },
        "creationTimestamp": "2022-03-28T13:49:43Z",
        "labels": {
            "kubernetes.io/metadata.name": "topic-st"
        },
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T13:49:43Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:labels": {
                            ".": {},
                            "f:kubernetes.io/metadata.name": {}
                        }
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T13:49:43Z"
            }
        ],
        "name": "topic-st",
        "resourceVersion": "82762",
        "uid": "3601d67a-ed0b-4c35-b101-6f41b78e5358"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 13:49:48 [ForkJoinPool-1-worker-11] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[http-bridge-scram-sha-st], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st]}
2022-03-28 13:49:48 [ForkJoinPool-1-worker-11] INFO  [KubeClusterResource:82] Client use Namespace: topic-st
2022-03-28 13:49:48 [ForkJoinPool-1-worker-11] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=topic-st, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 13:49:48 [ForkJoinPool-1-worker-11] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: topic-st
2022-03-28 13:49:48 [ForkJoinPool-1-worker-11] INFO  [TopicST:491] Deploying shared Kafka across all test cases in topic-st namespace
2022-03-28 13:49:48 [ForkJoinPool-1-worker-3] INFO  [TestSeparator:23] ############################################################################
2022-03-28 13:49:48 [ForkJoinPool-1-worker-3] INFO  [TestSeparator:24] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testCapacityFile-STARTED
2022-03-28 13:49:48 [ForkJoinPool-1-worker-3] DEBUG [AbstractST:658] ============================================================================
2022-03-28 13:49:48 [ForkJoinPool-1-worker-3] DEBUG [AbstractST:659] [cruisecontrol.CruiseControlConfigurationST - Before Each] - Setup test case environment
2022-03-28 13:49:48 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:77] [cruisecontrol.CruiseControlConfigurationST] - Adding parallel test: testCapacityFile
2022-03-28 13:49:48 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:81] [cruisecontrol.CruiseControlConfigurationST] - Parallel test count: 1
2022-03-28 13:49:48 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:230] testCapacityFile test now can proceed its execution
2022-03-28 13:49:48 [ForkJoinPool-1-worker-3] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 13:49:48 [ForkJoinPool-1-worker-3] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testCapacityFile=my-cluster-0531d5ce}
2022-03-28 13:49:48 [ForkJoinPool-1-worker-3] TRACE [AbstractST:607] USERS_NAME_MAP: {testCapacityFile=my-user-155969448-1270555822}
2022-03-28 13:49:48 [ForkJoinPool-1-worker-3] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testCapacityFile=my-topic-296605537-1008743104}
2022-03-28 13:49:48 [ForkJoinPool-1-worker-3] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testCapacityFile=my-cluster-0531d5ce-kafka-clients}
2022-03-28 13:49:48 [ForkJoinPool-1-worker-3] INFO  [TestSuiteNamespaceManager:163] Creating namespace:namespace-0 for test case:testCapacityFile
2022-03-28 13:49:48 [ForkJoinPool-1-worker-3] INFO  [KubeClusterResource:156] Creating Namespace: namespace-0
2022-03-28 13:49:48 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Namespace namespace-0
2022-03-28 13:49:49 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace topic-st get Namespace namespace-0 -o json
2022-03-28 13:49:49 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:155] Create/Update Kafka topic-cluster-name in namespace topic-st
2022-03-28 13:49:49 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:155] Create/Update Kafka user-cluster-name in namespace user-st
2022-03-28 13:49:49 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update Kafka http-bridge-tls-cluster-name in namespace http-bridge-tls-st
2022-03-28 13:49:49 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:155] Create/Update Kafka http-bridge-scram-sha-cluster-name in namespace http-bridge-scram-sha-st
2022-03-28 13:49:49 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:155] Create/Update Kafka quota-cluster in namespace throttling-quota-st
2022-03-28 13:49:49 [ForkJoinPool-1-worker-11] WARN  [VersionUsageUtils:60] The client is using resource type 'kafkas' with unstable version 'v1beta2'
2022-03-28 13:49:49 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:quota-cluster
2022-03-28 13:49:49 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace topic-st get Namespace namespace-0 -o json
2022-03-28 13:49:49 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 13:49:49 [ForkJoinPool-1-worker-3] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c31,c0",
            "openshift.io/sa.scc.supplemental-groups": "1000930000/10000",
            "openshift.io/sa.scc.uid-range": "1000930000/10000"
        },
        "creationTimestamp": "2022-03-28T13:49:44Z",
        "labels": {
            "kubernetes.io/metadata.name": "namespace-0"
        },
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T13:49:44Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:labels": {
                            ".": {},
                            "f:kubernetes.io/metadata.name": {}
                        }
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T13:49:44Z"
            }
        ],
        "name": "namespace-0",
        "resourceVersion": "82873",
        "uid": "c786e021-3d33-4092-9481-f55ab488c449"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 13:49:49 [ForkJoinPool-1-worker-3] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[http-bridge-scram-sha-st], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@3e255cd9=[namespace-0]}
2022-03-28 13:49:49 [ForkJoinPool-1-worker-3] INFO  [KubeClusterResource:82] Client use Namespace: namespace-0
2022-03-28 13:49:49 [ForkJoinPool-1-worker-3] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-0, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 13:49:49 [ForkJoinPool-1-worker-3] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-0
2022-03-28 13:49:49 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:http-bridge-tls-cluster-name
2022-03-28 13:49:49 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:user-cluster-name
2022-03-28 13:49:49 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:http-bridge-scram-sha-cluster-name
2022-03-28 13:49:49 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:topic-cluster-name
2022-03-28 13:49:49 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:433] Wait for Kafka: quota-cluster will have desired state: Ready
2022-03-28 13:49:49 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Kafka: quota-cluster will have desired state: Ready
2022-03-28 13:49:49 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:155] Create/Update Kafka my-cluster-0531d5ce in namespace namespace-0
2022-03-28 13:49:49 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:164] Using Namespace: namespace-0
2022-03-28 13:49:49 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:433] Wait for Kafka: http-bridge-tls-cluster-name will have desired state: Ready
2022-03-28 13:49:49 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:433] Wait for Kafka: topic-cluster-name will have desired state: Ready
2022-03-28 13:49:49 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:433] Wait for Kafka: user-cluster-name will have desired state: Ready
2022-03-28 13:49:49 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Kafka: topic-cluster-name will have desired state: Ready
2022-03-28 13:49:49 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Kafka: user-cluster-name will have desired state: Ready
2022-03-28 13:49:49 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Kafka: http-bridge-tls-cluster-name will have desired state: Ready
2022-03-28 13:49:49 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:433] Wait for Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready
2022-03-28 13:49:49 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready
2022-03-28 13:49:49 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (839820ms till timeout)
2022-03-28 13:49:49 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-0531d5ce
2022-03-28 13:49:49 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (839812ms till timeout)
2022-03-28 13:49:49 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (839812ms till timeout)
2022-03-28 13:49:49 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (839816ms till timeout)
2022-03-28 13:49:49 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (839807ms till timeout)
2022-03-28 13:49:50 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:433] Wait for Kafka: my-cluster-0531d5ce will have desired state: Ready
2022-03-28 13:49:50 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Kafka: my-cluster-0531d5ce will have desired state: Ready
2022-03-28 13:49:50 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1319901ms till timeout)
2022-03-28 13:49:50 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (838716ms till timeout)
2022-03-28 13:49:51 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (838706ms till timeout)
2022-03-28 13:49:51 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (838714ms till timeout)
2022-03-28 13:49:51 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (838706ms till timeout)
2022-03-28 13:49:51 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (838609ms till timeout)
2022-03-28 13:49:51 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1318798ms till timeout)
2022-03-28 13:49:52 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (837616ms till timeout)
2022-03-28 13:49:52 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (837613ms till timeout)
2022-03-28 13:49:52 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (837605ms till timeout)
2022-03-28 13:49:52 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (837604ms till timeout)
2022-03-28 13:49:52 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (837510ms till timeout)
2022-03-28 13:49:52 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 13:49:52 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1317696ms till timeout)
2022-03-28 13:49:53 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (836514ms till timeout)
2022-03-28 13:49:53 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (836503ms till timeout)
2022-03-28 13:49:53 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (836511ms till timeout)
2022-03-28 13:49:53 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (836498ms till timeout)
2022-03-28 13:49:53 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (836407ms till timeout)
2022-03-28 13:49:53 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1316593ms till timeout)
2022-03-28 13:49:54 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (835413ms till timeout)
2022-03-28 13:49:54 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (835402ms till timeout)
2022-03-28 13:49:54 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (835409ms till timeout)
2022-03-28 13:49:54 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (835306ms till timeout)
2022-03-28 13:49:54 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (835306ms till timeout)
2022-03-28 13:49:54 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1315494ms till timeout)
2022-03-28 13:49:55 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (834313ms till timeout)
2022-03-28 13:49:55 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (834309ms till timeout)
2022-03-28 13:49:55 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (834205ms till timeout)
2022-03-28 13:49:55 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (834205ms till timeout)
2022-03-28 13:49:55 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (834204ms till timeout)
2022-03-28 13:49:55 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1314394ms till timeout)
2022-03-28 13:49:56 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (833214ms till timeout)
2022-03-28 13:49:56 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (833210ms till timeout)
2022-03-28 13:49:56 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (833106ms till timeout)
2022-03-28 13:49:56 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (833009ms till timeout)
2022-03-28 13:49:56 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (833009ms till timeout)
2022-03-28 13:49:56 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1313199ms till timeout)
2022-03-28 13:49:57 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 13:49:57 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (832114ms till timeout)
2022-03-28 13:49:57 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (832110ms till timeout)
2022-03-28 13:49:57 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (832006ms till timeout)
2022-03-28 13:49:57 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (831910ms till timeout)
2022-03-28 13:49:57 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (831909ms till timeout)
2022-03-28 13:49:57 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1312098ms till timeout)
2022-03-28 13:49:58 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (831014ms till timeout)
2022-03-28 13:49:58 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (831010ms till timeout)
2022-03-28 13:49:58 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (830900ms till timeout)
2022-03-28 13:49:58 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (830805ms till timeout)
2022-03-28 13:49:58 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (830805ms till timeout)
2022-03-28 13:49:59 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1310994ms till timeout)
2022-03-28 13:49:59 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (829914ms till timeout)
2022-03-28 13:49:59 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (829910ms till timeout)
2022-03-28 13:49:59 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (829801ms till timeout)
2022-03-28 13:50:00 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (829704ms till timeout)
2022-03-28 13:50:00 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (829704ms till timeout)
2022-03-28 13:50:00 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1309893ms till timeout)
2022-03-28 13:50:00 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (828814ms till timeout)
2022-03-28 13:50:00 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (828811ms till timeout)
2022-03-28 13:50:01 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (828702ms till timeout)
2022-03-28 13:50:01 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (828605ms till timeout)
2022-03-28 13:50:01 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (828508ms till timeout)
2022-03-28 13:50:01 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1308793ms till timeout)
2022-03-28 13:50:02 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (827711ms till timeout)
2022-03-28 13:50:02 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (827707ms till timeout)
2022-03-28 13:50:02 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (827604ms till timeout)
2022-03-28 13:50:02 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (827506ms till timeout)
2022-03-28 13:50:02 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 13:50:02 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (827409ms till timeout)
2022-03-28 13:50:02 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1307596ms till timeout)
2022-03-28 13:50:03 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (826610ms till timeout)
2022-03-28 13:50:03 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (826608ms till timeout)
2022-03-28 13:50:03 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (826503ms till timeout)
2022-03-28 13:50:03 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (826404ms till timeout)
2022-03-28 13:50:03 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (826309ms till timeout)
2022-03-28 13:50:03 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1306496ms till timeout)
2022-03-28 13:50:04 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (825509ms till timeout)
2022-03-28 13:50:04 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (825506ms till timeout)
2022-03-28 13:50:04 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (825402ms till timeout)
2022-03-28 13:50:04 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (825305ms till timeout)
2022-03-28 13:50:04 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (825209ms till timeout)
2022-03-28 13:50:04 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1305397ms till timeout)
2022-03-28 13:50:05 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (824408ms till timeout)
2022-03-28 13:50:05 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (824406ms till timeout)
2022-03-28 13:50:05 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (824301ms till timeout)
2022-03-28 13:50:05 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (824204ms till timeout)
2022-03-28 13:50:05 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (824108ms till timeout)
2022-03-28 13:50:05 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1304298ms till timeout)
2022-03-28 13:50:06 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (823308ms till timeout)
2022-03-28 13:50:06 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (823304ms till timeout)
2022-03-28 13:50:06 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (823200ms till timeout)
2022-03-28 13:50:06 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (823099ms till timeout)
2022-03-28 13:50:06 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (822977ms till timeout)
2022-03-28 13:50:06 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1303158ms till timeout)
2022-03-28 13:50:07 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 13:50:07 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (822207ms till timeout)
2022-03-28 13:50:07 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (822204ms till timeout)
2022-03-28 13:50:07 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (822098ms till timeout)
2022-03-28 13:50:07 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (821999ms till timeout)
2022-03-28 13:50:07 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (821878ms till timeout)
2022-03-28 13:50:08 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1302058ms till timeout)
2022-03-28 13:50:08 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (821106ms till timeout)
2022-03-28 13:50:08 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (821102ms till timeout)
2022-03-28 13:50:08 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (820997ms till timeout)
2022-03-28 13:50:08 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (820900ms till timeout)
2022-03-28 13:50:09 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (820779ms till timeout)
2022-03-28 13:50:09 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1300958ms till timeout)
2022-03-28 13:50:09 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (820004ms till timeout)
2022-03-28 13:50:09 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (820001ms till timeout)
2022-03-28 13:50:09 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (819892ms till timeout)
2022-03-28 13:50:10 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (819795ms till timeout)
2022-03-28 13:50:10 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (819680ms till timeout)
2022-03-28 13:50:10 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1299858ms till timeout)
2022-03-28 13:50:10 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (818903ms till timeout)
2022-03-28 13:50:10 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (818900ms till timeout)
2022-03-28 13:50:11 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (818792ms till timeout)
2022-03-28 13:50:11 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (818696ms till timeout)
2022-03-28 13:50:11 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (818581ms till timeout)
2022-03-28 13:50:11 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1298758ms till timeout)
2022-03-28 13:50:11 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (817801ms till timeout)
2022-03-28 13:50:12 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (817796ms till timeout)
2022-03-28 13:50:12 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (817692ms till timeout)
2022-03-28 13:50:12 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (817595ms till timeout)
2022-03-28 13:50:12 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 13:50:12 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (817481ms till timeout)
2022-03-28 13:50:12 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1297659ms till timeout)
2022-03-28 13:50:13 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (816699ms till timeout)
2022-03-28 13:50:13 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (816696ms till timeout)
2022-03-28 13:50:13 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (816591ms till timeout)
2022-03-28 13:50:13 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (816495ms till timeout)
2022-03-28 13:50:13 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (816383ms till timeout)
2022-03-28 13:50:13 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1296559ms till timeout)
2022-03-28 13:50:14 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (815597ms till timeout)
2022-03-28 13:50:14 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (815594ms till timeout)
2022-03-28 13:50:14 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (815491ms till timeout)
2022-03-28 13:50:14 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (815395ms till timeout)
2022-03-28 13:50:14 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (815284ms till timeout)
2022-03-28 13:50:14 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1295459ms till timeout)
2022-03-28 13:50:15 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (814495ms till timeout)
2022-03-28 13:50:15 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (814493ms till timeout)
2022-03-28 13:50:15 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (814389ms till timeout)
2022-03-28 13:50:15 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (814293ms till timeout)
2022-03-28 13:50:15 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (814184ms till timeout)
2022-03-28 13:50:15 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1294356ms till timeout)
2022-03-28 13:50:16 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (813394ms till timeout)
2022-03-28 13:50:16 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (813392ms till timeout)
2022-03-28 13:50:16 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (813286ms till timeout)
2022-03-28 13:50:16 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (813191ms till timeout)
2022-03-28 13:50:16 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (813084ms till timeout)
2022-03-28 13:50:16 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1293257ms till timeout)
2022-03-28 13:50:17 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 13:50:17 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (812292ms till timeout)
2022-03-28 13:50:17 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (812290ms till timeout)
2022-03-28 13:50:17 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (812186ms till timeout)
2022-03-28 13:50:17 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (812090ms till timeout)
2022-03-28 13:50:17 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (811984ms till timeout)
2022-03-28 13:50:17 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1292156ms till timeout)
2022-03-28 13:50:18 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (811188ms till timeout)
2022-03-28 13:50:18 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (811186ms till timeout)
2022-03-28 13:50:18 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (811073ms till timeout)
2022-03-28 13:50:18 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (810985ms till timeout)
2022-03-28 13:50:18 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (810884ms till timeout)
2022-03-28 13:50:19 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1291056ms till timeout)
2022-03-28 13:50:19 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (810087ms till timeout)
2022-03-28 13:50:19 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (810084ms till timeout)
2022-03-28 13:50:19 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (809972ms till timeout)
2022-03-28 13:50:19 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (809876ms till timeout)
2022-03-28 13:50:20 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (809780ms till timeout)
2022-03-28 13:50:20 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1289955ms till timeout)
2022-03-28 13:50:20 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (808984ms till timeout)
2022-03-28 13:50:20 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (808982ms till timeout)
2022-03-28 13:50:20 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (808872ms till timeout)
2022-03-28 13:50:21 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (808775ms till timeout)
2022-03-28 13:50:21 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (808679ms till timeout)
2022-03-28 13:50:21 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1288855ms till timeout)
2022-03-28 13:50:21 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (807883ms till timeout)
2022-03-28 13:50:21 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (807881ms till timeout)
2022-03-28 13:50:22 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (807773ms till timeout)
2022-03-28 13:50:22 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (807668ms till timeout)
2022-03-28 13:50:22 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (807572ms till timeout)
2022-03-28 13:50:22 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 13:50:22 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1287754ms till timeout)
2022-03-28 13:50:22 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (806781ms till timeout)
2022-03-28 13:50:23 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (806779ms till timeout)
2022-03-28 13:50:23 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (806672ms till timeout)
2022-03-28 13:50:23 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (806568ms till timeout)
2022-03-28 13:50:23 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (806472ms till timeout)
2022-03-28 13:50:23 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1286656ms till timeout)
2022-03-28 13:50:24 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (805680ms till timeout)
2022-03-28 13:50:24 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (805677ms till timeout)
2022-03-28 13:50:24 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (805573ms till timeout)
2022-03-28 13:50:24 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (805469ms till timeout)
2022-03-28 13:50:24 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (805372ms till timeout)
2022-03-28 13:50:24 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1285556ms till timeout)
2022-03-28 13:50:25 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (804580ms till timeout)
2022-03-28 13:50:25 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (804578ms till timeout)
2022-03-28 13:50:25 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (804474ms till timeout)
2022-03-28 13:50:25 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (804370ms till timeout)
2022-03-28 13:50:25 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (804274ms till timeout)
2022-03-28 13:50:25 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1284458ms till timeout)
2022-03-28 13:50:26 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (803479ms till timeout)
2022-03-28 13:50:26 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (803476ms till timeout)
2022-03-28 13:50:26 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (803372ms till timeout)
2022-03-28 13:50:26 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (803264ms till timeout)
2022-03-28 13:50:26 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (803174ms till timeout)
2022-03-28 13:50:26 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1283356ms till timeout)
2022-03-28 13:50:27 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 13:50:27 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (802377ms till timeout)
2022-03-28 13:50:27 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (802375ms till timeout)
2022-03-28 13:50:27 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (802271ms till timeout)
2022-03-28 13:50:27 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (802165ms till timeout)
2022-03-28 13:50:27 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (802070ms till timeout)
2022-03-28 13:50:27 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1282256ms till timeout)
2022-03-28 13:50:28 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (801277ms till timeout)
2022-03-28 13:50:28 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (801274ms till timeout)
2022-03-28 13:50:28 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (801169ms till timeout)
2022-03-28 13:50:28 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (801066ms till timeout)
2022-03-28 13:50:28 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (800970ms till timeout)
2022-03-28 13:50:28 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1281156ms till timeout)
2022-03-28 13:50:29 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (800175ms till timeout)
2022-03-28 13:50:29 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (800173ms till timeout)
2022-03-28 13:50:29 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (800064ms till timeout)
2022-03-28 13:50:29 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (799966ms till timeout)
2022-03-28 13:50:29 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (799869ms till timeout)
2022-03-28 13:50:30 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1280057ms till timeout)
2022-03-28 13:50:30 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (799074ms till timeout)
2022-03-28 13:50:30 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (799070ms till timeout)
2022-03-28 13:50:30 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (798964ms till timeout)
2022-03-28 13:50:30 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (798867ms till timeout)
2022-03-28 13:50:31 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (798771ms till timeout)
2022-03-28 13:50:31 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1278955ms till timeout)
2022-03-28 13:50:31 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (797971ms till timeout)
2022-03-28 13:50:31 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (797967ms till timeout)
2022-03-28 13:50:31 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (797862ms till timeout)
2022-03-28 13:50:32 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (797766ms till timeout)
2022-03-28 13:50:32 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (797669ms till timeout)
2022-03-28 13:50:32 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1277857ms till timeout)
2022-03-28 13:50:32 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 13:50:32 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (796867ms till timeout)
2022-03-28 13:50:32 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (796859ms till timeout)
2022-03-28 13:50:33 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (796751ms till timeout)
2022-03-28 13:50:33 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (796656ms till timeout)
2022-03-28 13:50:33 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (796555ms till timeout)
2022-03-28 13:50:33 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1276744ms till timeout)
2022-03-28 13:50:33 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (795766ms till timeout)
2022-03-28 13:50:34 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (795760ms till timeout)
2022-03-28 13:50:34 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (795652ms till timeout)
2022-03-28 13:50:34 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (795555ms till timeout)
2022-03-28 13:50:34 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (795456ms till timeout)
2022-03-28 13:50:34 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1275644ms till timeout)
2022-03-28 13:50:35 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (794665ms till timeout)
2022-03-28 13:50:35 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (794661ms till timeout)
2022-03-28 13:50:35 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (794546ms till timeout)
2022-03-28 13:50:35 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (794455ms till timeout)
2022-03-28 13:50:35 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (794337ms till timeout)
2022-03-28 13:50:35 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1274527ms till timeout)
2022-03-28 13:50:36 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (793564ms till timeout)
2022-03-28 13:50:36 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (793561ms till timeout)
2022-03-28 13:50:36 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (793444ms till timeout)
2022-03-28 13:50:36 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (793347ms till timeout)
2022-03-28 13:50:36 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (793238ms till timeout)
2022-03-28 13:50:36 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1273427ms till timeout)
2022-03-28 13:50:37 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (792464ms till timeout)
2022-03-28 13:50:37 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 13:50:37 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (792457ms till timeout)
2022-03-28 13:50:37 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (792345ms till timeout)
2022-03-28 13:50:37 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (792247ms till timeout)
2022-03-28 13:50:37 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (792136ms till timeout)
2022-03-28 13:50:37 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1272324ms till timeout)
2022-03-28 13:50:38 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (791363ms till timeout)
2022-03-28 13:50:38 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (791357ms till timeout)
2022-03-28 13:50:38 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (791240ms till timeout)
2022-03-28 13:50:38 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (791143ms till timeout)
2022-03-28 13:50:38 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (791030ms till timeout)
2022-03-28 13:50:38 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1271219ms till timeout)
2022-03-28 13:50:39 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (790261ms till timeout)
2022-03-28 13:50:39 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (790258ms till timeout)
2022-03-28 13:50:39 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (790140ms till timeout)
2022-03-28 13:50:39 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (790045ms till timeout)
2022-03-28 13:50:39 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (789931ms till timeout)
2022-03-28 13:50:39 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1270120ms till timeout)
2022-03-28 13:50:40 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (789163ms till timeout)
2022-03-28 13:50:40 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (789160ms till timeout)
2022-03-28 13:50:40 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (789042ms till timeout)
2022-03-28 13:50:40 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (788946ms till timeout)
2022-03-28 13:50:40 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (788833ms till timeout)
2022-03-28 13:50:41 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1269022ms till timeout)
2022-03-28 13:50:41 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (788064ms till timeout)
2022-03-28 13:50:41 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (788058ms till timeout)
2022-03-28 13:50:41 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (787944ms till timeout)
2022-03-28 13:50:41 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (787848ms till timeout)
2022-03-28 13:50:42 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (787735ms till timeout)
2022-03-28 13:50:42 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1267923ms till timeout)
2022-03-28 13:50:42 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 13:50:42 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (786962ms till timeout)
2022-03-28 13:50:42 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (786957ms till timeout)
2022-03-28 13:50:42 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (786845ms till timeout)
2022-03-28 13:50:43 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (786749ms till timeout)
2022-03-28 13:50:43 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (786636ms till timeout)
2022-03-28 13:50:43 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1266823ms till timeout)
2022-03-28 13:50:43 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (785861ms till timeout)
2022-03-28 13:50:43 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (785857ms till timeout)
2022-03-28 13:50:44 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (785744ms till timeout)
2022-03-28 13:50:44 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (785649ms till timeout)
2022-03-28 13:50:44 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (785534ms till timeout)
2022-03-28 13:50:44 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1265722ms till timeout)
2022-03-28 13:50:44 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (784761ms till timeout)
2022-03-28 13:50:45 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (784758ms till timeout)
2022-03-28 13:50:45 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (784645ms till timeout)
2022-03-28 13:50:45 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (784548ms till timeout)
2022-03-28 13:50:45 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (784432ms till timeout)
2022-03-28 13:50:45 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1264621ms till timeout)
2022-03-28 13:50:46 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (783661ms till timeout)
2022-03-28 13:50:46 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (783658ms till timeout)
2022-03-28 13:50:46 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (783547ms till timeout)
2022-03-28 13:50:46 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (783450ms till timeout)
2022-03-28 13:50:46 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (783334ms till timeout)
2022-03-28 13:50:46 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1263522ms till timeout)
2022-03-28 13:50:47 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (782562ms till timeout)
2022-03-28 13:50:47 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (782559ms till timeout)
2022-03-28 13:50:47 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 13:50:47 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (782446ms till timeout)
2022-03-28 13:50:47 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (782350ms till timeout)
2022-03-28 13:50:47 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (782236ms till timeout)
2022-03-28 13:50:47 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1262424ms till timeout)
2022-03-28 13:50:48 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (781463ms till timeout)
2022-03-28 13:50:48 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (781459ms till timeout)
2022-03-28 13:50:48 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (781346ms till timeout)
2022-03-28 13:50:48 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (781250ms till timeout)
2022-03-28 13:50:48 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (781132ms till timeout)
2022-03-28 13:50:48 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1261318ms till timeout)
2022-03-28 13:50:49 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (780363ms till timeout)
2022-03-28 13:50:49 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (780360ms till timeout)
2022-03-28 13:50:49 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (780245ms till timeout)
2022-03-28 13:50:49 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (780149ms till timeout)
2022-03-28 13:50:49 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (780033ms till timeout)
2022-03-28 13:50:49 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1260217ms till timeout)
2022-03-28 13:50:50 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (779262ms till timeout)
2022-03-28 13:50:50 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (779259ms till timeout)
2022-03-28 13:50:50 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (779137ms till timeout)
2022-03-28 13:50:50 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (779037ms till timeout)
2022-03-28 13:50:50 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (778923ms till timeout)
2022-03-28 13:50:50 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1259111ms till timeout)
2022-03-28 13:50:51 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (778162ms till timeout)
2022-03-28 13:50:51 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (778159ms till timeout)
2022-03-28 13:50:51 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (778038ms till timeout)
2022-03-28 13:50:51 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (777937ms till timeout)
2022-03-28 13:50:51 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (777824ms till timeout)
2022-03-28 13:50:52 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1258012ms till timeout)
2022-03-28 13:50:52 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 13:50:52 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (777060ms till timeout)
2022-03-28 13:50:52 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (777057ms till timeout)
2022-03-28 13:50:52 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (776939ms till timeout)
2022-03-28 13:50:52 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (776839ms till timeout)
2022-03-28 13:50:53 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (776716ms till timeout)
2022-03-28 13:50:53 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1256903ms till timeout)
2022-03-28 13:50:53 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (775960ms till timeout)
2022-03-28 13:50:53 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (775958ms till timeout)
2022-03-28 13:50:53 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (775839ms till timeout)
2022-03-28 13:50:54 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (775740ms till timeout)
2022-03-28 13:50:54 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (775618ms till timeout)
2022-03-28 13:50:54 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1255806ms till timeout)
2022-03-28 13:50:54 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (774833ms till timeout)
2022-03-28 13:50:54 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (774830ms till timeout)
2022-03-28 13:50:55 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (774726ms till timeout)
2022-03-28 13:50:55 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (774631ms till timeout)
2022-03-28 13:50:55 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (774520ms till timeout)
2022-03-28 13:50:55 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1254707ms till timeout)
2022-03-28 13:50:55 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (773735ms till timeout)
2022-03-28 13:50:56 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (773731ms till timeout)
2022-03-28 13:50:56 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (773627ms till timeout)
2022-03-28 13:50:56 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (773530ms till timeout)
2022-03-28 13:50:56 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (773421ms till timeout)
2022-03-28 13:50:56 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1253608ms till timeout)
2022-03-28 13:50:57 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (772633ms till timeout)
2022-03-28 13:50:57 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (772630ms till timeout)
2022-03-28 13:50:57 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (772526ms till timeout)
2022-03-28 13:50:57 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 13:50:57 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (772430ms till timeout)
2022-03-28 13:50:57 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (772321ms till timeout)
2022-03-28 13:50:57 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1252506ms till timeout)
2022-03-28 13:50:58 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (771533ms till timeout)
2022-03-28 13:50:58 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (771531ms till timeout)
2022-03-28 13:50:58 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (771425ms till timeout)
2022-03-28 13:50:58 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (771329ms till timeout)
2022-03-28 13:50:58 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (771222ms till timeout)
2022-03-28 13:50:58 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1251407ms till timeout)
2022-03-28 13:50:59 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (770432ms till timeout)
2022-03-28 13:50:59 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (770430ms till timeout)
2022-03-28 13:50:59 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (770325ms till timeout)
2022-03-28 13:50:59 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (770230ms till timeout)
2022-03-28 13:50:59 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (770123ms till timeout)
2022-03-28 13:50:59 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1250308ms till timeout)
2022-03-28 13:51:00 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (769333ms till timeout)
2022-03-28 13:51:00 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (769323ms till timeout)
2022-03-28 13:51:00 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (769219ms till timeout)
2022-03-28 13:51:00 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (769123ms till timeout)
2022-03-28 13:51:00 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (769025ms till timeout)
2022-03-28 13:51:00 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1249209ms till timeout)
2022-03-28 13:51:01 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (768232ms till timeout)
2022-03-28 13:51:01 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (768223ms till timeout)
2022-03-28 13:51:01 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (768120ms till timeout)
2022-03-28 13:51:01 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (768024ms till timeout)
2022-03-28 13:51:01 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (767927ms till timeout)
2022-03-28 13:51:01 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1248111ms till timeout)
2022-03-28 13:51:02 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 13:51:02 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (767129ms till timeout)
2022-03-28 13:51:02 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (767125ms till timeout)
2022-03-28 13:51:02 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (767020ms till timeout)
2022-03-28 13:51:02 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (766923ms till timeout)
2022-03-28 13:51:02 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (766827ms till timeout)
2022-03-28 13:51:03 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1247011ms till timeout)
2022-03-28 13:51:03 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (766028ms till timeout)
2022-03-28 13:51:03 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (766027ms till timeout)
2022-03-28 13:51:03 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (765921ms till timeout)
2022-03-28 13:51:03 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (765824ms till timeout)
2022-03-28 13:51:04 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (765726ms till timeout)
2022-03-28 13:51:04 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1245911ms till timeout)
2022-03-28 13:51:04 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (764926ms till timeout)
2022-03-28 13:51:04 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (764923ms till timeout)
2022-03-28 13:51:04 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (764820ms till timeout)
2022-03-28 13:51:05 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (764721ms till timeout)
2022-03-28 13:51:05 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (764624ms till timeout)
2022-03-28 13:51:05 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1244811ms till timeout)
2022-03-28 13:51:05 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (763827ms till timeout)
2022-03-28 13:51:05 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (763823ms till timeout)
2022-03-28 13:51:06 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (763719ms till timeout)
2022-03-28 13:51:06 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (763621ms till timeout)
2022-03-28 13:51:06 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (763525ms till timeout)
2022-03-28 13:51:06 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1243710ms till timeout)
2022-03-28 13:51:06 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (762726ms till timeout)
2022-03-28 13:51:07 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (762723ms till timeout)
2022-03-28 13:51:07 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (762618ms till timeout)
2022-03-28 13:51:07 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (762520ms till timeout)
2022-03-28 13:51:07 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 13:51:07 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (762425ms till timeout)
2022-03-28 13:51:07 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1242611ms till timeout)
2022-03-28 13:51:08 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (761625ms till timeout)
2022-03-28 13:51:08 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (761622ms till timeout)
2022-03-28 13:51:08 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (761517ms till timeout)
2022-03-28 13:51:08 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (761421ms till timeout)
2022-03-28 13:51:08 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (761326ms till timeout)
2022-03-28 13:51:08 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1241511ms till timeout)
2022-03-28 13:51:09 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (760526ms till timeout)
2022-03-28 13:51:09 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (760523ms till timeout)
2022-03-28 13:51:09 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (760419ms till timeout)
2022-03-28 13:51:09 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (760322ms till timeout)
2022-03-28 13:51:09 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (760226ms till timeout)
2022-03-28 13:51:09 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1240412ms till timeout)
2022-03-28 13:51:10 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (759426ms till timeout)
2022-03-28 13:51:10 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (759423ms till timeout)
2022-03-28 13:51:10 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (759318ms till timeout)
2022-03-28 13:51:10 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (759220ms till timeout)
2022-03-28 13:51:10 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (759126ms till timeout)
2022-03-28 13:51:10 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1239312ms till timeout)
2022-03-28 13:51:11 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (758326ms till timeout)
2022-03-28 13:51:11 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (758323ms till timeout)
2022-03-28 13:51:11 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (758216ms till timeout)
2022-03-28 13:51:11 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (758119ms till timeout)
2022-03-28 13:51:11 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (758022ms till timeout)
2022-03-28 13:51:11 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1238210ms till timeout)
2022-03-28 13:51:12 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 13:51:12 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (757225ms till timeout)
2022-03-28 13:51:12 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (757222ms till timeout)
2022-03-28 13:51:12 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (757118ms till timeout)
2022-03-28 13:51:12 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (757019ms till timeout)
2022-03-28 13:51:12 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (756911ms till timeout)
2022-03-28 13:51:12 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1237099ms till timeout)
2022-03-28 13:51:13 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (756126ms till timeout)
2022-03-28 13:51:13 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (756123ms till timeout)
2022-03-28 13:51:13 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (756016ms till timeout)
2022-03-28 13:51:13 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (755920ms till timeout)
2022-03-28 13:51:13 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (755812ms till timeout)
2022-03-28 13:51:14 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1235999ms till timeout)
2022-03-28 13:51:14 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (755026ms till timeout)
2022-03-28 13:51:14 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (755023ms till timeout)
2022-03-28 13:51:14 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (754917ms till timeout)
2022-03-28 13:51:14 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (754820ms till timeout)
2022-03-28 13:51:15 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (754714ms till timeout)
2022-03-28 13:51:15 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1234894ms till timeout)
2022-03-28 13:51:15 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (753927ms till timeout)
2022-03-28 13:51:15 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (753924ms till timeout)
2022-03-28 13:51:15 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (753818ms till timeout)
2022-03-28 13:51:16 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (753723ms till timeout)
2022-03-28 13:51:16 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (753614ms till timeout)
2022-03-28 13:51:16 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1233793ms till timeout)
2022-03-28 13:51:16 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (752827ms till timeout)
2022-03-28 13:51:16 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (752824ms till timeout)
2022-03-28 13:51:17 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (752718ms till timeout)
2022-03-28 13:51:17 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (752622ms till timeout)
2022-03-28 13:51:17 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (752515ms till timeout)
2022-03-28 13:51:17 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 13:51:17 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1232695ms till timeout)
2022-03-28 13:51:17 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (751723ms till timeout)
2022-03-28 13:51:18 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (751720ms till timeout)
2022-03-28 13:51:18 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (751615ms till timeout)
2022-03-28 13:51:18 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (751519ms till timeout)
2022-03-28 13:51:18 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (751416ms till timeout)
2022-03-28 13:51:18 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1231594ms till timeout)
2022-03-28 13:51:19 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (750620ms till timeout)
2022-03-28 13:51:19 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (750616ms till timeout)
2022-03-28 13:51:19 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (750510ms till timeout)
2022-03-28 13:51:19 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (750414ms till timeout)
2022-03-28 13:51:19 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (750316ms till timeout)
2022-03-28 13:51:19 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1230493ms till timeout)
2022-03-28 13:51:20 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:444] Kafka: quota-cluster is in desired state: Ready
2022-03-28 13:51:20 [ForkJoinPool-1-worker-9] INFO  [TestSeparator:23] ############################################################################
2022-03-28 13:51:20 [ForkJoinPool-1-worker-9] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.topic.ThrottlingQuotaST.testThrottlingQuotasCreateTopic-STARTED
2022-03-28 13:51:20 [ForkJoinPool-1-worker-9] DEBUG [AbstractST:658] ============================================================================
2022-03-28 13:51:20 [ForkJoinPool-1-worker-9] DEBUG [AbstractST:659] [operators.topic.ThrottlingQuotaST - Before Each] - Setup test case environment
2022-03-28 13:51:20 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:77] [operators.topic.ThrottlingQuotaST] - Adding parallel test: testThrottlingQuotasCreateTopic
2022-03-28 13:51:20 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:81] [operators.topic.ThrottlingQuotaST] - Parallel test count: 2
2022-03-28 13:51:20 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:230] testThrottlingQuotasCreateTopic test now can proceed its execution
2022-03-28 13:51:20 [ForkJoinPool-1-worker-9] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 13:51:20 [ForkJoinPool-1-worker-9] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testThrottlingQuotasCreateTopic=my-cluster-ac2cadd6, testCapacityFile=my-cluster-0531d5ce}
2022-03-28 13:51:20 [ForkJoinPool-1-worker-9] TRACE [AbstractST:607] USERS_NAME_MAP: {testThrottlingQuotasCreateTopic=my-user-46611685-1079680568, testCapacityFile=my-user-155969448-1270555822}
2022-03-28 13:51:20 [ForkJoinPool-1-worker-9] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testThrottlingQuotasCreateTopic=my-topic-114922478-1506136333, testCapacityFile=my-topic-296605537-1008743104}
2022-03-28 13:51:20 [ForkJoinPool-1-worker-9] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testThrottlingQuotasCreateTopic=my-cluster-ac2cadd6-kafka-clients, testCapacityFile=my-cluster-0531d5ce-kafka-clients}
2022-03-28 13:51:20 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:155] Create/Update KafkaUser my-user-46611685-1079680568 in namespace throttling-quota-st
2022-03-28 13:51:20 [ForkJoinPool-1-worker-9] WARN  [VersionUsageUtils:60] The client is using resource type 'kafkausers' with unstable version 'v1beta2'
2022-03-28 13:51:20 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (749515ms till timeout)
2022-03-28 13:51:20 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-46611685-1079680568
2022-03-28 13:51:20 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (749315ms till timeout)
2022-03-28 13:51:20 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (749219ms till timeout)
2022-03-28 13:51:20 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:433] Wait for KafkaUser: my-user-46611685-1079680568 will have desired state: Ready
2022-03-28 13:51:20 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for KafkaUser: my-user-46611685-1079680568 will have desired state: Ready
2022-03-28 13:51:20 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (749124ms till timeout)
2022-03-28 13:51:20 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1229310ms till timeout)
2022-03-28 13:51:20 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] KafkaUser: my-user-46611685-1079680568 will have desired state: Ready not ready, will try again in 1000 ms (179808ms till timeout)
2022-03-28 13:51:21 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (748415ms till timeout)
2022-03-28 13:51:21 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (748216ms till timeout)
2022-03-28 13:51:21 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (748119ms till timeout)
2022-03-28 13:51:21 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (748024ms till timeout)
2022-03-28 13:51:22 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1228078ms till timeout)
2022-03-28 13:51:22 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:444] KafkaUser: my-user-46611685-1079680568 is in desired state: Ready
2022-03-28 13:51:22 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:155] Create/Update Job create-admin-my-cluster-ac2cadd6-kafka-clients in namespace throttling-quota-st
2022-03-28 13:51:22 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 13:51:22 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:create-admin-my-cluster-ac2cadd6-kafka-clients
2022-03-28 13:51:22 [ForkJoinPool-1-worker-9] INFO  [JobUtils:81] Waiting for job: create-admin-my-cluster-ac2cadd6-kafka-clients will be in active state
2022-03-28 13:51:22 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 13:51:22 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (747266ms till timeout)
2022-03-28 13:51:22 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (747065ms till timeout)
2022-03-28 13:51:22 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (746968ms till timeout)
2022-03-28 13:51:22 [ForkJoinPool-1-worker-9] INFO  [PodUtils:186] Waiting for message will be in the log
2022-03-28 13:51:22 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Waiting for message will be in the log
2022-03-28 13:51:22 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (746872ms till timeout)
2022-03-28 13:51:23 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (299799ms till timeout)
2022-03-28 13:51:23 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1226951ms till timeout)
2022-03-28 13:51:23 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (746167ms till timeout)
2022-03-28 13:51:23 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (745964ms till timeout)
2022-03-28 13:51:23 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (745866ms till timeout)
2022-03-28 13:51:24 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (745771ms till timeout)
2022-03-28 13:51:24 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1225824ms till timeout)
2022-03-28 13:51:24 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (298574ms till timeout)
2022-03-28 13:51:24 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (745067ms till timeout)
2022-03-28 13:51:24 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (744864ms till timeout)
2022-03-28 13:51:25 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (744768ms till timeout)
2022-03-28 13:51:25 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (744672ms till timeout)
2022-03-28 13:51:25 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1224726ms till timeout)
2022-03-28 13:51:25 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (297369ms till timeout)
2022-03-28 13:51:25 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (743968ms till timeout)
2022-03-28 13:51:26 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (743764ms till timeout)
2022-03-28 13:51:26 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (743669ms till timeout)
2022-03-28 13:51:26 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (743573ms till timeout)
2022-03-28 13:51:26 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1223627ms till timeout)
2022-03-28 13:51:26 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (296081ms till timeout)
2022-03-28 13:51:26 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (742869ms till timeout)
2022-03-28 13:51:27 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (742664ms till timeout)
2022-03-28 13:51:27 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (742569ms till timeout)
2022-03-28 13:51:27 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 13:51:27 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:444] Kafka: user-cluster-name is in desired state: Ready
2022-03-28 13:51:27 [ForkJoinPool-1-worker-13] INFO  [TestSeparator:23] ############################################################################
2022-03-28 13:51:27 [ForkJoinPool-1-worker-13] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.user.UserST.testCreatingUsersWithSecretPrefix-STARTED
2022-03-28 13:51:27 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:658] ============================================================================
2022-03-28 13:51:27 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:659] [operators.user.UserST - Before Each] - Setup test case environment
2022-03-28 13:51:27 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:77] [operators.user.UserST] - Adding parallel test: testCreatingUsersWithSecretPrefix
2022-03-28 13:51:27 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:81] [operators.user.UserST] - Parallel test count: 3
2022-03-28 13:51:27 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:230] testCreatingUsersWithSecretPrefix test now can proceed its execution
2022-03-28 13:51:27 [ForkJoinPool-1-worker-13] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 13:51:27 [ForkJoinPool-1-worker-13] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testThrottlingQuotasCreateTopic=my-cluster-ac2cadd6, testCapacityFile=my-cluster-0531d5ce, testCreatingUsersWithSecretPrefix=my-cluster-0c65d2dd}
2022-03-28 13:51:27 [ForkJoinPool-1-worker-13] TRACE [AbstractST:607] USERS_NAME_MAP: {testThrottlingQuotasCreateTopic=my-user-46611685-1079680568, testCapacityFile=my-user-155969448-1270555822, testCreatingUsersWithSecretPrefix=my-user-144001639-226807213}
2022-03-28 13:51:27 [ForkJoinPool-1-worker-13] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testThrottlingQuotasCreateTopic=my-topic-114922478-1506136333, testCapacityFile=my-topic-296605537-1008743104, testCreatingUsersWithSecretPrefix=my-topic-130462979-2061578809}
2022-03-28 13:51:27 [ForkJoinPool-1-worker-13] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testThrottlingQuotasCreateTopic=my-cluster-ac2cadd6-kafka-clients, testCapacityFile=my-cluster-0531d5ce-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-0c65d2dd-kafka-clients}
2022-03-28 13:51:27 [ForkJoinPool-1-worker-13] INFO  [TestSuiteNamespaceManager:163] Creating namespace:namespace-1 for test case:testCreatingUsersWithSecretPrefix
2022-03-28 13:51:27 [ForkJoinPool-1-worker-13] INFO  [KubeClusterResource:156] Creating Namespace: namespace-1
2022-03-28 13:51:27 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Namespace namespace-1
2022-03-28 13:51:27 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-0 get Namespace namespace-1 -o json
2022-03-28 13:51:27 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1222461ms till timeout)
2022-03-28 13:51:27 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (294887ms till timeout)
2022-03-28 13:51:28 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (741769ms till timeout)
2022-03-28 13:51:28 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (741566ms till timeout)
2022-03-28 13:51:28 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (741470ms till timeout)
2022-03-28 13:51:28 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1221362ms till timeout)
2022-03-28 13:51:29 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (293692ms till timeout)
2022-03-28 13:51:29 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (740660ms till timeout)
2022-03-28 13:51:29 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (740465ms till timeout)
2022-03-28 13:51:29 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:444] Kafka: http-bridge-tls-cluster-name is in desired state: Ready
2022-03-28 13:51:29 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update KafkaUser my-user-829879141-597480023 in namespace http-bridge-tls-st
2022-03-28 13:51:29 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-829879141-597480023
2022-03-28 13:51:29 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:433] Wait for KafkaUser: my-user-829879141-597480023 will have desired state: Ready
2022-03-28 13:51:29 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for KafkaUser: my-user-829879141-597480023 will have desired state: Ready
2022-03-28 13:51:29 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaUser: my-user-829879141-597480023 will have desired state: Ready not ready, will try again in 1000 ms (179902ms till timeout)
2022-03-28 13:51:29 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1220242ms till timeout)
2022-03-28 13:51:30 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (292499ms till timeout)
2022-03-28 13:51:30 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:444] Kafka: http-bridge-scram-sha-cluster-name is in desired state: Ready
2022-03-28 13:51:30 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:155] Create/Update KafkaUser my-user-609064688-1843560098 in namespace http-bridge-scram-sha-st
2022-03-28 13:51:30 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:444] Kafka: topic-cluster-name is in desired state: Ready
2022-03-28 13:51:30 [ForkJoinPool-1-worker-1] INFO  [TestSeparator:23] ############################################################################
2022-03-28 13:51:30 [ForkJoinPool-1-worker-1] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.topic.ThrottlingQuotaST.testThrottlingQuotasCreateAlterPartitions-STARTED
2022-03-28 13:51:30 [ForkJoinPool-1-worker-1] DEBUG [AbstractST:658] ============================================================================
2022-03-28 13:51:30 [ForkJoinPool-1-worker-1] DEBUG [AbstractST:659] [operators.topic.ThrottlingQuotaST - Before Each] - Setup test case environment
2022-03-28 13:51:30 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:77] [operators.topic.ThrottlingQuotaST] - Adding parallel test: testThrottlingQuotasCreateAlterPartitions
2022-03-28 13:51:30 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:81] [operators.topic.ThrottlingQuotaST] - Parallel test count: 4
2022-03-28 13:51:30 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:230] testThrottlingQuotasCreateAlterPartitions test now can proceed its execution
2022-03-28 13:51:30 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-609064688-1843560098
2022-03-28 13:51:30 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:433] Wait for KafkaUser: my-user-609064688-1843560098 will have desired state: Ready
2022-03-28 13:51:30 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for KafkaUser: my-user-609064688-1843560098 will have desired state: Ready
2022-03-28 13:51:30 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaUser: my-user-609064688-1843560098 will have desired state: Ready not ready, will try again in 1000 ms (179903ms till timeout)
2022-03-28 13:51:30 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:444] KafkaUser: my-user-829879141-597480023 is in desired state: Ready
2022-03-28 13:51:30 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1219143ms till timeout)
2022-03-28 13:51:31 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update Deployment http-bridge-tls-st-kafka-clients in namespace http-bridge-tls-st
2022-03-28 13:51:31 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:http-bridge-tls-st-kafka-clients
2022-03-28 13:51:31 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (291363ms till timeout)
2022-03-28 13:51:31 [ForkJoinPool-1-worker-7] INFO  [DeploymentUtils:161] Wait for Deployment: http-bridge-tls-st-kafka-clients will be ready
2022-03-28 13:51:31 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Wait for Deployment: http-bridge-tls-st-kafka-clients will be ready
2022-03-28 13:51:31 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Wait for Deployment: http-bridge-tls-st-kafka-clients will be ready not ready, will try again in 1000 ms (479902ms till timeout)
2022-03-28 13:51:31 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:444] KafkaUser: my-user-609064688-1843560098 is in desired state: Ready
2022-03-28 13:51:32 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1218045ms till timeout)
2022-03-28 13:51:32 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 13:51:32 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:155] Create/Update Deployment http-bridge-scram-sha-st-shared-kafka-clients in namespace http-bridge-scram-sha-st
2022-03-28 13:51:32 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:http-bridge-scram-sha-st-shared-kafka-clients
2022-03-28 13:51:32 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (290181ms till timeout)
2022-03-28 13:51:32 [ForkJoinPool-1-worker-5] INFO  [DeploymentUtils:161] Wait for Deployment: http-bridge-scram-sha-st-shared-kafka-clients will be ready
2022-03-28 13:51:32 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Wait for Deployment: http-bridge-scram-sha-st-shared-kafka-clients will be ready
2022-03-28 13:51:32 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Wait for Deployment: http-bridge-tls-st-kafka-clients will be ready not ready, will try again in 1000 ms (478718ms till timeout)
2022-03-28 13:51:32 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Wait for Deployment: http-bridge-scram-sha-st-shared-kafka-clients will be ready not ready, will try again in 1000 ms (479811ms till timeout)
2022-03-28 13:51:33 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-0 get Namespace namespace-1 -o json
2022-03-28 13:51:33 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 13:51:33 [ForkJoinPool-1-worker-13] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c31,c5",
            "openshift.io/sa.scc.supplemental-groups": "1000940000/10000",
            "openshift.io/sa.scc.uid-range": "1000940000/10000"
        },
        "creationTimestamp": "2022-03-28T13:51:23Z",
        "labels": {
            "kubernetes.io/metadata.name": "namespace-1"
        },
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T13:51:23Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:labels": {
                            ".": {},
                            "f:kubernetes.io/metadata.name": {}
                        }
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T13:51:23Z"
            }
        ],
        "name": "namespace-1",
        "resourceVersion": "84558",
        "uid": "a2826b8b-4be4-42d2-84db-3517505ce8f1"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 13:51:33 [ForkJoinPool-1-worker-13] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[http-bridge-scram-sha-st], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[namespace-1], io.strimzi.test.logs.CollectorElement@3e255cd9=[namespace-0]}
2022-03-28 13:51:33 [ForkJoinPool-1-worker-13] INFO  [KubeClusterResource:82] Client use Namespace: namespace-1
2022-03-28 13:51:33 [ForkJoinPool-1-worker-13] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-1, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 13:51:33 [ForkJoinPool-1-worker-13] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-1
2022-03-28 13:51:33 [ForkJoinPool-1-worker-1] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 13:51:33 [ForkJoinPool-1-worker-1] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testThrottlingQuotasCreateTopic=my-cluster-ac2cadd6, testThrottlingQuotasCreateAlterPartitions=my-cluster-8f6c28cb, testCapacityFile=my-cluster-0531d5ce, testCreatingUsersWithSecretPrefix=my-cluster-0c65d2dd}
2022-03-28 13:51:33 [ForkJoinPool-1-worker-1] TRACE [AbstractST:607] USERS_NAME_MAP: {testThrottlingQuotasCreateTopic=my-user-46611685-1079680568, testThrottlingQuotasCreateAlterPartitions=my-user-1557959119-1722636050, testCapacityFile=my-user-155969448-1270555822, testCreatingUsersWithSecretPrefix=my-user-144001639-226807213}
2022-03-28 13:51:33 [ForkJoinPool-1-worker-1] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testThrottlingQuotasCreateTopic=my-topic-114922478-1506136333, testThrottlingQuotasCreateAlterPartitions=my-topic-1449977352-344480794, testCapacityFile=my-topic-296605537-1008743104, testCreatingUsersWithSecretPrefix=my-topic-130462979-2061578809}
2022-03-28 13:51:33 [ForkJoinPool-1-worker-1] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testThrottlingQuotasCreateTopic=my-cluster-ac2cadd6-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-8f6c28cb-kafka-clients, testCapacityFile=my-cluster-0531d5ce-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-0c65d2dd-kafka-clients}
2022-03-28 13:51:33 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:155] Create/Update KafkaUser my-user-1557959119-1722636050 in namespace throttling-quota-st
2022-03-28 13:51:33 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:155] Create/Update Kafka my-cluster-0c65d2dd in namespace namespace-1
2022-03-28 13:51:33 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:164] Using Namespace: namespace-1
2022-03-28 13:51:33 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1216947ms till timeout)
2022-03-28 13:51:33 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-1557959119-1722636050
2022-03-28 13:51:33 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-0c65d2dd
2022-03-28 13:51:33 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:433] Wait for KafkaUser: my-user-1557959119-1722636050 will have desired state: Ready
2022-03-28 13:51:33 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for KafkaUser: my-user-1557959119-1722636050 will have desired state: Ready
2022-03-28 13:51:33 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:433] Wait for Kafka: my-cluster-0c65d2dd will have desired state: Ready
2022-03-28 13:51:33 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Kafka: my-cluster-0c65d2dd will have desired state: Ready
2022-03-28 13:51:33 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:444] KafkaUser: my-user-1557959119-1722636050 is in desired state: Ready
2022-03-28 13:51:33 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-0c65d2dd will have desired state: Ready not ready, will try again in 1000 ms (839810ms till timeout)
2022-03-28 13:51:33 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:155] Create/Update Job create-admin-my-cluster-8f6c28cb-kafka-clients in namespace throttling-quota-st
2022-03-28 13:51:33 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (289022ms till timeout)
2022-03-28 13:51:33 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:create-admin-my-cluster-8f6c28cb-kafka-clients
2022-03-28 13:51:33 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Wait for Deployment: http-bridge-tls-st-kafka-clients will be ready not ready, will try again in 1000 ms (477564ms till timeout)
2022-03-28 13:51:34 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Wait for Deployment: http-bridge-scram-sha-st-shared-kafka-clients will be ready not ready, will try again in 1000 ms (478657ms till timeout)
2022-03-28 13:51:34 [ForkJoinPool-1-worker-1] INFO  [JobUtils:81] Waiting for job: create-admin-my-cluster-8f6c28cb-kafka-clients will be in active state
2022-03-28 13:51:34 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 13:51:34 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1215796ms till timeout)
2022-03-28 13:51:34 [ForkJoinPool-1-worker-1] INFO  [PodUtils:186] Waiting for message will be in the log
2022-03-28 13:51:34 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Waiting for message will be in the log
2022-03-28 13:51:34 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (299889ms till timeout)
2022-03-28 13:51:34 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-0c65d2dd will have desired state: Ready not ready, will try again in 1000 ms (838710ms till timeout)
2022-03-28 13:51:34 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (287917ms till timeout)
2022-03-28 13:51:35 [ForkJoinPool-1-worker-7] INFO  [DeploymentUtils:168] Deployment: http-bridge-tls-st-kafka-clients is ready
2022-03-28 13:51:35 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Wait for Deployment: http-bridge-scram-sha-st-shared-kafka-clients will be ready not ready, will try again in 1000 ms (477557ms till timeout)
2022-03-28 13:51:35 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update KafkaBridge http-bridge-tls-cluster-name in namespace http-bridge-tls-st
2022-03-28 13:51:35 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1214697ms till timeout)
2022-03-28 13:51:35 [ForkJoinPool-1-worker-7] WARN  [VersionUsageUtils:60] The client is using resource type 'kafkabridges' with unstable version 'v1beta2'
2022-03-28 13:51:35 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaBridge:http-bridge-tls-cluster-name
2022-03-28 13:51:35 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (298755ms till timeout)
2022-03-28 13:51:35 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:433] Wait for KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready
2022-03-28 13:51:35 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready
2022-03-28 13:51:35 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-0c65d2dd will have desired state: Ready not ready, will try again in 1000 ms (837560ms till timeout)
2022-03-28 13:51:35 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (479800ms till timeout)
2022-03-28 13:51:36 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (286774ms till timeout)
2022-03-28 13:51:36 [ForkJoinPool-1-worker-5] INFO  [DeploymentUtils:168] Deployment: http-bridge-scram-sha-st-shared-kafka-clients is ready
2022-03-28 13:51:36 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:155] Create/Update KafkaBridge http-bridge-scram-sha-cluster-name in namespace http-bridge-scram-sha-st
2022-03-28 13:51:36 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1213596ms till timeout)
2022-03-28 13:51:36 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaBridge:http-bridge-scram-sha-cluster-name
2022-03-28 13:51:36 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:433] Wait for KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready
2022-03-28 13:51:36 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready
2022-03-28 13:51:36 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (297585ms till timeout)
2022-03-28 13:51:36 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (479758ms till timeout)
2022-03-28 13:51:37 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-0c65d2dd will have desired state: Ready not ready, will try again in 1000 ms (836390ms till timeout)
2022-03-28 13:51:37 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (478632ms till timeout)
2022-03-28 13:51:37 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (285604ms till timeout)
2022-03-28 13:51:37 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 13:51:37 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1212497ms till timeout)
2022-03-28 13:51:37 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (296480ms till timeout)
2022-03-28 13:51:38 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (478658ms till timeout)
2022-03-28 13:51:38 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-0c65d2dd will have desired state: Ready not ready, will try again in 1000 ms (835292ms till timeout)
2022-03-28 13:51:38 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (477533ms till timeout)
2022-03-28 13:51:38 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (284404ms till timeout)
2022-03-28 13:51:38 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1211399ms till timeout)
2022-03-28 13:51:39 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (295378ms till timeout)
2022-03-28 13:51:39 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (477558ms till timeout)
2022-03-28 13:51:39 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-0c65d2dd will have desired state: Ready not ready, will try again in 1000 ms (834194ms till timeout)
2022-03-28 13:51:39 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (476435ms till timeout)
2022-03-28 13:51:39 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (283210ms till timeout)
2022-03-28 13:51:39 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1210299ms till timeout)
2022-03-28 13:51:40 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (294261ms till timeout)
2022-03-28 13:51:40 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (476456ms till timeout)
2022-03-28 13:51:40 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-0c65d2dd will have desired state: Ready not ready, will try again in 1000 ms (833093ms till timeout)
2022-03-28 13:51:40 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (475335ms till timeout)
2022-03-28 13:51:40 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (281996ms till timeout)
2022-03-28 13:51:40 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1209200ms till timeout)
2022-03-28 13:51:41 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (293160ms till timeout)
2022-03-28 13:51:41 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (475353ms till timeout)
2022-03-28 13:51:41 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-0c65d2dd will have desired state: Ready not ready, will try again in 1000 ms (831996ms till timeout)
2022-03-28 13:51:41 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (474238ms till timeout)
2022-03-28 13:51:42 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (280799ms till timeout)
2022-03-28 13:51:42 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1208047ms till timeout)
2022-03-28 13:51:42 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 13:51:42 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (292058ms till timeout)
2022-03-28 13:51:42 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (474254ms till timeout)
2022-03-28 13:51:42 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-0c65d2dd will have desired state: Ready not ready, will try again in 1000 ms (830897ms till timeout)
2022-03-28 13:51:42 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (473140ms till timeout)
2022-03-28 13:51:43 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1206948ms till timeout)
2022-03-28 13:51:43 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (279512ms till timeout)
2022-03-28 13:51:43 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (290956ms till timeout)
2022-03-28 13:51:43 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (473155ms till timeout)
2022-03-28 13:51:43 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-0c65d2dd will have desired state: Ready not ready, will try again in 1000 ms (829799ms till timeout)
2022-03-28 13:51:43 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (472041ms till timeout)
2022-03-28 13:51:44 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1205849ms till timeout)
2022-03-28 13:51:44 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (278316ms till timeout)
2022-03-28 13:51:44 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (289854ms till timeout)
2022-03-28 13:51:44 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (472053ms till timeout)
2022-03-28 13:51:44 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-0c65d2dd will have desired state: Ready not ready, will try again in 1000 ms (828697ms till timeout)
2022-03-28 13:51:44 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (470938ms till timeout)
2022-03-28 13:51:45 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1204748ms till timeout)
2022-03-28 13:51:45 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (277115ms till timeout)
2022-03-28 13:51:45 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (288650ms till timeout)
2022-03-28 13:51:45 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (470869ms till timeout)
2022-03-28 13:51:45 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-0c65d2dd will have desired state: Ready not ready, will try again in 1000 ms (827512ms till timeout)
2022-03-28 13:51:46 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (469754ms till timeout)
2022-03-28 13:51:46 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1203649ms till timeout)
2022-03-28 13:51:46 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (275904ms till timeout)
2022-03-28 13:51:46 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (469752ms till timeout)
2022-03-28 13:51:46 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (287457ms till timeout)
2022-03-28 13:51:47 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-0c65d2dd will have desired state: Ready not ready, will try again in 1000 ms (826408ms till timeout)
2022-03-28 13:51:47 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (468650ms till timeout)
2022-03-28 13:51:47 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 13:51:47 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1202546ms till timeout)
2022-03-28 13:51:48 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (468653ms till timeout)
2022-03-28 13:51:48 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-0c65d2dd will have desired state: Ready not ready, will try again in 1000 ms (825294ms till timeout)
2022-03-28 13:51:48 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (286256ms till timeout)
2022-03-28 13:51:48 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (274609ms till timeout)
2022-03-28 13:51:48 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (467448ms till timeout)
2022-03-28 13:51:48 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1201447ms till timeout)
2022-03-28 13:51:49 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (467552ms till timeout)
2022-03-28 13:51:49 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-0c65d2dd will have desired state: Ready not ready, will try again in 1000 ms (824196ms till timeout)
2022-03-28 13:51:49 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (285063ms till timeout)
2022-03-28 13:51:49 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (466335ms till timeout)
2022-03-28 13:51:49 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (273318ms till timeout)
2022-03-28 13:51:49 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1200348ms till timeout)
2022-03-28 13:51:50 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (466452ms till timeout)
2022-03-28 13:51:50 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-0c65d2dd will have desired state: Ready not ready, will try again in 1000 ms (823097ms till timeout)
2022-03-28 13:51:50 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (283961ms till timeout)
2022-03-28 13:51:50 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (465237ms till timeout)
2022-03-28 13:51:50 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (272121ms till timeout)
2022-03-28 13:51:50 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1199244ms till timeout)
2022-03-28 13:51:51 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (465352ms till timeout)
2022-03-28 13:51:51 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-0c65d2dd will have desired state: Ready not ready, will try again in 1000 ms (821992ms till timeout)
2022-03-28 13:51:51 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (282859ms till timeout)
2022-03-28 13:51:51 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (464135ms till timeout)
2022-03-28 13:51:51 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (270927ms till timeout)
2022-03-28 13:51:51 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1198143ms till timeout)
2022-03-28 13:51:52 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 13:51:52 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (464253ms till timeout)
2022-03-28 13:51:52 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-0c65d2dd will have desired state: Ready not ready, will try again in 1000 ms (820894ms till timeout)
2022-03-28 13:51:52 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (281759ms till timeout)
2022-03-28 13:51:52 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (463035ms till timeout)
2022-03-28 13:51:53 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (269734ms till timeout)
2022-03-28 13:51:53 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1196979ms till timeout)
2022-03-28 13:51:53 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (463155ms till timeout)
2022-03-28 13:51:53 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-0c65d2dd will have desired state: Ready not ready, will try again in 1000 ms (819795ms till timeout)
2022-03-28 13:51:53 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (280657ms till timeout)
2022-03-28 13:51:53 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (461934ms till timeout)
2022-03-28 13:51:54 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (268540ms till timeout)
2022-03-28 13:51:54 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1195784ms till timeout)
2022-03-28 13:51:54 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (462056ms till timeout)
2022-03-28 13:51:54 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-0c65d2dd will have desired state: Ready not ready, will try again in 1000 ms (818697ms till timeout)
2022-03-28 13:51:54 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (279556ms till timeout)
2022-03-28 13:51:54 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (460832ms till timeout)
2022-03-28 13:51:55 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (267345ms till timeout)
2022-03-28 13:51:55 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1194502ms till timeout)
2022-03-28 13:51:55 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (460957ms till timeout)
2022-03-28 13:51:55 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-0c65d2dd will have desired state: Ready not ready, will try again in 1000 ms (817599ms till timeout)
2022-03-28 13:51:55 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (278456ms till timeout)
2022-03-28 13:51:56 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (459729ms till timeout)
2022-03-28 13:51:56 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (266152ms till timeout)
2022-03-28 13:51:56 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1193396ms till timeout)
2022-03-28 13:51:56 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (459859ms till timeout)
2022-03-28 13:51:56 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-0c65d2dd will have desired state: Ready not ready, will try again in 1000 ms (816501ms till timeout)
2022-03-28 13:51:57 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (277352ms till timeout)
2022-03-28 13:51:57 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:444] KafkaBridge: http-bridge-tls-cluster-name is in desired state: Ready
2022-03-28 13:51:57 [ForkJoinPool-1-worker-7] INFO  [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-03-28 13:51:57 [ForkJoinPool-1-worker-7] INFO  [TestSeparator:23] ############################################################################
2022-03-28 13:51:57 [ForkJoinPool-1-worker-7] INFO  [TestSeparator:24] io.strimzi.systemtest.bridge.HttpBridgeTlsST.testSendSimpleMessageTls-STARTED
2022-03-28 13:51:57 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:658] ============================================================================
2022-03-28 13:51:57 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:659] [bridge.HttpBridgeTlsST - Before Each] - Setup test case environment
2022-03-28 13:51:57 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:77] [bridge.HttpBridgeTlsST] - Adding parallel test: testSendSimpleMessageTls
2022-03-28 13:51:57 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:81] [bridge.HttpBridgeTlsST] - Parallel test count: 5
2022-03-28 13:51:57 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:230] testSendSimpleMessageTls test now can proceed its execution
2022-03-28 13:51:57 [ForkJoinPool-1-worker-7] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 13:51:57 [ForkJoinPool-1-worker-7] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testThrottlingQuotasCreateTopic=my-cluster-ac2cadd6, testThrottlingQuotasCreateAlterPartitions=my-cluster-8f6c28cb, testCapacityFile=my-cluster-0531d5ce, testSendSimpleMessageTls=my-cluster-fbc845dd, testCreatingUsersWithSecretPrefix=my-cluster-0c65d2dd}
2022-03-28 13:51:57 [ForkJoinPool-1-worker-7] TRACE [AbstractST:607] USERS_NAME_MAP: {testThrottlingQuotasCreateTopic=my-user-46611685-1079680568, testThrottlingQuotasCreateAlterPartitions=my-user-1557959119-1722636050, testCapacityFile=my-user-155969448-1270555822, testSendSimpleMessageTls=my-user-1754673553-2077518027, testCreatingUsersWithSecretPrefix=my-user-144001639-226807213}
2022-03-28 13:51:57 [ForkJoinPool-1-worker-7] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testThrottlingQuotasCreateTopic=my-topic-114922478-1506136333, testThrottlingQuotasCreateAlterPartitions=my-topic-1449977352-344480794, testCapacityFile=my-topic-296605537-1008743104, testSendSimpleMessageTls=my-topic-115477502-2025274778, testCreatingUsersWithSecretPrefix=my-topic-130462979-2061578809}
2022-03-28 13:51:57 [ForkJoinPool-1-worker-7] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testThrottlingQuotasCreateTopic=my-cluster-ac2cadd6-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-8f6c28cb-kafka-clients, testCapacityFile=my-cluster-0531d5ce-kafka-clients, testSendSimpleMessageTls=my-cluster-fbc845dd-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-0c65d2dd-kafka-clients}
2022-03-28 13:51:57 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update KafkaTopic my-topic-779240419-754104585 in namespace http-bridge-tls-st
2022-03-28 13:51:57 [ForkJoinPool-1-worker-7] WARN  [VersionUsageUtils:60] The client is using resource type 'kafkatopics' with unstable version 'v1beta2'
2022-03-28 13:51:57 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-779240419-754104585
2022-03-28 13:51:57 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 13:51:57 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:433] Wait for KafkaTopic: my-topic-779240419-754104585 will have desired state: Ready
2022-03-28 13:51:57 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for KafkaTopic: my-topic-779240419-754104585 will have desired state: Ready
2022-03-28 13:51:57 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:444] KafkaTopic: my-topic-779240419-754104585 is in desired state: Ready
2022-03-28 13:51:57 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update Job producer-97559606 in namespace http-bridge-tls-st
2022-03-28 13:51:57 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:producer-97559606
2022-03-28 13:51:57 [ForkJoinPool-1-worker-7] INFO  [JobUtils:81] Waiting for job: producer-97559606 will be in active state
2022-03-28 13:51:57 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 13:51:57 [ForkJoinPool-1-worker-7] INFO  [ClientUtils:76] Waiting for producer/consumer:producer-97559606 to finished
2022-03-28 13:51:57 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for job finished
2022-03-28 13:51:57 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1192203ms till timeout)
2022-03-28 13:51:57 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (264951ms till timeout)
2022-03-28 13:51:57 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:444] KafkaBridge: http-bridge-scram-sha-cluster-name is in desired state: Ready
2022-03-28 13:51:57 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job producer-97559606 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:51:53Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:51:57 [ForkJoinPool-1-worker-5] INFO  [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-03-28 13:51:57 [ForkJoinPool-1-worker-5] INFO  [TestSeparator:23] ############################################################################
2022-03-28 13:51:57 [ForkJoinPool-1-worker-5] INFO  [TestSeparator:24] io.strimzi.systemtest.bridge.HttpBridgeScramShaST.testReceiveSimpleMessageTlsScramSha-STARTED
2022-03-28 13:51:57 [ForkJoinPool-1-worker-5] DEBUG [AbstractST:658] ============================================================================
2022-03-28 13:51:57 [ForkJoinPool-1-worker-5] DEBUG [AbstractST:659] [bridge.HttpBridgeScramShaST - Before Each] - Setup test case environment
2022-03-28 13:51:57 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:77] [bridge.HttpBridgeScramShaST] - Adding parallel test: testReceiveSimpleMessageTlsScramSha
2022-03-28 13:51:57 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:81] [bridge.HttpBridgeScramShaST] - Parallel test count: 6
2022-03-28 13:51:57 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:230] testReceiveSimpleMessageTlsScramSha test now can proceed its execution
2022-03-28 13:51:57 [ForkJoinPool-1-worker-5] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 13:51:57 [ForkJoinPool-1-worker-5] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-af298d01, testThrottlingQuotasCreateTopic=my-cluster-ac2cadd6, testThrottlingQuotasCreateAlterPartitions=my-cluster-8f6c28cb, testCapacityFile=my-cluster-0531d5ce, testSendSimpleMessageTls=my-cluster-fbc845dd, testCreatingUsersWithSecretPrefix=my-cluster-0c65d2dd}
2022-03-28 13:51:57 [ForkJoinPool-1-worker-5] TRACE [AbstractST:607] USERS_NAME_MAP: {testReceiveSimpleMessageTlsScramSha=my-user-1597210213-707359243, testThrottlingQuotasCreateTopic=my-user-46611685-1079680568, testThrottlingQuotasCreateAlterPartitions=my-user-1557959119-1722636050, testCapacityFile=my-user-155969448-1270555822, testSendSimpleMessageTls=my-user-1754673553-2077518027, testCreatingUsersWithSecretPrefix=my-user-144001639-226807213}
2022-03-28 13:51:57 [ForkJoinPool-1-worker-5] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-topic-649423016-1623551209, testThrottlingQuotasCreateTopic=my-topic-114922478-1506136333, testThrottlingQuotasCreateAlterPartitions=my-topic-1449977352-344480794, testCapacityFile=my-topic-296605537-1008743104, testSendSimpleMessageTls=my-topic-115477502-2025274778, testCreatingUsersWithSecretPrefix=my-topic-130462979-2061578809}
2022-03-28 13:51:57 [ForkJoinPool-1-worker-5] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-af298d01-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-ac2cadd6-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-8f6c28cb-kafka-clients, testCapacityFile=my-cluster-0531d5ce-kafka-clients, testSendSimpleMessageTls=my-cluster-fbc845dd-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-0c65d2dd-kafka-clients}
2022-03-28 13:51:57 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:155] Create/Update KafkaTopic my-topic-205198573-1992555764 in namespace http-bridge-scram-sha-st
2022-03-28 13:51:58 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-0c65d2dd will have desired state: Ready not ready, will try again in 1000 ms (815352ms till timeout)
2022-03-28 13:51:58 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (219617ms till timeout)
2022-03-28 13:51:58 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-205198573-1992555764
2022-03-28 13:51:58 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (276210ms till timeout)
2022-03-28 13:51:58 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:433] Wait for KafkaTopic: my-topic-205198573-1992555764 will have desired state: Ready
2022-03-28 13:51:58 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for KafkaTopic: my-topic-205198573-1992555764 will have desired state: Ready
2022-03-28 13:51:58 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaTopic: my-topic-205198573-1992555764 will have desired state: Ready not ready, will try again in 1000 ms (179903ms till timeout)
2022-03-28 13:51:58 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1191103ms till timeout)
2022-03-28 13:51:59 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (263664ms till timeout)
2022-03-28 13:51:59 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-0c65d2dd will have desired state: Ready not ready, will try again in 1000 ms (814249ms till timeout)
2022-03-28 13:51:59 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job producer-97559606 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:51:53Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:51:59 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (275025ms till timeout)
2022-03-28 13:51:59 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (218322ms till timeout)
2022-03-28 13:51:59 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:444] KafkaTopic: my-topic-205198573-1992555764 is in desired state: Ready
2022-03-28 13:51:59 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:155] Create/Update Job consumer-408340879 in namespace http-bridge-scram-sha-st
2022-03-28 13:51:59 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:consumer-408340879
2022-03-28 13:51:59 [ForkJoinPool-1-worker-5] INFO  [JobUtils:81] Waiting for job: consumer-408340879 will be in active state
2022-03-28 13:51:59 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 13:51:59 [ForkJoinPool-1-worker-5] INFO  [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-03-28 13:51:59 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:155] Create/Update Job producer-828793479 in namespace http-bridge-scram-sha-st
2022-03-28 13:52:00 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:producer-828793479
2022-03-28 13:52:00 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1189912ms till timeout)
2022-03-28 13:52:00 [ForkJoinPool-1-worker-5] INFO  [JobUtils:81] Waiting for job: producer-828793479 will be in active state
2022-03-28 13:52:00 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 13:52:00 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-0c65d2dd will have desired state: Ready not ready, will try again in 1000 ms (813060ms till timeout)
2022-03-28 13:52:00 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (262467ms till timeout)
2022-03-28 13:52:00 [ForkJoinPool-1-worker-5] INFO  [ClientUtils:61] Waiting till producer producer-828793479 and consumer consumer-408340879 finish
2022-03-28 13:52:00 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for clients finished
2022-03-28 13:52:00 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (273826ms till timeout)
2022-03-28 13:52:00 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job producer-97559606 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:51:53Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:52:00 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (219804ms till timeout)
2022-03-28 13:52:00 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (217030ms till timeout)
2022-03-28 13:52:01 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1188811ms till timeout)
2022-03-28 13:52:01 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-0c65d2dd will have desired state: Ready not ready, will try again in 1000 ms (811958ms till timeout)
2022-03-28 13:52:01 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (261269ms till timeout)
2022-03-28 13:52:01 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (272723ms till timeout)
2022-03-28 13:52:01 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (218700ms till timeout)
2022-03-28 13:52:01 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job producer-97559606 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:51:53Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:52:01 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (215830ms till timeout)
2022-03-28 13:52:02 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 13:52:02 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-0531d5ce will have desired state: Ready not ready, will try again in 1000 ms (1187711ms till timeout)
2022-03-28 13:52:02 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-0c65d2dd will have desired state: Ready not ready, will try again in 1000 ms (810858ms till timeout)
2022-03-28 13:52:02 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (260164ms till timeout)
2022-03-28 13:52:02 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (271621ms till timeout)
2022-03-28 13:52:02 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (217598ms till timeout)
2022-03-28 13:52:03 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job producer-97559606 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:51:53Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:52:03 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (214632ms till timeout)
2022-03-28 13:52:03 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:444] Kafka: my-cluster-0531d5ce is in desired state: Ready
2022-03-28 13:52:03 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-0c65d2dd will have desired state: Ready not ready, will try again in 1000 ms (809756ms till timeout)
2022-03-28 13:52:03 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-0 exec my-cluster-0531d5ce-cruise-control-c866c8ddb-qx869 -- /bin/bash -c cat /tmp/capacity.json
2022-03-28 13:52:03 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (259047ms till timeout)
2022-03-28 13:52:03 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (270508ms till timeout)
2022-03-28 13:52:03 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (216484ms till timeout)
2022-03-28 13:52:04 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job producer-97559606 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:51:53Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:52:04 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (213434ms till timeout)
2022-03-28 13:52:04 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-0c65d2dd will have desired state: Ready not ready, will try again in 1000 ms (808657ms till timeout)
2022-03-28 13:52:04 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (257942ms till timeout)
2022-03-28 13:52:04 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (269400ms till timeout)
2022-03-28 13:52:05 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (215377ms till timeout)
2022-03-28 13:52:05 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job producer-97559606 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:51:53Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:52:05 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (212239ms till timeout)
2022-03-28 13:52:05 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-0c65d2dd will have desired state: Ready not ready, will try again in 1000 ms (807559ms till timeout)
2022-03-28 13:52:06 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (256748ms till timeout)
2022-03-28 13:52:06 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (268295ms till timeout)
2022-03-28 13:52:06 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (214279ms till timeout)
2022-03-28 13:52:06 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job producer-97559606 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:51:53Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:52:06 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (211043ms till timeout)
2022-03-28 13:52:06 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-0c65d2dd will have desired state: Ready not ready, will try again in 1000 ms (806462ms till timeout)
2022-03-28 13:52:07 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (255554ms till timeout)
2022-03-28 13:52:07 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (213178ms till timeout)
2022-03-28 13:52:07 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (267101ms till timeout)
2022-03-28 13:52:07 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 13:52:07 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job producer-97559606 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:51:53Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:52:07 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (209847ms till timeout)
2022-03-28 13:52:08 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-0c65d2dd will have desired state: Ready not ready, will try again in 1000 ms (805241ms till timeout)
2022-03-28 13:52:08 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (254360ms till timeout)
2022-03-28 13:52:08 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (211983ms till timeout)
2022-03-28 13:52:08 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (265903ms till timeout)
2022-03-28 13:52:09 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job producer-97559606 in namespace http-bridge-tls-st, has status JobStatus(active=null, completedIndexes=null, completionTime=2022-03-28T13:52:04Z, conditions=[JobCondition(lastProbeTime=2022-03-28T13:52:04Z, lastTransitionTime=2022-03-28T13:52:04Z, message=null, reason=null, status=True, type=Complete, additionalProperties={})], failed=null, ready=null, startTime=2022-03-28T13:51:53Z, succeeded=1, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:52:09 [ForkJoinPool-1-worker-7] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment producer-97559606 deletion
2022-03-28 13:52:09 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for ReplicaSet producer-97559606 to be deleted
2022-03-28 13:52:09 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-0c65d2dd will have desired state: Ready not ready, will try again in 1000 ms (804092ms till timeout)
2022-03-28 13:52:09 [ForkJoinPool-1-worker-7] DEBUG [JobUtils:40] Job producer-97559606 was deleted
2022-03-28 13:52:09 [ForkJoinPool-1-worker-7] INFO  [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-03-28 13:52:09 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update Job consumer-1209157228 in namespace http-bridge-tls-st
2022-03-28 13:52:09 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:consumer-1209157228
2022-03-28 13:52:09 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (264761ms till timeout)
2022-03-28 13:52:09 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (253115ms till timeout)
2022-03-28 13:52:09 [ForkJoinPool-1-worker-7] INFO  [JobUtils:81] Waiting for job: consumer-1209157228 will be in active state
2022-03-28 13:52:09 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 13:52:09 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (210647ms till timeout)
2022-03-28 13:52:09 [ForkJoinPool-1-worker-7] INFO  [ClientUtils:76] Waiting for producer/consumer:consumer-1209157228 to finished
2022-03-28 13:52:09 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for job finished
2022-03-28 13:52:10 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job consumer-1209157228 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:52:05Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:52:10 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (219804ms till timeout)
2022-03-28 13:52:10 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Command: oc --namespace namespace-0 exec my-cluster-0531d5ce-cruise-control-c866c8ddb-qx869 -- /bin/bash -c cat /tmp/capacity.json
2022-03-28 13:52:10 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 13:52:10 [ForkJoinPool-1-worker-3] INFO  [CruiseControlConfigurationST:80] We got only one configuration of broker-capacities
2022-03-28 13:52:10 [ForkJoinPool-1-worker-3] INFO  [CruiseControlConfigurationST:83] Verifying cruise control configuration.
2022-03-28 13:52:10 [ForkJoinPool-1-worker-3] INFO  [CruiseControlConfigurationST:92] Verifying default cruise control capacities
2022-03-28 13:52:10 [ForkJoinPool-1-worker-3] DEBUG [AbstractST:674] ============================================================================
2022-03-28 13:52:10 [ForkJoinPool-1-worker-3] DEBUG [AbstractST:675] [cruisecontrol.CruiseControlConfigurationST - After Each] - Clean up after test
2022-03-28 13:52:10 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:344] ############################################################################
2022-03-28 13:52:10 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:348] Delete all resources for testCapacityFile
2022-03-28 13:52:10 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:241] Delete of Kafka my-cluster-0531d5ce in namespace namespace-0
2022-03-28 13:52:10 [ForkJoinPool-1-worker-3] INFO  [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-0, for cruise control Kafka cluster my-cluster-0531d5ce
2022-03-28 13:52:10 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-0c65d2dd will have desired state: Ready not ready, will try again in 1000 ms (802927ms till timeout)
2022-03-28 13:52:10 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (263562ms till timeout)
2022-03-28 13:52:10 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (252004ms till timeout)
2022-03-28 13:52:11 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job consumer-1209157228 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:52:05Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:52:11 [ForkJoinPool-1-worker-5] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment producer-828793479 deletion
2022-03-28 13:52:11 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for ReplicaSet producer-828793479 to be deleted
2022-03-28 13:52:11 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (218430ms till timeout)
2022-03-28 13:52:11 [ForkJoinPool-1-worker-5] DEBUG [JobUtils:40] Job producer-828793479 was deleted
2022-03-28 13:52:11 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-0531d5ce
2022-03-28 13:52:11 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-0c65d2dd will have desired state: Ready not ready, will try again in 1000 ms (801732ms till timeout)
2022-03-28 13:52:11 [ForkJoinPool-1-worker-5] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment consumer-408340879 deletion
2022-03-28 13:52:11 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for ReplicaSet consumer-408340879 to be deleted
2022-03-28 13:52:11 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:369] ############################################################################
2022-03-28 13:52:11 [ForkJoinPool-1-worker-3] INFO  [TestSuiteNamespaceManager:200] Deleting namespace:namespace-0 for test case:testCapacityFile
2022-03-28 13:52:11 [ForkJoinPool-1-worker-5] DEBUG [JobUtils:40] Job consumer-408340879 was deleted
2022-03-28 13:52:11 [ForkJoinPool-1-worker-5] DEBUG [AbstractST:674] ============================================================================
2022-03-28 13:52:11 [ForkJoinPool-1-worker-5] DEBUG [AbstractST:675] [bridge.HttpBridgeScramShaST - After Each] - Clean up after test
2022-03-28 13:52:11 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:344] ############################################################################
2022-03-28 13:52:11 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:348] Delete all resources for testReceiveSimpleMessageTlsScramSha
2022-03-28 13:52:11 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:241] Delete of Job consumer-408340879 in namespace http-bridge-scram-sha-st
2022-03-28 13:52:12 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Namespace namespace-0 removal
2022-03-28 13:52:12 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (262363ms till timeout)
2022-03-28 13:52:12 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 13:52:12 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (250808ms till timeout)
2022-03-28 13:52:12 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:consumer-408340879
2022-03-28 13:52:12 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:241] Delete of Job producer-828793479 in namespace http-bridge-scram-sha-st
2022-03-28 13:52:12 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 13:52:12 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:producer-828793479
2022-03-28 13:52:12 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:241] Delete of KafkaTopic my-topic-205198573-1992555764 in namespace http-bridge-scram-sha-st
2022-03-28 13:52:12 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 13:52:12 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 13:52:12 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (479488ms till timeout)
2022-03-28 13:52:12 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job consumer-1209157228 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:52:05Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:52:12 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-205198573-1992555764
2022-03-28 13:52:12 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (217137ms till timeout)
2022-03-28 13:52:12 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-0c65d2dd will have desired state: Ready not ready, will try again in 1000 ms (800554ms till timeout)
2022-03-28 13:52:12 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:369] ############################################################################
2022-03-28 13:52:12 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:267] testReceiveSimpleMessageTlsScramSha - Notifies waiting test cases:[testCapacityFile, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testThrottlingQuotasCreateAlterPartitions, testSendSimpleMessageTls, testReceiveSimpleMessageTlsScramSha] to and randomly select one to start execution
2022-03-28 13:52:12 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:93] [bridge.HttpBridgeScramShaST] - Removing parallel test: testReceiveSimpleMessageTlsScramSha
2022-03-28 13:52:12 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:97] [bridge.HttpBridgeScramShaST] - Parallel test count: 5
2022-03-28 13:52:12 [ForkJoinPool-1-worker-5] INFO  [TestSeparator:29] io.strimzi.systemtest.bridge.HttpBridgeScramShaST.testReceiveSimpleMessageTlsScramSha-FINISHED
2022-03-28 13:52:12 [ForkJoinPool-1-worker-5] INFO  [TestSeparator:30] ############################################################################
2022-03-28 13:52:12 [ForkJoinPool-1-worker-5] INFO  [TestSeparator:23] ############################################################################
2022-03-28 13:52:12 [ForkJoinPool-1-worker-5] INFO  [TestSeparator:24] io.strimzi.systemtest.bridge.HttpBridgeScramShaST.testSendSimpleMessageTlsScramSha-STARTED
2022-03-28 13:52:12 [ForkJoinPool-1-worker-5] DEBUG [AbstractST:658] ============================================================================
2022-03-28 13:52:12 [ForkJoinPool-1-worker-5] DEBUG [AbstractST:659] [bridge.HttpBridgeScramShaST - Before Each] - Setup test case environment
2022-03-28 13:52:12 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:77] [bridge.HttpBridgeScramShaST] - Adding parallel test: testSendSimpleMessageTlsScramSha
2022-03-28 13:52:12 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:81] [bridge.HttpBridgeScramShaST] - Parallel test count: 6
2022-03-28 13:52:12 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:230] testSendSimpleMessageTlsScramSha test now can proceed its execution
2022-03-28 13:52:12 [ForkJoinPool-1-worker-5] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 13:52:12 [ForkJoinPool-1-worker-5] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-af298d01, testThrottlingQuotasCreateTopic=my-cluster-ac2cadd6, testThrottlingQuotasCreateAlterPartitions=my-cluster-8f6c28cb, testCapacityFile=my-cluster-0531d5ce, testSendSimpleMessageTls=my-cluster-fbc845dd, testCreatingUsersWithSecretPrefix=my-cluster-0c65d2dd, testSendSimpleMessageTlsScramSha=my-cluster-e353214b}
2022-03-28 13:52:12 [ForkJoinPool-1-worker-5] TRACE [AbstractST:607] USERS_NAME_MAP: {testReceiveSimpleMessageTlsScramSha=my-user-1597210213-707359243, testThrottlingQuotasCreateTopic=my-user-46611685-1079680568, testThrottlingQuotasCreateAlterPartitions=my-user-1557959119-1722636050, testCapacityFile=my-user-155969448-1270555822, testSendSimpleMessageTls=my-user-1754673553-2077518027, testCreatingUsersWithSecretPrefix=my-user-144001639-226807213, testSendSimpleMessageTlsScramSha=my-user-182357028-1132873010}
2022-03-28 13:52:12 [ForkJoinPool-1-worker-5] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-topic-649423016-1623551209, testThrottlingQuotasCreateTopic=my-topic-114922478-1506136333, testThrottlingQuotasCreateAlterPartitions=my-topic-1449977352-344480794, testCapacityFile=my-topic-296605537-1008743104, testSendSimpleMessageTls=my-topic-115477502-2025274778, testCreatingUsersWithSecretPrefix=my-topic-130462979-2061578809, testSendSimpleMessageTlsScramSha=my-topic-652506937-1802310709}
2022-03-28 13:52:12 [ForkJoinPool-1-worker-5] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-af298d01-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-ac2cadd6-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-8f6c28cb-kafka-clients, testCapacityFile=my-cluster-0531d5ce-kafka-clients, testSendSimpleMessageTls=my-cluster-fbc845dd-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-0c65d2dd-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-e353214b-kafka-clients}
2022-03-28 13:52:12 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:155] Create/Update KafkaTopic my-topic-2010366894-647311872 in namespace http-bridge-scram-sha-st
2022-03-28 13:52:13 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-2010366894-647311872
2022-03-28 13:52:13 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (261256ms till timeout)
2022-03-28 13:52:13 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:433] Wait for KafkaTopic: my-topic-2010366894-647311872 will have desired state: Ready
2022-03-28 13:52:13 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for KafkaTopic: my-topic-2010366894-647311872 will have desired state: Ready
2022-03-28 13:52:13 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (249609ms till timeout)
2022-03-28 13:52:13 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaTopic: my-topic-2010366894-647311872 will have desired state: Ready not ready, will try again in 1000 ms (179903ms till timeout)
2022-03-28 13:52:13 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 13:52:13 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job consumer-1209157228 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:52:05Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:52:13 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-0c65d2dd will have desired state: Ready not ready, will try again in 1000 ms (799450ms till timeout)
2022-03-28 13:52:14 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (215841ms till timeout)
2022-03-28 13:52:14 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 13:52:14 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 13:52:14 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (477859ms till timeout)
2022-03-28 13:52:14 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (260152ms till timeout)
2022-03-28 13:52:14 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (248499ms till timeout)
2022-03-28 13:52:14 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:444] KafkaTopic: my-topic-2010366894-647311872 is in desired state: Ready
2022-03-28 13:52:14 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:155] Create/Update Job producer-536361300 in namespace http-bridge-scram-sha-st
2022-03-28 13:52:14 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:producer-536361300
2022-03-28 13:52:14 [ForkJoinPool-1-worker-5] INFO  [JobUtils:81] Waiting for job: producer-536361300 will be in active state
2022-03-28 13:52:14 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 13:52:14 [ForkJoinPool-1-worker-5] INFO  [ClientUtils:76] Waiting for producer/consumer:producer-536361300 to finished
2022-03-28 13:52:14 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for job finished
2022-03-28 13:52:14 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job producer-536361300 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:52:10Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:52:14 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (219803ms till timeout)
2022-03-28 13:52:15 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-0c65d2dd will have desired state: Ready not ready, will try again in 1000 ms (798351ms till timeout)
2022-03-28 13:52:15 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 13:52:15 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job consumer-1209157228 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:52:05Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:52:15 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (214644ms till timeout)
2022-03-28 13:52:15 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (259015ms till timeout)
2022-03-28 13:52:15 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (247362ms till timeout)
2022-03-28 13:52:16 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job producer-536361300 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:52:10Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:52:16 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (218605ms till timeout)
2022-03-28 13:52:16 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-0c65d2dd will have desired state: Ready not ready, will try again in 1000 ms (797202ms till timeout)
2022-03-28 13:52:16 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job consumer-1209157228 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:52:05Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:52:16 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (213448ms till timeout)
2022-03-28 13:52:16 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (257828ms till timeout)
2022-03-28 13:52:16 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (246175ms till timeout)
2022-03-28 13:52:17 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job producer-536361300 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:52:10Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:52:17 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 13:52:17 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-0c65d2dd will have desired state: Ready not ready, will try again in 1000 ms (796098ms till timeout)
2022-03-28 13:52:17 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (217311ms till timeout)
2022-03-28 13:52:17 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job consumer-1209157228 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:52:05Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:52:17 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (256726ms till timeout)
2022-03-28 13:52:17 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (212149ms till timeout)
2022-03-28 13:52:17 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (244967ms till timeout)
2022-03-28 13:52:18 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-0c65d2dd will have desired state: Ready not ready, will try again in 1000 ms (794998ms till timeout)
2022-03-28 13:52:18 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job producer-536361300 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:52:10Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:52:18 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (216111ms till timeout)
2022-03-28 13:52:18 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (255625ms till timeout)
2022-03-28 13:52:18 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job consumer-1209157228 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:52:05Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:52:18 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (210948ms till timeout)
2022-03-28 13:52:19 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (243772ms till timeout)
2022-03-28 13:52:19 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-0c65d2dd will have desired state: Ready not ready, will try again in 1000 ms (793896ms till timeout)
2022-03-28 13:52:19 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job producer-536361300 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:52:10Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:52:19 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (214913ms till timeout)
2022-03-28 13:52:19 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (254474ms till timeout)
2022-03-28 13:52:20 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job consumer-1209157228 in namespace http-bridge-tls-st, has status JobStatus(active=null, completedIndexes=null, completionTime=2022-03-28T13:52:15Z, conditions=[JobCondition(lastProbeTime=2022-03-28T13:52:15Z, lastTransitionTime=2022-03-28T13:52:15Z, message=null, reason=null, status=True, type=Complete, additionalProperties={})], failed=null, ready=null, startTime=2022-03-28T13:52:05Z, succeeded=1, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:52:20 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (242576ms till timeout)
2022-03-28 13:52:20 [ForkJoinPool-1-worker-7] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment consumer-1209157228 deletion
2022-03-28 13:52:20 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for ReplicaSet consumer-1209157228 to be deleted
2022-03-28 13:52:20 [ForkJoinPool-1-worker-7] DEBUG [JobUtils:40] Job consumer-1209157228 was deleted
2022-03-28 13:52:20 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:674] ============================================================================
2022-03-28 13:52:20 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:675] [bridge.HttpBridgeTlsST - After Each] - Clean up after test
2022-03-28 13:52:20 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:344] ############################################################################
2022-03-28 13:52:20 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:348] Delete all resources for testSendSimpleMessageTls
2022-03-28 13:52:20 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:241] Delete of Job producer-97559606 in namespace http-bridge-tls-st
2022-03-28 13:52:20 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:producer-97559606
2022-03-28 13:52:20 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-0c65d2dd will have desired state: Ready not ready, will try again in 1000 ms (792770ms till timeout)
2022-03-28 13:52:20 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:241] Delete of Job consumer-1209157228 in namespace http-bridge-tls-st
2022-03-28 13:52:20 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 13:52:20 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 13:52:20 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (471270ms till timeout)
2022-03-28 13:52:20 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:consumer-1209157228
2022-03-28 13:52:20 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job producer-536361300 in namespace http-bridge-scram-sha-st, has status JobStatus(active=null, completedIndexes=null, completionTime=2022-03-28T13:52:15Z, conditions=[JobCondition(lastProbeTime=2022-03-28T13:52:15Z, lastTransitionTime=2022-03-28T13:52:15Z, message=null, reason=null, status=True, type=Complete, additionalProperties={})], failed=null, ready=null, startTime=2022-03-28T13:52:10Z, succeeded=1, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:52:21 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:241] Delete of KafkaTopic my-topic-779240419-754104585 in namespace http-bridge-tls-st
2022-03-28 13:52:21 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (253350ms till timeout)
2022-03-28 13:52:21 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-779240419-754104585
2022-03-28 13:52:21 [ForkJoinPool-1-worker-5] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment producer-536361300 deletion
2022-03-28 13:52:21 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for ReplicaSet producer-536361300 to be deleted
2022-03-28 13:52:21 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:369] ############################################################################
2022-03-28 13:52:21 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:267] testSendSimpleMessageTls - Notifies waiting test cases:[testCapacityFile, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testThrottlingQuotasCreateAlterPartitions, testSendSimpleMessageTls, testReceiveSimpleMessageTlsScramSha, testSendSimpleMessageTlsScramSha] to and randomly select one to start execution
2022-03-28 13:52:21 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:93] [bridge.HttpBridgeTlsST] - Removing parallel test: testSendSimpleMessageTls
2022-03-28 13:52:21 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:97] [bridge.HttpBridgeTlsST] - Parallel test count: 5
2022-03-28 13:52:21 [ForkJoinPool-1-worker-7] INFO  [TestSeparator:29] io.strimzi.systemtest.bridge.HttpBridgeTlsST.testSendSimpleMessageTls-FINISHED
2022-03-28 13:52:21 [ForkJoinPool-1-worker-7] INFO  [TestSeparator:30] ############################################################################
2022-03-28 13:52:21 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (241416ms till timeout)
2022-03-28 13:52:21 [ForkJoinPool-1-worker-7] INFO  [TestSeparator:23] ############################################################################
2022-03-28 13:52:21 [ForkJoinPool-1-worker-7] INFO  [TestSeparator:24] io.strimzi.systemtest.bridge.HttpBridgeTlsST.testReceiveSimpleMessageTls-STARTED
2022-03-28 13:52:21 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:658] ============================================================================
2022-03-28 13:52:21 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:659] [bridge.HttpBridgeTlsST - Before Each] - Setup test case environment
2022-03-28 13:52:21 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:77] [bridge.HttpBridgeTlsST] - Adding parallel test: testReceiveSimpleMessageTls
2022-03-28 13:52:21 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:81] [bridge.HttpBridgeTlsST] - Parallel test count: 6
2022-03-28 13:52:21 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:230] testReceiveSimpleMessageTls test now can proceed its execution
2022-03-28 13:52:21 [ForkJoinPool-1-worker-7] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 13:52:21 [ForkJoinPool-1-worker-7] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-af298d01, testThrottlingQuotasCreateTopic=my-cluster-ac2cadd6, testThrottlingQuotasCreateAlterPartitions=my-cluster-8f6c28cb, testCapacityFile=my-cluster-0531d5ce, testSendSimpleMessageTls=my-cluster-fbc845dd, testCreatingUsersWithSecretPrefix=my-cluster-0c65d2dd, testReceiveSimpleMessageTls=my-cluster-ace1fa24, testSendSimpleMessageTlsScramSha=my-cluster-e353214b}
2022-03-28 13:52:21 [ForkJoinPool-1-worker-7] TRACE [AbstractST:607] USERS_NAME_MAP: {testReceiveSimpleMessageTlsScramSha=my-user-1597210213-707359243, testThrottlingQuotasCreateTopic=my-user-46611685-1079680568, testThrottlingQuotasCreateAlterPartitions=my-user-1557959119-1722636050, testCapacityFile=my-user-155969448-1270555822, testSendSimpleMessageTls=my-user-1754673553-2077518027, testCreatingUsersWithSecretPrefix=my-user-144001639-226807213, testReceiveSimpleMessageTls=my-user-1418772225-1024845009, testSendSimpleMessageTlsScramSha=my-user-182357028-1132873010}
2022-03-28 13:52:21 [ForkJoinPool-1-worker-7] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-topic-649423016-1623551209, testThrottlingQuotasCreateTopic=my-topic-114922478-1506136333, testThrottlingQuotasCreateAlterPartitions=my-topic-1449977352-344480794, testCapacityFile=my-topic-296605537-1008743104, testSendSimpleMessageTls=my-topic-115477502-2025274778, testCreatingUsersWithSecretPrefix=my-topic-130462979-2061578809, testReceiveSimpleMessageTls=my-topic-1131768783-120051821, testSendSimpleMessageTlsScramSha=my-topic-652506937-1802310709}
2022-03-28 13:52:21 [ForkJoinPool-1-worker-7] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-af298d01-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-ac2cadd6-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-8f6c28cb-kafka-clients, testCapacityFile=my-cluster-0531d5ce-kafka-clients, testSendSimpleMessageTls=my-cluster-fbc845dd-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-0c65d2dd-kafka-clients, testReceiveSimpleMessageTls=my-cluster-ace1fa24-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-e353214b-kafka-clients}
2022-03-28 13:52:21 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update KafkaTopic my-topic-348502360-1448005789 in namespace http-bridge-tls-st
2022-03-28 13:52:21 [ForkJoinPool-1-worker-5] DEBUG [JobUtils:40] Job producer-536361300 was deleted
2022-03-28 13:52:21 [ForkJoinPool-1-worker-5] INFO  [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-03-28 13:52:21 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-348502360-1448005789
2022-03-28 13:52:21 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:155] Create/Update Job consumer-518626488 in namespace http-bridge-scram-sha-st
2022-03-28 13:52:21 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 13:52:21 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-0c65d2dd will have desired state: Ready not ready, will try again in 1000 ms (791630ms till timeout)
2022-03-28 13:52:21 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:433] Wait for KafkaTopic: my-topic-348502360-1448005789 will have desired state: Ready
2022-03-28 13:52:21 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for KafkaTopic: my-topic-348502360-1448005789 will have desired state: Ready
2022-03-28 13:52:21 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:consumer-518626488
2022-03-28 13:52:21 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaTopic: my-topic-348502360-1448005789 will have desired state: Ready not ready, will try again in 1000 ms (179814ms till timeout)
2022-03-28 13:52:22 [ForkJoinPool-1-worker-5] INFO  [JobUtils:81] Waiting for job: consumer-518626488 will be in active state
2022-03-28 13:52:22 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 13:52:22 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (252214ms till timeout)
2022-03-28 13:52:22 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 13:52:22 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 13:52:22 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (469799ms till timeout)
2022-03-28 13:52:22 [ForkJoinPool-1-worker-5] INFO  [ClientUtils:76] Waiting for producer/consumer:consumer-518626488 to finished
2022-03-28 13:52:22 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for job finished
2022-03-28 13:52:22 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 13:52:22 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job consumer-518626488 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:52:17Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:52:22 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (219800ms till timeout)
2022-03-28 13:52:22 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (240264ms till timeout)
2022-03-28 13:52:22 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-0c65d2dd will have desired state: Ready not ready, will try again in 1000 ms (790530ms till timeout)
2022-03-28 13:52:23 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:444] KafkaTopic: my-topic-348502360-1448005789 is in desired state: Ready
2022-03-28 13:52:23 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update Job consumer-996751443 in namespace http-bridge-tls-st
2022-03-28 13:52:23 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:consumer-996751443
2022-03-28 13:52:23 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 13:52:23 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (251113ms till timeout)
2022-03-28 13:52:23 [ForkJoinPool-1-worker-7] INFO  [JobUtils:81] Waiting for job: consumer-996751443 will be in active state
2022-03-28 13:52:23 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 13:52:23 [ForkJoinPool-1-worker-7] INFO  [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-03-28 13:52:23 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update Job producer-645321552 in namespace http-bridge-tls-st
2022-03-28 13:52:23 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job consumer-518626488 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:52:17Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:52:23 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:producer-645321552
2022-03-28 13:52:23 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 13:52:23 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 13:52:23 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (468346ms till timeout)
2022-03-28 13:52:23 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (218515ms till timeout)
2022-03-28 13:52:23 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (238987ms till timeout)
2022-03-28 13:52:23 [ForkJoinPool-1-worker-7] INFO  [JobUtils:81] Waiting for job: producer-645321552 will be in active state
2022-03-28 13:52:23 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 13:52:23 [ForkJoinPool-1-worker-7] INFO  [ClientUtils:61] Waiting till producer producer-645321552 and consumer consumer-996751443 finish
2022-03-28 13:52:23 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for clients finished
2022-03-28 13:52:24 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-0c65d2dd will have desired state: Ready not ready, will try again in 1000 ms (789377ms till timeout)
2022-03-28 13:52:24 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (219807ms till timeout)
2022-03-28 13:52:24 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (250012ms till timeout)
2022-03-28 13:52:24 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 13:52:24 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job consumer-518626488 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:52:17Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:52:24 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (237878ms till timeout)
2022-03-28 13:52:25 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (217222ms till timeout)
2022-03-28 13:52:25 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 13:52:25 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 13:52:25 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (466907ms till timeout)
2022-03-28 13:52:25 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-0c65d2dd will have desired state: Ready not ready, will try again in 1000 ms (788278ms till timeout)
2022-03-28 13:52:25 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (218710ms till timeout)
2022-03-28 13:52:25 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (248908ms till timeout)
2022-03-28 13:52:26 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (236773ms till timeout)
2022-03-28 13:52:26 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 13:52:26 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job consumer-518626488 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:52:17Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:52:26 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-0c65d2dd will have desired state: Ready not ready, will try again in 1000 ms (787178ms till timeout)
2022-03-28 13:52:26 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (215932ms till timeout)
2022-03-28 13:52:26 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (217609ms till timeout)
2022-03-28 13:52:26 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 13:52:26 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 13:52:26 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (465445ms till timeout)
2022-03-28 13:52:26 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (247802ms till timeout)
2022-03-28 13:52:27 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (235670ms till timeout)
2022-03-28 13:52:27 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 13:52:27 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-0c65d2dd will have desired state: Ready not ready, will try again in 1000 ms (786079ms till timeout)
2022-03-28 13:52:27 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job consumer-518626488 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:52:17Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:52:27 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (216510ms till timeout)
2022-03-28 13:52:27 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (214734ms till timeout)
2022-03-28 13:52:27 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 13:52:27 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (246701ms till timeout)
2022-03-28 13:52:28 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 13:52:28 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 13:52:28 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (463970ms till timeout)
2022-03-28 13:52:28 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (234447ms till timeout)
2022-03-28 13:52:28 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-0c65d2dd will have desired state: Ready not ready, will try again in 1000 ms (784980ms till timeout)
2022-03-28 13:52:28 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (215412ms till timeout)
2022-03-28 13:52:28 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job consumer-518626488 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:52:17Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:52:28 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (213538ms till timeout)
2022-03-28 13:52:28 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (245556ms till timeout)
2022-03-28 13:52:29 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 13:52:29 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 13:52:29 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 13:52:29 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (462507ms till timeout)
2022-03-28 13:52:29 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (233254ms till timeout)
2022-03-28 13:52:29 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-0c65d2dd will have desired state: Ready not ready, will try again in 1000 ms (783835ms till timeout)
2022-03-28 13:52:29 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (214177ms till timeout)
2022-03-28 13:52:29 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job consumer-518626488 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:52:17Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:52:29 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (244423ms till timeout)
2022-03-28 13:52:30 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (212211ms till timeout)
2022-03-28 13:52:30 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 13:52:30 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (232059ms till timeout)
2022-03-28 13:52:30 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-0c65d2dd will have desired state: Ready not ready, will try again in 1000 ms (782642ms till timeout)
2022-03-28 13:52:30 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 13:52:30 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 13:52:30 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (461064ms till timeout)
2022-03-28 13:52:31 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (243322ms till timeout)
2022-03-28 13:52:31 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job consumer-518626488 in namespace http-bridge-scram-sha-st, has status JobStatus(active=null, completedIndexes=null, completionTime=2022-03-28T13:52:26Z, conditions=[JobCondition(lastProbeTime=2022-03-28T13:52:26Z, lastTransitionTime=2022-03-28T13:52:26Z, message=null, reason=null, status=True, type=Complete, additionalProperties={})], failed=null, ready=null, startTime=2022-03-28T13:52:17Z, succeeded=1, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:52:31 [ForkJoinPool-1-worker-7] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment producer-645321552 deletion
2022-03-28 13:52:31 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for ReplicaSet producer-645321552 to be deleted
2022-03-28 13:52:31 [ForkJoinPool-1-worker-7] DEBUG [JobUtils:40] Job producer-645321552 was deleted
2022-03-28 13:52:31 [ForkJoinPool-1-worker-5] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment consumer-518626488 deletion
2022-03-28 13:52:31 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for ReplicaSet consumer-518626488 to be deleted
2022-03-28 13:52:31 [ForkJoinPool-1-worker-7] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment consumer-996751443 deletion
2022-03-28 13:52:31 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for ReplicaSet consumer-996751443 to be deleted
2022-03-28 13:52:31 [ForkJoinPool-1-worker-5] DEBUG [JobUtils:40] Job consumer-518626488 was deleted
2022-03-28 13:52:31 [ForkJoinPool-1-worker-5] DEBUG [AbstractST:674] ============================================================================
2022-03-28 13:52:31 [ForkJoinPool-1-worker-5] DEBUG [AbstractST:675] [bridge.HttpBridgeScramShaST - After Each] - Clean up after test
2022-03-28 13:52:31 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:344] ############################################################################
2022-03-28 13:52:31 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:348] Delete all resources for testSendSimpleMessageTlsScramSha
2022-03-28 13:52:31 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:241] Delete of Job producer-536361300 in namespace http-bridge-scram-sha-st
2022-03-28 13:52:31 [ForkJoinPool-1-worker-7] DEBUG [JobUtils:40] Job consumer-996751443 was deleted
2022-03-28 13:52:31 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:674] ============================================================================
2022-03-28 13:52:31 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:675] [bridge.HttpBridgeTlsST - After Each] - Clean up after test
2022-03-28 13:52:31 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:344] ############################################################################
2022-03-28 13:52:31 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:348] Delete all resources for testReceiveSimpleMessageTls
2022-03-28 13:52:31 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:241] Delete of Job consumer-996751443 in namespace http-bridge-tls-st
2022-03-28 13:52:31 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:producer-536361300
2022-03-28 13:52:31 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-0c65d2dd will have desired state: Ready not ready, will try again in 1000 ms (781501ms till timeout)
2022-03-28 13:52:31 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:consumer-996751443
2022-03-28 13:52:31 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (230908ms till timeout)
2022-03-28 13:52:31 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 13:52:32 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:241] Delete of Job consumer-518626488 in namespace http-bridge-scram-sha-st
2022-03-28 13:52:32 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:241] Delete of Job producer-645321552 in namespace http-bridge-tls-st
2022-03-28 13:52:32 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:consumer-518626488
2022-03-28 13:52:32 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (242170ms till timeout)
2022-03-28 13:52:32 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:producer-645321552
2022-03-28 13:52:32 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 13:52:32 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:241] Delete of KafkaTopic my-topic-2010366894-647311872 in namespace http-bridge-scram-sha-st
2022-03-28 13:52:32 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 13:52:32 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 13:52:32 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (459597ms till timeout)
2022-03-28 13:52:32 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:241] Delete of KafkaTopic my-topic-348502360-1448005789 in namespace http-bridge-tls-st
2022-03-28 13:52:32 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-2010366894-647311872
2022-03-28 13:52:32 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-348502360-1448005789
2022-03-28 13:52:32 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:369] ############################################################################
2022-03-28 13:52:32 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:267] testSendSimpleMessageTlsScramSha - Notifies waiting test cases:[testCapacityFile, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testThrottlingQuotasCreateAlterPartitions, testSendSimpleMessageTls, testReceiveSimpleMessageTlsScramSha, testSendSimpleMessageTlsScramSha, testReceiveSimpleMessageTls] to and randomly select one to start execution
2022-03-28 13:52:32 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:93] [bridge.HttpBridgeScramShaST] - Removing parallel test: testSendSimpleMessageTlsScramSha
2022-03-28 13:52:32 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:97] [bridge.HttpBridgeScramShaST] - Parallel test count: 5
2022-03-28 13:52:32 [ForkJoinPool-1-worker-5] INFO  [TestSeparator:29] io.strimzi.systemtest.bridge.HttpBridgeScramShaST.testSendSimpleMessageTlsScramSha-FINISHED
2022-03-28 13:52:32 [ForkJoinPool-1-worker-5] INFO  [TestSeparator:30] ############################################################################
2022-03-28 13:52:32 [ForkJoinPool-1-worker-5] DEBUG [AbstractST:689] ============================================================================
2022-03-28 13:52:32 [ForkJoinPool-1-worker-5] DEBUG [AbstractST:690] [bridge.HttpBridgeScramShaST - After All] - Clean up after test suite
2022-03-28 13:52:32 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:344] ############################################################################
2022-03-28 13:52:32 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:348] Delete all resources for HttpBridgeScramShaST
2022-03-28 13:52:32 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:241] Delete of Deployment http-bridge-scram-sha-st-shared-kafka-clients in namespace http-bridge-scram-sha-st
2022-03-28 13:52:32 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:369] ############################################################################
2022-03-28 13:52:32 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:267] testReceiveSimpleMessageTls - Notifies waiting test cases:[testCapacityFile, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testThrottlingQuotasCreateAlterPartitions, testSendSimpleMessageTls, testReceiveSimpleMessageTlsScramSha, testSendSimpleMessageTlsScramSha, testReceiveSimpleMessageTls] to and randomly select one to start execution
2022-03-28 13:52:32 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:93] [bridge.HttpBridgeTlsST] - Removing parallel test: testReceiveSimpleMessageTls
2022-03-28 13:52:32 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:97] [bridge.HttpBridgeTlsST] - Parallel test count: 4
2022-03-28 13:52:32 [ForkJoinPool-1-worker-7] INFO  [TestSeparator:29] io.strimzi.systemtest.bridge.HttpBridgeTlsST.testReceiveSimpleMessageTls-FINISHED
2022-03-28 13:52:32 [ForkJoinPool-1-worker-7] INFO  [TestSeparator:30] ############################################################################
2022-03-28 13:52:32 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:689] ============================================================================
2022-03-28 13:52:32 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:690] [bridge.HttpBridgeTlsST - After All] - Clean up after test suite
2022-03-28 13:52:32 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:344] ############################################################################
2022-03-28 13:52:32 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:348] Delete all resources for HttpBridgeTlsST
2022-03-28 13:52:32 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:241] Delete of Deployment http-bridge-tls-st-kafka-clients in namespace http-bridge-tls-st
2022-03-28 13:52:32 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:http-bridge-scram-sha-st-shared-kafka-clients
2022-03-28 13:52:33 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-0c65d2dd will have desired state: Ready not ready, will try again in 1000 ms (780366ms till timeout)
2022-03-28 13:52:33 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:http-bridge-tls-st-kafka-clients
2022-03-28 13:52:33 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (229769ms till timeout)
2022-03-28 13:52:33 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (241035ms till timeout)
2022-03-28 13:52:33 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 13:52:33 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:http-bridge-scram-sha-st-shared-kafka-clients not ready, will try again in 10000 ms (479427ms till timeout)
2022-03-28 13:52:33 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:http-bridge-tls-st-kafka-clients not ready, will try again in 10000 ms (479426ms till timeout)
2022-03-28 13:52:33 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 13:52:33 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 13:52:33 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (458162ms till timeout)
2022-03-28 13:52:34 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-0c65d2dd will have desired state: Ready not ready, will try again in 1000 ms (779268ms till timeout)
2022-03-28 13:52:34 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (228580ms till timeout)
2022-03-28 13:52:34 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (239934ms till timeout)
2022-03-28 13:52:34 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 13:52:35 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-0c65d2dd will have desired state: Ready not ready, will try again in 1000 ms (778168ms till timeout)
2022-03-28 13:52:35 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 13:52:35 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 13:52:35 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (456708ms till timeout)
2022-03-28 13:52:35 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (227382ms till timeout)
2022-03-28 13:52:35 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (238830ms till timeout)
2022-03-28 13:52:36 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 13:52:36 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-0c65d2dd will have desired state: Ready not ready, will try again in 1000 ms (777070ms till timeout)
2022-03-28 13:52:36 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (226187ms till timeout)
2022-03-28 13:52:36 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (237649ms till timeout)
2022-03-28 13:52:36 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 13:52:36 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 13:52:36 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (455257ms till timeout)
2022-03-28 13:52:37 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 13:52:37 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-0c65d2dd will have desired state: Ready not ready, will try again in 1000 ms (775971ms till timeout)
2022-03-28 13:52:37 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 13:52:37 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (224994ms till timeout)
2022-03-28 13:52:37 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (236457ms till timeout)
2022-03-28 13:52:38 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 13:52:38 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 13:52:38 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (453823ms till timeout)
2022-03-28 13:52:38 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-0c65d2dd will have desired state: Ready not ready, will try again in 1000 ms (774871ms till timeout)
2022-03-28 13:52:39 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (223709ms till timeout)
2022-03-28 13:52:39 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (235263ms till timeout)
2022-03-28 13:52:39 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 13:52:39 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-0c65d2dd will have desired state: Ready not ready, will try again in 1000 ms (773771ms till timeout)
2022-03-28 13:52:39 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 13:52:39 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 13:52:39 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (452362ms till timeout)
2022-03-28 13:52:40 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (234163ms till timeout)
2022-03-28 13:52:40 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (222423ms till timeout)
2022-03-28 13:52:40 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 13:52:40 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-0c65d2dd will have desired state: Ready not ready, will try again in 1000 ms (772671ms till timeout)
2022-03-28 13:52:41 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 13:52:41 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 13:52:41 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (450915ms till timeout)
2022-03-28 13:52:41 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (233063ms till timeout)
2022-03-28 13:52:41 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (221229ms till timeout)
2022-03-28 13:52:41 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-0c65d2dd will have desired state: Ready not ready, will try again in 1000 ms (771571ms till timeout)
2022-03-28 13:52:42 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 13:52:42 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 13:52:42 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (231962ms till timeout)
2022-03-28 13:52:42 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 13:52:42 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 13:52:42 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (449484ms till timeout)
2022-03-28 13:52:42 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (220035ms till timeout)
2022-03-28 13:52:42 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-0c65d2dd will have desired state: Ready not ready, will try again in 1000 ms (770472ms till timeout)
2022-03-28 13:52:43 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (230862ms till timeout)
2022-03-28 13:52:43 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 13:52:44 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 13:52:44 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 13:52:44 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (448023ms till timeout)
2022-03-28 13:52:44 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (218818ms till timeout)
2022-03-28 13:52:44 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:http-bridge-scram-sha-st-shared-kafka-clients not ready, will try again in 10000 ms (468945ms till timeout)
2022-03-28 13:52:44 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-0c65d2dd will have desired state: Ready not ready, will try again in 1000 ms (769310ms till timeout)
2022-03-28 13:52:44 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:http-bridge-tls-st-kafka-clients not ready, will try again in 10000 ms (468945ms till timeout)
2022-03-28 13:52:44 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (229758ms till timeout)
2022-03-28 13:52:45 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 13:52:45 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (217624ms till timeout)
2022-03-28 13:52:45 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-0c65d2dd will have desired state: Ready not ready, will try again in 1000 ms (768208ms till timeout)
2022-03-28 13:52:45 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 13:52:45 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 13:52:45 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (446546ms till timeout)
2022-03-28 13:52:45 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (228652ms till timeout)
2022-03-28 13:52:46 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (216429ms till timeout)
2022-03-28 13:52:46 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-0c65d2dd will have desired state: Ready not ready, will try again in 1000 ms (767015ms till timeout)
2022-03-28 13:52:46 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 13:52:46 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (227549ms till timeout)
2022-03-28 13:52:47 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 13:52:47 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 13:52:47 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (444983ms till timeout)
2022-03-28 13:52:47 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 13:52:47 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-0c65d2dd will have desired state: Ready not ready, will try again in 1000 ms (765916ms till timeout)
2022-03-28 13:52:47 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (215043ms till timeout)
2022-03-28 13:52:47 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (226448ms till timeout)
2022-03-28 13:52:48 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 13:52:48 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 13:52:48 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 13:52:48 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (443513ms till timeout)
2022-03-28 13:52:48 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:444] Kafka: my-cluster-0c65d2dd is in desired state: Ready
2022-03-28 13:52:48 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:155] Create/Update KafkaTopic my-topic-130462979-2061578809 in namespace namespace-1
2022-03-28 13:52:48 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:164] Using Namespace: namespace-1
2022-03-28 13:52:48 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-130462979-2061578809
2022-03-28 13:52:48 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:433] Wait for KafkaTopic: my-topic-130462979-2061578809 will have desired state: Ready
2022-03-28 13:52:48 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for KafkaTopic: my-topic-130462979-2061578809 will have desired state: Ready
2022-03-28 13:52:49 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (213742ms till timeout)
2022-03-28 13:52:49 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:444] KafkaTopic: my-topic-130462979-2061578809 is in desired state: Ready
2022-03-28 13:52:49 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:155] Create/Update KafkaUser encrypted-leopold in namespace namespace-1
2022-03-28 13:52:49 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:164] Using Namespace: namespace-1
2022-03-28 13:52:49 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (225289ms till timeout)
2022-03-28 13:52:49 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:encrypted-leopold
2022-03-28 13:52:49 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:433] Wait for KafkaUser: encrypted-leopold will have desired state: Ready
2022-03-28 13:52:49 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for KafkaUser: encrypted-leopold will have desired state: Ready
2022-03-28 13:52:49 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] KafkaUser: encrypted-leopold will have desired state: Ready not ready, will try again in 1000 ms (179903ms till timeout)
2022-03-28 13:52:49 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 13:52:50 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 13:52:50 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 13:52:50 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (442016ms till timeout)
2022-03-28 13:52:50 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (212457ms till timeout)
2022-03-28 13:52:50 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (224010ms till timeout)
2022-03-28 13:52:50 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:444] KafkaUser: encrypted-leopold is in desired state: Ready
2022-03-28 13:52:50 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:155] Create/Update KafkaUser scramed-leopold in namespace namespace-1
2022-03-28 13:52:50 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:164] Using Namespace: namespace-1
2022-03-28 13:52:50 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:scramed-leopold
2022-03-28 13:52:50 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:433] Wait for KafkaUser: scramed-leopold will have desired state: Ready
2022-03-28 13:52:50 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for KafkaUser: scramed-leopold will have desired state: Ready
2022-03-28 13:52:50 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:444] KafkaUser: scramed-leopold is in desired state: Ready
2022-03-28 13:52:50 [ForkJoinPool-1-worker-13] INFO  [UserST:346] Deploying KafkaClients pod for TLS listener
2022-03-28 13:52:51 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 13:52:51 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:155] Create/Update Deployment my-cluster-0c65d2dd-tls-kafka-clients in namespace namespace-1
2022-03-28 13:52:51 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:164] Using Namespace: namespace-1
2022-03-28 13:52:51 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:my-cluster-0c65d2dd-tls-kafka-clients
2022-03-28 13:52:51 [ForkJoinPool-1-worker-13] INFO  [DeploymentUtils:161] Wait for Deployment: my-cluster-0c65d2dd-tls-kafka-clients will be ready
2022-03-28 13:52:51 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Wait for Deployment: my-cluster-0c65d2dd-tls-kafka-clients will be ready
2022-03-28 13:52:51 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0c65d2dd-tls-kafka-clients will be ready not ready, will try again in 1000 ms (479902ms till timeout)
2022-03-28 13:52:51 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 13:52:51 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 13:52:51 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (440560ms till timeout)
2022-03-28 13:52:51 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (222870ms till timeout)
2022-03-28 13:52:51 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (211312ms till timeout)
2022-03-28 13:52:52 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 13:52:52 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 13:52:52 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0c65d2dd-tls-kafka-clients will be ready not ready, will try again in 1000 ms (478803ms till timeout)
2022-03-28 13:52:52 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (221768ms till timeout)
2022-03-28 13:52:52 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (210021ms till timeout)
2022-03-28 13:52:52 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 13:52:52 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 13:52:52 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (439073ms till timeout)
2022-03-28 13:52:53 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0c65d2dd-tls-kafka-clients will be ready not ready, will try again in 1000 ms (477704ms till timeout)
2022-03-28 13:52:53 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (220667ms till timeout)
2022-03-28 13:52:53 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 13:52:54 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (208827ms till timeout)
2022-03-28 13:52:54 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 13:52:54 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 13:52:54 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (437634ms till timeout)
2022-03-28 13:52:54 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:http-bridge-scram-sha-st-shared-kafka-clients not ready, will try again in 10000 ms (458462ms till timeout)
2022-03-28 13:52:54 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:http-bridge-tls-st-kafka-clients not ready, will try again in 10000 ms (458461ms till timeout)
2022-03-28 13:52:54 [ForkJoinPool-1-worker-13] INFO  [DeploymentUtils:168] Deployment: my-cluster-0c65d2dd-tls-kafka-clients is ready
2022-03-28 13:52:54 [ForkJoinPool-1-worker-13] INFO  [UserST:350] Deploying KafkaClients pod for PLAIN listener
2022-03-28 13:52:54 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (219474ms till timeout)
2022-03-28 13:52:55 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (207722ms till timeout)
2022-03-28 13:52:55 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:155] Create/Update Deployment my-cluster-0c65d2dd-plain-kafka-clients in namespace namespace-1
2022-03-28 13:52:55 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:164] Using Namespace: namespace-1
2022-03-28 13:52:55 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:my-cluster-0c65d2dd-plain-kafka-clients
2022-03-28 13:52:55 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 13:52:55 [ForkJoinPool-1-worker-13] INFO  [DeploymentUtils:161] Wait for Deployment: my-cluster-0c65d2dd-plain-kafka-clients will be ready
2022-03-28 13:52:55 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Wait for Deployment: my-cluster-0c65d2dd-plain-kafka-clients will be ready
2022-03-28 13:52:55 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0c65d2dd-plain-kafka-clients will be ready not ready, will try again in 1000 ms (479903ms till timeout)
2022-03-28 13:52:55 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 13:52:55 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 13:52:55 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (436168ms till timeout)
2022-03-28 13:52:56 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (218344ms till timeout)
2022-03-28 13:52:56 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (206598ms till timeout)
2022-03-28 13:52:56 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0c65d2dd-plain-kafka-clients will be ready not ready, will try again in 1000 ms (478803ms till timeout)
2022-03-28 13:52:56 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 13:52:57 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (217244ms till timeout)
2022-03-28 13:52:57 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 13:52:57 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 13:52:57 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 13:52:57 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (434674ms till timeout)
2022-03-28 13:52:57 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (205394ms till timeout)
2022-03-28 13:52:57 [ForkJoinPool-1-worker-13] INFO  [DeploymentUtils:168] Deployment: my-cluster-0c65d2dd-plain-kafka-clients is ready
2022-03-28 13:52:58 [ForkJoinPool-1-worker-13] INFO  [UserST:357] Checking if user secrets with secret prefixes exists
2022-03-28 13:52:58 [ForkJoinPool-1-worker-13] INFO  [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-03-28 13:52:58 [ForkJoinPool-1-worker-13] INFO  [UserST:373] Checking if TLS user is able to send messages
2022-03-28 13:52:58 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (216090ms till timeout)
2022-03-28 13:52:58 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 13:52:58 [ForkJoinPool-1-worker-13] DEBUG [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@4f8a5b98, which are set.
2022-03-28 13:52:58 [ForkJoinPool-1-worker-13] INFO  [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@7c142b32, messages=[], arguments=[--topic, my-topic-205198573-1992555764, --bootstrap-server, my-cluster-0c65d2dd-kafka-bootstrap.namespace-1.svc:9093, USER=top_secret_encrypted_leopold, --max-messages, 100], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-0c65d2dd-tls-kafka-clients-664b5d7666-2swbr', podNamespace='namespace-1', bootstrapServer='my-cluster-0c65d2dd-kafka-bootstrap.namespace-1.svc:9093', topicName='my-topic-205198573-1992555764', maxMessages=100, kafkaUsername='top-secret-encrypted-leopold', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@4f8a5b98}
2022-03-28 13:52:58 [ForkJoinPool-1-worker-13] INFO  [InternalKafkaClient:124] Producing 100 messages to my-cluster-0c65d2dd-kafka-bootstrap.namespace-1.svc:9093:my-topic-205198573-1992555764 from pod my-cluster-0c65d2dd-tls-kafka-clients-664b5d7666-2swbr
2022-03-28 13:52:58 [ForkJoinPool-1-worker-13] INFO  [VerifiableClient:192] Client command: oc exec my-cluster-0c65d2dd-tls-kafka-clients-664b5d7666-2swbr -n namespace-1 -- /opt/kafka/producer.sh --topic my-topic-205198573-1992555764 --bootstrap-server my-cluster-0c65d2dd-kafka-bootstrap.namespace-1.svc:9093 USER=top_secret_encrypted_leopold --max-messages 100
2022-03-28 13:52:58 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc exec my-cluster-0c65d2dd-tls-kafka-clients-664b5d7666-2swbr -n namespace-1 -- /opt/kafka/producer.sh --topic my-topic-205198573-1992555764 --bootstrap-server my-cluster-0c65d2dd-kafka-bootstrap.namespace-1.svc:9093 USER=top_secret_encrypted_leopold --max-messages 100
2022-03-28 13:52:58 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (204288ms till timeout)
2022-03-28 13:52:58 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 13:52:58 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 1
2022-03-28 13:52:58 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 13:52:58 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Error from server (NotFound): namespaces "namespace-0" not found
2022-03-28 13:52:58 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] ======STDERR END======
2022-03-28 13:52:58 [ForkJoinPool-1-worker-3] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[http-bridge-scram-sha-st], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[namespace-1], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 13:52:58 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:267] testCapacityFile - Notifies waiting test cases:[testCapacityFile, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testThrottlingQuotasCreateAlterPartitions, testSendSimpleMessageTls, testReceiveSimpleMessageTlsScramSha, testSendSimpleMessageTlsScramSha, testReceiveSimpleMessageTls] to and randomly select one to start execution
2022-03-28 13:52:58 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:93] [cruisecontrol.CruiseControlConfigurationST] - Removing parallel test: testCapacityFile
2022-03-28 13:52:58 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:97] [cruisecontrol.CruiseControlConfigurationST] - Parallel test count: 3
2022-03-28 13:52:58 [ForkJoinPool-1-worker-3] INFO  [TestSeparator:29] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testCapacityFile-FINISHED
2022-03-28 13:52:58 [ForkJoinPool-1-worker-3] INFO  [TestSeparator:30] ############################################################################
2022-03-28 13:52:58 [ForkJoinPool-1-worker-3] INFO  [TestSeparator:23] ############################################################################
2022-03-28 13:52:58 [ForkJoinPool-1-worker-3] INFO  [TestSeparator:24] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods-STARTED
2022-03-28 13:52:58 [ForkJoinPool-1-worker-3] DEBUG [AbstractST:658] ============================================================================
2022-03-28 13:52:58 [ForkJoinPool-1-worker-3] DEBUG [AbstractST:659] [cruisecontrol.CruiseControlConfigurationST - Before Each] - Setup test case environment
2022-03-28 13:52:58 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:77] [cruisecontrol.CruiseControlConfigurationST] - Adding parallel test: testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods
2022-03-28 13:52:58 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:81] [cruisecontrol.CruiseControlConfigurationST] - Parallel test count: 4
2022-03-28 13:52:58 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:230] testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods test now can proceed its execution
2022-03-28 13:52:58 [ForkJoinPool-1-worker-3] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 13:52:58 [ForkJoinPool-1-worker-3] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-af298d01, testThrottlingQuotasCreateTopic=my-cluster-ac2cadd6, testThrottlingQuotasCreateAlterPartitions=my-cluster-8f6c28cb, testCapacityFile=my-cluster-0531d5ce, testSendSimpleMessageTls=my-cluster-fbc845dd, testCreatingUsersWithSecretPrefix=my-cluster-0c65d2dd, testReceiveSimpleMessageTls=my-cluster-ace1fa24, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-8af61d52, testSendSimpleMessageTlsScramSha=my-cluster-e353214b}
2022-03-28 13:52:58 [ForkJoinPool-1-worker-3] TRACE [AbstractST:607] USERS_NAME_MAP: {testReceiveSimpleMessageTlsScramSha=my-user-1597210213-707359243, testThrottlingQuotasCreateTopic=my-user-46611685-1079680568, testThrottlingQuotasCreateAlterPartitions=my-user-1557959119-1722636050, testCapacityFile=my-user-155969448-1270555822, testSendSimpleMessageTls=my-user-1754673553-2077518027, testCreatingUsersWithSecretPrefix=my-user-144001639-226807213, testReceiveSimpleMessageTls=my-user-1418772225-1024845009, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-1807142870-188660517, testSendSimpleMessageTlsScramSha=my-user-182357028-1132873010}
2022-03-28 13:52:58 [ForkJoinPool-1-worker-3] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-topic-649423016-1623551209, testThrottlingQuotasCreateTopic=my-topic-114922478-1506136333, testThrottlingQuotasCreateAlterPartitions=my-topic-1449977352-344480794, testCapacityFile=my-topic-296605537-1008743104, testSendSimpleMessageTls=my-topic-115477502-2025274778, testCreatingUsersWithSecretPrefix=my-topic-130462979-2061578809, testReceiveSimpleMessageTls=my-topic-1131768783-120051821, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-1367124108-1748498395, testSendSimpleMessageTlsScramSha=my-topic-652506937-1802310709}
2022-03-28 13:52:58 [ForkJoinPool-1-worker-3] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-af298d01-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-ac2cadd6-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-8f6c28cb-kafka-clients, testCapacityFile=my-cluster-0531d5ce-kafka-clients, testSendSimpleMessageTls=my-cluster-fbc845dd-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-0c65d2dd-kafka-clients, testReceiveSimpleMessageTls=my-cluster-ace1fa24-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-8af61d52-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-e353214b-kafka-clients}
2022-03-28 13:52:58 [ForkJoinPool-1-worker-3] INFO  [TestSuiteNamespaceManager:163] Creating namespace:namespace-2 for test case:testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods
2022-03-28 13:52:59 [ForkJoinPool-1-worker-3] INFO  [KubeClusterResource:156] Creating Namespace: namespace-2
2022-03-28 13:52:59 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Namespace namespace-2
2022-03-28 13:52:59 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-1 get Namespace namespace-2 -o json
2022-03-28 13:52:59 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (214985ms till timeout)
2022-03-28 13:52:59 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-1 get Namespace namespace-2 -o json
2022-03-28 13:52:59 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 13:52:59 [ForkJoinPool-1-worker-3] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c31,c10",
            "openshift.io/sa.scc.supplemental-groups": "1000950000/10000",
            "openshift.io/sa.scc.uid-range": "1000950000/10000"
        },
        "creationTimestamp": "2022-03-28T13:52:54Z",
        "labels": {
            "kubernetes.io/metadata.name": "namespace-2"
        },
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T13:52:54Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:labels": {
                            ".": {},
                            "f:kubernetes.io/metadata.name": {}
                        }
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T13:52:54Z"
            }
        ],
        "name": "namespace-2",
        "resourceVersion": "86477",
        "uid": "1893df08-84a2-4f6a-8c9f-a3bc15b8ae02"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 13:52:59 [ForkJoinPool-1-worker-3] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@3881d5f2=[namespace-2], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[http-bridge-scram-sha-st], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[namespace-1], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 13:52:59 [ForkJoinPool-1-worker-3] INFO  [KubeClusterResource:82] Client use Namespace: namespace-2
2022-03-28 13:52:59 [ForkJoinPool-1-worker-3] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-2, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 13:52:59 [ForkJoinPool-1-worker-3] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-2
2022-03-28 13:52:59 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:155] Create/Update Kafka my-cluster-8af61d52 in namespace namespace-2
2022-03-28 13:52:59 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:164] Using Namespace: namespace-2
2022-03-28 13:52:59 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (203096ms till timeout)
2022-03-28 13:52:59 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-8af61d52
2022-03-28 13:52:59 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:433] Wait for Kafka: my-cluster-8af61d52 will have desired state: Ready
2022-03-28 13:52:59 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Kafka: my-cluster-8af61d52 will have desired state: Ready
2022-03-28 13:52:59 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1319902ms till timeout)
2022-03-28 13:53:00 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (213883ms till timeout)
2022-03-28 13:53:00 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (201903ms till timeout)
2022-03-28 13:53:01 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1318802ms till timeout)
2022-03-28 13:53:01 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (212771ms till timeout)
2022-03-28 13:53:02 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (200707ms till timeout)
2022-03-28 13:53:02 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1317629ms till timeout)
2022-03-28 13:53:02 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 13:53:02 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (211668ms till timeout)
2022-03-28 13:53:03 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (199508ms till timeout)
2022-03-28 13:53:03 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1316433ms till timeout)
2022-03-28 13:53:03 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (210567ms till timeout)
2022-03-28 13:53:04 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (198224ms till timeout)
2022-03-28 13:53:04 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1315237ms till timeout)
2022-03-28 13:53:04 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:241] Delete of KafkaBridge http-bridge-scram-sha-cluster-name in namespace http-bridge-scram-sha-st
2022-03-28 13:53:04 [ForkJoinPool-1-worker-13] INFO  [InternalKafkaClient:127] Producer finished correctly: true
2022-03-28 13:53:04 [ForkJoinPool-1-worker-13] INFO  [InternalKafkaClient:131] Producer produced 100 messages
2022-03-28 13:53:04 [ForkJoinPool-1-worker-13] DEBUG [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@68d28a43, which are set.
2022-03-28 13:53:04 [ForkJoinPool-1-worker-13] INFO  [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@333495ac, messages=[], arguments=[--group-id, my-consumer-group-1276100178, --topic, my-topic-205198573-1992555764, --group-instance-id, instance1871030279, --bootstrap-server, my-cluster-0c65d2dd-kafka-bootstrap.namespace-1.svc:9093, USER=top_secret_encrypted_leopold, --max-messages, 100], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-0c65d2dd-tls-kafka-clients-664b5d7666-2swbr', podNamespace='namespace-1', bootstrapServer='my-cluster-0c65d2dd-kafka-bootstrap.namespace-1.svc:9093', topicName='my-topic-205198573-1992555764', maxMessages=100, kafkaUsername='top-secret-encrypted-leopold', consumerGroupName='my-consumer-group-1276100178', consumerInstanceId='instance1871030279', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@68d28a43}
2022-03-28 13:53:04 [ForkJoinPool-1-worker-13] INFO  [InternalKafkaClient:192] Consuming 100 messages from my-cluster-0c65d2dd-kafka-bootstrap.namespace-1.svc:9093:my-topic-205198573-1992555764 from pod my-cluster-0c65d2dd-tls-kafka-clients-664b5d7666-2swbr
2022-03-28 13:53:04 [ForkJoinPool-1-worker-13] INFO  [VerifiableClient:192] Client command: oc exec my-cluster-0c65d2dd-tls-kafka-clients-664b5d7666-2swbr -n namespace-1 -- /opt/kafka/consumer.sh --group-id my-consumer-group-1276100178 --topic my-topic-205198573-1992555764 --group-instance-id instance1871030279 --bootstrap-server my-cluster-0c65d2dd-kafka-bootstrap.namespace-1.svc:9093 USER=top_secret_encrypted_leopold --max-messages 100
2022-03-28 13:53:04 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc exec my-cluster-0c65d2dd-tls-kafka-clients-664b5d7666-2swbr -n namespace-1 -- /opt/kafka/consumer.sh --group-id my-consumer-group-1276100178 --topic my-topic-205198573-1992555764 --group-instance-id instance1871030279 --bootstrap-server my-cluster-0c65d2dd-kafka-bootstrap.namespace-1.svc:9093 USER=top_secret_encrypted_leopold --max-messages 100
2022-03-28 13:53:04 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaBridge:http-bridge-scram-sha-cluster-name
2022-03-28 13:53:05 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:241] Delete of KafkaUser my-user-609064688-1843560098 in namespace http-bridge-scram-sha-st
2022-03-28 13:53:05 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (209352ms till timeout)
2022-03-28 13:53:05 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:http-bridge-tls-st-kafka-clients not ready, will try again in 10000 ms (447936ms till timeout)
2022-03-28 13:53:05 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-609064688-1843560098
2022-03-28 13:53:05 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:241] Delete of Kafka http-bridge-scram-sha-cluster-name in namespace http-bridge-scram-sha-st
2022-03-28 13:53:05 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:http-bridge-scram-sha-cluster-name
2022-03-28 13:53:05 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:http-bridge-scram-sha-cluster-name not ready, will try again in 10000 ms (839888ms till timeout)
2022-03-28 13:53:05 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (197071ms till timeout)
2022-03-28 13:53:05 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1314003ms till timeout)
2022-03-28 13:53:06 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (208251ms till timeout)
2022-03-28 13:53:06 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (195870ms till timeout)
2022-03-28 13:53:06 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1312880ms till timeout)
2022-03-28 13:53:07 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (207150ms till timeout)
2022-03-28 13:53:07 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 13:53:08 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (194676ms till timeout)
2022-03-28 13:53:08 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1311684ms till timeout)
2022-03-28 13:53:08 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (206049ms till timeout)
2022-03-28 13:53:09 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (193384ms till timeout)
2022-03-28 13:53:09 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1310398ms till timeout)
2022-03-28 13:53:09 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (204938ms till timeout)
2022-03-28 13:53:10 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1309298ms till timeout)
2022-03-28 13:53:10 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (192100ms till timeout)
2022-03-28 13:53:10 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (203647ms till timeout)
2022-03-28 13:53:11 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1308198ms till timeout)
2022-03-28 13:53:12 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (190814ms till timeout)
2022-03-28 13:53:12 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (202368ms till timeout)
2022-03-28 13:53:12 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 13:53:12 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1307095ms till timeout)
2022-03-28 13:53:13 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (189530ms till timeout)
2022-03-28 13:53:13 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (201083ms till timeout)
2022-03-28 13:53:13 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1305995ms till timeout)
2022-03-28 13:53:14 [ForkJoinPool-1-worker-13] INFO  [InternalKafkaClient:195] Consumer finished correctly: true
2022-03-28 13:53:14 [ForkJoinPool-1-worker-13] INFO  [InternalKafkaClient:198] Consumer consumed 100 messages
2022-03-28 13:53:14 [ForkJoinPool-1-worker-13] INFO  [UserST:386] Checking if SCRAM-SHA user is able to send messages
2022-03-28 13:53:14 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (188240ms till timeout)
2022-03-28 13:53:14 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (199794ms till timeout)
2022-03-28 13:53:14 [ForkJoinPool-1-worker-13] DEBUG [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@5e78f752, which are set.
2022-03-28 13:53:14 [ForkJoinPool-1-worker-13] INFO  [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@1fbe01d6, messages=[], arguments=[--topic, my-topic-205198573-1992555764, --bootstrap-server, my-cluster-0c65d2dd-kafka-bootstrap.namespace-1.svc:9092, USER=top_secret_scramed_leopold, --max-messages, 100], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-0c65d2dd-plain-kafka-clients-74558b569-s82wz', podNamespace='namespace-1', bootstrapServer='my-cluster-0c65d2dd-kafka-bootstrap.namespace-1.svc:9092', topicName='my-topic-205198573-1992555764', maxMessages=100, kafkaUsername='top-secret-scramed-leopold', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@5e78f752}
2022-03-28 13:53:14 [ForkJoinPool-1-worker-13] INFO  [InternalKafkaClient:94] Producing 100 messages to my-cluster-0c65d2dd-kafka-bootstrap.namespace-1.svc:9092:my-topic-205198573-1992555764 from pod my-cluster-0c65d2dd-plain-kafka-clients-74558b569-s82wz
2022-03-28 13:53:14 [ForkJoinPool-1-worker-13] INFO  [VerifiableClient:192] Client command: oc exec my-cluster-0c65d2dd-plain-kafka-clients-74558b569-s82wz -n namespace-1 -- /opt/kafka/producer.sh --topic my-topic-205198573-1992555764 --bootstrap-server my-cluster-0c65d2dd-kafka-bootstrap.namespace-1.svc:9092 USER=top_secret_scramed_leopold --max-messages 100
2022-03-28 13:53:14 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc exec my-cluster-0c65d2dd-plain-kafka-clients-74558b569-s82wz -n namespace-1 -- /opt/kafka/producer.sh --topic my-topic-205198573-1992555764 --bootstrap-server my-cluster-0c65d2dd-kafka-bootstrap.namespace-1.svc:9092 USER=top_secret_scramed_leopold --max-messages 100
2022-03-28 13:53:14 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1304896ms till timeout)
2022-03-28 13:53:15 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:241] Delete of KafkaBridge http-bridge-tls-cluster-name in namespace http-bridge-tls-st
2022-03-28 13:53:15 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaBridge:http-bridge-tls-cluster-name
2022-03-28 13:53:15 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:241] Delete of KafkaUser my-user-829879141-597480023 in namespace http-bridge-tls-st
2022-03-28 13:53:15 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-829879141-597480023
2022-03-28 13:53:15 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:241] Delete of Kafka http-bridge-tls-cluster-name in namespace http-bridge-tls-st
2022-03-28 13:53:15 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:369] ############################################################################
2022-03-28 13:53:15 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (198649ms till timeout)
2022-03-28 13:53:15 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (187003ms till timeout)
2022-03-28 13:53:15 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:http-bridge-tls-cluster-name
2022-03-28 13:53:15 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Namespace http-bridge-scram-sha-st removal
2022-03-28 13:53:15 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-2 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 13:53:16 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:369] ############################################################################
2022-03-28 13:53:16 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1303696ms till timeout)
2022-03-28 13:53:16 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Namespace http-bridge-tls-st removal
2022-03-28 13:53:16 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-28 13:53:16 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-2 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 13:53:16 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 13:53:16 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace http-bridge-scram-sha-st removal not ready, will try again in 1000 ms (479549ms till timeout)
2022-03-28 13:53:16 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-28 13:53:16 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 13:53:16 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (479444ms till timeout)
2022-03-28 13:53:16 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (197548ms till timeout)
2022-03-28 13:53:17 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (185804ms till timeout)
2022-03-28 13:53:17 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1302595ms till timeout)
2022-03-28 13:53:17 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 13:53:17 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-2 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 13:53:17 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-28 13:53:17 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-2 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 13:53:17 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 13:53:17 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace http-bridge-scram-sha-st removal not ready, will try again in 1000 ms (478027ms till timeout)
2022-03-28 13:53:17 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (196434ms till timeout)
2022-03-28 13:53:18 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (184610ms till timeout)
2022-03-28 13:53:18 [ForkJoinPool-1-worker-13] INFO  [InternalKafkaClient:97] Producer finished correctly: true
2022-03-28 13:53:18 [ForkJoinPool-1-worker-13] INFO  [InternalKafkaClient:101] Producer produced 100 messages
2022-03-28 13:53:18 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1301493ms till timeout)
2022-03-28 13:53:18 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-28 13:53:18 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 13:53:18 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (477851ms till timeout)
2022-03-28 13:53:18 [ForkJoinPool-1-worker-13] DEBUG [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@634210f0, which are set.
2022-03-28 13:53:18 [ForkJoinPool-1-worker-13] INFO  [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@74cc70c1, messages=[], arguments=[--group-id, my-consumer-group-1276100178, --topic, my-topic-205198573-1992555764, --group-instance-id, instance289308191, --bootstrap-server, my-cluster-0c65d2dd-kafka-bootstrap.namespace-1.svc:9092, USER=top_secret_scramed_leopold, --max-messages, 100], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-0c65d2dd-plain-kafka-clients-74558b569-s82wz', podNamespace='namespace-1', bootstrapServer='my-cluster-0c65d2dd-kafka-bootstrap.namespace-1.svc:9092', topicName='my-topic-205198573-1992555764', maxMessages=100, kafkaUsername='top-secret-scramed-leopold', consumerGroupName='my-consumer-group-1276100178', consumerInstanceId='instance289308191', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@634210f0}
2022-03-28 13:53:18 [ForkJoinPool-1-worker-13] INFO  [InternalKafkaClient:157] Consuming 100 messages from my-cluster-0c65d2dd-kafka-bootstrap.namespace-1.svc:9092#my-topic-205198573-1992555764 from pod my-cluster-0c65d2dd-plain-kafka-clients-74558b569-s82wz
2022-03-28 13:53:18 [ForkJoinPool-1-worker-13] INFO  [VerifiableClient:192] Client command: oc exec my-cluster-0c65d2dd-plain-kafka-clients-74558b569-s82wz -n namespace-1 -- /opt/kafka/consumer.sh --group-id my-consumer-group-1276100178 --topic my-topic-205198573-1992555764 --group-instance-id instance289308191 --bootstrap-server my-cluster-0c65d2dd-kafka-bootstrap.namespace-1.svc:9092 USER=top_secret_scramed_leopold --max-messages 100
2022-03-28 13:53:18 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc exec my-cluster-0c65d2dd-plain-kafka-clients-74558b569-s82wz -n namespace-1 -- /opt/kafka/consumer.sh --group-id my-consumer-group-1276100178 --topic my-topic-205198573-1992555764 --group-instance-id instance289308191 --bootstrap-server my-cluster-0c65d2dd-kafka-bootstrap.namespace-1.svc:9092 USER=top_secret_scramed_leopold --max-messages 100
2022-03-28 13:53:18 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-2 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 13:53:19 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (195331ms till timeout)
2022-03-28 13:53:19 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-28 13:53:19 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (183416ms till timeout)
2022-03-28 13:53:19 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-2 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 13:53:19 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 13:53:19 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace http-bridge-scram-sha-st removal not ready, will try again in 1000 ms (476509ms till timeout)
2022-03-28 13:53:19 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1300394ms till timeout)
2022-03-28 13:53:19 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-28 13:53:19 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 13:53:19 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (476360ms till timeout)
2022-03-28 13:53:20 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (194231ms till timeout)
2022-03-28 13:53:20 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-2 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 13:53:20 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (182104ms till timeout)
2022-03-28 13:53:20 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1299117ms till timeout)
2022-03-28 13:53:20 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-28 13:53:20 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-2 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 13:53:20 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 13:53:20 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace http-bridge-scram-sha-st removal not ready, will try again in 1000 ms (475025ms till timeout)
2022-03-28 13:53:21 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (193125ms till timeout)
2022-03-28 13:53:21 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-28 13:53:21 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 13:53:21 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (474895ms till timeout)
2022-03-28 13:53:21 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-2 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 13:53:22 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (180809ms till timeout)
2022-03-28 13:53:22 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1297822ms till timeout)
2022-03-28 13:53:22 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 13:53:22 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (192024ms till timeout)
2022-03-28 13:53:22 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-28 13:53:22 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-2 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 13:53:22 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 13:53:22 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace http-bridge-scram-sha-st removal not ready, will try again in 1000 ms (473566ms till timeout)
2022-03-28 13:53:22 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-28 13:53:22 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 13:53:22 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (473415ms till timeout)
2022-03-28 13:53:23 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (179523ms till timeout)
2022-03-28 13:53:23 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1296537ms till timeout)
2022-03-28 13:53:23 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-2 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 13:53:23 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (190923ms till timeout)
2022-03-28 13:53:23 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-28 13:53:23 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-2 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 13:53:23 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 1
2022-03-28 13:53:23 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 13:53:23 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Error from server (NotFound): namespaces "http-bridge-scram-sha-st" not found
2022-03-28 13:53:23 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] ======STDERR END======
2022-03-28 13:53:23 [ForkJoinPool-1-worker-5] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@3881d5f2=[namespace-2], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[namespace-1], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 13:53:23 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:254] HttpBridgeScramShaST - Notifies waiting test suites:[UserST, ThrottlingQuotaST, TopicST, CruiseControlConfigurationST, HttpBridgeScramShaST, HttpBridgeTlsST, ReconciliationST] to and randomly select one to start execution
2022-03-28 13:53:23 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:85] [bridge.HttpBridgeScramShaST] - Removing parallel suite: HttpBridgeScramShaST
2022-03-28 13:53:23 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:89] [bridge.HttpBridgeScramShaST] - Parallel suites count: 6
[INFO] Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 273.856 s - in io.strimzi.systemtest.bridge.HttpBridgeScramShaST
2022-03-28 13:53:23 [ForkJoinPool-1-worker-5] INFO  [TestSeparator:23] ############################################################################
2022-03-28 13:53:23 [ForkJoinPool-1-worker-5] INFO  [TestSeparator:24] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testConfigurationFileIsCreated-STARTED
2022-03-28 13:53:23 [ForkJoinPool-1-worker-5] DEBUG [AbstractST:658] ============================================================================
2022-03-28 13:53:23 [ForkJoinPool-1-worker-5] DEBUG [AbstractST:659] [cruisecontrol.CruiseControlConfigurationST - Before Each] - Setup test case environment
2022-03-28 13:53:23 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:77] [cruisecontrol.CruiseControlConfigurationST] - Adding parallel test: testConfigurationFileIsCreated
2022-03-28 13:53:23 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:81] [cruisecontrol.CruiseControlConfigurationST] - Parallel test count: 5
2022-03-28 13:53:23 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:230] testConfigurationFileIsCreated test now can proceed its execution
2022-03-28 13:53:23 [ForkJoinPool-1-worker-5] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 13:53:23 [ForkJoinPool-1-worker-5] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-af298d01, testConfigurationFileIsCreated=my-cluster-3244df0d, testThrottlingQuotasCreateTopic=my-cluster-ac2cadd6, testThrottlingQuotasCreateAlterPartitions=my-cluster-8f6c28cb, testCapacityFile=my-cluster-0531d5ce, testSendSimpleMessageTls=my-cluster-fbc845dd, testCreatingUsersWithSecretPrefix=my-cluster-0c65d2dd, testReceiveSimpleMessageTls=my-cluster-ace1fa24, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-8af61d52, testSendSimpleMessageTlsScramSha=my-cluster-e353214b}
2022-03-28 13:53:23 [ForkJoinPool-1-worker-5] TRACE [AbstractST:607] USERS_NAME_MAP: {testReceiveSimpleMessageTlsScramSha=my-user-1597210213-707359243, testConfigurationFileIsCreated=my-user-1799841978-1468805631, testThrottlingQuotasCreateTopic=my-user-46611685-1079680568, testThrottlingQuotasCreateAlterPartitions=my-user-1557959119-1722636050, testCapacityFile=my-user-155969448-1270555822, testSendSimpleMessageTls=my-user-1754673553-2077518027, testCreatingUsersWithSecretPrefix=my-user-144001639-226807213, testReceiveSimpleMessageTls=my-user-1418772225-1024845009, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-1807142870-188660517, testSendSimpleMessageTlsScramSha=my-user-182357028-1132873010}
2022-03-28 13:53:23 [ForkJoinPool-1-worker-5] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-topic-649423016-1623551209, testConfigurationFileIsCreated=my-topic-1435014906-681104826, testThrottlingQuotasCreateTopic=my-topic-114922478-1506136333, testThrottlingQuotasCreateAlterPartitions=my-topic-1449977352-344480794, testCapacityFile=my-topic-296605537-1008743104, testSendSimpleMessageTls=my-topic-115477502-2025274778, testCreatingUsersWithSecretPrefix=my-topic-130462979-2061578809, testReceiveSimpleMessageTls=my-topic-1131768783-120051821, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-1367124108-1748498395, testSendSimpleMessageTlsScramSha=my-topic-652506937-1802310709}
2022-03-28 13:53:23 [ForkJoinPool-1-worker-5] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-af298d01-kafka-clients, testConfigurationFileIsCreated=my-cluster-3244df0d-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-ac2cadd6-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-8f6c28cb-kafka-clients, testCapacityFile=my-cluster-0531d5ce-kafka-clients, testSendSimpleMessageTls=my-cluster-fbc845dd-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-0c65d2dd-kafka-clients, testReceiveSimpleMessageTls=my-cluster-ace1fa24-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-8af61d52-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-e353214b-kafka-clients}
2022-03-28 13:53:23 [ForkJoinPool-1-worker-5] INFO  [TestSuiteNamespaceManager:163] Creating namespace:namespace-3 for test case:testConfigurationFileIsCreated
2022-03-28 13:53:23 [ForkJoinPool-1-worker-5] INFO  [KubeClusterResource:156] Creating Namespace: namespace-3
2022-03-28 13:53:24 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Namespace namespace-3
2022-03-28 13:53:24 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-2 get Namespace namespace-3 -o json
2022-03-28 13:53:24 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-28 13:53:24 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 13:53:24 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (471955ms till timeout)
2022-03-28 13:53:24 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-2 get Namespace namespace-3 -o json
2022-03-28 13:53:24 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 13:53:24 [ForkJoinPool-1-worker-5] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c31,c15",
            "openshift.io/sa.scc.supplemental-groups": "1000960000/10000",
            "openshift.io/sa.scc.uid-range": "1000960000/10000"
        },
        "creationTimestamp": "2022-03-28T13:53:19Z",
        "labels": {
            "kubernetes.io/metadata.name": "namespace-3"
        },
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T13:53:19Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:labels": {
                            ".": {},
                            "f:kubernetes.io/metadata.name": {}
                        }
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T13:53:19Z"
            }
        ],
        "name": "namespace-3",
        "resourceVersion": "87415",
        "uid": "94ec91fa-92b2-449b-a957-f60324e1816a"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 13:53:24 [ForkJoinPool-1-worker-5] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@3881d5f2=[namespace-2], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[namespace-1], io.strimzi.test.logs.CollectorElement@3e255cd9=[], io.strimzi.test.logs.CollectorElement@aec142ef=[namespace-3]}
2022-03-28 13:53:24 [ForkJoinPool-1-worker-5] INFO  [KubeClusterResource:82] Client use Namespace: namespace-3
2022-03-28 13:53:24 [ForkJoinPool-1-worker-5] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-3, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 13:53:24 [ForkJoinPool-1-worker-5] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-3
2022-03-28 13:53:24 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:155] Create/Update Kafka my-cluster-3244df0d in namespace namespace-3
2022-03-28 13:53:24 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:164] Using Namespace: namespace-3
2022-03-28 13:53:24 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (189787ms till timeout)
2022-03-28 13:53:24 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (178233ms till timeout)
2022-03-28 13:53:24 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1295246ms till timeout)
2022-03-28 13:53:24 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-3244df0d
2022-03-28 13:53:24 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:433] Wait for Kafka: my-cluster-3244df0d will have desired state: Ready
2022-03-28 13:53:24 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Kafka: my-cluster-3244df0d will have desired state: Ready
2022-03-28 13:53:24 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1319900ms till timeout)
2022-03-28 13:53:25 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-28 13:53:25 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1294149ms till timeout)
2022-03-28 13:53:25 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-28 13:53:25 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 13:53:25 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (470499ms till timeout)
2022-03-28 13:53:25 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (188590ms till timeout)
2022-03-28 13:53:25 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (176851ms till timeout)
2022-03-28 13:53:26 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1318799ms till timeout)
2022-03-28 13:53:26 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-28 13:53:26 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1293050ms till timeout)
2022-03-28 13:53:26 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (187484ms till timeout)
2022-03-28 13:53:27 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (175656ms till timeout)
2022-03-28 13:53:27 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1317649ms till timeout)
2022-03-28 13:53:27 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-28 13:53:27 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 13:53:27 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (469018ms till timeout)
2022-03-28 13:53:27 [ForkJoinPool-1-worker-15] DEBUG [SuiteThreadController:184] ReconciliationST suite now can proceed its execution
2022-03-28 13:53:27 [ForkJoinPool-1-worker-15] DEBUG [TestSuiteNamespaceManager:128] Content of the test suite namespaces map:
{HttpBridgeTlsST=[http-bridge-tls-st], CruiseControlST=[cruise-control-st], AbstractST=[abstract-st], OpaIntegrationST=[opa-integration-st], CruiseControlApiST=[cruise-control-api-st], HttpBridgeScramShaST=[http-bridge-scram-sha-st], ReconciliationST=[reconciliation-st], DynamicConfST=[dynamic-conf-st], AbstractUpgradeST=[abstract-upgrade-st], QuotasST=[quotas-st], DynamicConfSharedST=[dynamic-conf-shared-st], RollingUpdateST=[rolling-update-st], OlmAbstractST=[olm-abstract-st], CustomAuthorizerST=[custom-authorizer-st], AlternativeReconcileTriggersST=[alternative-reconcile-triggers-st], LogSettingST=[log-setting-st], ListenersST=[listeners-st], UserST=[user-st], TopicST=[topic-st], SecurityST=[security-st], TracingST=[tracing-st], AbstractNamespaceST=[abstract-namespace-st], CruiseControlConfigurationST=[cruise-control-configuration-st], HttpBridgeCorsST=[http-bridge-cors-st], ConfigProviderST=[config-provider-st], KafkaST=[kafka-st], LoggingChangeST=[logging-change-st], HttpBridgeKafkaExternalListenersST=[http-bridge-kafka-external-listeners-st], OauthAbstractST=[oauth-abstract-st], ThrottlingQuotaST=[throttling-quota-st], MultipleListenersST=[multiple-listeners-st]}
2022-03-28 13:53:27 [ForkJoinPool-1-worker-15] DEBUG [TestSuiteNamespaceManager:129] Test suite `ReconciliationST` creates these additional namespaces:[reconciliation-st]
2022-03-28 13:53:27 [ForkJoinPool-1-worker-15] INFO  [KubeClusterResource:156] Creating Namespace: reconciliation-st
2022-03-28 13:53:27 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Namespace reconciliation-st
2022-03-28 13:53:27 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-3 get Namespace reconciliation-st -o json
2022-03-28 13:53:27 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1291952ms till timeout)
2022-03-28 13:53:27 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-3 get Namespace reconciliation-st -o json
2022-03-28 13:53:27 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 13:53:27 [ForkJoinPool-1-worker-15] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c31,c20",
            "openshift.io/sa.scc.supplemental-groups": "1000970000/10000",
            "openshift.io/sa.scc.uid-range": "1000970000/10000"
        },
        "creationTimestamp": "2022-03-28T13:53:23Z",
        "labels": {
            "kubernetes.io/metadata.name": "reconciliation-st"
        },
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T13:53:23Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:labels": {
                            ".": {},
                            "f:kubernetes.io/metadata.name": {}
                        }
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T13:53:23Z"
            }
        ],
        "name": "reconciliation-st",
        "resourceVersion": "87530",
        "uid": "1762853b-b173-477d-90da-f67571af08db"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 13:53:27 [ForkJoinPool-1-worker-15] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@3881d5f2=[namespace-2], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[namespace-1], io.strimzi.test.logs.CollectorElement@3e255cd9=[], io.strimzi.test.logs.CollectorElement@aec142ef=[namespace-3]}
2022-03-28 13:53:27 [ForkJoinPool-1-worker-15] INFO  [KubeClusterResource:82] Client use Namespace: reconciliation-st
2022-03-28 13:53:27 [ForkJoinPool-1-worker-15] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=reconciliation-st, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 13:53:27 [ForkJoinPool-1-worker-15] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: reconciliation-st
2022-03-28 13:53:27 [ForkJoinPool-1-worker-15] INFO  [TestSeparator:23] ############################################################################
2022-03-28 13:53:27 [ForkJoinPool-1-worker-15] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.ReconciliationST.testPauseReconciliationInKafkaRebalanceAndTopic-STARTED
2022-03-28 13:53:27 [ForkJoinPool-1-worker-15] DEBUG [AbstractST:658] ============================================================================
2022-03-28 13:53:27 [ForkJoinPool-1-worker-15] DEBUG [AbstractST:659] [operators.ReconciliationST - Before Each] - Setup test case environment
2022-03-28 13:53:27 [ForkJoinPool-1-worker-15] DEBUG [SuiteThreadController:77] [operators.ReconciliationST] - Adding parallel test: testPauseReconciliationInKafkaRebalanceAndTopic
2022-03-28 13:53:27 [ForkJoinPool-1-worker-15] DEBUG [SuiteThreadController:81] [operators.ReconciliationST] - Parallel test count: 6
2022-03-28 13:53:27 [ForkJoinPool-1-worker-15] DEBUG [SuiteThreadController:230] testPauseReconciliationInKafkaRebalanceAndTopic test now can proceed its execution
2022-03-28 13:53:27 [ForkJoinPool-1-worker-15] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 13:53:27 [ForkJoinPool-1-worker-15] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-0444229e, testReceiveSimpleMessageTlsScramSha=my-cluster-af298d01, testConfigurationFileIsCreated=my-cluster-3244df0d, testThrottlingQuotasCreateTopic=my-cluster-ac2cadd6, testThrottlingQuotasCreateAlterPartitions=my-cluster-8f6c28cb, testCapacityFile=my-cluster-0531d5ce, testSendSimpleMessageTls=my-cluster-fbc845dd, testCreatingUsersWithSecretPrefix=my-cluster-0c65d2dd, testReceiveSimpleMessageTls=my-cluster-ace1fa24, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-8af61d52, testSendSimpleMessageTlsScramSha=my-cluster-e353214b}
2022-03-28 13:53:27 [ForkJoinPool-1-worker-15] TRACE [AbstractST:607] USERS_NAME_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-user-1010649803-214635564, testReceiveSimpleMessageTlsScramSha=my-user-1597210213-707359243, testConfigurationFileIsCreated=my-user-1799841978-1468805631, testThrottlingQuotasCreateTopic=my-user-46611685-1079680568, testThrottlingQuotasCreateAlterPartitions=my-user-1557959119-1722636050, testCapacityFile=my-user-155969448-1270555822, testSendSimpleMessageTls=my-user-1754673553-2077518027, testCreatingUsersWithSecretPrefix=my-user-144001639-226807213, testReceiveSimpleMessageTls=my-user-1418772225-1024845009, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-1807142870-188660517, testSendSimpleMessageTlsScramSha=my-user-182357028-1132873010}
2022-03-28 13:53:27 [ForkJoinPool-1-worker-15] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-889283546-691691584, testReceiveSimpleMessageTlsScramSha=my-topic-649423016-1623551209, testConfigurationFileIsCreated=my-topic-1435014906-681104826, testThrottlingQuotasCreateTopic=my-topic-114922478-1506136333, testThrottlingQuotasCreateAlterPartitions=my-topic-1449977352-344480794, testCapacityFile=my-topic-296605537-1008743104, testSendSimpleMessageTls=my-topic-115477502-2025274778, testCreatingUsersWithSecretPrefix=my-topic-130462979-2061578809, testReceiveSimpleMessageTls=my-topic-1131768783-120051821, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-1367124108-1748498395, testSendSimpleMessageTlsScramSha=my-topic-652506937-1802310709}
2022-03-28 13:53:27 [ForkJoinPool-1-worker-15] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-0444229e-kafka-clients, testReceiveSimpleMessageTlsScramSha=my-cluster-af298d01-kafka-clients, testConfigurationFileIsCreated=my-cluster-3244df0d-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-ac2cadd6-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-8f6c28cb-kafka-clients, testCapacityFile=my-cluster-0531d5ce-kafka-clients, testSendSimpleMessageTls=my-cluster-fbc845dd-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-0c65d2dd-kafka-clients, testReceiveSimpleMessageTls=my-cluster-ace1fa24-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-8af61d52-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-e353214b-kafka-clients}
2022-03-28 13:53:27 [ForkJoinPool-1-worker-15] INFO  [TestSuiteNamespaceManager:163] Creating namespace:namespace-4 for test case:testPauseReconciliationInKafkaRebalanceAndTopic
2022-03-28 13:53:28 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (186384ms till timeout)
2022-03-28 13:53:28 [ForkJoinPool-1-worker-15] INFO  [KubeClusterResource:156] Creating Namespace: namespace-4
2022-03-28 13:53:28 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Namespace namespace-4
2022-03-28 13:53:28 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace reconciliation-st get Namespace namespace-4 -o json
2022-03-28 13:53:28 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-28 13:53:28 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1316521ms till timeout)
2022-03-28 13:53:28 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (174426ms till timeout)
2022-03-28 13:53:28 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace reconciliation-st get Namespace namespace-4 -o json
2022-03-28 13:53:28 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 13:53:28 [ForkJoinPool-1-worker-15] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c31,c25",
            "openshift.io/sa.scc.supplemental-groups": "1000980000/10000",
            "openshift.io/sa.scc.uid-range": "1000980000/10000"
        },
        "creationTimestamp": "2022-03-28T13:53:23Z",
        "labels": {
            "kubernetes.io/metadata.name": "namespace-4"
        },
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T13:53:23Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:labels": {
                            ".": {},
                            "f:kubernetes.io/metadata.name": {}
                        }
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T13:53:23Z"
            }
        ],
        "name": "namespace-4",
        "resourceVersion": "87575",
        "uid": "2bb1c244-46b2-48e3-a859-1803aa2430a4"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 13:53:28 [ForkJoinPool-1-worker-15] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[namespace-1], io.strimzi.test.logs.CollectorElement@aec142ef=[namespace-3], io.strimzi.test.logs.CollectorElement@3881d5f2=[namespace-2], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[namespace-4], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 13:53:28 [ForkJoinPool-1-worker-15] INFO  [KubeClusterResource:82] Client use Namespace: namespace-4
2022-03-28 13:53:28 [ForkJoinPool-1-worker-15] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-4, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 13:53:28 [ForkJoinPool-1-worker-15] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-4
2022-03-28 13:53:28 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:155] Create/Update Kafka my-cluster-0444229e in namespace namespace-4
2022-03-28 13:53:28 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:164] Using Namespace: namespace-4
2022-03-28 13:53:28 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-28 13:53:28 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 13:53:28 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (467532ms till timeout)
2022-03-28 13:53:28 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-0444229e
2022-03-28 13:53:28 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:433] Wait for Kafka: my-cluster-0444229e will have desired state: Ready
2022-03-28 13:53:28 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Kafka: my-cluster-0444229e will have desired state: Ready
2022-03-28 13:53:28 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1319900ms till timeout)
2022-03-28 13:53:29 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1290755ms till timeout)
2022-03-28 13:53:29 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (185194ms till timeout)
2022-03-28 13:53:29 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1315421ms till timeout)
2022-03-28 13:53:29 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (173232ms till timeout)
2022-03-28 13:53:29 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-28 13:53:30 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1318801ms till timeout)
2022-03-28 13:53:30 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1289657ms till timeout)
2022-03-28 13:53:30 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-28 13:53:30 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 13:53:30 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (466004ms till timeout)
2022-03-28 13:53:30 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (184093ms till timeout)
2022-03-28 13:53:30 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1314320ms till timeout)
2022-03-28 13:53:30 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (172039ms till timeout)
2022-03-28 13:53:31 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1317703ms till timeout)
2022-03-28 13:53:31 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-28 13:53:31 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1288558ms till timeout)
2022-03-28 13:53:31 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (182994ms till timeout)
2022-03-28 13:53:31 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1313222ms till timeout)
2022-03-28 13:53:31 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-28 13:53:31 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 13:53:31 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (464472ms till timeout)
2022-03-28 13:53:31 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (170843ms till timeout)
2022-03-28 13:53:32 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1316600ms till timeout)
2022-03-28 13:53:32 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1287460ms till timeout)
2022-03-28 13:53:32 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (181882ms till timeout)
2022-03-28 13:53:32 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1312124ms till timeout)
2022-03-28 13:53:32 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-28 13:53:33 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (169649ms till timeout)
2022-03-28 13:53:33 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1315499ms till timeout)
2022-03-28 13:53:33 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1286358ms till timeout)
2022-03-28 13:53:33 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (180783ms till timeout)
2022-03-28 13:53:33 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1311026ms till timeout)
2022-03-28 13:53:34 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (168454ms till timeout)
2022-03-28 13:53:34 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1314401ms till timeout)
2022-03-28 13:53:34 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1285260ms till timeout)
2022-03-28 13:53:34 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (179678ms till timeout)
2022-03-28 13:53:34 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1309927ms till timeout)
2022-03-28 13:53:35 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (167260ms till timeout)
2022-03-28 13:53:35 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1313301ms till timeout)
2022-03-28 13:53:35 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1284152ms till timeout)
2022-03-28 13:53:35 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (178577ms till timeout)
2022-03-28 13:53:36 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1308828ms till timeout)
2022-03-28 13:53:36 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (166065ms till timeout)
2022-03-28 13:53:36 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1312119ms till timeout)
2022-03-28 13:53:36 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1282985ms till timeout)
2022-03-28 13:53:36 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (177426ms till timeout)
2022-03-28 13:53:37 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1307729ms till timeout)
2022-03-28 13:53:37 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (164864ms till timeout)
2022-03-28 13:53:38 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1310831ms till timeout)
2022-03-28 13:53:38 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1281786ms till timeout)
2022-03-28 13:53:38 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (176321ms till timeout)
2022-03-28 13:53:38 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1306539ms till timeout)
2022-03-28 13:53:38 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-28 13:53:38 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 13:53:38 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (457912ms till timeout)
2022-03-28 13:53:39 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (163657ms till timeout)
2022-03-28 13:53:39 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1280667ms till timeout)
2022-03-28 13:53:39 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1309625ms till timeout)
2022-03-28 13:53:39 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (175119ms till timeout)
2022-03-28 13:53:39 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-28 13:53:39 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1305420ms till timeout)
2022-03-28 13:53:39 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-28 13:53:39 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 13:53:39 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (456299ms till timeout)
2022-03-28 13:53:40 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1279471ms till timeout)
2022-03-28 13:53:40 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1308515ms till timeout)
2022-03-28 13:53:40 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (162457ms till timeout)
2022-03-28 13:53:40 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (173920ms till timeout)
2022-03-28 13:53:40 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1304322ms till timeout)
2022-03-28 13:53:40 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-28 13:53:41 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1278362ms till timeout)
2022-03-28 13:53:41 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1307406ms till timeout)
2022-03-28 13:53:41 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (172699ms till timeout)
2022-03-28 13:53:41 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (161054ms till timeout)
2022-03-28 13:53:41 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1303050ms till timeout)
2022-03-28 13:53:41 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-28 13:53:41 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 13:53:41 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (454344ms till timeout)
2022-03-28 13:53:42 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1306307ms till timeout)
2022-03-28 13:53:42 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1277263ms till timeout)
2022-03-28 13:53:42 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (171598ms till timeout)
2022-03-28 13:53:42 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1301944ms till timeout)
2022-03-28 13:53:42 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-28 13:53:42 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (159846ms till timeout)
2022-03-28 13:53:43 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-28 13:53:43 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 13:53:43 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (452823ms till timeout)
2022-03-28 13:53:43 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1276163ms till timeout)
2022-03-28 13:53:43 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1305112ms till timeout)
2022-03-28 13:53:43 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (170491ms till timeout)
2022-03-28 13:53:43 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1300844ms till timeout)
2022-03-28 13:53:44 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (158651ms till timeout)
2022-03-28 13:53:44 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-28 13:53:44 [ForkJoinPool-1-worker-13] INFO  [InternalKafkaClient:160] Consumer finished correctly: true
2022-03-28 13:53:44 [ForkJoinPool-1-worker-13] INFO  [InternalKafkaClient:163] Consumer consumed 100 messages
2022-03-28 13:53:44 [ForkJoinPool-1-worker-13] INFO  [UserST:392] Checking owner reference - if the secret will be deleted when we delete KafkaUser
2022-03-28 13:53:44 [ForkJoinPool-1-worker-13] INFO  [UserST:394] Deleting KafkaUser:encrypted-leopold
2022-03-28 13:53:44 [ForkJoinPool-1-worker-13] INFO  [KafkaUserUtils:62] Waiting for KafkaUser deletion encrypted-leopold
2022-03-28 13:53:44 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for KafkaUser deletion encrypted-leopold
2022-03-28 13:53:44 [ForkJoinPool-1-worker-13] INFO  [KafkaUserUtils:75] KafkaUser encrypted-leopold deleted
2022-03-28 13:53:44 [ForkJoinPool-1-worker-13] INFO  [UserST:398] Deleting KafkaUser:scramed-leopold
2022-03-28 13:53:44 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1275049ms till timeout)
2022-03-28 13:53:44 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1303997ms till timeout)
2022-03-28 13:53:44 [ForkJoinPool-1-worker-13] INFO  [KafkaUserUtils:62] Waiting for KafkaUser deletion scramed-leopold
2022-03-28 13:53:44 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for KafkaUser deletion scramed-leopold
2022-03-28 13:53:44 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-28 13:53:44 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 13:53:44 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (451350ms till timeout)
2022-03-28 13:53:45 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (169386ms till timeout)
2022-03-28 13:53:45 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1299737ms till timeout)
2022-03-28 13:53:45 [ForkJoinPool-1-worker-13] INFO  [KafkaUserUtils:75] KafkaUser scramed-leopold deleted
2022-03-28 13:53:45 [ForkJoinPool-1-worker-13] INFO  [UserST:402] Checking if secrets are deleted
2022-03-28 13:53:45 [ForkJoinPool-1-worker-13] INFO  [SecretUtils:54] Waiting for Secret deletion top-secret-encrypted-leopold
2022-03-28 13:53:45 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Expected secret top-secret-encrypted-leopold deleted
2022-03-28 13:53:45 [ForkJoinPool-1-worker-13] INFO  [SecretUtils:58] Secret top-secret-encrypted-leopold deleted
2022-03-28 13:53:45 [ForkJoinPool-1-worker-13] INFO  [SecretUtils:54] Waiting for Secret deletion top-secret-scramed-leopold
2022-03-28 13:53:45 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Expected secret top-secret-scramed-leopold deleted
2022-03-28 13:53:45 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (157448ms till timeout)
2022-03-28 13:53:45 [ForkJoinPool-1-worker-13] INFO  [SecretUtils:58] Secret top-secret-scramed-leopold deleted
2022-03-28 13:53:45 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:674] ============================================================================
2022-03-28 13:53:45 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:675] [operators.user.UserST - After Each] - Clean up after test
2022-03-28 13:53:45 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:344] ############################################################################
2022-03-28 13:53:45 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:348] Delete all resources for testCreatingUsersWithSecretPrefix
2022-03-28 13:53:45 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of KafkaUser scramed-leopold in namespace namespace-1
2022-03-28 13:53:45 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:scramed-leopold
2022-03-28 13:53:45 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of Deployment my-cluster-0c65d2dd-plain-kafka-clients in namespace namespace-1
2022-03-28 13:53:45 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:my-cluster-0c65d2dd-plain-kafka-clients
2022-03-28 13:53:45 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-28 13:53:45 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1273877ms till timeout)
2022-03-28 13:53:46 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1302823ms till timeout)
2022-03-28 13:53:46 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (168215ms till timeout)
2022-03-28 13:53:46 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1298566ms till timeout)
2022-03-28 13:53:46 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-0c65d2dd-plain-kafka-clients not ready, will try again in 10000 ms (479521ms till timeout)
2022-03-28 13:53:46 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-28 13:53:46 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 1
2022-03-28 13:53:46 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 13:53:46 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Error from server (NotFound): namespaces "http-bridge-tls-st" not found
2022-03-28 13:53:46 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] ======STDERR END======
2022-03-28 13:53:46 [ForkJoinPool-1-worker-7] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[namespace-1], io.strimzi.test.logs.CollectorElement@aec142ef=[namespace-3], io.strimzi.test.logs.CollectorElement@3881d5f2=[namespace-2], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[namespace-4], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 13:53:46 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:254] HttpBridgeTlsST - Notifies waiting test suites:[UserST, ThrottlingQuotaST, TopicST, CruiseControlConfigurationST, HttpBridgeScramShaST, HttpBridgeTlsST] to and randomly select one to start execution
2022-03-28 13:53:46 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:85] [bridge.HttpBridgeTlsST] - Removing parallel suite: HttpBridgeTlsST
2022-03-28 13:53:46 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:89] [bridge.HttpBridgeTlsST] - Parallel suites count: 5
[INFO] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 296.362 s - in io.strimzi.systemtest.bridge.HttpBridgeTlsST
2022-03-28 13:53:46 [ForkJoinPool-1-worker-7] INFO  [TestSeparator:23] ############################################################################
2022-03-28 13:53:46 [ForkJoinPool-1-worker-7] INFO  [TestSeparator:24] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testDeployAndUnDeployCruiseControl-STARTED
2022-03-28 13:53:46 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:658] ============================================================================
2022-03-28 13:53:46 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:659] [cruisecontrol.CruiseControlConfigurationST - Before Each] - Setup test case environment
2022-03-28 13:53:46 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:77] [cruisecontrol.CruiseControlConfigurationST] - Adding parallel test: testDeployAndUnDeployCruiseControl
2022-03-28 13:53:46 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:81] [cruisecontrol.CruiseControlConfigurationST] - Parallel test count: 7
2022-03-28 13:53:46 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:205] [testDeployAndUnDeployCruiseControl] moved to the WaitZone, because current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 13:53:46 [ForkJoinPool-1-worker-7] TRACE [SuiteThreadController:210] testDeployAndUnDeployCruiseControl is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 13:53:46 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (156338ms till timeout)
2022-03-28 13:53:47 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1272778ms till timeout)
2022-03-28 13:53:47 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1301724ms till timeout)
2022-03-28 13:53:47 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (167115ms till timeout)
2022-03-28 13:53:47 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1297460ms till timeout)
2022-03-28 13:53:47 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (155235ms till timeout)
2022-03-28 13:53:48 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1271678ms till timeout)
2022-03-28 13:53:48 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1300626ms till timeout)
2022-03-28 13:53:48 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1296335ms till timeout)
2022-03-28 13:53:48 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (165891ms till timeout)
2022-03-28 13:53:48 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (154131ms till timeout)
2022-03-28 13:53:49 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1270577ms till timeout)
2022-03-28 13:53:49 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1299523ms till timeout)
2022-03-28 13:53:49 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1295233ms till timeout)
2022-03-28 13:53:49 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (164790ms till timeout)
2022-03-28 13:53:49 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (153027ms till timeout)
2022-03-28 13:53:50 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1269474ms till timeout)
2022-03-28 13:53:50 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1298422ms till timeout)
2022-03-28 13:53:50 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1294131ms till timeout)
2022-03-28 13:53:50 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (163688ms till timeout)
2022-03-28 13:53:50 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (151923ms till timeout)
2022-03-28 13:53:51 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:230] testDeployAndUnDeployCruiseControl test now can proceed its execution
2022-03-28 13:53:51 [ForkJoinPool-1-worker-7] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 13:53:51 [ForkJoinPool-1-worker-7] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-0444229e, testReceiveSimpleMessageTlsScramSha=my-cluster-af298d01, testDeployAndUnDeployCruiseControl=my-cluster-844045c3, testConfigurationFileIsCreated=my-cluster-3244df0d, testThrottlingQuotasCreateTopic=my-cluster-ac2cadd6, testThrottlingQuotasCreateAlterPartitions=my-cluster-8f6c28cb, testCapacityFile=my-cluster-0531d5ce, testSendSimpleMessageTls=my-cluster-fbc845dd, testCreatingUsersWithSecretPrefix=my-cluster-0c65d2dd, testReceiveSimpleMessageTls=my-cluster-ace1fa24, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-8af61d52, testSendSimpleMessageTlsScramSha=my-cluster-e353214b}
2022-03-28 13:53:51 [ForkJoinPool-1-worker-7] TRACE [AbstractST:607] USERS_NAME_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-user-1010649803-214635564, testReceiveSimpleMessageTlsScramSha=my-user-1597210213-707359243, testDeployAndUnDeployCruiseControl=my-user-2117120687-604608920, testConfigurationFileIsCreated=my-user-1799841978-1468805631, testThrottlingQuotasCreateTopic=my-user-46611685-1079680568, testThrottlingQuotasCreateAlterPartitions=my-user-1557959119-1722636050, testCapacityFile=my-user-155969448-1270555822, testSendSimpleMessageTls=my-user-1754673553-2077518027, testCreatingUsersWithSecretPrefix=my-user-144001639-226807213, testReceiveSimpleMessageTls=my-user-1418772225-1024845009, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-1807142870-188660517, testSendSimpleMessageTlsScramSha=my-user-182357028-1132873010}
2022-03-28 13:53:51 [ForkJoinPool-1-worker-7] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-889283546-691691584, testReceiveSimpleMessageTlsScramSha=my-topic-649423016-1623551209, testDeployAndUnDeployCruiseControl=my-topic-1101175281-2107378523, testConfigurationFileIsCreated=my-topic-1435014906-681104826, testThrottlingQuotasCreateTopic=my-topic-114922478-1506136333, testThrottlingQuotasCreateAlterPartitions=my-topic-1449977352-344480794, testCapacityFile=my-topic-296605537-1008743104, testSendSimpleMessageTls=my-topic-115477502-2025274778, testCreatingUsersWithSecretPrefix=my-topic-130462979-2061578809, testReceiveSimpleMessageTls=my-topic-1131768783-120051821, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-1367124108-1748498395, testSendSimpleMessageTlsScramSha=my-topic-652506937-1802310709}
2022-03-28 13:53:51 [ForkJoinPool-1-worker-7] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-0444229e-kafka-clients, testReceiveSimpleMessageTlsScramSha=my-cluster-af298d01-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-844045c3-kafka-clients, testConfigurationFileIsCreated=my-cluster-3244df0d-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-ac2cadd6-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-8f6c28cb-kafka-clients, testCapacityFile=my-cluster-0531d5ce-kafka-clients, testSendSimpleMessageTls=my-cluster-fbc845dd-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-0c65d2dd-kafka-clients, testReceiveSimpleMessageTls=my-cluster-ace1fa24-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-8af61d52-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-e353214b-kafka-clients}
2022-03-28 13:53:51 [ForkJoinPool-1-worker-7] INFO  [TestSuiteNamespaceManager:163] Creating namespace:namespace-5 for test case:testDeployAndUnDeployCruiseControl
2022-03-28 13:53:51 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1268375ms till timeout)
2022-03-28 13:53:51 [ForkJoinPool-1-worker-7] INFO  [KubeClusterResource:156] Creating Namespace: namespace-5
2022-03-28 13:53:51 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1297322ms till timeout)
2022-03-28 13:53:51 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Namespace namespace-5
2022-03-28 13:53:51 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-4 get Namespace namespace-5 -o json
2022-03-28 13:53:51 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1292933ms till timeout)
2022-03-28 13:53:51 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (162483ms till timeout)
2022-03-28 13:53:52 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (150806ms till timeout)
2022-03-28 13:53:52 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-4 get Namespace namespace-5 -o json
2022-03-28 13:53:52 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 13:53:52 [ForkJoinPool-1-worker-7] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c31,c30",
            "openshift.io/sa.scc.supplemental-groups": "1000990000/10000",
            "openshift.io/sa.scc.uid-range": "1000990000/10000"
        },
        "creationTimestamp": "2022-03-28T13:53:47Z",
        "labels": {
            "kubernetes.io/metadata.name": "namespace-5"
        },
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T13:53:47Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:labels": {
                            ".": {},
                            "f:kubernetes.io/metadata.name": {}
                        }
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T13:53:47Z"
            }
        ],
        "name": "namespace-5",
        "resourceVersion": "88099",
        "uid": "e3656937-cd31-49df-954b-6b2c81919df9"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 13:53:52 [ForkJoinPool-1-worker-7] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[namespace-1], io.strimzi.test.logs.CollectorElement@aec142ef=[namespace-3], io.strimzi.test.logs.CollectorElement@3881d5f2=[namespace-2], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[namespace-4], io.strimzi.test.logs.CollectorElement@f4405b3b=[namespace-5], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 13:53:52 [ForkJoinPool-1-worker-7] INFO  [KubeClusterResource:82] Client use Namespace: namespace-5
2022-03-28 13:53:52 [ForkJoinPool-1-worker-7] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-5, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 13:53:52 [ForkJoinPool-1-worker-7] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-5
2022-03-28 13:53:52 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update Kafka my-cluster-844045c3 in namespace namespace-5
2022-03-28 13:53:52 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:164] Using Namespace: namespace-5
2022-03-28 13:53:52 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-844045c3
2022-03-28 13:53:52 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:433] Wait for Kafka: my-cluster-844045c3 will have desired state: Ready
2022-03-28 13:53:52 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Kafka: my-cluster-844045c3 will have desired state: Ready
2022-03-28 13:53:52 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1267214ms till timeout)
2022-03-28 13:53:52 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1319808ms till timeout)
2022-03-28 13:53:52 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1296161ms till timeout)
2022-03-28 13:53:52 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1291835ms till timeout)
2022-03-28 13:53:53 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (161293ms till timeout)
2022-03-28 13:53:53 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (149639ms till timeout)
2022-03-28 13:53:53 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1266113ms till timeout)
2022-03-28 13:53:53 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1295053ms till timeout)
2022-03-28 13:53:53 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1318699ms till timeout)
2022-03-28 13:53:54 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1290735ms till timeout)
2022-03-28 13:53:54 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (160191ms till timeout)
2022-03-28 13:53:54 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (148524ms till timeout)
2022-03-28 13:53:54 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1265012ms till timeout)
2022-03-28 13:53:54 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1293954ms till timeout)
2022-03-28 13:53:55 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1317505ms till timeout)
2022-03-28 13:53:55 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1289637ms till timeout)
2022-03-28 13:53:55 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (159091ms till timeout)
2022-03-28 13:53:55 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (147418ms till timeout)
2022-03-28 13:53:55 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1263888ms till timeout)
2022-03-28 13:53:56 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1292836ms till timeout)
2022-03-28 13:53:56 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1316386ms till timeout)
2022-03-28 13:53:56 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1288539ms till timeout)
2022-03-28 13:53:56 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (157989ms till timeout)
2022-03-28 13:53:56 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (146227ms till timeout)
2022-03-28 13:53:56 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-0c65d2dd-plain-kafka-clients not ready, will try again in 10000 ms (469088ms till timeout)
2022-03-28 13:53:57 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1262790ms till timeout)
2022-03-28 13:53:57 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1291735ms till timeout)
2022-03-28 13:53:57 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1315286ms till timeout)
2022-03-28 13:53:57 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1287441ms till timeout)
2022-03-28 13:53:57 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (156887ms till timeout)
2022-03-28 13:53:57 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (145121ms till timeout)
2022-03-28 13:53:58 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1261691ms till timeout)
2022-03-28 13:53:58 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1290631ms till timeout)
2022-03-28 13:53:58 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1314182ms till timeout)
2022-03-28 13:53:58 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1286342ms till timeout)
2022-03-28 13:53:58 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (155785ms till timeout)
2022-03-28 13:53:58 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (144014ms till timeout)
2022-03-28 13:53:59 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1260584ms till timeout)
2022-03-28 13:53:59 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1289533ms till timeout)
2022-03-28 13:53:59 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1313083ms till timeout)
2022-03-28 13:53:59 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1285242ms till timeout)
2022-03-28 13:53:59 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (154683ms till timeout)
2022-03-28 13:53:59 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (142902ms till timeout)
2022-03-28 13:54:00 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1259485ms till timeout)
2022-03-28 13:54:00 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1288430ms till timeout)
2022-03-28 13:54:00 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1311981ms till timeout)
2022-03-28 13:54:00 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1284139ms till timeout)
2022-03-28 13:54:00 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (153575ms till timeout)
2022-03-28 13:54:01 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (141789ms till timeout)
2022-03-28 13:54:01 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1258384ms till timeout)
2022-03-28 13:54:01 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1287333ms till timeout)
2022-03-28 13:54:01 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1310882ms till timeout)
2022-03-28 13:54:01 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1283040ms till timeout)
2022-03-28 13:54:01 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (152468ms till timeout)
2022-03-28 13:54:02 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (140650ms till timeout)
2022-03-28 13:54:02 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1257285ms till timeout)
2022-03-28 13:54:02 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1286232ms till timeout)
2022-03-28 13:54:02 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1309782ms till timeout)
2022-03-28 13:54:02 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1281942ms till timeout)
2022-03-28 13:54:03 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (151344ms till timeout)
2022-03-28 13:54:03 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (139544ms till timeout)
2022-03-28 13:54:03 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1256185ms till timeout)
2022-03-28 13:54:03 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1285134ms till timeout)
2022-03-28 13:54:03 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1308684ms till timeout)
2022-03-28 13:54:03 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1280844ms till timeout)
2022-03-28 13:54:04 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (150228ms till timeout)
2022-03-28 13:54:04 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (138437ms till timeout)
2022-03-28 13:54:04 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1255086ms till timeout)
2022-03-28 13:54:04 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1284033ms till timeout)
2022-03-28 13:54:04 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1307584ms till timeout)
2022-03-28 13:54:05 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1279745ms till timeout)
2022-03-28 13:54:05 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (149125ms till timeout)
2022-03-28 13:54:05 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (137328ms till timeout)
2022-03-28 13:54:05 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1253987ms till timeout)
2022-03-28 13:54:05 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1282935ms till timeout)
2022-03-28 13:54:06 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1306482ms till timeout)
2022-03-28 13:54:06 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1278647ms till timeout)
2022-03-28 13:54:06 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (148021ms till timeout)
2022-03-28 13:54:06 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (136221ms till timeout)
2022-03-28 13:54:06 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1252856ms till timeout)
2022-03-28 13:54:07 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1281802ms till timeout)
2022-03-28 13:54:07 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1305352ms till timeout)
2022-03-28 13:54:07 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1277547ms till timeout)
2022-03-28 13:54:07 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-0c65d2dd-plain-kafka-clients not ready, will try again in 10000 ms (458600ms till timeout)
2022-03-28 13:54:07 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (146919ms till timeout)
2022-03-28 13:54:07 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (135118ms till timeout)
2022-03-28 13:54:08 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1251757ms till timeout)
2022-03-28 13:54:08 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1280704ms till timeout)
2022-03-28 13:54:08 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1304254ms till timeout)
2022-03-28 13:54:08 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1276449ms till timeout)
2022-03-28 13:54:08 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (145810ms till timeout)
2022-03-28 13:54:08 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (134015ms till timeout)
2022-03-28 13:54:09 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1250658ms till timeout)
2022-03-28 13:54:09 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1279606ms till timeout)
2022-03-28 13:54:09 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1303155ms till timeout)
2022-03-28 13:54:09 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1275347ms till timeout)
2022-03-28 13:54:09 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (144708ms till timeout)
2022-03-28 13:54:09 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (132911ms till timeout)
2022-03-28 13:54:10 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1249547ms till timeout)
2022-03-28 13:54:10 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1278507ms till timeout)
2022-03-28 13:54:10 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1302057ms till timeout)
2022-03-28 13:54:10 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1274249ms till timeout)
2022-03-28 13:54:10 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (143605ms till timeout)
2022-03-28 13:54:11 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (131807ms till timeout)
2022-03-28 13:54:11 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1248447ms till timeout)
2022-03-28 13:54:11 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1277391ms till timeout)
2022-03-28 13:54:11 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1300941ms till timeout)
2022-03-28 13:54:11 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1273136ms till timeout)
2022-03-28 13:54:11 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (142502ms till timeout)
2022-03-28 13:54:12 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (130700ms till timeout)
2022-03-28 13:54:12 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1247347ms till timeout)
2022-03-28 13:54:12 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1276293ms till timeout)
2022-03-28 13:54:12 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1299839ms till timeout)
2022-03-28 13:54:12 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1272034ms till timeout)
2022-03-28 13:54:12 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (141394ms till timeout)
2022-03-28 13:54:13 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (129595ms till timeout)
2022-03-28 13:54:13 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1246243ms till timeout)
2022-03-28 13:54:13 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1275191ms till timeout)
2022-03-28 13:54:13 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1298733ms till timeout)
2022-03-28 13:54:13 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1270929ms till timeout)
2022-03-28 13:54:14 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (140291ms till timeout)
2022-03-28 13:54:14 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (128493ms till timeout)
2022-03-28 13:54:14 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1245139ms till timeout)
2022-03-28 13:54:14 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1274086ms till timeout)
2022-03-28 13:54:14 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1297635ms till timeout)
2022-03-28 13:54:15 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1269830ms till timeout)
2022-03-28 13:54:15 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (139174ms till timeout)
2022-03-28 13:54:15 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (127386ms till timeout)
2022-03-28 13:54:15 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1244039ms till timeout)
2022-03-28 13:54:15 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1272986ms till timeout)
2022-03-28 13:54:16 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1296536ms till timeout)
2022-03-28 13:54:16 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1268731ms till timeout)
2022-03-28 13:54:16 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (138073ms till timeout)
2022-03-28 13:54:16 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (126278ms till timeout)
2022-03-28 13:54:16 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1242935ms till timeout)
2022-03-28 13:54:17 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1271870ms till timeout)
2022-03-28 13:54:17 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1295421ms till timeout)
2022-03-28 13:54:17 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1267616ms till timeout)
2022-03-28 13:54:17 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of Deployment my-cluster-0c65d2dd-tls-kafka-clients in namespace namespace-1
2022-03-28 13:54:17 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (136903ms till timeout)
2022-03-28 13:54:17 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:my-cluster-0c65d2dd-tls-kafka-clients
2022-03-28 13:54:17 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (125151ms till timeout)
2022-03-28 13:54:17 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-0c65d2dd-tls-kafka-clients not ready, will try again in 10000 ms (479613ms till timeout)
2022-03-28 13:54:18 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1241785ms till timeout)
2022-03-28 13:54:18 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1270733ms till timeout)
2022-03-28 13:54:18 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1294284ms till timeout)
2022-03-28 13:54:18 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1266478ms till timeout)
2022-03-28 13:54:18 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (135796ms till timeout)
2022-03-28 13:54:18 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (124042ms till timeout)
2022-03-28 13:54:19 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1240686ms till timeout)
2022-03-28 13:54:19 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1269634ms till timeout)
2022-03-28 13:54:19 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1293182ms till timeout)
2022-03-28 13:54:19 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1265376ms till timeout)
2022-03-28 13:54:19 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (134691ms till timeout)
2022-03-28 13:54:19 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (122932ms till timeout)
2022-03-28 13:54:20 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1239587ms till timeout)
2022-03-28 13:54:20 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1268535ms till timeout)
2022-03-28 13:54:20 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1292078ms till timeout)
2022-03-28 13:54:20 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1264274ms till timeout)
2022-03-28 13:54:20 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (133589ms till timeout)
2022-03-28 13:54:21 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (121800ms till timeout)
2022-03-28 13:54:21 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1238486ms till timeout)
2022-03-28 13:54:21 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1267433ms till timeout)
2022-03-28 13:54:21 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1290980ms till timeout)
2022-03-28 13:54:21 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1263173ms till timeout)
2022-03-28 13:54:21 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (132486ms till timeout)
2022-03-28 13:54:22 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (120694ms till timeout)
2022-03-28 13:54:22 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1237384ms till timeout)
2022-03-28 13:54:22 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1266335ms till timeout)
2022-03-28 13:54:22 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1289881ms till timeout)
2022-03-28 13:54:22 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1262074ms till timeout)
2022-03-28 13:54:23 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (131385ms till timeout)
2022-03-28 13:54:23 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (119589ms till timeout)
2022-03-28 13:54:23 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1236280ms till timeout)
2022-03-28 13:54:23 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1265216ms till timeout)
2022-03-28 13:54:23 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1288767ms till timeout)
2022-03-28 13:54:23 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1260963ms till timeout)
2022-03-28 13:54:24 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (130274ms till timeout)
2022-03-28 13:54:24 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (118485ms till timeout)
2022-03-28 13:54:24 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1235181ms till timeout)
2022-03-28 13:54:24 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1264118ms till timeout)
2022-03-28 13:54:24 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1287664ms till timeout)
2022-03-28 13:54:24 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1259864ms till timeout)
2022-03-28 13:54:25 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (129172ms till timeout)
2022-03-28 13:54:25 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (117381ms till timeout)
2022-03-28 13:54:25 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1234081ms till timeout)
2022-03-28 13:54:25 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1263020ms till timeout)
2022-03-28 13:54:25 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1286566ms till timeout)
2022-03-28 13:54:26 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1258761ms till timeout)
2022-03-28 13:54:26 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (128064ms till timeout)
2022-03-28 13:54:26 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (116271ms till timeout)
2022-03-28 13:54:26 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1232980ms till timeout)
2022-03-28 13:54:26 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1261911ms till timeout)
2022-03-28 13:54:27 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1285454ms till timeout)
2022-03-28 13:54:27 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1257646ms till timeout)
2022-03-28 13:54:27 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (126957ms till timeout)
2022-03-28 13:54:27 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (115167ms till timeout)
2022-03-28 13:54:27 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1231881ms till timeout)
2022-03-28 13:54:28 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1260724ms till timeout)
2022-03-28 13:54:28 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1284275ms till timeout)
2022-03-28 13:54:28 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1256471ms till timeout)
2022-03-28 13:54:28 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-0c65d2dd-tls-kafka-clients not ready, will try again in 10000 ms (469127ms till timeout)
2022-03-28 13:54:28 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (125824ms till timeout)
2022-03-28 13:54:28 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (114043ms till timeout)
2022-03-28 13:54:29 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1230778ms till timeout)
2022-03-28 13:54:29 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1259626ms till timeout)
2022-03-28 13:54:29 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1283177ms till timeout)
2022-03-28 13:54:29 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1255372ms till timeout)
2022-03-28 13:54:29 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (124709ms till timeout)
2022-03-28 13:54:29 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (112937ms till timeout)
2022-03-28 13:54:30 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1229678ms till timeout)
2022-03-28 13:54:30 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1258528ms till timeout)
2022-03-28 13:54:30 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1282078ms till timeout)
2022-03-28 13:54:30 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1254267ms till timeout)
2022-03-28 13:54:30 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (123608ms till timeout)
2022-03-28 13:54:31 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (111829ms till timeout)
2022-03-28 13:54:31 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1228580ms till timeout)
2022-03-28 13:54:31 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1257428ms till timeout)
2022-03-28 13:54:31 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1280978ms till timeout)
2022-03-28 13:54:31 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1253168ms till timeout)
2022-03-28 13:54:31 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (122504ms till timeout)
2022-03-28 13:54:32 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (110722ms till timeout)
2022-03-28 13:54:32 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1227480ms till timeout)
2022-03-28 13:54:32 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1256328ms till timeout)
2022-03-28 13:54:32 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1279879ms till timeout)
2022-03-28 13:54:32 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1252061ms till timeout)
2022-03-28 13:54:32 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (121400ms till timeout)
2022-03-28 13:54:33 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (109617ms till timeout)
2022-03-28 13:54:33 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1226374ms till timeout)
2022-03-28 13:54:33 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1255230ms till timeout)
2022-03-28 13:54:33 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1278780ms till timeout)
2022-03-28 13:54:33 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1250841ms till timeout)
2022-03-28 13:54:34 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (120286ms till timeout)
2022-03-28 13:54:34 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (108503ms till timeout)
2022-03-28 13:54:34 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1225274ms till timeout)
2022-03-28 13:54:34 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1254126ms till timeout)
2022-03-28 13:54:34 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1277678ms till timeout)
2022-03-28 13:54:35 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1249736ms till timeout)
2022-03-28 13:54:35 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (119180ms till timeout)
2022-03-28 13:54:35 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (107399ms till timeout)
2022-03-28 13:54:35 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1224174ms till timeout)
2022-03-28 13:54:35 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1253027ms till timeout)
2022-03-28 13:54:35 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1276577ms till timeout)
2022-03-28 13:54:36 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1248636ms till timeout)
2022-03-28 13:54:36 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (118074ms till timeout)
2022-03-28 13:54:36 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (106293ms till timeout)
2022-03-28 13:54:36 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1223076ms till timeout)
2022-03-28 13:54:36 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1251926ms till timeout)
2022-03-28 13:54:37 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1275478ms till timeout)
2022-03-28 13:54:37 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1247537ms till timeout)
2022-03-28 13:54:37 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (116970ms till timeout)
2022-03-28 13:54:37 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (105190ms till timeout)
2022-03-28 13:54:37 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1221976ms till timeout)
2022-03-28 13:54:38 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1250795ms till timeout)
2022-03-28 13:54:38 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1274339ms till timeout)
2022-03-28 13:54:38 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1246439ms till timeout)
2022-03-28 13:54:38 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (115868ms till timeout)
2022-03-28 13:54:38 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (104018ms till timeout)
2022-03-28 13:54:38 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-0c65d2dd-tls-kafka-clients not ready, will try again in 10000 ms (458672ms till timeout)
2022-03-28 13:54:39 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1220844ms till timeout)
2022-03-28 13:54:39 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1249693ms till timeout)
2022-03-28 13:54:39 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1273240ms till timeout)
2022-03-28 13:54:39 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1245340ms till timeout)
2022-03-28 13:54:39 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (114767ms till timeout)
2022-03-28 13:54:39 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (102911ms till timeout)
2022-03-28 13:54:40 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1219744ms till timeout)
2022-03-28 13:54:40 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1248593ms till timeout)
2022-03-28 13:54:40 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1272142ms till timeout)
2022-03-28 13:54:40 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1244241ms till timeout)
2022-03-28 13:54:40 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (113664ms till timeout)
2022-03-28 13:54:41 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (101804ms till timeout)
2022-03-28 13:54:41 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1218644ms till timeout)
2022-03-28 13:54:41 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1247493ms till timeout)
2022-03-28 13:54:41 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1271044ms till timeout)
2022-03-28 13:54:41 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1243143ms till timeout)
2022-03-28 13:54:41 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (112563ms till timeout)
2022-03-28 13:54:42 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (100691ms till timeout)
2022-03-28 13:54:42 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1217476ms till timeout)
2022-03-28 13:54:42 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1246394ms till timeout)
2022-03-28 13:54:42 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1269945ms till timeout)
2022-03-28 13:54:42 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1242044ms till timeout)
2022-03-28 13:54:42 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (111461ms till timeout)
2022-03-28 13:54:43 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (99589ms till timeout)
2022-03-28 13:54:43 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1216377ms till timeout)
2022-03-28 13:54:43 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1245295ms till timeout)
2022-03-28 13:54:43 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1268846ms till timeout)
2022-03-28 13:54:43 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1240945ms till timeout)
2022-03-28 13:54:44 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (110360ms till timeout)
2022-03-28 13:54:44 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (98476ms till timeout)
2022-03-28 13:54:44 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1215277ms till timeout)
2022-03-28 13:54:44 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1244196ms till timeout)
2022-03-28 13:54:44 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1267747ms till timeout)
2022-03-28 13:54:44 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1239846ms till timeout)
2022-03-28 13:54:45 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (109259ms till timeout)
2022-03-28 13:54:45 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (97372ms till timeout)
2022-03-28 13:54:45 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1214177ms till timeout)
2022-03-28 13:54:45 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1243098ms till timeout)
2022-03-28 13:54:45 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1266650ms till timeout)
2022-03-28 13:54:46 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1238747ms till timeout)
2022-03-28 13:54:46 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (108158ms till timeout)
2022-03-28 13:54:46 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (96266ms till timeout)
2022-03-28 13:54:46 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1213078ms till timeout)
2022-03-28 13:54:46 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1241998ms till timeout)
2022-03-28 13:54:46 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1265548ms till timeout)
2022-03-28 13:54:47 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1237648ms till timeout)
2022-03-28 13:54:47 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (107057ms till timeout)
2022-03-28 13:54:47 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (95161ms till timeout)
2022-03-28 13:54:47 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1211973ms till timeout)
2022-03-28 13:54:47 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1240899ms till timeout)
2022-03-28 13:54:48 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1264449ms till timeout)
2022-03-28 13:54:48 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1236550ms till timeout)
2022-03-28 13:54:48 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (105955ms till timeout)
2022-03-28 13:54:48 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (94053ms till timeout)
2022-03-28 13:54:48 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1210873ms till timeout)
2022-03-28 13:54:49 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of KafkaTopic my-topic-130462979-2061578809 in namespace namespace-1
2022-03-28 13:54:49 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1239719ms till timeout)
2022-03-28 13:54:49 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1263270ms till timeout)
2022-03-28 13:54:49 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-130462979-2061578809
2022-03-28 13:54:49 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of KafkaUser encrypted-leopold in namespace namespace-1
2022-03-28 13:54:49 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1235364ms till timeout)
2022-03-28 13:54:49 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:encrypted-leopold
2022-03-28 13:54:49 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (104821ms till timeout)
2022-03-28 13:54:49 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of Kafka my-cluster-0c65d2dd in namespace namespace-1
2022-03-28 13:54:49 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-0c65d2dd
2022-03-28 13:54:49 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster-0c65d2dd not ready, will try again in 10000 ms (839880ms till timeout)
2022-03-28 13:54:49 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (92837ms till timeout)
2022-03-28 13:54:50 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1209760ms till timeout)
2022-03-28 13:54:50 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1238620ms till timeout)
2022-03-28 13:54:50 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1262171ms till timeout)
2022-03-28 13:54:50 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1234265ms till timeout)
2022-03-28 13:54:50 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (103721ms till timeout)
2022-03-28 13:54:51 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (91733ms till timeout)
2022-03-28 13:54:51 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1208652ms till timeout)
2022-03-28 13:54:51 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1237520ms till timeout)
2022-03-28 13:54:51 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1261072ms till timeout)
2022-03-28 13:54:51 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1233166ms till timeout)
2022-03-28 13:54:51 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (102615ms till timeout)
2022-03-28 13:54:52 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (90620ms till timeout)
2022-03-28 13:54:52 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1207544ms till timeout)
2022-03-28 13:54:52 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1236418ms till timeout)
2022-03-28 13:54:52 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1259957ms till timeout)
2022-03-28 13:54:52 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1232066ms till timeout)
2022-03-28 13:54:52 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (101514ms till timeout)
2022-03-28 13:54:53 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (89515ms till timeout)
2022-03-28 13:54:53 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1206435ms till timeout)
2022-03-28 13:54:53 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1235319ms till timeout)
2022-03-28 13:54:53 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1258851ms till timeout)
2022-03-28 13:54:53 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1230967ms till timeout)
2022-03-28 13:54:53 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (100413ms till timeout)
2022-03-28 13:54:54 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (88410ms till timeout)
2022-03-28 13:54:54 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1205331ms till timeout)
2022-03-28 13:54:54 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1234206ms till timeout)
2022-03-28 13:54:54 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1257752ms till timeout)
2022-03-28 13:54:54 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1229868ms till timeout)
2022-03-28 13:54:55 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (99311ms till timeout)
2022-03-28 13:54:55 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (87307ms till timeout)
2022-03-28 13:54:55 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1204201ms till timeout)
2022-03-28 13:54:55 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1233109ms till timeout)
2022-03-28 13:54:55 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1256643ms till timeout)
2022-03-28 13:54:56 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1228769ms till timeout)
2022-03-28 13:54:56 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (98135ms till timeout)
2022-03-28 13:54:56 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (86203ms till timeout)
2022-03-28 13:54:56 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1203102ms till timeout)
2022-03-28 13:54:56 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1232011ms till timeout)
2022-03-28 13:54:56 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1255544ms till timeout)
2022-03-28 13:54:57 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1227670ms till timeout)
2022-03-28 13:54:57 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (97033ms till timeout)
2022-03-28 13:54:57 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (85067ms till timeout)
2022-03-28 13:54:57 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1201985ms till timeout)
2022-03-28 13:54:57 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1230913ms till timeout)
2022-03-28 13:54:58 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1254446ms till timeout)
2022-03-28 13:54:58 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1226572ms till timeout)
2022-03-28 13:54:58 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (95931ms till timeout)
2022-03-28 13:54:58 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (83963ms till timeout)
2022-03-28 13:54:58 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1200882ms till timeout)
2022-03-28 13:54:59 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1229815ms till timeout)
2022-03-28 13:54:59 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1253348ms till timeout)
2022-03-28 13:54:59 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1225473ms till timeout)
2022-03-28 13:54:59 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (94831ms till timeout)
2022-03-28 13:54:59 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (82860ms till timeout)
2022-03-28 13:55:00 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:369] ############################################################################
2022-03-28 13:55:00 [ForkJoinPool-1-worker-13] INFO  [TestSuiteNamespaceManager:200] Deleting namespace:namespace-1 for test case:testCreatingUsersWithSecretPrefix
2022-03-28 13:55:00 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1199779ms till timeout)
2022-03-28 13:55:00 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Namespace namespace-1 removal
2022-03-28 13:55:00 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-5 get Namespace namespace-1 -o yaml
2022-03-28 13:55:00 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1228631ms till timeout)
2022-03-28 13:55:00 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1252183ms till timeout)
2022-03-28 13:55:00 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1224375ms till timeout)
2022-03-28 13:55:00 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-5 get Namespace namespace-1 -o yaml
2022-03-28 13:55:00 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:00 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (479534ms till timeout)
2022-03-28 13:55:00 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (93730ms till timeout)
2022-03-28 13:55:01 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (81756ms till timeout)
2022-03-28 13:55:01 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1198676ms till timeout)
2022-03-28 13:55:01 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1227534ms till timeout)
2022-03-28 13:55:01 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1251085ms till timeout)
2022-03-28 13:55:01 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1223276ms till timeout)
2022-03-28 13:55:01 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-5 get Namespace namespace-1 -o yaml
2022-03-28 13:55:01 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (92629ms till timeout)
2022-03-28 13:55:02 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (80648ms till timeout)
2022-03-28 13:55:02 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-5 get Namespace namespace-1 -o yaml
2022-03-28 13:55:02 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:02 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (477949ms till timeout)
2022-03-28 13:55:02 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1197571ms till timeout)
2022-03-28 13:55:02 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1226434ms till timeout)
2022-03-28 13:55:02 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1249980ms till timeout)
2022-03-28 13:55:02 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1222175ms till timeout)
2022-03-28 13:55:02 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (91525ms till timeout)
2022-03-28 13:55:03 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-5 get Namespace namespace-1 -o yaml
2022-03-28 13:55:03 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (79538ms till timeout)
2022-03-28 13:55:03 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1196462ms till timeout)
2022-03-28 13:55:03 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1225334ms till timeout)
2022-03-28 13:55:03 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1248882ms till timeout)
2022-03-28 13:55:03 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1221077ms till timeout)
2022-03-28 13:55:03 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-5 get Namespace namespace-1 -o yaml
2022-03-28 13:55:03 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:03 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (476415ms till timeout)
2022-03-28 13:55:03 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (90424ms till timeout)
2022-03-28 13:55:04 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (78432ms till timeout)
2022-03-28 13:55:04 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-8af61d52 will have desired state: Ready not ready, will try again in 1000 ms (1195353ms till timeout)
2022-03-28 13:55:04 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1224234ms till timeout)
2022-03-28 13:55:04 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1247785ms till timeout)
2022-03-28 13:55:04 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-5 get Namespace namespace-1 -o yaml
2022-03-28 13:55:04 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1219978ms till timeout)
2022-03-28 13:55:05 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (89324ms till timeout)
2022-03-28 13:55:05 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-5 get Namespace namespace-1 -o yaml
2022-03-28 13:55:05 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:05 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (474907ms till timeout)
2022-03-28 13:55:05 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (77326ms till timeout)
2022-03-28 13:55:05 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:444] Kafka: my-cluster-8af61d52 is in desired state: Ready
2022-03-28 13:55:05 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1223097ms till timeout)
2022-03-28 13:55:05 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1246648ms till timeout)
2022-03-28 13:55:05 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1218842ms till timeout)
2022-03-28 13:55:06 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (88202ms till timeout)
2022-03-28 13:55:06 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-5 get Namespace namespace-1 -o yaml
2022-03-28 13:55:06 [ForkJoinPool-1-worker-3] INFO  [CruiseControlConfigurationST:157] Changing the broker capacity of the cruise control
2022-03-28 13:55:06 [ForkJoinPool-1-worker-3] INFO  [CruiseControlConfigurationST:168] Verifying that CC pod is rolling, because of change size of disk
2022-03-28 13:55:06 [ForkJoinPool-1-worker-3] INFO  [DeploymentUtils:136] Waiting for Deployment my-cluster-8af61d52-cruise-control rolling update
2022-03-28 13:55:06 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Deployment my-cluster-8af61d52-cruise-control rolling update in namespace:namespace-2
2022-03-28 13:55:06 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-8af61d52-cruise-control-f7786667b-ltx5c=04ae3803-1af4-46cd-ae48-030c1ceece94}
2022-03-28 13:55:06 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-8af61d52-cruise-control-f7786667b-ltx5c=04ae3803-1af4-46cd-ae48-030c1ceece94}
2022-03-28 13:55:06 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-8af61d52-cruise-control-f7786667b-ltx5c=04ae3803-1af4-46cd-ae48-030c1ceece94}
2022-03-28 13:55:06 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Deployment my-cluster-8af61d52-cruise-control rolling update in namespace:namespace-2 not ready, will try again in 5000 ms (599804ms till timeout)
2022-03-28 13:55:06 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (76138ms till timeout)
2022-03-28 13:55:06 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-5 get Namespace namespace-1 -o yaml
2022-03-28 13:55:06 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:06 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (473429ms till timeout)
2022-03-28 13:55:06 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1221997ms till timeout)
2022-03-28 13:55:06 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1245547ms till timeout)
2022-03-28 13:55:07 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1217736ms till timeout)
2022-03-28 13:55:07 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (87098ms till timeout)
2022-03-28 13:55:07 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-5 get Namespace namespace-1 -o yaml
2022-03-28 13:55:07 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (75034ms till timeout)
2022-03-28 13:55:07 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1220897ms till timeout)
2022-03-28 13:55:08 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1244449ms till timeout)
2022-03-28 13:55:08 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-5 get Namespace namespace-1 -o yaml
2022-03-28 13:55:08 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:08 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (471978ms till timeout)
2022-03-28 13:55:08 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1216638ms till timeout)
2022-03-28 13:55:08 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (85995ms till timeout)
2022-03-28 13:55:08 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (73930ms till timeout)
2022-03-28 13:55:09 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1219796ms till timeout)
2022-03-28 13:55:09 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-5 get Namespace namespace-1 -o yaml
2022-03-28 13:55:09 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1243347ms till timeout)
2022-03-28 13:55:09 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1215541ms till timeout)
2022-03-28 13:55:09 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (84892ms till timeout)
2022-03-28 13:55:09 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-5 get Namespace namespace-1 -o yaml
2022-03-28 13:55:09 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:09 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (470465ms till timeout)
2022-03-28 13:55:10 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (72825ms till timeout)
2022-03-28 13:55:10 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1218697ms till timeout)
2022-03-28 13:55:10 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1242247ms till timeout)
2022-03-28 13:55:10 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1214442ms till timeout)
2022-03-28 13:55:10 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (83793ms till timeout)
2022-03-28 13:55:10 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-5 get Namespace namespace-1 -o yaml
2022-03-28 13:55:11 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (71719ms till timeout)
2022-03-28 13:55:11 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-5 get Namespace namespace-1 -o yaml
2022-03-28 13:55:11 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:11 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (469001ms till timeout)
2022-03-28 13:55:11 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1217597ms till timeout)
2022-03-28 13:55:11 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1241148ms till timeout)
2022-03-28 13:55:11 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1213342ms till timeout)
2022-03-28 13:55:11 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-8af61d52-cruise-control-f7786667b-ltx5c=04ae3803-1af4-46cd-ae48-030c1ceece94}
2022-03-28 13:55:11 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (82691ms till timeout)
2022-03-28 13:55:11 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-8af61d52-cruise-control-789676d5cc-4dxnv=5dbdbd50-3498-455b-beeb-ccd3fd6c68cd, my-cluster-8af61d52-cruise-control-f7786667b-ltx5c=04ae3803-1af4-46cd-ae48-030c1ceece94}
2022-03-28 13:55:11 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-8af61d52-cruise-control-f7786667b-ltx5c=04ae3803-1af4-46cd-ae48-030c1ceece94}
2022-03-28 13:55:11 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Deployment my-cluster-8af61d52-cruise-control rolling update in namespace:namespace-2 not ready, will try again in 5000 ms (594508ms till timeout)
2022-03-28 13:55:12 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-5 get Namespace namespace-1 -o yaml
2022-03-28 13:55:12 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (70617ms till timeout)
2022-03-28 13:55:12 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1216499ms till timeout)
2022-03-28 13:55:12 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1240047ms till timeout)
2022-03-28 13:55:12 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1212242ms till timeout)
2022-03-28 13:55:12 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-5 get Namespace namespace-1 -o yaml
2022-03-28 13:55:12 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:12 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (467525ms till timeout)
2022-03-28 13:55:12 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (81590ms till timeout)
2022-03-28 13:55:13 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (69512ms till timeout)
2022-03-28 13:55:13 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1215398ms till timeout)
2022-03-28 13:55:13 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1238937ms till timeout)
2022-03-28 13:55:13 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-5 get Namespace namespace-1 -o yaml
2022-03-28 13:55:13 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1211119ms till timeout)
2022-03-28 13:55:13 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (80489ms till timeout)
2022-03-28 13:55:14 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-5 get Namespace namespace-1 -o yaml
2022-03-28 13:55:14 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 1
2022-03-28 13:55:14 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 13:55:14 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Error from server (NotFound): namespaces "namespace-1" not found
2022-03-28 13:55:14 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] ======STDERR END======
2022-03-28 13:55:14 [ForkJoinPool-1-worker-13] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[], io.strimzi.test.logs.CollectorElement@aec142ef=[namespace-3], io.strimzi.test.logs.CollectorElement@3881d5f2=[namespace-2], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[namespace-4], io.strimzi.test.logs.CollectorElement@f4405b3b=[namespace-5], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 13:55:14 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:267] testCreatingUsersWithSecretPrefix - Notifies waiting test cases:[testCapacityFile, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testThrottlingQuotasCreateAlterPartitions, testSendSimpleMessageTls, testReceiveSimpleMessageTlsScramSha, testSendSimpleMessageTlsScramSha, testReceiveSimpleMessageTls, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testConfigurationFileIsCreated, testPauseReconciliationInKafkaRebalanceAndTopic] to and randomly select one to start execution
2022-03-28 13:55:14 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:93] [operators.user.UserST] - Removing parallel test: testCreatingUsersWithSecretPrefix
2022-03-28 13:55:14 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:97] [operators.user.UserST] - Parallel test count: 6
2022-03-28 13:55:14 [ForkJoinPool-1-worker-13] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.user.UserST.testCreatingUsersWithSecretPrefix-FINISHED
2022-03-28 13:55:14 [ForkJoinPool-1-worker-13] INFO  [TestSeparator:30] ############################################################################
2022-03-28 13:55:14 [ForkJoinPool-1-worker-13] INFO  [TestSeparator:23] ############################################################################
2022-03-28 13:55:14 [ForkJoinPool-1-worker-13] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.user.UserST.testScramUserWithQuotas-STARTED
2022-03-28 13:55:14 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:658] ============================================================================
2022-03-28 13:55:14 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:659] [operators.user.UserST - Before Each] - Setup test case environment
2022-03-28 13:55:14 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:77] [operators.user.UserST] - Adding parallel test: testScramUserWithQuotas
2022-03-28 13:55:14 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:81] [operators.user.UserST] - Parallel test count: 7
2022-03-28 13:55:14 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:205] [testScramUserWithQuotas] moved to the WaitZone, because current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 13:55:14 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testScramUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 13:55:14 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (68404ms till timeout)
2022-03-28 13:55:14 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1214298ms till timeout)
2022-03-28 13:55:14 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1237840ms till timeout)
2022-03-28 13:55:14 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1210021ms till timeout)
2022-03-28 13:55:15 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (79387ms till timeout)
2022-03-28 13:55:15 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (67299ms till timeout)
2022-03-28 13:55:15 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1213199ms till timeout)
2022-03-28 13:55:15 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1236742ms till timeout)
2022-03-28 13:55:15 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-3244df0d will have desired state: Ready not ready, will try again in 1000 ms (1208923ms till timeout)
2022-03-28 13:55:16 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (78285ms till timeout)
2022-03-28 13:55:16 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (66194ms till timeout)
2022-03-28 13:55:16 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1212099ms till timeout)
2022-03-28 13:55:16 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1235644ms till timeout)
2022-03-28 13:55:16 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-8af61d52-cruise-control-f7786667b-ltx5c=04ae3803-1af4-46cd-ae48-030c1ceece94}
2022-03-28 13:55:17 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:444] Kafka: my-cluster-3244df0d is in desired state: Ready
2022-03-28 13:55:17 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-8af61d52-cruise-control-789676d5cc-4dxnv=5dbdbd50-3498-455b-beeb-ccd3fd6c68cd, my-cluster-8af61d52-cruise-control-f7786667b-ltx5c=04ae3803-1af4-46cd-ae48-030c1ceece94}
2022-03-28 13:55:17 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-8af61d52-cruise-control-f7786667b-ltx5c=04ae3803-1af4-46cd-ae48-030c1ceece94}
2022-03-28 13:55:17 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Deployment my-cluster-8af61d52-cruise-control rolling update in namespace:namespace-2 not ready, will try again in 5000 ms (589213ms till timeout)
2022-03-28 13:55:17 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (77099ms till timeout)
2022-03-28 13:55:17 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-3 exec my-cluster-3244df0d-cruise-control-84454fbbc8-bjzf9 -- /bin/bash -c cat /tmp/cruisecontrol.properties
2022-03-28 13:55:17 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (65091ms till timeout)
2022-03-28 13:55:17 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1211001ms till timeout)
2022-03-28 13:55:17 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1234547ms till timeout)
2022-03-28 13:55:18 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (75999ms till timeout)
2022-03-28 13:55:18 [ForkJoinPool-1-worker-5] INFO  [Exec:417] Command: oc --namespace namespace-3 exec my-cluster-3244df0d-cruise-control-84454fbbc8-bjzf9 -- /bin/bash -c cat /tmp/cruisecontrol.properties
2022-03-28 13:55:18 [ForkJoinPool-1-worker-5] INFO  [Exec:417] Return code: 0
2022-03-28 13:55:18 [ForkJoinPool-1-worker-5] DEBUG [AbstractST:674] ============================================================================
2022-03-28 13:55:18 [ForkJoinPool-1-worker-5] DEBUG [AbstractST:675] [cruisecontrol.CruiseControlConfigurationST - After Each] - Clean up after test
2022-03-28 13:55:18 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:344] ############################################################################
2022-03-28 13:55:18 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:348] Delete all resources for testConfigurationFileIsCreated
2022-03-28 13:55:18 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:241] Delete of Kafka my-cluster-3244df0d in namespace namespace-3
2022-03-28 13:55:18 [ForkJoinPool-1-worker-5] INFO  [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-3, for cruise control Kafka cluster my-cluster-3244df0d
2022-03-28 13:55:19 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (63835ms till timeout)
2022-03-28 13:55:19 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1209874ms till timeout)
2022-03-28 13:55:19 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1233427ms till timeout)
2022-03-28 13:55:19 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:230] testScramUserWithQuotas test now can proceed its execution
2022-03-28 13:55:19 [ForkJoinPool-1-worker-13] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 13:55:19 [ForkJoinPool-1-worker-13] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testScramUserWithQuotas=my-cluster-6f653060, testReceiveSimpleMessageTlsScramSha=my-cluster-af298d01, testConfigurationFileIsCreated=my-cluster-3244df0d, testThrottlingQuotasCreateAlterPartitions=my-cluster-8f6c28cb, testSendSimpleMessageTls=my-cluster-fbc845dd, testCreatingUsersWithSecretPrefix=my-cluster-0c65d2dd, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-0444229e, testDeployAndUnDeployCruiseControl=my-cluster-844045c3, testThrottlingQuotasCreateTopic=my-cluster-ac2cadd6, testCapacityFile=my-cluster-0531d5ce, testReceiveSimpleMessageTls=my-cluster-ace1fa24, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-8af61d52, testSendSimpleMessageTlsScramSha=my-cluster-e353214b}
2022-03-28 13:55:19 [ForkJoinPool-1-worker-13] TRACE [AbstractST:607] USERS_NAME_MAP: {testScramUserWithQuotas=my-user-27563760-1632863338, testReceiveSimpleMessageTlsScramSha=my-user-1597210213-707359243, testConfigurationFileIsCreated=my-user-1799841978-1468805631, testThrottlingQuotasCreateAlterPartitions=my-user-1557959119-1722636050, testSendSimpleMessageTls=my-user-1754673553-2077518027, testCreatingUsersWithSecretPrefix=my-user-144001639-226807213, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-1010649803-214635564, testDeployAndUnDeployCruiseControl=my-user-2117120687-604608920, testThrottlingQuotasCreateTopic=my-user-46611685-1079680568, testCapacityFile=my-user-155969448-1270555822, testReceiveSimpleMessageTls=my-user-1418772225-1024845009, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-1807142870-188660517, testSendSimpleMessageTlsScramSha=my-user-182357028-1132873010}
2022-03-28 13:55:19 [ForkJoinPool-1-worker-13] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testScramUserWithQuotas=my-topic-1345794255-226872460, testReceiveSimpleMessageTlsScramSha=my-topic-649423016-1623551209, testConfigurationFileIsCreated=my-topic-1435014906-681104826, testThrottlingQuotasCreateAlterPartitions=my-topic-1449977352-344480794, testSendSimpleMessageTls=my-topic-115477502-2025274778, testCreatingUsersWithSecretPrefix=my-topic-130462979-2061578809, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-889283546-691691584, testDeployAndUnDeployCruiseControl=my-topic-1101175281-2107378523, testThrottlingQuotasCreateTopic=my-topic-114922478-1506136333, testCapacityFile=my-topic-296605537-1008743104, testReceiveSimpleMessageTls=my-topic-1131768783-120051821, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-1367124108-1748498395, testSendSimpleMessageTlsScramSha=my-topic-652506937-1802310709}
2022-03-28 13:55:19 [ForkJoinPool-1-worker-13] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testScramUserWithQuotas=my-cluster-6f653060-kafka-clients, testReceiveSimpleMessageTlsScramSha=my-cluster-af298d01-kafka-clients, testConfigurationFileIsCreated=my-cluster-3244df0d-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-8f6c28cb-kafka-clients, testSendSimpleMessageTls=my-cluster-fbc845dd-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-0c65d2dd-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-0444229e-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-844045c3-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-ac2cadd6-kafka-clients, testCapacityFile=my-cluster-0531d5ce-kafka-clients, testReceiveSimpleMessageTls=my-cluster-ace1fa24-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-8af61d52-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-e353214b-kafka-clients}
2022-03-28 13:55:19 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:155] Create/Update KafkaUser scramed-arnost in namespace user-st
2022-03-28 13:55:19 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:scramed-arnost
2022-03-28 13:55:19 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:433] Wait for KafkaUser: scramed-arnost will have desired state: Ready
2022-03-28 13:55:19 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for KafkaUser: scramed-arnost will have desired state: Ready
2022-03-28 13:55:19 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (74898ms till timeout)
2022-03-28 13:55:19 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-3244df0d
2022-03-28 13:55:19 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] KafkaUser: scramed-arnost will have desired state: Ready not ready, will try again in 1000 ms (179806ms till timeout)
2022-03-28 13:55:19 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:369] ############################################################################
2022-03-28 13:55:19 [ForkJoinPool-1-worker-5] INFO  [TestSuiteNamespaceManager:200] Deleting namespace:namespace-3 for test case:testConfigurationFileIsCreated
2022-03-28 13:55:19 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Namespace namespace-3 removal
2022-03-28 13:55:19 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-5 get Namespace namespace-3 -o yaml
2022-03-28 13:55:20 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1208728ms till timeout)
2022-03-28 13:55:20 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (62662ms till timeout)
2022-03-28 13:55:20 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1232292ms till timeout)
2022-03-28 13:55:20 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-5 get Namespace namespace-3 -o yaml
2022-03-28 13:55:20 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:20 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (479518ms till timeout)
2022-03-28 13:55:20 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (73792ms till timeout)
2022-03-28 13:55:20 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:444] KafkaUser: scramed-arnost is in desired state: Ready
2022-03-28 13:55:20 [ForkJoinPool-1-worker-13] DEBUG [UserST:274] Command for kafka-configs.sh bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user scramed-arnost
2022-03-28 13:55:20 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace user-st exec user-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user scramed-arnost
2022-03-28 13:55:21 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1207625ms till timeout)
2022-03-28 13:55:21 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1231169ms till timeout)
2022-03-28 13:55:21 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-5 get Namespace namespace-3 -o yaml
2022-03-28 13:55:21 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (61345ms till timeout)
2022-03-28 13:55:21 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (72691ms till timeout)
2022-03-28 13:55:21 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-5 get Namespace namespace-3 -o yaml
2022-03-28 13:55:21 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:21 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (478027ms till timeout)
2022-03-28 13:55:22 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-8af61d52-cruise-control-f7786667b-ltx5c=04ae3803-1af4-46cd-ae48-030c1ceece94}
2022-03-28 13:55:22 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1206506ms till timeout)
2022-03-28 13:55:22 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1230057ms till timeout)
2022-03-28 13:55:22 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-8af61d52-cruise-control-789676d5cc-4dxnv=5dbdbd50-3498-455b-beeb-ccd3fd6c68cd, my-cluster-8af61d52-cruise-control-f7786667b-ltx5c=04ae3803-1af4-46cd-ae48-030c1ceece94}
2022-03-28 13:55:22 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-8af61d52-cruise-control-f7786667b-ltx5c=04ae3803-1af4-46cd-ae48-030c1ceece94}
2022-03-28 13:55:22 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Deployment my-cluster-8af61d52-cruise-control rolling update in namespace:namespace-2 not ready, will try again in 5000 ms (583917ms till timeout)
2022-03-28 13:55:22 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (60241ms till timeout)
2022-03-28 13:55:22 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (71589ms till timeout)
2022-03-28 13:55:22 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-5 get Namespace namespace-3 -o yaml
2022-03-28 13:55:23 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-5 get Namespace namespace-3 -o yaml
2022-03-28 13:55:23 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:23 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (476559ms till timeout)
2022-03-28 13:55:23 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1205408ms till timeout)
2022-03-28 13:55:23 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1228958ms till timeout)
2022-03-28 13:55:23 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (59047ms till timeout)
2022-03-28 13:55:23 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (70482ms till timeout)
2022-03-28 13:55:24 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-5 get Namespace namespace-3 -o yaml
2022-03-28 13:55:24 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (1204308ms till timeout)
2022-03-28 13:55:24 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-844045c3 will have desired state: Ready not ready, will try again in 1000 ms (1227859ms till timeout)
2022-03-28 13:55:24 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-5 get Namespace namespace-3 -o yaml
2022-03-28 13:55:24 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:24 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (475080ms till timeout)
2022-03-28 13:55:24 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (57852ms till timeout)
2022-03-28 13:55:25 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (69314ms till timeout)
2022-03-28 13:55:25 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:444] Kafka: my-cluster-0444229e is in desired state: Ready
2022-03-28 13:55:25 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:155] Create/Update KafkaTopic my-topic-889283546-691691584 in namespace namespace-5
2022-03-28 13:55:25 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:164] Using Namespace: namespace-4
2022-03-28 13:55:25 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:444] Kafka: my-cluster-844045c3 is in desired state: Ready
2022-03-28 13:55:25 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-5 get Namespace namespace-3 -o yaml
2022-03-28 13:55:25 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-889283546-691691584
2022-03-28 13:55:26 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:433] Wait for KafkaTopic: my-topic-889283546-691691584 will have desired state: Ready
2022-03-28 13:55:26 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for KafkaTopic: my-topic-889283546-691691584 will have desired state: Ready
2022-03-28 13:55:26 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (56584ms till timeout)
2022-03-28 13:55:26 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:444] KafkaTopic: my-topic-889283546-691691584 is in desired state: Ready
2022-03-28 13:55:26 [ForkJoinPool-1-worker-15] INFO  [ReconciliationST:147] Adding pause annotation into KafkaTopic resource and changing replication factor
2022-03-28 13:55:26 [ForkJoinPool-1-worker-7] INFO  [CruiseControlConfigurationST:111] Removing Cruise Control to the classic Kafka.
2022-03-28 13:55:26 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (68136ms till timeout)
2022-03-28 13:55:26 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-5 get Namespace namespace-3 -o yaml
2022-03-28 13:55:26 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:26 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (473619ms till timeout)
2022-03-28 13:55:26 [ForkJoinPool-1-worker-13] INFO  [Exec:417] Command: oc --namespace user-st exec user-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user scramed-arnost
2022-03-28 13:55:26 [ForkJoinPool-1-worker-13] INFO  [Exec:417] Return code: 0
2022-03-28 13:55:26 [ForkJoinPool-1-worker-7] INFO  [RollingUpdateUtils:73] Waiting for component with name: my-cluster-844045c3-kafka rolling update
2022-03-28 13:55:26 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:74] Waiting for rolling update of component matching LabelSelector: LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-844045c3-kafka, strimzi.io/cluster=my-cluster-844045c3, strimzi.io/kind=Kafka}, additionalProperties={})
2022-03-28 13:55:26 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for component with name my-cluster-844045c3-kafka rolling update
2022-03-28 13:55:26 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-844045c3-kafka-0=7a441eef-cc28-4155-958e-c8c61cc69438, my-cluster-844045c3-kafka-1=8a676e65-d331-46fd-b04b-b47fb87d1932, my-cluster-844045c3-kafka-2=009a7082-9aa0-4208-b610-501b2bffd9cd}
2022-03-28 13:55:26 [ForkJoinPool-1-worker-13] INFO  [KafkaUserUtils:62] Waiting for KafkaUser deletion scramed-arnost
2022-03-28 13:55:26 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for KafkaUser deletion scramed-arnost
2022-03-28 13:55:26 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:433] Wait for KafkaTopic: my-topic-889283546-691691584 will have desired state: ReconciliationPaused
2022-03-28 13:55:26 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for KafkaTopic: my-topic-889283546-691691584 will have desired state: ReconciliationPaused
2022-03-28 13:55:26 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-844045c3-kafka-0=7a441eef-cc28-4155-958e-c8c61cc69438, my-cluster-844045c3-kafka-1=8a676e65-d331-46fd-b04b-b47fb87d1932, my-cluster-844045c3-kafka-2=009a7082-9aa0-4208-b610-501b2bffd9cd}
2022-03-28 13:55:26 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-844045c3-kafka-0=7a441eef-cc28-4155-958e-c8c61cc69438, my-cluster-844045c3-kafka-1=8a676e65-d331-46fd-b04b-b47fb87d1932, my-cluster-844045c3-kafka-2=009a7082-9aa0-4208-b610-501b2bffd9cd}
2022-03-28 13:55:26 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:50] At least my-cluster-844045c3-kafka-1 hasn't rolled
2022-03-28 13:55:26 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] component with name my-cluster-844045c3-kafka rolling update not ready, will try again in 5000 ms (1799832ms till timeout)
2022-03-28 13:55:26 [ForkJoinPool-1-worker-13] INFO  [KafkaUserUtils:75] KafkaUser scramed-arnost deleted
2022-03-28 13:55:26 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic: my-topic-889283546-691691584 will have desired state: ReconciliationPaused not ready, will try again in 1000 ms (179817ms till timeout)
2022-03-28 13:55:26 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for all KafkaUser scramed-arnost attributes will be cleaned
2022-03-28 13:55:26 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace user-st exec user-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user scramed-arnost
2022-03-28 13:55:27 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-5 get Namespace namespace-3 -o yaml
2022-03-28 13:55:27 [ForkJoinPool-1-worker-9] INFO  [PodUtils:189] Message org.apache.kafka.common.errors.ThrottlingQuotaExceededException: The throttling quota has been exceeded. found in create-admin-my-cluster-ac2cadd6-kafka-clients-gk42l log
2022-03-28 13:55:27 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (66939ms till timeout)
2022-03-28 13:55:27 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-8af61d52-cruise-control-f7786667b-ltx5c=04ae3803-1af4-46cd-ae48-030c1ceece94}
2022-03-28 13:55:27 [ForkJoinPool-1-worker-9] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment create-admin-my-cluster-ac2cadd6-kafka-clients deletion
2022-03-28 13:55:27 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for ReplicaSet create-admin-my-cluster-ac2cadd6-kafka-clients to be deleted
2022-03-28 13:55:27 [ForkJoinPool-1-worker-9] DEBUG [JobUtils:40] Job create-admin-my-cluster-ac2cadd6-kafka-clients was deleted
2022-03-28 13:55:27 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-5 get Namespace namespace-3 -o yaml
2022-03-28 13:55:27 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:27 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (472165ms till timeout)
2022-03-28 13:55:27 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:444] KafkaTopic: my-topic-889283546-691691584 is in desired state: ReconciliationPaused
2022-03-28 13:55:27 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-0444229e-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-889283546-691691584 --describe --bootstrap-server my-cluster-0444229e-kafka-bootstrap:9092
2022-03-28 13:55:27 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-8af61d52-cruise-control-789676d5cc-4dxnv=5dbdbd50-3498-455b-beeb-ccd3fd6c68cd, my-cluster-8af61d52-cruise-control-f7786667b-ltx5c=04ae3803-1af4-46cd-ae48-030c1ceece94}
2022-03-28 13:55:27 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-8af61d52-cruise-control-f7786667b-ltx5c=04ae3803-1af4-46cd-ae48-030c1ceece94}
2022-03-28 13:55:27 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Deployment my-cluster-8af61d52-cruise-control rolling update in namespace:namespace-2 not ready, will try again in 5000 ms (578572ms till timeout)
2022-03-28 13:55:28 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-0
2022-03-28 13:55:28 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (65836ms till timeout)
2022-03-28 13:55:28 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-5 get Namespace namespace-3 -o yaml
2022-03-28 13:55:29 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-0
2022-03-28 13:55:29 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:29 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-1
2022-03-28 13:55:29 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-5 get Namespace namespace-3 -o yaml
2022-03-28 13:55:29 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:29 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (470539ms till timeout)
2022-03-28 13:55:29 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (64640ms till timeout)
2022-03-28 13:55:29 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-1
2022-03-28 13:55:29 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:29 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-10
2022-03-28 13:55:30 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-5 get Namespace namespace-3 -o yaml
2022-03-28 13:55:30 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-10
2022-03-28 13:55:30 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:30 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-100
2022-03-28 13:55:30 [ForkJoinPool-1-worker-13] INFO  [Exec:417] Command: oc --namespace user-st exec user-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user scramed-arnost
2022-03-28 13:55:30 [ForkJoinPool-1-worker-13] INFO  [Exec:417] Return code: 0
2022-03-28 13:55:30 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:674] ============================================================================
2022-03-28 13:55:30 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:675] [operators.user.UserST - After Each] - Clean up after test
2022-03-28 13:55:30 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:344] ############################################################################
2022-03-28 13:55:30 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:348] Delete all resources for testScramUserWithQuotas
2022-03-28 13:55:30 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of KafkaUser scramed-arnost in namespace user-st
2022-03-28 13:55:30 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:scramed-arnost
2022-03-28 13:55:30 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-5 get Namespace namespace-3 -o yaml
2022-03-28 13:55:30 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:30 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (469077ms till timeout)
2022-03-28 13:55:30 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (63447ms till timeout)
2022-03-28 13:55:30 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:369] ############################################################################
2022-03-28 13:55:30 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:267] testScramUserWithQuotas - Notifies waiting test cases:[testCapacityFile, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testThrottlingQuotasCreateAlterPartitions, testSendSimpleMessageTls, testReceiveSimpleMessageTlsScramSha, testSendSimpleMessageTlsScramSha, testReceiveSimpleMessageTls, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testConfigurationFileIsCreated, testPauseReconciliationInKafkaRebalanceAndTopic] to and randomly select one to start execution
2022-03-28 13:55:30 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:93] [operators.user.UserST] - Removing parallel test: testScramUserWithQuotas
2022-03-28 13:55:30 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:97] [operators.user.UserST] - Parallel test count: 6
2022-03-28 13:55:30 [ForkJoinPool-1-worker-13] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.user.UserST.testScramUserWithQuotas-FINISHED
2022-03-28 13:55:30 [ForkJoinPool-1-worker-13] INFO  [TestSeparator:30] ############################################################################
2022-03-28 13:55:30 [ForkJoinPool-1-worker-13] INFO  [TestSeparator:23] ############################################################################
2022-03-28 13:55:30 [ForkJoinPool-1-worker-13] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.user.UserST.testTlsUserWithQuotas-STARTED
2022-03-28 13:55:30 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:658] ============================================================================
2022-03-28 13:55:30 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:659] [operators.user.UserST - Before Each] - Setup test case environment
2022-03-28 13:55:30 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:77] [operators.user.UserST] - Adding parallel test: testTlsUserWithQuotas
2022-03-28 13:55:30 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:81] [operators.user.UserST] - Parallel test count: 7
2022-03-28 13:55:30 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:205] [testTlsUserWithQuotas] moved to the WaitZone, because current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 13:55:30 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testTlsUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 13:55:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-100
2022-03-28 13:55:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:31 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-102
2022-03-28 13:55:31 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-844045c3-kafka-0=7a441eef-cc28-4155-958e-c8c61cc69438, my-cluster-844045c3-kafka-1=8a676e65-d331-46fd-b04b-b47fb87d1932, my-cluster-844045c3-kafka-2=009a7082-9aa0-4208-b610-501b2bffd9cd}
2022-03-28 13:55:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-102
2022-03-28 13:55:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:31 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-104
2022-03-28 13:55:31 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-844045c3-kafka-0=7a441eef-cc28-4155-958e-c8c61cc69438, my-cluster-844045c3-kafka-1=8a676e65-d331-46fd-b04b-b47fb87d1932, my-cluster-844045c3-kafka-2=009a7082-9aa0-4208-b610-501b2bffd9cd}
2022-03-28 13:55:31 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-844045c3-kafka-0=7a441eef-cc28-4155-958e-c8c61cc69438, my-cluster-844045c3-kafka-1=8a676e65-d331-46fd-b04b-b47fb87d1932, my-cluster-844045c3-kafka-2=009a7082-9aa0-4208-b610-501b2bffd9cd}
2022-03-28 13:55:31 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:50] At least my-cluster-844045c3-kafka-1 hasn't rolled
2022-03-28 13:55:31 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] component with name my-cluster-844045c3-kafka rolling update not ready, will try again in 5000 ms (1794637ms till timeout)
2022-03-28 13:55:31 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-5 get Namespace namespace-3 -o yaml
2022-03-28 13:55:32 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (62346ms till timeout)
2022-03-28 13:55:32 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-104
2022-03-28 13:55:32 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:32 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-108
2022-03-28 13:55:32 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-5 get Namespace namespace-3 -o yaml
2022-03-28 13:55:32 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:32 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (467629ms till timeout)
2022-03-28 13:55:32 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-108
2022-03-28 13:55:32 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:32 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-11
2022-03-28 13:55:32 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-8af61d52-cruise-control-f7786667b-ltx5c=04ae3803-1af4-46cd-ae48-030c1ceece94}
2022-03-28 13:55:33 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-8af61d52-cruise-control-789676d5cc-4dxnv=5dbdbd50-3498-455b-beeb-ccd3fd6c68cd, my-cluster-8af61d52-cruise-control-f7786667b-ltx5c=04ae3803-1af4-46cd-ae48-030c1ceece94}
2022-03-28 13:55:33 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-8af61d52-cruise-control-f7786667b-ltx5c=04ae3803-1af4-46cd-ae48-030c1ceece94}
2022-03-28 13:55:33 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Deployment my-cluster-8af61d52-cruise-control rolling update in namespace:namespace-2 not ready, will try again in 5000 ms (573188ms till timeout)
2022-03-28 13:55:33 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (61170ms till timeout)
2022-03-28 13:55:33 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-5 get Namespace namespace-3 -o yaml
2022-03-28 13:55:33 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-11
2022-03-28 13:55:33 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:33 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-112
2022-03-28 13:55:33 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-4 exec my-cluster-0444229e-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-889283546-691691584 --describe --bootstrap-server my-cluster-0444229e-kafka-bootstrap:9092
2022-03-28 13:55:33 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 13:55:33 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for KafkaTopic's spec will be stable
2022-03-28 13:55:33 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-0444229e-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-889283546-691691584 --describe --bootstrap-server my-cluster-0444229e-kafka-bootstrap:9092
2022-03-28 13:55:33 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-5 get Namespace namespace-3 -o yaml
2022-03-28 13:55:33 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:33 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (466162ms till timeout)
2022-03-28 13:55:33 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-112
2022-03-28 13:55:33 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:33 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-113
2022-03-28 13:55:34 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-113
2022-03-28 13:55:34 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:34 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-117
2022-03-28 13:55:34 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (59976ms till timeout)
2022-03-28 13:55:34 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-5 get Namespace namespace-3 -o yaml
2022-03-28 13:55:34 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-117
2022-03-28 13:55:34 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:34 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-119
2022-03-28 13:55:35 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-5 get Namespace namespace-3 -o yaml
2022-03-28 13:55:35 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:35 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (464670ms till timeout)
2022-03-28 13:55:35 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-119
2022-03-28 13:55:35 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:35 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-12
2022-03-28 13:55:35 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (58784ms till timeout)
2022-03-28 13:55:35 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:230] testTlsUserWithQuotas test now can proceed its execution
2022-03-28 13:55:35 [ForkJoinPool-1-worker-13] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 13:55:35 [ForkJoinPool-1-worker-13] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testScramUserWithQuotas=my-cluster-6f653060, testReceiveSimpleMessageTlsScramSha=my-cluster-af298d01, testConfigurationFileIsCreated=my-cluster-3244df0d, testThrottlingQuotasCreateAlterPartitions=my-cluster-8f6c28cb, testSendSimpleMessageTls=my-cluster-fbc845dd, testCreatingUsersWithSecretPrefix=my-cluster-0c65d2dd, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-0444229e, testDeployAndUnDeployCruiseControl=my-cluster-844045c3, testTlsUserWithQuotas=my-cluster-9832c9ae, testThrottlingQuotasCreateTopic=my-cluster-ac2cadd6, testCapacityFile=my-cluster-0531d5ce, testReceiveSimpleMessageTls=my-cluster-ace1fa24, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-8af61d52, testSendSimpleMessageTlsScramSha=my-cluster-e353214b}
2022-03-28 13:55:35 [ForkJoinPool-1-worker-13] TRACE [AbstractST:607] USERS_NAME_MAP: {testScramUserWithQuotas=my-user-27563760-1632863338, testReceiveSimpleMessageTlsScramSha=my-user-1597210213-707359243, testConfigurationFileIsCreated=my-user-1799841978-1468805631, testThrottlingQuotasCreateAlterPartitions=my-user-1557959119-1722636050, testSendSimpleMessageTls=my-user-1754673553-2077518027, testCreatingUsersWithSecretPrefix=my-user-144001639-226807213, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-1010649803-214635564, testDeployAndUnDeployCruiseControl=my-user-2117120687-604608920, testTlsUserWithQuotas=my-user-1602931187-443256751, testThrottlingQuotasCreateTopic=my-user-46611685-1079680568, testCapacityFile=my-user-155969448-1270555822, testReceiveSimpleMessageTls=my-user-1418772225-1024845009, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-1807142870-188660517, testSendSimpleMessageTlsScramSha=my-user-182357028-1132873010}
2022-03-28 13:55:35 [ForkJoinPool-1-worker-13] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testScramUserWithQuotas=my-topic-1345794255-226872460, testReceiveSimpleMessageTlsScramSha=my-topic-649423016-1623551209, testConfigurationFileIsCreated=my-topic-1435014906-681104826, testThrottlingQuotasCreateAlterPartitions=my-topic-1449977352-344480794, testSendSimpleMessageTls=my-topic-115477502-2025274778, testCreatingUsersWithSecretPrefix=my-topic-130462979-2061578809, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-889283546-691691584, testDeployAndUnDeployCruiseControl=my-topic-1101175281-2107378523, testTlsUserWithQuotas=my-topic-985048738-437665866, testThrottlingQuotasCreateTopic=my-topic-114922478-1506136333, testCapacityFile=my-topic-296605537-1008743104, testReceiveSimpleMessageTls=my-topic-1131768783-120051821, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-1367124108-1748498395, testSendSimpleMessageTlsScramSha=my-topic-652506937-1802310709}
2022-03-28 13:55:35 [ForkJoinPool-1-worker-13] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testScramUserWithQuotas=my-cluster-6f653060-kafka-clients, testReceiveSimpleMessageTlsScramSha=my-cluster-af298d01-kafka-clients, testConfigurationFileIsCreated=my-cluster-3244df0d-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-8f6c28cb-kafka-clients, testSendSimpleMessageTls=my-cluster-fbc845dd-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-0c65d2dd-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-0444229e-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-844045c3-kafka-clients, testTlsUserWithQuotas=my-cluster-9832c9ae-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-ac2cadd6-kafka-clients, testCapacityFile=my-cluster-0531d5ce-kafka-clients, testReceiveSimpleMessageTls=my-cluster-ace1fa24-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-8af61d52-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-e353214b-kafka-clients}
2022-03-28 13:55:35 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:155] Create/Update KafkaUser encrypted-arnost in namespace user-st
2022-03-28 13:55:36 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-12
2022-03-28 13:55:36 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:36 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-128
2022-03-28 13:55:36 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:encrypted-arnost
2022-03-28 13:55:36 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:433] Wait for KafkaUser: encrypted-arnost will have desired state: Ready
2022-03-28 13:55:36 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for KafkaUser: encrypted-arnost will have desired state: Ready
2022-03-28 13:55:36 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-5 get Namespace namespace-3 -o yaml
2022-03-28 13:55:36 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] KafkaUser: encrypted-arnost will have desired state: Ready not ready, will try again in 1000 ms (179903ms till timeout)
2022-03-28 13:55:36 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-128
2022-03-28 13:55:36 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:36 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-13
2022-03-28 13:55:36 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-5 get Namespace namespace-3 -o yaml
2022-03-28 13:55:36 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:36 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (463182ms till timeout)
2022-03-28 13:55:36 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (57592ms till timeout)
2022-03-28 13:55:36 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-844045c3-kafka-0=7a441eef-cc28-4155-958e-c8c61cc69438, my-cluster-844045c3-kafka-1=8a676e65-d331-46fd-b04b-b47fb87d1932, my-cluster-844045c3-kafka-2=009a7082-9aa0-4208-b610-501b2bffd9cd}
2022-03-28 13:55:37 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-844045c3-kafka-0=e24c016e-3d31-4327-92e5-6a4d2019dc61, my-cluster-844045c3-kafka-1=8a676e65-d331-46fd-b04b-b47fb87d1932, my-cluster-844045c3-kafka-2=009a7082-9aa0-4208-b610-501b2bffd9cd}
2022-03-28 13:55:37 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-844045c3-kafka-0=e24c016e-3d31-4327-92e5-6a4d2019dc61, my-cluster-844045c3-kafka-1=8a676e65-d331-46fd-b04b-b47fb87d1932, my-cluster-844045c3-kafka-2=009a7082-9aa0-4208-b610-501b2bffd9cd}
2022-03-28 13:55:37 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:50] At least my-cluster-844045c3-kafka-1 hasn't rolled
2022-03-28 13:55:37 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] component with name my-cluster-844045c3-kafka rolling update not ready, will try again in 5000 ms (1789443ms till timeout)
2022-03-28 13:55:37 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-13
2022-03-28 13:55:37 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:37 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-133
2022-03-28 13:55:37 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:444] KafkaUser: encrypted-arnost is in desired state: Ready
2022-03-28 13:55:37 [ForkJoinPool-1-worker-13] DEBUG [UserST:274] Command for kafka-configs.sh bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user CN=encrypted-arnost
2022-03-28 13:55:37 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace user-st exec user-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user CN=encrypted-arnost
2022-03-28 13:55:37 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-133
2022-03-28 13:55:37 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:37 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-135
2022-03-28 13:55:37 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-5 get Namespace namespace-3 -o yaml
2022-03-28 13:55:37 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-4 exec my-cluster-0444229e-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-889283546-691691584 --describe --bootstrap-server my-cluster-0444229e-kafka-bootstrap:9092
2022-03-28 13:55:37 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 13:55:37 [ForkJoinPool-1-worker-15] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 19 polls
2022-03-28 13:55:37 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (175754ms till timeout)
2022-03-28 13:55:37 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (56399ms till timeout)
2022-03-28 13:55:38 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-8af61d52-cruise-control-f7786667b-ltx5c=04ae3803-1af4-46cd-ae48-030c1ceece94}
2022-03-28 13:55:38 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-5 get Namespace namespace-3 -o yaml
2022-03-28 13:55:38 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:38 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (461707ms till timeout)
2022-03-28 13:55:38 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-135
2022-03-28 13:55:38 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:38 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-14
2022-03-28 13:55:38 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-8af61d52-cruise-control-789676d5cc-4dxnv=5dbdbd50-3498-455b-beeb-ccd3fd6c68cd}
2022-03-28 13:55:38 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:119] All pods seem to have rolled
2022-03-28 13:55:38 [ForkJoinPool-1-worker-3] INFO  [DeploymentUtils:161] Wait for Deployment: my-cluster-8af61d52-cruise-control will be ready
2022-03-28 13:55:38 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Wait for Deployment: my-cluster-8af61d52-cruise-control will be ready
2022-03-28 13:55:38 [ForkJoinPool-1-worker-3] INFO  [DeploymentUtils:168] Deployment: my-cluster-8af61d52-cruise-control is ready
2022-03-28 13:55:38 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-8af61d52, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-8af61d52-cruise-control}, additionalProperties={})to be ready
2022-03-28 13:55:38 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-8af61d52-cruise-control-789676d5cc-4dxnv not ready: cruise-control)
2022-03-28 13:55:38 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-8af61d52-cruise-control-789676d5cc-4dxnv not ready: tls-sidecar)
2022-03-28 13:55:38 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:106] Pods my-cluster-8af61d52-cruise-control-789676d5cc-4dxnv are ready
2022-03-28 13:55:38 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-8af61d52, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-8af61d52-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599901ms till timeout)
2022-03-28 13:55:38 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-14
2022-03-28 13:55:38 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:38 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-143
2022-03-28 13:55:38 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-0444229e-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-889283546-691691584 --describe --bootstrap-server my-cluster-0444229e-kafka-bootstrap:9092
2022-03-28 13:55:39 [ForkJoinPool-1-worker-1] INFO  [PodUtils:189] Message org.apache.kafka.common.errors.ThrottlingQuotaExceededException: The throttling quota has been exceeded. found in create-admin-my-cluster-8f6c28cb-kafka-clients-cwgxv log
2022-03-28 13:55:39 [ForkJoinPool-1-worker-1] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment create-admin-my-cluster-8f6c28cb-kafka-clients deletion
2022-03-28 13:55:39 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for ReplicaSet create-admin-my-cluster-8f6c28cb-kafka-clients to be deleted
2022-03-28 13:55:39 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-5 get Namespace namespace-3 -o yaml
2022-03-28 13:55:39 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] ReplicaSet create-admin-my-cluster-8f6c28cb-kafka-clients to be deleted not ready, will try again in 5000 ms (179901ms till timeout)
2022-03-28 13:55:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-143
2022-03-28 13:55:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:39 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-144
2022-03-28 13:55:39 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-5 get Namespace namespace-3 -o yaml
2022-03-28 13:55:39 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:39 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (460262ms till timeout)
2022-03-28 13:55:39 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-8af61d52-cruise-control-789676d5cc-4dxnv not ready: cruise-control)
2022-03-28 13:55:39 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-8af61d52-cruise-control-789676d5cc-4dxnv not ready: tls-sidecar)
2022-03-28 13:55:39 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:106] Pods my-cluster-8af61d52-cruise-control-789676d5cc-4dxnv are ready
2022-03-28 13:55:39 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-8af61d52, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-8af61d52-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598709ms till timeout)
2022-03-28 13:55:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-144
2022-03-28 13:55:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:40 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-146
2022-03-28 13:55:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-146
2022-03-28 13:55:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:40 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-148
2022-03-28 13:55:40 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-5 get Namespace namespace-3 -o yaml
2022-03-28 13:55:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-148
2022-03-28 13:55:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:41 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-15
2022-03-28 13:55:41 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-5 get Namespace namespace-3 -o yaml
2022-03-28 13:55:41 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:41 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (458824ms till timeout)
2022-03-28 13:55:41 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-8af61d52-cruise-control-789676d5cc-4dxnv not ready: cruise-control)
2022-03-28 13:55:41 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-8af61d52-cruise-control-789676d5cc-4dxnv not ready: tls-sidecar)
2022-03-28 13:55:41 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:106] Pods my-cluster-8af61d52-cruise-control-789676d5cc-4dxnv are ready
2022-03-28 13:55:41 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-8af61d52, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-8af61d52-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597518ms till timeout)
2022-03-28 13:55:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-15
2022-03-28 13:55:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:41 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-151
2022-03-28 13:55:42 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-844045c3-kafka-0=7a441eef-cc28-4155-958e-c8c61cc69438, my-cluster-844045c3-kafka-1=8a676e65-d331-46fd-b04b-b47fb87d1932, my-cluster-844045c3-kafka-2=009a7082-9aa0-4208-b610-501b2bffd9cd}
2022-03-28 13:55:42 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-5 get Namespace namespace-3 -o yaml
2022-03-28 13:55:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-151
2022-03-28 13:55:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:42 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-152
2022-03-28 13:55:42 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-844045c3-kafka-0=e24c016e-3d31-4327-92e5-6a4d2019dc61, my-cluster-844045c3-kafka-1=8a676e65-d331-46fd-b04b-b47fb87d1932, my-cluster-844045c3-kafka-2=009a7082-9aa0-4208-b610-501b2bffd9cd}
2022-03-28 13:55:42 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-844045c3-kafka-0=e24c016e-3d31-4327-92e5-6a4d2019dc61, my-cluster-844045c3-kafka-1=8a676e65-d331-46fd-b04b-b47fb87d1932, my-cluster-844045c3-kafka-2=009a7082-9aa0-4208-b610-501b2bffd9cd}
2022-03-28 13:55:42 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:50] At least my-cluster-844045c3-kafka-1 hasn't rolled
2022-03-28 13:55:42 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] component with name my-cluster-844045c3-kafka rolling update not ready, will try again in 5000 ms (1784244ms till timeout)
2022-03-28 13:55:42 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-8af61d52-cruise-control-789676d5cc-4dxnv not ready: cruise-control)
2022-03-28 13:55:42 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-8af61d52-cruise-control-789676d5cc-4dxnv not ready: tls-sidecar)
2022-03-28 13:55:42 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:106] Pods my-cluster-8af61d52-cruise-control-789676d5cc-4dxnv are ready
2022-03-28 13:55:42 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-8af61d52, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-8af61d52-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596389ms till timeout)
2022-03-28 13:55:42 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-5 get Namespace namespace-3 -o yaml
2022-03-28 13:55:42 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:42 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (457367ms till timeout)
2022-03-28 13:55:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-152
2022-03-28 13:55:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:42 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-154
2022-03-28 13:55:43 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-154
2022-03-28 13:55:43 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:43 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-155
2022-03-28 13:55:43 [ForkJoinPool-1-worker-13] INFO  [Exec:417] Command: oc --namespace user-st exec user-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user CN=encrypted-arnost
2022-03-28 13:55:43 [ForkJoinPool-1-worker-13] INFO  [Exec:417] Return code: 0
2022-03-28 13:55:43 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-8af61d52-cruise-control-789676d5cc-4dxnv not ready: cruise-control)
2022-03-28 13:55:43 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-8af61d52-cruise-control-789676d5cc-4dxnv not ready: tls-sidecar)
2022-03-28 13:55:43 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:106] Pods my-cluster-8af61d52-cruise-control-789676d5cc-4dxnv are ready
2022-03-28 13:55:43 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-8af61d52, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-8af61d52-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595194ms till timeout)
2022-03-28 13:55:43 [ForkJoinPool-1-worker-13] INFO  [KafkaUserUtils:62] Waiting for KafkaUser deletion encrypted-arnost
2022-03-28 13:55:43 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for KafkaUser deletion encrypted-arnost
2022-03-28 13:55:43 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-5 get Namespace namespace-3 -o yaml
2022-03-28 13:55:43 [ForkJoinPool-1-worker-13] INFO  [KafkaUserUtils:75] KafkaUser encrypted-arnost deleted
2022-03-28 13:55:43 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for all KafkaUser CN=encrypted-arnost attributes will be cleaned
2022-03-28 13:55:43 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace user-st exec user-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user CN=encrypted-arnost
2022-03-28 13:55:43 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-155
2022-03-28 13:55:43 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:43 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-157
2022-03-28 13:55:44 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-5 get Namespace namespace-3 -o yaml
2022-03-28 13:55:44 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:44 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (455892ms till timeout)
2022-03-28 13:55:44 [ForkJoinPool-1-worker-1] DEBUG [JobUtils:40] Job create-admin-my-cluster-8f6c28cb-kafka-clients was deleted
2022-03-28 13:55:44 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-157
2022-03-28 13:55:44 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:44 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-158
2022-03-28 13:55:44 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-8af61d52-cruise-control-789676d5cc-4dxnv not ready: cruise-control)
2022-03-28 13:55:44 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-8af61d52-cruise-control-789676d5cc-4dxnv not ready: tls-sidecar)
2022-03-28 13:55:44 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:106] Pods my-cluster-8af61d52-cruise-control-789676d5cc-4dxnv are ready
2022-03-28 13:55:44 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-8af61d52, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-8af61d52-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594049ms till timeout)
2022-03-28 13:55:44 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-30
2022-03-28 13:55:44 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-4 exec my-cluster-0444229e-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-889283546-691691584 --describe --bootstrap-server my-cluster-0444229e-kafka-bootstrap:9092
2022-03-28 13:55:44 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 13:55:44 [ForkJoinPool-1-worker-15] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 18 polls
2022-03-28 13:55:44 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (168776ms till timeout)
2022-03-28 13:55:45 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-158
2022-03-28 13:55:45 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:45 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-159
2022-03-28 13:55:45 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-5 get Namespace namespace-3 -o yaml
2022-03-28 13:55:45 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-30
2022-03-28 13:55:45 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:45 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-31
2022-03-28 13:55:45 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-5 get Namespace namespace-3 -o yaml
2022-03-28 13:55:45 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:45 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (454315ms till timeout)
2022-03-28 13:55:45 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-159
2022-03-28 13:55:45 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:45 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-16
2022-03-28 13:55:45 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-8af61d52-cruise-control-789676d5cc-4dxnv not ready: cruise-control)
2022-03-28 13:55:45 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-8af61d52-cruise-control-789676d5cc-4dxnv not ready: tls-sidecar)
2022-03-28 13:55:45 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:106] Pods my-cluster-8af61d52-cruise-control-789676d5cc-4dxnv are ready
2022-03-28 13:55:45 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-8af61d52, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-8af61d52-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592854ms till timeout)
2022-03-28 13:55:45 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-0444229e-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-889283546-691691584 --describe --bootstrap-server my-cluster-0444229e-kafka-bootstrap:9092
2022-03-28 13:55:45 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-31
2022-03-28 13:55:45 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:45 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-32
2022-03-28 13:55:46 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-16
2022-03-28 13:55:46 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:46 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-160
2022-03-28 13:55:46 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-32
2022-03-28 13:55:46 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:46 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:155] Create/Update Job create-admin-my-cluster-8f6c28cb-kafka-clients in namespace throttling-quota-st
2022-03-28 13:55:46 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-5 get Namespace namespace-3 -o yaml
2022-03-28 13:55:46 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:create-admin-my-cluster-8f6c28cb-kafka-clients
2022-03-28 13:55:46 [ForkJoinPool-1-worker-1] INFO  [JobUtils:81] Waiting for job: create-admin-my-cluster-8f6c28cb-kafka-clients will be in active state
2022-03-28 13:55:46 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 13:55:46 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-160
2022-03-28 13:55:46 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:46 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-161
2022-03-28 13:55:47 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-8af61d52-cruise-control-789676d5cc-4dxnv not ready: cruise-control)
2022-03-28 13:55:47 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-8af61d52-cruise-control-789676d5cc-4dxnv not ready: tls-sidecar)
2022-03-28 13:55:47 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:106] Pods my-cluster-8af61d52-cruise-control-789676d5cc-4dxnv are ready
2022-03-28 13:55:47 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-8af61d52, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-8af61d52-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591593ms till timeout)
2022-03-28 13:55:47 [ForkJoinPool-1-worker-1] INFO  [PodUtils:186] Waiting for message will be in the log
2022-03-28 13:55:47 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Waiting for message will be in the log
2022-03-28 13:55:47 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-5 get Namespace namespace-3 -o yaml
2022-03-28 13:55:47 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:47 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (452787ms till timeout)
2022-03-28 13:55:47 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (299900ms till timeout)
2022-03-28 13:55:47 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-844045c3-kafka-0=7a441eef-cc28-4155-958e-c8c61cc69438, my-cluster-844045c3-kafka-1=8a676e65-d331-46fd-b04b-b47fb87d1932, my-cluster-844045c3-kafka-2=009a7082-9aa0-4208-b610-501b2bffd9cd}
2022-03-28 13:55:47 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-844045c3-kafka-0=e24c016e-3d31-4327-92e5-6a4d2019dc61, my-cluster-844045c3-kafka-1=8a676e65-d331-46fd-b04b-b47fb87d1932, my-cluster-844045c3-kafka-2=009a7082-9aa0-4208-b610-501b2bffd9cd}
2022-03-28 13:55:47 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-844045c3-kafka-0=e24c016e-3d31-4327-92e5-6a4d2019dc61, my-cluster-844045c3-kafka-1=8a676e65-d331-46fd-b04b-b47fb87d1932, my-cluster-844045c3-kafka-2=009a7082-9aa0-4208-b610-501b2bffd9cd}
2022-03-28 13:55:47 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:50] At least my-cluster-844045c3-kafka-1 hasn't rolled
2022-03-28 13:55:47 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] component with name my-cluster-844045c3-kafka rolling update not ready, will try again in 5000 ms (1779140ms till timeout)
2022-03-28 13:55:47 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-161
2022-03-28 13:55:47 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:47 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-166
2022-03-28 13:55:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-166
2022-03-28 13:55:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:48 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-17
2022-03-28 13:55:48 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-5 get Namespace namespace-3 -o yaml
2022-03-28 13:55:48 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-8af61d52-cruise-control-789676d5cc-4dxnv not ready: cruise-control)
2022-03-28 13:55:48 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-8af61d52-cruise-control-789676d5cc-4dxnv not ready: tls-sidecar)
2022-03-28 13:55:48 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:106] Pods my-cluster-8af61d52-cruise-control-789676d5cc-4dxnv are ready
2022-03-28 13:55:48 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-8af61d52, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-8af61d52-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590397ms till timeout)
2022-03-28 13:55:48 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (298800ms till timeout)
2022-03-28 13:55:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-17
2022-03-28 13:55:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:48 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-171
2022-03-28 13:55:48 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-5 get Namespace namespace-3 -o yaml
2022-03-28 13:55:48 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:48 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (451313ms till timeout)
2022-03-28 13:55:49 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-171
2022-03-28 13:55:49 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:49 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-172
2022-03-28 13:55:49 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-8af61d52-cruise-control-789676d5cc-4dxnv not ready: cruise-control)
2022-03-28 13:55:49 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-8af61d52-cruise-control-789676d5cc-4dxnv not ready: tls-sidecar)
2022-03-28 13:55:49 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:106] Pods my-cluster-8af61d52-cruise-control-789676d5cc-4dxnv are ready
2022-03-28 13:55:49 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-8af61d52, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-8af61d52-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (589205ms till timeout)
2022-03-28 13:55:49 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (297610ms till timeout)
2022-03-28 13:55:49 [ForkJoinPool-1-worker-13] INFO  [Exec:417] Command: oc --namespace user-st exec user-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user CN=encrypted-arnost
2022-03-28 13:55:49 [ForkJoinPool-1-worker-13] INFO  [Exec:417] Return code: 0
2022-03-28 13:55:49 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:674] ============================================================================
2022-03-28 13:55:49 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:675] [operators.user.UserST - After Each] - Clean up after test
2022-03-28 13:55:49 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:344] ############################################################################
2022-03-28 13:55:49 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:348] Delete all resources for testTlsUserWithQuotas
2022-03-28 13:55:49 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of KafkaUser encrypted-arnost in namespace user-st
2022-03-28 13:55:49 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-5 get Namespace namespace-3 -o yaml
2022-03-28 13:55:49 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:encrypted-arnost
2022-03-28 13:55:49 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-172
2022-03-28 13:55:49 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:49 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-177
2022-03-28 13:55:49 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:369] ############################################################################
2022-03-28 13:55:49 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:267] testTlsUserWithQuotas - Notifies waiting test cases:[testCapacityFile, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testThrottlingQuotasCreateAlterPartitions, testSendSimpleMessageTls, testReceiveSimpleMessageTlsScramSha, testSendSimpleMessageTlsScramSha, testReceiveSimpleMessageTls, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testConfigurationFileIsCreated, testPauseReconciliationInKafkaRebalanceAndTopic] to and randomly select one to start execution
2022-03-28 13:55:49 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:93] [operators.user.UserST] - Removing parallel test: testTlsUserWithQuotas
2022-03-28 13:55:49 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:97] [operators.user.UserST] - Parallel test count: 6
2022-03-28 13:55:49 [ForkJoinPool-1-worker-13] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.user.UserST.testTlsUserWithQuotas-FINISHED
2022-03-28 13:55:49 [ForkJoinPool-1-worker-13] INFO  [TestSeparator:30] ############################################################################
2022-03-28 13:55:49 [ForkJoinPool-1-worker-13] INFO  [TestSeparator:23] ############################################################################
2022-03-28 13:55:49 [ForkJoinPool-1-worker-13] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.user.UserST.testUserWithNameMoreThan64Chars-STARTED
2022-03-28 13:55:49 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:658] ============================================================================
2022-03-28 13:55:49 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:659] [operators.user.UserST - Before Each] - Setup test case environment
2022-03-28 13:55:49 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:77] [operators.user.UserST] - Adding parallel test: testUserWithNameMoreThan64Chars
2022-03-28 13:55:49 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:81] [operators.user.UserST] - Parallel test count: 7
2022-03-28 13:55:49 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:205] [testUserWithNameMoreThan64Chars] moved to the WaitZone, because current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 13:55:49 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testUserWithNameMoreThan64Chars is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 13:55:50 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-5 get Namespace namespace-3 -o yaml
2022-03-28 13:55:50 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:50 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (449859ms till timeout)
2022-03-28 13:55:50 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-177
2022-03-28 13:55:50 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:50 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-178
2022-03-28 13:55:50 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-8af61d52-cruise-control-789676d5cc-4dxnv not ready: cruise-control)
2022-03-28 13:55:50 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-8af61d52-cruise-control-789676d5cc-4dxnv not ready: tls-sidecar)
2022-03-28 13:55:50 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:106] Pods my-cluster-8af61d52-cruise-control-789676d5cc-4dxnv are ready
2022-03-28 13:55:50 [ForkJoinPool-1-worker-3] INFO  [DeploymentUtils:141] Deployment my-cluster-8af61d52-cruise-control rolling update finished
2022-03-28 13:55:50 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (296414ms till timeout)
2022-03-28 13:55:50 [ForkJoinPool-1-worker-3] INFO  [CruiseControlConfigurationST:171] Verifying that Kafka pods did not roll
2022-03-28 13:55:50 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Waiting for stability of rolling update will be not triggered
2022-03-28 13:55:50 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:55:50 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-178
2022-03-28 13:55:50 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:50 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-18
2022-03-28 13:55:50 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:55:50 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:55:50 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-8af61d52-kafka-0 hasn't rolled
2022-03-28 13:55:50 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f} pods didn't roll. Remaining seconds for stability: 50
2022-03-28 13:55:50 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (299899ms till timeout)
2022-03-28 13:55:51 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-5 get Namespace namespace-3 -o yaml
2022-03-28 13:55:51 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-4 exec my-cluster-0444229e-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-889283546-691691584 --describe --bootstrap-server my-cluster-0444229e-kafka-bootstrap:9092
2022-03-28 13:55:51 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 13:55:51 [ForkJoinPool-1-worker-15] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 17 polls
2022-03-28 13:55:51 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (162401ms till timeout)
2022-03-28 13:55:51 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-18
2022-03-28 13:55:51 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:51 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-180
2022-03-28 13:55:51 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-5 get Namespace namespace-3 -o yaml
2022-03-28 13:55:51 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:51 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (448350ms till timeout)
2022-03-28 13:55:51 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (295312ms till timeout)
2022-03-28 13:55:51 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:55:52 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-180
2022-03-28 13:55:52 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:52 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-184
2022-03-28 13:55:52 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:55:52 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:55:52 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-8af61d52-kafka-0 hasn't rolled
2022-03-28 13:55:52 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f} pods didn't roll. Remaining seconds for stability: 49
2022-03-28 13:55:52 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (298695ms till timeout)
2022-03-28 13:55:52 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-0444229e-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-889283546-691691584 --describe --bootstrap-server my-cluster-0444229e-kafka-bootstrap:9092
2022-03-28 13:55:52 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-844045c3-kafka-0=7a441eef-cc28-4155-958e-c8c61cc69438, my-cluster-844045c3-kafka-1=8a676e65-d331-46fd-b04b-b47fb87d1932, my-cluster-844045c3-kafka-2=009a7082-9aa0-4208-b610-501b2bffd9cd}
2022-03-28 13:55:52 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-844045c3-kafka-0=e24c016e-3d31-4327-92e5-6a4d2019dc61, my-cluster-844045c3-kafka-1=8a676e65-d331-46fd-b04b-b47fb87d1932, my-cluster-844045c3-kafka-2=009a7082-9aa0-4208-b610-501b2bffd9cd}
2022-03-28 13:55:52 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-844045c3-kafka-0=e24c016e-3d31-4327-92e5-6a4d2019dc61, my-cluster-844045c3-kafka-1=8a676e65-d331-46fd-b04b-b47fb87d1932, my-cluster-844045c3-kafka-2=009a7082-9aa0-4208-b610-501b2bffd9cd}
2022-03-28 13:55:52 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:50] At least my-cluster-844045c3-kafka-1 hasn't rolled
2022-03-28 13:55:52 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] component with name my-cluster-844045c3-kafka rolling update not ready, will try again in 5000 ms (1774038ms till timeout)
2022-03-28 13:55:52 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-5 get Namespace namespace-3 -o yaml
2022-03-28 13:55:52 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-184
2022-03-28 13:55:52 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:52 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-187
2022-03-28 13:55:52 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (294211ms till timeout)
2022-03-28 13:55:53 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-5 get Namespace namespace-3 -o yaml
2022-03-28 13:55:53 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:53 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (446870ms till timeout)
2022-03-28 13:55:53 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:55:53 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-187
2022-03-28 13:55:53 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:53 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-19
2022-03-28 13:55:53 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:55:53 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:55:53 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-8af61d52-kafka-0 hasn't rolled
2022-03-28 13:55:53 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f} pods didn't roll. Remaining seconds for stability: 48
2022-03-28 13:55:53 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (297501ms till timeout)
2022-03-28 13:55:53 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-19
2022-03-28 13:55:53 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:53 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-193
2022-03-28 13:55:54 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (293109ms till timeout)
2022-03-28 13:55:54 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-5 get Namespace namespace-3 -o yaml
2022-03-28 13:55:54 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-193
2022-03-28 13:55:54 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:54 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-195
2022-03-28 13:55:54 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:55:54 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-5 get Namespace namespace-3 -o yaml
2022-03-28 13:55:54 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:54 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (445444ms till timeout)
2022-03-28 13:55:54 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:55:54 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:55:54 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-8af61d52-kafka-0 hasn't rolled
2022-03-28 13:55:54 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f} pods didn't roll. Remaining seconds for stability: 47
2022-03-28 13:55:54 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (296305ms till timeout)
2022-03-28 13:55:54 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:230] testUserWithNameMoreThan64Chars test now can proceed its execution
2022-03-28 13:55:54 [ForkJoinPool-1-worker-13] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 13:55:54 [ForkJoinPool-1-worker-13] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testScramUserWithQuotas=my-cluster-6f653060, testReceiveSimpleMessageTlsScramSha=my-cluster-af298d01, testConfigurationFileIsCreated=my-cluster-3244df0d, testThrottlingQuotasCreateAlterPartitions=my-cluster-8f6c28cb, testSendSimpleMessageTls=my-cluster-fbc845dd, testCreatingUsersWithSecretPrefix=my-cluster-0c65d2dd, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-0444229e, testUserWithNameMoreThan64Chars=my-cluster-02e25de6, testDeployAndUnDeployCruiseControl=my-cluster-844045c3, testTlsUserWithQuotas=my-cluster-9832c9ae, testThrottlingQuotasCreateTopic=my-cluster-ac2cadd6, testCapacityFile=my-cluster-0531d5ce, testReceiveSimpleMessageTls=my-cluster-ace1fa24, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-8af61d52, testSendSimpleMessageTlsScramSha=my-cluster-e353214b}
2022-03-28 13:55:54 [ForkJoinPool-1-worker-13] TRACE [AbstractST:607] USERS_NAME_MAP: {testScramUserWithQuotas=my-user-27563760-1632863338, testReceiveSimpleMessageTlsScramSha=my-user-1597210213-707359243, testConfigurationFileIsCreated=my-user-1799841978-1468805631, testThrottlingQuotasCreateAlterPartitions=my-user-1557959119-1722636050, testSendSimpleMessageTls=my-user-1754673553-2077518027, testCreatingUsersWithSecretPrefix=my-user-144001639-226807213, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-1010649803-214635564, testUserWithNameMoreThan64Chars=my-user-2124392695-340403978, testDeployAndUnDeployCruiseControl=my-user-2117120687-604608920, testTlsUserWithQuotas=my-user-1602931187-443256751, testThrottlingQuotasCreateTopic=my-user-46611685-1079680568, testCapacityFile=my-user-155969448-1270555822, testReceiveSimpleMessageTls=my-user-1418772225-1024845009, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-1807142870-188660517, testSendSimpleMessageTlsScramSha=my-user-182357028-1132873010}
2022-03-28 13:55:54 [ForkJoinPool-1-worker-13] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testScramUserWithQuotas=my-topic-1345794255-226872460, testReceiveSimpleMessageTlsScramSha=my-topic-649423016-1623551209, testConfigurationFileIsCreated=my-topic-1435014906-681104826, testThrottlingQuotasCreateAlterPartitions=my-topic-1449977352-344480794, testSendSimpleMessageTls=my-topic-115477502-2025274778, testCreatingUsersWithSecretPrefix=my-topic-130462979-2061578809, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-889283546-691691584, testUserWithNameMoreThan64Chars=my-topic-1053060011-1199293498, testDeployAndUnDeployCruiseControl=my-topic-1101175281-2107378523, testTlsUserWithQuotas=my-topic-985048738-437665866, testThrottlingQuotasCreateTopic=my-topic-114922478-1506136333, testCapacityFile=my-topic-296605537-1008743104, testReceiveSimpleMessageTls=my-topic-1131768783-120051821, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-1367124108-1748498395, testSendSimpleMessageTlsScramSha=my-topic-652506937-1802310709}
2022-03-28 13:55:54 [ForkJoinPool-1-worker-13] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testScramUserWithQuotas=my-cluster-6f653060-kafka-clients, testReceiveSimpleMessageTlsScramSha=my-cluster-af298d01-kafka-clients, testConfigurationFileIsCreated=my-cluster-3244df0d-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-8f6c28cb-kafka-clients, testSendSimpleMessageTls=my-cluster-fbc845dd-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-0c65d2dd-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-0444229e-kafka-clients, testUserWithNameMoreThan64Chars=my-cluster-02e25de6-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-844045c3-kafka-clients, testTlsUserWithQuotas=my-cluster-9832c9ae-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-ac2cadd6-kafka-clients, testCapacityFile=my-cluster-0531d5ce-kafka-clients, testReceiveSimpleMessageTls=my-cluster-ace1fa24-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-8af61d52-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-e353214b-kafka-clients}
2022-03-28 13:55:54 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:155] Create/Update KafkaUser user-with-correct-nameabcdefghijklmnopqrstuvxyzabcdefghijklmnopq in namespace user-st
2022-03-28 13:55:54 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-195
2022-03-28 13:55:54 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:54 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-197
2022-03-28 13:55:54 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:user-with-correct-nameabcdefghijklmnopqrstuvxyzabcdefghijklmnopq
2022-03-28 13:55:54 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:433] Wait for KafkaUser: user-with-correct-nameabcdefghijklmnopqrstuvxyzabcdefghijklmnopq will have desired state: Ready
2022-03-28 13:55:54 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for KafkaUser: user-with-correct-nameabcdefghijklmnopqrstuvxyzabcdefghijklmnopq will have desired state: Ready
2022-03-28 13:55:55 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:444] KafkaUser: user-with-correct-nameabcdefghijklmnopqrstuvxyzabcdefghijklmnopq is in desired state: Ready
2022-03-28 13:55:55 [ForkJoinPool-1-worker-13] INFO  [KafkaUserUtils:90] Wait until KafkaUser user-with-correct-nameabcdefghijklmnopqrstuvxyzabcdefghijklmnopq status is available
2022-03-28 13:55:55 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for KafkaUser user-with-correct-nameabcdefghijklmnopqrstuvxyzabcdefghijklmnopq status is available
2022-03-28 13:55:55 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (291913ms till timeout)
2022-03-28 13:55:55 [ForkJoinPool-1-worker-13] INFO  [KafkaUserUtils:95] KafkaUser user-with-correct-nameabcdefghijklmnopqrstuvxyzabcdefghijklmnopq status is available
2022-03-28 13:55:55 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:155] Create/Update KafkaUser sasl-userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdef in namespace user-st
2022-03-28 13:55:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-197
2022-03-28 13:55:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:55 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-199
2022-03-28 13:55:55 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-5 get Namespace namespace-3 -o yaml
2022-03-28 13:55:55 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:sasl-userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdef
2022-03-28 13:55:55 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:55:55 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:433] Wait for KafkaUser: sasl-userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdef will have desired state: Ready
2022-03-28 13:55:55 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for KafkaUser: sasl-userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdef will have desired state: Ready
2022-03-28 13:55:55 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:55:55 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:55:55 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-8af61d52-kafka-0 hasn't rolled
2022-03-28 13:55:55 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f} pods didn't roll. Remaining seconds for stability: 46
2022-03-28 13:55:55 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (295154ms till timeout)
2022-03-28 13:55:55 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:444] KafkaUser: sasl-userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdef is in desired state: Ready
2022-03-28 13:55:55 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:155] Create/Update KafkaUser userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdefghijk in namespace user-st
2022-03-28 13:55:55 [ForkJoinPool-1-worker-13] INFO  [KafkaUserUtils:90] Wait until KafkaUser userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdefghijk status is available
2022-03-28 13:55:55 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for KafkaUser userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdefghijk status is available
2022-03-28 13:55:55 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-5 get Namespace namespace-3 -o yaml
2022-03-28 13:55:55 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 1
2022-03-28 13:55:55 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 13:55:55 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Error from server (NotFound): namespaces "namespace-3" not found
2022-03-28 13:55:55 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] ======STDERR END======
2022-03-28 13:55:55 [ForkJoinPool-1-worker-5] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[], io.strimzi.test.logs.CollectorElement@aec142ef=[], io.strimzi.test.logs.CollectorElement@3881d5f2=[namespace-2], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[namespace-4], io.strimzi.test.logs.CollectorElement@f4405b3b=[namespace-5], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 13:55:55 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:267] testConfigurationFileIsCreated - Notifies waiting test cases:[testCapacityFile, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testThrottlingQuotasCreateAlterPartitions, testSendSimpleMessageTls, testReceiveSimpleMessageTlsScramSha, testSendSimpleMessageTlsScramSha, testReceiveSimpleMessageTls, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testConfigurationFileIsCreated, testPauseReconciliationInKafkaRebalanceAndTopic] to and randomly select one to start execution
2022-03-28 13:55:55 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:93] [cruisecontrol.CruiseControlConfigurationST] - Removing parallel test: testConfigurationFileIsCreated
2022-03-28 13:55:55 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:97] [cruisecontrol.CruiseControlConfigurationST] - Parallel test count: 6
2022-03-28 13:55:55 [ForkJoinPool-1-worker-5] INFO  [TestSeparator:29] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testConfigurationFileIsCreated-FINISHED
2022-03-28 13:55:55 [ForkJoinPool-1-worker-5] INFO  [TestSeparator:30] ############################################################################
2022-03-28 13:55:55 [ForkJoinPool-1-worker-5] INFO  [TestSeparator:23] ############################################################################
2022-03-28 13:55:55 [ForkJoinPool-1-worker-5] INFO  [TestSeparator:24] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testConfigurationPerformanceOptions-STARTED
2022-03-28 13:55:55 [ForkJoinPool-1-worker-5] DEBUG [AbstractST:658] ============================================================================
2022-03-28 13:55:55 [ForkJoinPool-1-worker-5] DEBUG [AbstractST:659] [cruisecontrol.CruiseControlConfigurationST - Before Each] - Setup test case environment
2022-03-28 13:55:55 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:77] [cruisecontrol.CruiseControlConfigurationST] - Adding parallel test: testConfigurationPerformanceOptions
2022-03-28 13:55:55 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:81] [cruisecontrol.CruiseControlConfigurationST] - Parallel test count: 7
2022-03-28 13:55:55 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:205] [testConfigurationPerformanceOptions] moved to the WaitZone, because current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 13:55:55 [ForkJoinPool-1-worker-5] TRACE [SuiteThreadController:210] testConfigurationPerformanceOptions is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 13:55:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-199
2022-03-28 13:55:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:55 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-2
2022-03-28 13:55:55 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-4 exec my-cluster-0444229e-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-889283546-691691584 --describe --bootstrap-server my-cluster-0444229e-kafka-bootstrap:9092
2022-03-28 13:55:55 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 13:55:55 [ForkJoinPool-1-worker-15] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 16 polls
2022-03-28 13:55:55 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (157683ms till timeout)
2022-03-28 13:55:56 [ForkJoinPool-1-worker-13] INFO  [KafkaUserUtils:95] KafkaUser userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdefghijk status is available
2022-03-28 13:55:56 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:674] ============================================================================
2022-03-28 13:55:56 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:675] [operators.user.UserST - After Each] - Clean up after test
2022-03-28 13:55:56 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:344] ############################################################################
2022-03-28 13:55:56 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:348] Delete all resources for testUserWithNameMoreThan64Chars
2022-03-28 13:55:56 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of KafkaUser sasl-userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdef in namespace user-st
2022-03-28 13:55:56 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:sasl-userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdef
2022-03-28 13:55:56 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (290720ms till timeout)
2022-03-28 13:55:56 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of KafkaUser userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdefghijk in namespace user-st
2022-03-28 13:55:56 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-2
2022-03-28 13:55:56 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:56 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-20
2022-03-28 13:55:56 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdefghijk
2022-03-28 13:55:56 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of KafkaUser user-with-correct-nameabcdefghijklmnopqrstuvxyzabcdefghijklmnopq in namespace user-st
2022-03-28 13:55:56 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:55:56 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:user-with-correct-nameabcdefghijklmnopqrstuvxyzabcdefghijklmnopq
2022-03-28 13:55:56 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:55:56 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:55:56 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-8af61d52-kafka-0 hasn't rolled
2022-03-28 13:55:56 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f} pods didn't roll. Remaining seconds for stability: 45
2022-03-28 13:55:56 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (294009ms till timeout)
2022-03-28 13:55:56 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:369] ############################################################################
2022-03-28 13:55:56 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:267] testUserWithNameMoreThan64Chars - Notifies waiting test cases:[testCapacityFile, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testThrottlingQuotasCreateAlterPartitions, testSendSimpleMessageTls, testReceiveSimpleMessageTlsScramSha, testSendSimpleMessageTlsScramSha, testReceiveSimpleMessageTls, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testConfigurationFileIsCreated, testPauseReconciliationInKafkaRebalanceAndTopic, testConfigurationPerformanceOptions] to and randomly select one to start execution
2022-03-28 13:55:56 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:93] [operators.user.UserST] - Removing parallel test: testUserWithNameMoreThan64Chars
2022-03-28 13:55:56 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:97] [operators.user.UserST] - Parallel test count: 6
2022-03-28 13:55:56 [ForkJoinPool-1-worker-13] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.user.UserST.testUserWithNameMoreThan64Chars-FINISHED
2022-03-28 13:55:56 [ForkJoinPool-1-worker-13] INFO  [TestSeparator:30] ############################################################################
2022-03-28 13:55:56 [ForkJoinPool-1-worker-13] INFO  [TestSeparator:23] ############################################################################
2022-03-28 13:55:56 [ForkJoinPool-1-worker-13] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.user.UserST.testTlsExternalUser-STARTED
2022-03-28 13:55:56 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:658] ============================================================================
2022-03-28 13:55:56 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:659] [operators.user.UserST - Before Each] - Setup test case environment
2022-03-28 13:55:56 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:77] [operators.user.UserST] - Adding parallel test: testTlsExternalUser
2022-03-28 13:55:56 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:81] [operators.user.UserST] - Parallel test count: 7
2022-03-28 13:55:56 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:205] [testTlsExternalUser] moved to the WaitZone, because current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 13:55:56 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testTlsExternalUser is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 13:55:56 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-0444229e-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-889283546-691691584 --describe --bootstrap-server my-cluster-0444229e-kafka-bootstrap:9092
2022-03-28 13:55:57 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-20
2022-03-28 13:55:57 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:57 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-200
2022-03-28 13:55:57 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-844045c3-kafka-0=7a441eef-cc28-4155-958e-c8c61cc69438, my-cluster-844045c3-kafka-1=8a676e65-d331-46fd-b04b-b47fb87d1932, my-cluster-844045c3-kafka-2=009a7082-9aa0-4208-b610-501b2bffd9cd}
2022-03-28 13:55:57 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (289616ms till timeout)
2022-03-28 13:55:57 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-844045c3-kafka-0=e24c016e-3d31-4327-92e5-6a4d2019dc61, my-cluster-844045c3-kafka-1=8a676e65-d331-46fd-b04b-b47fb87d1932, my-cluster-844045c3-kafka-2=009a7082-9aa0-4208-b610-501b2bffd9cd}
2022-03-28 13:55:57 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-844045c3-kafka-0=e24c016e-3d31-4327-92e5-6a4d2019dc61, my-cluster-844045c3-kafka-1=8a676e65-d331-46fd-b04b-b47fb87d1932, my-cluster-844045c3-kafka-2=009a7082-9aa0-4208-b610-501b2bffd9cd}
2022-03-28 13:55:57 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:50] At least my-cluster-844045c3-kafka-1 hasn't rolled
2022-03-28 13:55:57 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] component with name my-cluster-844045c3-kafka rolling update not ready, will try again in 5000 ms (1768875ms till timeout)
2022-03-28 13:55:57 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-200
2022-03-28 13:55:57 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:57 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-201
2022-03-28 13:55:57 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:55:57 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:55:57 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:55:57 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-8af61d52-kafka-0 hasn't rolled
2022-03-28 13:55:57 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f} pods didn't roll. Remaining seconds for stability: 44
2022-03-28 13:55:57 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (292904ms till timeout)
2022-03-28 13:55:58 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-201
2022-03-28 13:55:58 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:58 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-202
2022-03-28 13:55:58 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (288512ms till timeout)
2022-03-28 13:55:58 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:55:59 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-202
2022-03-28 13:55:59 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:59 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-203
2022-03-28 13:55:59 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:55:59 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:55:59 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-8af61d52-kafka-0 hasn't rolled
2022-03-28 13:55:59 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f} pods didn't roll. Remaining seconds for stability: 43
2022-03-28 13:55:59 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (291710ms till timeout)
2022-03-28 13:55:59 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-203
2022-03-28 13:55:59 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:55:59 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-204
2022-03-28 13:55:59 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (287411ms till timeout)
2022-03-28 13:56:00 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-204
2022-03-28 13:56:00 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:56:00 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-205
2022-03-28 13:56:00 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:00 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:00 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:00 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-8af61d52-kafka-0 hasn't rolled
2022-03-28 13:56:00 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f} pods didn't roll. Remaining seconds for stability: 42
2022-03-28 13:56:00 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (290512ms till timeout)
2022-03-28 13:56:00 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-4 exec my-cluster-0444229e-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-889283546-691691584 --describe --bootstrap-server my-cluster-0444229e-kafka-bootstrap:9092
2022-03-28 13:56:00 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 13:56:00 [ForkJoinPool-1-worker-15] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 15 polls
2022-03-28 13:56:00 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (153112ms till timeout)
2022-03-28 13:56:00 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-205
2022-03-28 13:56:00 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:56:00 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-206
2022-03-28 13:56:00 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (286308ms till timeout)
2022-03-28 13:56:00 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:230] testConfigurationPerformanceOptions test now can proceed its execution
2022-03-28 13:56:00 [ForkJoinPool-1-worker-5] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 13:56:00 [ForkJoinPool-1-worker-5] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testScramUserWithQuotas=my-cluster-6f653060, testReceiveSimpleMessageTlsScramSha=my-cluster-af298d01, testConfigurationFileIsCreated=my-cluster-3244df0d, testThrottlingQuotasCreateAlterPartitions=my-cluster-8f6c28cb, testSendSimpleMessageTls=my-cluster-fbc845dd, testCreatingUsersWithSecretPrefix=my-cluster-0c65d2dd, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-0444229e, testUserWithNameMoreThan64Chars=my-cluster-02e25de6, testDeployAndUnDeployCruiseControl=my-cluster-844045c3, testTlsUserWithQuotas=my-cluster-9832c9ae, testThrottlingQuotasCreateTopic=my-cluster-ac2cadd6, testCapacityFile=my-cluster-0531d5ce, testReceiveSimpleMessageTls=my-cluster-ace1fa24, testConfigurationPerformanceOptions=my-cluster-4994018d, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-8af61d52, testSendSimpleMessageTlsScramSha=my-cluster-e353214b}
2022-03-28 13:56:00 [ForkJoinPool-1-worker-5] TRACE [AbstractST:607] USERS_NAME_MAP: {testScramUserWithQuotas=my-user-27563760-1632863338, testReceiveSimpleMessageTlsScramSha=my-user-1597210213-707359243, testConfigurationFileIsCreated=my-user-1799841978-1468805631, testThrottlingQuotasCreateAlterPartitions=my-user-1557959119-1722636050, testSendSimpleMessageTls=my-user-1754673553-2077518027, testCreatingUsersWithSecretPrefix=my-user-144001639-226807213, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-1010649803-214635564, testUserWithNameMoreThan64Chars=my-user-2124392695-340403978, testDeployAndUnDeployCruiseControl=my-user-2117120687-604608920, testTlsUserWithQuotas=my-user-1602931187-443256751, testThrottlingQuotasCreateTopic=my-user-46611685-1079680568, testCapacityFile=my-user-155969448-1270555822, testReceiveSimpleMessageTls=my-user-1418772225-1024845009, testConfigurationPerformanceOptions=my-user-882948629-1821489961, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-1807142870-188660517, testSendSimpleMessageTlsScramSha=my-user-182357028-1132873010}
2022-03-28 13:56:00 [ForkJoinPool-1-worker-5] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testScramUserWithQuotas=my-topic-1345794255-226872460, testReceiveSimpleMessageTlsScramSha=my-topic-649423016-1623551209, testConfigurationFileIsCreated=my-topic-1435014906-681104826, testThrottlingQuotasCreateAlterPartitions=my-topic-1449977352-344480794, testSendSimpleMessageTls=my-topic-115477502-2025274778, testCreatingUsersWithSecretPrefix=my-topic-130462979-2061578809, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-889283546-691691584, testUserWithNameMoreThan64Chars=my-topic-1053060011-1199293498, testDeployAndUnDeployCruiseControl=my-topic-1101175281-2107378523, testTlsUserWithQuotas=my-topic-985048738-437665866, testThrottlingQuotasCreateTopic=my-topic-114922478-1506136333, testCapacityFile=my-topic-296605537-1008743104, testReceiveSimpleMessageTls=my-topic-1131768783-120051821, testConfigurationPerformanceOptions=my-topic-627301835-843876360, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-1367124108-1748498395, testSendSimpleMessageTlsScramSha=my-topic-652506937-1802310709}
2022-03-28 13:56:00 [ForkJoinPool-1-worker-5] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testScramUserWithQuotas=my-cluster-6f653060-kafka-clients, testReceiveSimpleMessageTlsScramSha=my-cluster-af298d01-kafka-clients, testConfigurationFileIsCreated=my-cluster-3244df0d-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-8f6c28cb-kafka-clients, testSendSimpleMessageTls=my-cluster-fbc845dd-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-0c65d2dd-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-0444229e-kafka-clients, testUserWithNameMoreThan64Chars=my-cluster-02e25de6-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-844045c3-kafka-clients, testTlsUserWithQuotas=my-cluster-9832c9ae-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-ac2cadd6-kafka-clients, testCapacityFile=my-cluster-0531d5ce-kafka-clients, testReceiveSimpleMessageTls=my-cluster-ace1fa24-kafka-clients, testConfigurationPerformanceOptions=my-cluster-4994018d-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-8af61d52-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-e353214b-kafka-clients}
2022-03-28 13:56:00 [ForkJoinPool-1-worker-5] INFO  [TestSuiteNamespaceManager:163] Creating namespace:namespace-6 for test case:testConfigurationPerformanceOptions
2022-03-28 13:56:01 [ForkJoinPool-1-worker-5] INFO  [KubeClusterResource:156] Creating Namespace: namespace-6
2022-03-28 13:56:01 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Namespace namespace-6
2022-03-28 13:56:01 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-5 get Namespace namespace-6 -o json
2022-03-28 13:56:01 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-206
2022-03-28 13:56:01 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:56:01 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-207
2022-03-28 13:56:01 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:01 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-0444229e-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-889283546-691691584 --describe --bootstrap-server my-cluster-0444229e-kafka-bootstrap:9092
2022-03-28 13:56:01 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:01 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:01 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-8af61d52-kafka-0 hasn't rolled
2022-03-28 13:56:01 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f} pods didn't roll. Remaining seconds for stability: 41
2022-03-28 13:56:01 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (289316ms till timeout)
2022-03-28 13:56:01 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-5 get Namespace namespace-6 -o json
2022-03-28 13:56:01 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 13:56:01 [ForkJoinPool-1-worker-5] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c32,c4",
            "openshift.io/sa.scc.supplemental-groups": "1001000000/10000",
            "openshift.io/sa.scc.uid-range": "1001000000/10000"
        },
        "creationTimestamp": "2022-03-28T13:55:56Z",
        "labels": {
            "kubernetes.io/metadata.name": "namespace-6"
        },
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T13:55:56Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:labels": {
                            ".": {},
                            "f:kubernetes.io/metadata.name": {}
                        }
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T13:55:56Z"
            }
        ],
        "name": "namespace-6",
        "resourceVersion": "90961",
        "uid": "880fcc20-54e3-4ef9-86b3-931f11c1e84e"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 13:56:01 [ForkJoinPool-1-worker-5] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[], io.strimzi.test.logs.CollectorElement@aec142ef=[], io.strimzi.test.logs.CollectorElement@f851b6c3=[namespace-6], io.strimzi.test.logs.CollectorElement@3881d5f2=[namespace-2], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[namespace-4], io.strimzi.test.logs.CollectorElement@f4405b3b=[namespace-5], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 13:56:01 [ForkJoinPool-1-worker-5] INFO  [KubeClusterResource:82] Client use Namespace: namespace-6
2022-03-28 13:56:01 [ForkJoinPool-1-worker-5] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-6, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 13:56:01 [ForkJoinPool-1-worker-5] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-6
2022-03-28 13:56:01 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:155] Create/Update Kafka my-cluster-4994018d in namespace namespace-6
2022-03-28 13:56:01 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:164] Using Namespace: namespace-6
2022-03-28 13:56:01 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-4994018d
2022-03-28 13:56:01 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:433] Wait for Kafka: my-cluster-4994018d will have desired state: Ready
2022-03-28 13:56:01 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Kafka: my-cluster-4994018d will have desired state: Ready
2022-03-28 13:56:01 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-207
2022-03-28 13:56:01 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:56:01 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-208
2022-03-28 13:56:01 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1319903ms till timeout)
2022-03-28 13:56:01 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testTlsExternalUser is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 13:56:02 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (285067ms till timeout)
2022-03-28 13:56:02 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-208
2022-03-28 13:56:02 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:56:02 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-209
2022-03-28 13:56:02 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:02 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-844045c3-kafka-0=7a441eef-cc28-4155-958e-c8c61cc69438, my-cluster-844045c3-kafka-1=8a676e65-d331-46fd-b04b-b47fb87d1932, my-cluster-844045c3-kafka-2=009a7082-9aa0-4208-b610-501b2bffd9cd}
2022-03-28 13:56:02 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:02 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:02 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-8af61d52-kafka-0 hasn't rolled
2022-03-28 13:56:02 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f} pods didn't roll. Remaining seconds for stability: 40
2022-03-28 13:56:02 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (288123ms till timeout)
2022-03-28 13:56:02 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-844045c3-kafka-0=e24c016e-3d31-4327-92e5-6a4d2019dc61, my-cluster-844045c3-kafka-1=8a676e65-d331-46fd-b04b-b47fb87d1932, my-cluster-844045c3-kafka-2=009a7082-9aa0-4208-b610-501b2bffd9cd}
2022-03-28 13:56:02 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-844045c3-kafka-0=e24c016e-3d31-4327-92e5-6a4d2019dc61, my-cluster-844045c3-kafka-1=8a676e65-d331-46fd-b04b-b47fb87d1932, my-cluster-844045c3-kafka-2=009a7082-9aa0-4208-b610-501b2bffd9cd}
2022-03-28 13:56:02 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:50] At least my-cluster-844045c3-kafka-1 hasn't rolled
2022-03-28 13:56:02 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] component with name my-cluster-844045c3-kafka rolling update not ready, will try again in 5000 ms (1763606ms till timeout)
2022-03-28 13:56:02 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1318804ms till timeout)
2022-03-28 13:56:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-209
2022-03-28 13:56:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:56:03 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-21
2022-03-28 13:56:03 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (283965ms till timeout)
2022-03-28 13:56:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-21
2022-03-28 13:56:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:56:03 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-210
2022-03-28 13:56:03 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:03 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:03 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:03 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-8af61d52-kafka-0 hasn't rolled
2022-03-28 13:56:03 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f} pods didn't roll. Remaining seconds for stability: 39
2022-03-28 13:56:03 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (286929ms till timeout)
2022-03-28 13:56:04 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1317707ms till timeout)
2022-03-28 13:56:04 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-210
2022-03-28 13:56:04 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:56:04 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-211
2022-03-28 13:56:04 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (282851ms till timeout)
2022-03-28 13:56:04 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-211
2022-03-28 13:56:04 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:56:04 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-212
2022-03-28 13:56:04 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:05 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:05 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:05 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-8af61d52-kafka-0 hasn't rolled
2022-03-28 13:56:05 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f} pods didn't roll. Remaining seconds for stability: 38
2022-03-28 13:56:05 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (285735ms till timeout)
2022-03-28 13:56:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-212
2022-03-28 13:56:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:56:05 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-213
2022-03-28 13:56:05 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1316609ms till timeout)
2022-03-28 13:56:05 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (281750ms till timeout)
2022-03-28 13:56:05 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-4 exec my-cluster-0444229e-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-889283546-691691584 --describe --bootstrap-server my-cluster-0444229e-kafka-bootstrap:9092
2022-03-28 13:56:05 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 13:56:05 [ForkJoinPool-1-worker-15] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 14 polls
2022-03-28 13:56:05 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (148295ms till timeout)
2022-03-28 13:56:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-213
2022-03-28 13:56:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:56:05 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-214
2022-03-28 13:56:06 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-214
2022-03-28 13:56:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:56:06 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-215
2022-03-28 13:56:06 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:06 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:06 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-8af61d52-kafka-0 hasn't rolled
2022-03-28 13:56:06 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f} pods didn't roll. Remaining seconds for stability: 37
2022-03-28 13:56:06 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (284543ms till timeout)
2022-03-28 13:56:06 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1315449ms till timeout)
2022-03-28 13:56:06 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-0444229e-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-889283546-691691584 --describe --bootstrap-server my-cluster-0444229e-kafka-bootstrap:9092
2022-03-28 13:56:06 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (280648ms till timeout)
2022-03-28 13:56:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-215
2022-03-28 13:56:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:56:06 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-216
2022-03-28 13:56:06 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testTlsExternalUser is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 13:56:07 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:07 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-216
2022-03-28 13:56:07 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:56:07 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-217
2022-03-28 13:56:07 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:07 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:07 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-8af61d52-kafka-0 hasn't rolled
2022-03-28 13:56:07 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f} pods didn't roll. Remaining seconds for stability: 36
2022-03-28 13:56:07 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (283351ms till timeout)
2022-03-28 13:56:07 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1314218ms till timeout)
2022-03-28 13:56:07 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (279455ms till timeout)
2022-03-28 13:56:07 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-844045c3-kafka-0=7a441eef-cc28-4155-958e-c8c61cc69438, my-cluster-844045c3-kafka-1=8a676e65-d331-46fd-b04b-b47fb87d1932, my-cluster-844045c3-kafka-2=009a7082-9aa0-4208-b610-501b2bffd9cd}
2022-03-28 13:56:07 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-217
2022-03-28 13:56:07 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:56:07 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-218
2022-03-28 13:56:08 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-844045c3-kafka-0=e24c016e-3d31-4327-92e5-6a4d2019dc61, my-cluster-844045c3-kafka-1=8a676e65-d331-46fd-b04b-b47fb87d1932, my-cluster-844045c3-kafka-2=009a7082-9aa0-4208-b610-501b2bffd9cd}
2022-03-28 13:56:08 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-844045c3-kafka-0=e24c016e-3d31-4327-92e5-6a4d2019dc61, my-cluster-844045c3-kafka-1=8a676e65-d331-46fd-b04b-b47fb87d1932, my-cluster-844045c3-kafka-2=009a7082-9aa0-4208-b610-501b2bffd9cd}
2022-03-28 13:56:08 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:50] At least my-cluster-844045c3-kafka-1 hasn't rolled
2022-03-28 13:56:08 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] component with name my-cluster-844045c3-kafka rolling update not ready, will try again in 5000 ms (1758412ms till timeout)
2022-03-28 13:56:08 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-218
2022-03-28 13:56:08 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:56:08 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-219
2022-03-28 13:56:08 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:08 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:08 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:08 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-8af61d52-kafka-0 hasn't rolled
2022-03-28 13:56:08 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f} pods didn't roll. Remaining seconds for stability: 35
2022-03-28 13:56:08 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (282157ms till timeout)
2022-03-28 13:56:08 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1313063ms till timeout)
2022-03-28 13:56:08 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (278278ms till timeout)
2022-03-28 13:56:09 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-219
2022-03-28 13:56:09 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:56:09 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-22
2022-03-28 13:56:09 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-22
2022-03-28 13:56:09 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:56:09 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-220
2022-03-28 13:56:09 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:09 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:09 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:09 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-8af61d52-kafka-0 hasn't rolled
2022-03-28 13:56:09 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f} pods didn't roll. Remaining seconds for stability: 34
2022-03-28 13:56:09 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (280963ms till timeout)
2022-03-28 13:56:09 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1311833ms till timeout)
2022-03-28 13:56:10 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (277085ms till timeout)
2022-03-28 13:56:10 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-220
2022-03-28 13:56:10 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:56:10 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-221
2022-03-28 13:56:10 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-221
2022-03-28 13:56:10 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:56:10 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-226
2022-03-28 13:56:10 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:11 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:11 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:11 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-8af61d52-kafka-0 hasn't rolled
2022-03-28 13:56:11 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f} pods didn't roll. Remaining seconds for stability: 33
2022-03-28 13:56:11 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (279770ms till timeout)
2022-03-28 13:56:11 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1310677ms till timeout)
2022-03-28 13:56:11 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (275893ms till timeout)
2022-03-28 13:56:11 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-226
2022-03-28 13:56:11 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:56:11 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-228
2022-03-28 13:56:11 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-4 exec my-cluster-0444229e-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-889283546-691691584 --describe --bootstrap-server my-cluster-0444229e-kafka-bootstrap:9092
2022-03-28 13:56:11 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 13:56:11 [ForkJoinPool-1-worker-15] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 13 polls
2022-03-28 13:56:11 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (141960ms till timeout)
2022-03-28 13:56:11 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-228
2022-03-28 13:56:11 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:56:11 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-23
2022-03-28 13:56:11 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testTlsExternalUser is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 13:56:12 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:12 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-23
2022-03-28 13:56:12 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:56:12 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-230
2022-03-28 13:56:12 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:12 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:12 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-8af61d52-kafka-0 hasn't rolled
2022-03-28 13:56:12 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f} pods didn't roll. Remaining seconds for stability: 32
2022-03-28 13:56:12 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (278578ms till timeout)
2022-03-28 13:56:12 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1309485ms till timeout)
2022-03-28 13:56:12 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (274700ms till timeout)
2022-03-28 13:56:12 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-0444229e-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-889283546-691691584 --describe --bootstrap-server my-cluster-0444229e-kafka-bootstrap:9092
2022-03-28 13:56:12 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-230
2022-03-28 13:56:12 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:56:12 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-234
2022-03-28 13:56:13 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-844045c3-kafka-0=7a441eef-cc28-4155-958e-c8c61cc69438, my-cluster-844045c3-kafka-1=8a676e65-d331-46fd-b04b-b47fb87d1932, my-cluster-844045c3-kafka-2=009a7082-9aa0-4208-b610-501b2bffd9cd}
2022-03-28 13:56:13 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-844045c3-kafka-0=e24c016e-3d31-4327-92e5-6a4d2019dc61, my-cluster-844045c3-kafka-1=8a676e65-d331-46fd-b04b-b47fb87d1932, my-cluster-844045c3-kafka-2=009a7082-9aa0-4208-b610-501b2bffd9cd}
2022-03-28 13:56:13 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-844045c3-kafka-0=e24c016e-3d31-4327-92e5-6a4d2019dc61, my-cluster-844045c3-kafka-1=8a676e65-d331-46fd-b04b-b47fb87d1932, my-cluster-844045c3-kafka-2=009a7082-9aa0-4208-b610-501b2bffd9cd}
2022-03-28 13:56:13 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:50] At least my-cluster-844045c3-kafka-1 hasn't rolled
2022-03-28 13:56:13 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] component with name my-cluster-844045c3-kafka rolling update not ready, will try again in 5000 ms (1753217ms till timeout)
2022-03-28 13:56:13 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:13 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:13 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:13 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-8af61d52-kafka-0 hasn't rolled
2022-03-28 13:56:13 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f} pods didn't roll. Remaining seconds for stability: 31
2022-03-28 13:56:13 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (277471ms till timeout)
2022-03-28 13:56:13 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-234
2022-03-28 13:56:13 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:56:13 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-24
2022-03-28 13:56:13 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1308286ms till timeout)
2022-03-28 13:56:13 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (273588ms till timeout)
2022-03-28 13:56:14 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-24
2022-03-28 13:56:14 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:56:14 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-240
2022-03-28 13:56:14 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:14 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:14 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:14 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-8af61d52-kafka-0 hasn't rolled
2022-03-28 13:56:14 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f} pods didn't roll. Remaining seconds for stability: 30
2022-03-28 13:56:14 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (276276ms till timeout)
2022-03-28 13:56:14 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1307184ms till timeout)
2022-03-28 13:56:14 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-240
2022-03-28 13:56:14 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:56:14 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-241
2022-03-28 13:56:14 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (272400ms till timeout)
2022-03-28 13:56:15 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-241
2022-03-28 13:56:15 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:56:15 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-242
2022-03-28 13:56:15 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:15 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-242
2022-03-28 13:56:15 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:56:15 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-243
2022-03-28 13:56:15 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:15 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:15 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-8af61d52-kafka-0 hasn't rolled
2022-03-28 13:56:15 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f} pods didn't roll. Remaining seconds for stability: 29
2022-03-28 13:56:15 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (275080ms till timeout)
2022-03-28 13:56:15 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1305985ms till timeout)
2022-03-28 13:56:15 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (271211ms till timeout)
2022-03-28 13:56:16 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-4 exec my-cluster-0444229e-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-889283546-691691584 --describe --bootstrap-server my-cluster-0444229e-kafka-bootstrap:9092
2022-03-28 13:56:16 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 13:56:16 [ForkJoinPool-1-worker-15] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 12 polls
2022-03-28 13:56:16 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (137456ms till timeout)
2022-03-28 13:56:16 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-243
2022-03-28 13:56:16 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:56:16 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-249
2022-03-28 13:56:16 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:16 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-249
2022-03-28 13:56:16 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:56:16 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-25
2022-03-28 13:56:16 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testTlsExternalUser is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 13:56:17 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:17 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:17 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-8af61d52-kafka-0 hasn't rolled
2022-03-28 13:56:17 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f} pods didn't roll. Remaining seconds for stability: 28
2022-03-28 13:56:17 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (273887ms till timeout)
2022-03-28 13:56:17 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1304794ms till timeout)
2022-03-28 13:56:17 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (270012ms till timeout)
2022-03-28 13:56:17 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-0444229e-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-889283546-691691584 --describe --bootstrap-server my-cluster-0444229e-kafka-bootstrap:9092
2022-03-28 13:56:17 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-25
2022-03-28 13:56:17 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:56:17 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-251
2022-03-28 13:56:18 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:18 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-251
2022-03-28 13:56:18 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:56:18 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-259
2022-03-28 13:56:18 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:18 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:18 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-8af61d52-kafka-0 hasn't rolled
2022-03-28 13:56:18 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f} pods didn't roll. Remaining seconds for stability: 27
2022-03-28 13:56:18 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (272693ms till timeout)
2022-03-28 13:56:18 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1303600ms till timeout)
2022-03-28 13:56:18 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-844045c3-kafka-0=7a441eef-cc28-4155-958e-c8c61cc69438, my-cluster-844045c3-kafka-1=8a676e65-d331-46fd-b04b-b47fb87d1932, my-cluster-844045c3-kafka-2=009a7082-9aa0-4208-b610-501b2bffd9cd}
2022-03-28 13:56:18 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (268823ms till timeout)
2022-03-28 13:56:18 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-844045c3-kafka-0=e24c016e-3d31-4327-92e5-6a4d2019dc61, my-cluster-844045c3-kafka-1=8a676e65-d331-46fd-b04b-b47fb87d1932, my-cluster-844045c3-kafka-2=009a7082-9aa0-4208-b610-501b2bffd9cd}
2022-03-28 13:56:18 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-844045c3-kafka-0=e24c016e-3d31-4327-92e5-6a4d2019dc61, my-cluster-844045c3-kafka-1=8a676e65-d331-46fd-b04b-b47fb87d1932, my-cluster-844045c3-kafka-2=009a7082-9aa0-4208-b610-501b2bffd9cd}
2022-03-28 13:56:18 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:50] At least my-cluster-844045c3-kafka-1 hasn't rolled
2022-03-28 13:56:18 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] component with name my-cluster-844045c3-kafka rolling update not ready, will try again in 5000 ms (1748116ms till timeout)
2022-03-28 13:56:18 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-259
2022-03-28 13:56:18 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:56:18 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-26
2022-03-28 13:56:19 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-26
2022-03-28 13:56:19 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:56:19 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:19 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-264
2022-03-28 13:56:19 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:19 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:19 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-8af61d52-kafka-0 hasn't rolled
2022-03-28 13:56:19 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f} pods didn't roll. Remaining seconds for stability: 26
2022-03-28 13:56:19 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (271499ms till timeout)
2022-03-28 13:56:19 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1302407ms till timeout)
2022-03-28 13:56:19 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (267629ms till timeout)
2022-03-28 13:56:19 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-264
2022-03-28 13:56:19 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:56:19 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-269
2022-03-28 13:56:20 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-269
2022-03-28 13:56:20 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:56:20 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-27
2022-03-28 13:56:20 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:20 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:20 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:20 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-8af61d52-kafka-0 hasn't rolled
2022-03-28 13:56:20 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f} pods didn't roll. Remaining seconds for stability: 25
2022-03-28 13:56:20 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (270302ms till timeout)
2022-03-28 13:56:20 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1301206ms till timeout)
2022-03-28 13:56:20 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (266433ms till timeout)
2022-03-28 13:56:20 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-27
2022-03-28 13:56:20 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:56:20 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-270
2022-03-28 13:56:21 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-4 exec my-cluster-0444229e-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-889283546-691691584 --describe --bootstrap-server my-cluster-0444229e-kafka-bootstrap:9092
2022-03-28 13:56:21 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 13:56:21 [ForkJoinPool-1-worker-15] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 11 polls
2022-03-28 13:56:21 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (132159ms till timeout)
2022-03-28 13:56:21 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-270
2022-03-28 13:56:21 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:56:21 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-28
2022-03-28 13:56:21 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:21 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:21 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:21 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-8af61d52-kafka-0 hasn't rolled
2022-03-28 13:56:21 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f} pods didn't roll. Remaining seconds for stability: 24
2022-03-28 13:56:21 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (269098ms till timeout)
2022-03-28 13:56:21 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1300004ms till timeout)
2022-03-28 13:56:21 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (265224ms till timeout)
2022-03-28 13:56:21 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testTlsExternalUser is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 13:56:22 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-28
2022-03-28 13:56:22 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:56:22 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-282
2022-03-28 13:56:22 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-0444229e-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-889283546-691691584 --describe --bootstrap-server my-cluster-0444229e-kafka-bootstrap:9092
2022-03-28 13:56:22 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:22 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-282
2022-03-28 13:56:22 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:56:22 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-283
2022-03-28 13:56:22 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:22 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:22 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-8af61d52-kafka-0 hasn't rolled
2022-03-28 13:56:22 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f} pods didn't roll. Remaining seconds for stability: 23
2022-03-28 13:56:22 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (267903ms till timeout)
2022-03-28 13:56:22 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1298807ms till timeout)
2022-03-28 13:56:23 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (264034ms till timeout)
2022-03-28 13:56:23 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-844045c3-kafka-0=7a441eef-cc28-4155-958e-c8c61cc69438, my-cluster-844045c3-kafka-1=8a676e65-d331-46fd-b04b-b47fb87d1932, my-cluster-844045c3-kafka-2=009a7082-9aa0-4208-b610-501b2bffd9cd}
2022-03-28 13:56:23 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-283
2022-03-28 13:56:23 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:56:23 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-285
2022-03-28 13:56:23 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-844045c3-kafka-0=e24c016e-3d31-4327-92e5-6a4d2019dc61, my-cluster-844045c3-kafka-1=8a676e65-d331-46fd-b04b-b47fb87d1932, my-cluster-844045c3-kafka-2=009a7082-9aa0-4208-b610-501b2bffd9cd}
2022-03-28 13:56:23 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-844045c3-kafka-0=e24c016e-3d31-4327-92e5-6a4d2019dc61, my-cluster-844045c3-kafka-1=8a676e65-d331-46fd-b04b-b47fb87d1932, my-cluster-844045c3-kafka-2=009a7082-9aa0-4208-b610-501b2bffd9cd}
2022-03-28 13:56:23 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:50] At least my-cluster-844045c3-kafka-1 hasn't rolled
2022-03-28 13:56:23 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] component with name my-cluster-844045c3-kafka rolling update not ready, will try again in 5000 ms (1742917ms till timeout)
2022-03-28 13:56:23 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:23 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-285
2022-03-28 13:56:23 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:56:23 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-287
2022-03-28 13:56:24 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:24 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:24 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-8af61d52-kafka-0 hasn't rolled
2022-03-28 13:56:24 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f} pods didn't roll. Remaining seconds for stability: 22
2022-03-28 13:56:24 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (266710ms till timeout)
2022-03-28 13:56:24 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1297617ms till timeout)
2022-03-28 13:56:24 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (262920ms till timeout)
2022-03-28 13:56:24 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-287
2022-03-28 13:56:24 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:56:24 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-29
2022-03-28 13:56:25 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-29
2022-03-28 13:56:25 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:56:25 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-296
2022-03-28 13:56:25 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:25 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:25 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:25 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-8af61d52-kafka-0 hasn't rolled
2022-03-28 13:56:25 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f} pods didn't roll. Remaining seconds for stability: 21
2022-03-28 13:56:25 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (265518ms till timeout)
2022-03-28 13:56:25 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1296424ms till timeout)
2022-03-28 13:56:25 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (261648ms till timeout)
2022-03-28 13:56:25 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-296
2022-03-28 13:56:25 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:56:25 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-30
2022-03-28 13:56:26 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-30
2022-03-28 13:56:26 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:56:26 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-300
2022-03-28 13:56:26 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:26 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:26 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:26 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-8af61d52-kafka-0 hasn't rolled
2022-03-28 13:56:26 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f} pods didn't roll. Remaining seconds for stability: 20
2022-03-28 13:56:26 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (264325ms till timeout)
2022-03-28 13:56:26 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1295232ms till timeout)
2022-03-28 13:56:26 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (260455ms till timeout)
2022-03-28 13:56:26 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-4 exec my-cluster-0444229e-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-889283546-691691584 --describe --bootstrap-server my-cluster-0444229e-kafka-bootstrap:9092
2022-03-28 13:56:26 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 13:56:26 [ForkJoinPool-1-worker-15] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 10 polls
2022-03-28 13:56:26 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (126928ms till timeout)
2022-03-28 13:56:26 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-300
2022-03-28 13:56:26 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:56:26 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-301
2022-03-28 13:56:26 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testTlsExternalUser is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 13:56:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-301
2022-03-28 13:56:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:56:27 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-302
2022-03-28 13:56:27 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:27 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-0444229e-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-889283546-691691584 --describe --bootstrap-server my-cluster-0444229e-kafka-bootstrap:9092
2022-03-28 13:56:27 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:27 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:27 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-8af61d52-kafka-0 hasn't rolled
2022-03-28 13:56:27 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f} pods didn't roll. Remaining seconds for stability: 19
2022-03-28 13:56:27 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (263127ms till timeout)
2022-03-28 13:56:27 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1294035ms till timeout)
2022-03-28 13:56:27 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (259257ms till timeout)
2022-03-28 13:56:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-302
2022-03-28 13:56:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:56:27 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-303
2022-03-28 13:56:28 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-303
2022-03-28 13:56:28 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:56:28 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-304
2022-03-28 13:56:28 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-844045c3-kafka-0=7a441eef-cc28-4155-958e-c8c61cc69438, my-cluster-844045c3-kafka-1=8a676e65-d331-46fd-b04b-b47fb87d1932, my-cluster-844045c3-kafka-2=009a7082-9aa0-4208-b610-501b2bffd9cd}
2022-03-28 13:56:28 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-844045c3-kafka-0=e24c016e-3d31-4327-92e5-6a4d2019dc61, my-cluster-844045c3-kafka-1=8a676e65-d331-46fd-b04b-b47fb87d1932, my-cluster-844045c3-kafka-2=c477a607-0ed7-44a0-a864-53d364b682a1}
2022-03-28 13:56:28 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-844045c3-kafka-0=e24c016e-3d31-4327-92e5-6a4d2019dc61, my-cluster-844045c3-kafka-1=8a676e65-d331-46fd-b04b-b47fb87d1932, my-cluster-844045c3-kafka-2=c477a607-0ed7-44a0-a864-53d364b682a1}
2022-03-28 13:56:28 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:50] At least my-cluster-844045c3-kafka-1 hasn't rolled
2022-03-28 13:56:28 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] component with name my-cluster-844045c3-kafka rolling update not ready, will try again in 5000 ms (1737723ms till timeout)
2022-03-28 13:56:28 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:28 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:28 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:28 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-8af61d52-kafka-0 hasn't rolled
2022-03-28 13:56:28 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f} pods didn't roll. Remaining seconds for stability: 18
2022-03-28 13:56:28 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (262026ms till timeout)
2022-03-28 13:56:28 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1292840ms till timeout)
2022-03-28 13:56:28 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (258149ms till timeout)
2022-03-28 13:56:29 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-304
2022-03-28 13:56:29 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:56:29 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-305
2022-03-28 13:56:29 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:30 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:30 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:30 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-8af61d52-kafka-0 hasn't rolled
2022-03-28 13:56:30 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f} pods didn't roll. Remaining seconds for stability: 17
2022-03-28 13:56:30 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (260832ms till timeout)
2022-03-28 13:56:30 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1291737ms till timeout)
2022-03-28 13:56:30 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (256962ms till timeout)
2022-03-28 13:56:30 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-305
2022-03-28 13:56:30 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:56:30 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-306
2022-03-28 13:56:31 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:31 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:31 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:31 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-8af61d52-kafka-0 hasn't rolled
2022-03-28 13:56:31 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f} pods didn't roll. Remaining seconds for stability: 16
2022-03-28 13:56:31 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (259638ms till timeout)
2022-03-28 13:56:31 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1290545ms till timeout)
2022-03-28 13:56:31 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (255768ms till timeout)
2022-03-28 13:56:31 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testTlsExternalUser is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 13:56:32 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:32 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:32 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:32 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-8af61d52-kafka-0 hasn't rolled
2022-03-28 13:56:32 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f} pods didn't roll. Remaining seconds for stability: 15
2022-03-28 13:56:32 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (258442ms till timeout)
2022-03-28 13:56:32 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1289350ms till timeout)
2022-03-28 13:56:32 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (254572ms till timeout)
2022-03-28 13:56:33 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:33 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:33 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:33 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-8af61d52-kafka-0 hasn't rolled
2022-03-28 13:56:33 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f} pods didn't roll. Remaining seconds for stability: 14
2022-03-28 13:56:33 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (257249ms till timeout)
2022-03-28 13:56:33 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1288155ms till timeout)
2022-03-28 13:56:33 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (253378ms till timeout)
2022-03-28 13:56:33 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-844045c3-kafka-0=7a441eef-cc28-4155-958e-c8c61cc69438, my-cluster-844045c3-kafka-1=8a676e65-d331-46fd-b04b-b47fb87d1932, my-cluster-844045c3-kafka-2=009a7082-9aa0-4208-b610-501b2bffd9cd}
2022-03-28 13:56:33 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-4 exec my-cluster-0444229e-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-889283546-691691584 --describe --bootstrap-server my-cluster-0444229e-kafka-bootstrap:9092
2022-03-28 13:56:33 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 13:56:33 [ForkJoinPool-1-worker-15] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 9 polls
2022-03-28 13:56:33 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (119853ms till timeout)
2022-03-28 13:56:33 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-844045c3-kafka-0=e24c016e-3d31-4327-92e5-6a4d2019dc61, my-cluster-844045c3-kafka-1=8a676e65-d331-46fd-b04b-b47fb87d1932, my-cluster-844045c3-kafka-2=c477a607-0ed7-44a0-a864-53d364b682a1}
2022-03-28 13:56:33 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-844045c3-kafka-0=e24c016e-3d31-4327-92e5-6a4d2019dc61, my-cluster-844045c3-kafka-1=8a676e65-d331-46fd-b04b-b47fb87d1932, my-cluster-844045c3-kafka-2=c477a607-0ed7-44a0-a864-53d364b682a1}
2022-03-28 13:56:33 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:50] At least my-cluster-844045c3-kafka-1 hasn't rolled
2022-03-28 13:56:33 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] component with name my-cluster-844045c3-kafka rolling update not ready, will try again in 5000 ms (1732623ms till timeout)
2022-03-28 13:56:34 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:34 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-0444229e-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-889283546-691691584 --describe --bootstrap-server my-cluster-0444229e-kafka-bootstrap:9092
2022-03-28 13:56:34 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:34 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:34 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-8af61d52-kafka-0 hasn't rolled
2022-03-28 13:56:34 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f} pods didn't roll. Remaining seconds for stability: 13
2022-03-28 13:56:34 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (256055ms till timeout)
2022-03-28 13:56:34 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1286957ms till timeout)
2022-03-28 13:56:34 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (252184ms till timeout)
2022-03-28 13:56:35 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:35 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-306
2022-03-28 13:56:35 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:56:35 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-307
2022-03-28 13:56:36 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:36 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:36 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-8af61d52-kafka-0 hasn't rolled
2022-03-28 13:56:36 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f} pods didn't roll. Remaining seconds for stability: 12
2022-03-28 13:56:36 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (254862ms till timeout)
2022-03-28 13:56:36 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1285732ms till timeout)
2022-03-28 13:56:36 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (250992ms till timeout)
2022-03-28 13:56:36 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testTlsExternalUser is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 13:56:37 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:37 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:37 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:37 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-8af61d52-kafka-0 hasn't rolled
2022-03-28 13:56:37 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f} pods didn't roll. Remaining seconds for stability: 11
2022-03-28 13:56:37 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (253668ms till timeout)
2022-03-28 13:56:37 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1284568ms till timeout)
2022-03-28 13:56:37 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (249798ms till timeout)
2022-03-28 13:56:38 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:38 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:38 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:38 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-8af61d52-kafka-0 hasn't rolled
2022-03-28 13:56:38 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f} pods didn't roll. Remaining seconds for stability: 10
2022-03-28 13:56:38 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (252473ms till timeout)
2022-03-28 13:56:38 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1283380ms till timeout)
2022-03-28 13:56:38 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (248604ms till timeout)
2022-03-28 13:56:38 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-844045c3-kafka-0=7a441eef-cc28-4155-958e-c8c61cc69438, my-cluster-844045c3-kafka-1=8a676e65-d331-46fd-b04b-b47fb87d1932, my-cluster-844045c3-kafka-2=009a7082-9aa0-4208-b610-501b2bffd9cd}
2022-03-28 13:56:39 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-844045c3-kafka-0=e24c016e-3d31-4327-92e5-6a4d2019dc61, my-cluster-844045c3-kafka-1=8a676e65-d331-46fd-b04b-b47fb87d1932, my-cluster-844045c3-kafka-2=c477a607-0ed7-44a0-a864-53d364b682a1}
2022-03-28 13:56:39 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-844045c3-kafka-0=e24c016e-3d31-4327-92e5-6a4d2019dc61, my-cluster-844045c3-kafka-1=8a676e65-d331-46fd-b04b-b47fb87d1932, my-cluster-844045c3-kafka-2=c477a607-0ed7-44a0-a864-53d364b682a1}
2022-03-28 13:56:39 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:50] At least my-cluster-844045c3-kafka-1 hasn't rolled
2022-03-28 13:56:39 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] component with name my-cluster-844045c3-kafka rolling update not ready, will try again in 5000 ms (1727413ms till timeout)
2022-03-28 13:56:39 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:39 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:39 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:39 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-8af61d52-kafka-0 hasn't rolled
2022-03-28 13:56:39 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f} pods didn't roll. Remaining seconds for stability: 9
2022-03-28 13:56:39 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (251280ms till timeout)
2022-03-28 13:56:39 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1282187ms till timeout)
2022-03-28 13:56:39 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (247492ms till timeout)
2022-03-28 13:56:40 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:40 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:40 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:40 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-8af61d52-kafka-0 hasn't rolled
2022-03-28 13:56:40 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f} pods didn't roll. Remaining seconds for stability: 8
2022-03-28 13:56:40 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (250087ms till timeout)
2022-03-28 13:56:40 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1280994ms till timeout)
2022-03-28 13:56:40 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (246216ms till timeout)
2022-03-28 13:56:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-307
2022-03-28 13:56:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:56:41 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-308
2022-03-28 13:56:41 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:41 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testTlsExternalUser is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 13:56:41 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:41 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:41 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-8af61d52-kafka-0 hasn't rolled
2022-03-28 13:56:41 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f} pods didn't roll. Remaining seconds for stability: 7
2022-03-28 13:56:41 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (248891ms till timeout)
2022-03-28 13:56:42 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1279798ms till timeout)
2022-03-28 13:56:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-308
2022-03-28 13:56:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:56:42 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-309
2022-03-28 13:56:42 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (245022ms till timeout)
2022-03-28 13:56:42 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:43 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:43 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:43 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-8af61d52-kafka-0 hasn't rolled
2022-03-28 13:56:43 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f} pods didn't roll. Remaining seconds for stability: 6
2022-03-28 13:56:43 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (247697ms till timeout)
2022-03-28 13:56:43 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1278604ms till timeout)
2022-03-28 13:56:43 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (243827ms till timeout)
2022-03-28 13:56:43 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-4 exec my-cluster-0444229e-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-889283546-691691584 --describe --bootstrap-server my-cluster-0444229e-kafka-bootstrap:9092
2022-03-28 13:56:43 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 13:56:43 [ForkJoinPool-1-worker-15] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 8 polls
2022-03-28 13:56:43 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (109711ms till timeout)
2022-03-28 13:56:44 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-844045c3-kafka-0=7a441eef-cc28-4155-958e-c8c61cc69438, my-cluster-844045c3-kafka-1=8a676e65-d331-46fd-b04b-b47fb87d1932, my-cluster-844045c3-kafka-2=009a7082-9aa0-4208-b610-501b2bffd9cd}
2022-03-28 13:56:44 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:44 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-844045c3-kafka-0=e24c016e-3d31-4327-92e5-6a4d2019dc61, my-cluster-844045c3-kafka-1=8a676e65-d331-46fd-b04b-b47fb87d1932, my-cluster-844045c3-kafka-2=c477a607-0ed7-44a0-a864-53d364b682a1}
2022-03-28 13:56:44 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-844045c3-kafka-0=e24c016e-3d31-4327-92e5-6a4d2019dc61, my-cluster-844045c3-kafka-1=8a676e65-d331-46fd-b04b-b47fb87d1932, my-cluster-844045c3-kafka-2=c477a607-0ed7-44a0-a864-53d364b682a1}
2022-03-28 13:56:44 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:50] At least my-cluster-844045c3-kafka-1 hasn't rolled
2022-03-28 13:56:44 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] component with name my-cluster-844045c3-kafka rolling update not ready, will try again in 5000 ms (1722220ms till timeout)
2022-03-28 13:56:44 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:44 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:44 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-8af61d52-kafka-0 hasn't rolled
2022-03-28 13:56:44 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f} pods didn't roll. Remaining seconds for stability: 5
2022-03-28 13:56:44 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (246552ms till timeout)
2022-03-28 13:56:44 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1277411ms till timeout)
2022-03-28 13:56:44 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (242720ms till timeout)
2022-03-28 13:56:44 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-0444229e-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-889283546-691691584 --describe --bootstrap-server my-cluster-0444229e-kafka-bootstrap:9092
2022-03-28 13:56:45 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:45 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:45 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:45 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-8af61d52-kafka-0 hasn't rolled
2022-03-28 13:56:45 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f} pods didn't roll. Remaining seconds for stability: 4
2022-03-28 13:56:45 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (245358ms till timeout)
2022-03-28 13:56:45 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1276265ms till timeout)
2022-03-28 13:56:45 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (241488ms till timeout)
2022-03-28 13:56:46 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:46 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:46 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:46 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-8af61d52-kafka-0 hasn't rolled
2022-03-28 13:56:46 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f} pods didn't roll. Remaining seconds for stability: 3
2022-03-28 13:56:46 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (244165ms till timeout)
2022-03-28 13:56:46 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1275073ms till timeout)
2022-03-28 13:56:46 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (240294ms till timeout)
2022-03-28 13:56:46 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testTlsExternalUser is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 13:56:47 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:47 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:47 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:47 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-8af61d52-kafka-0 hasn't rolled
2022-03-28 13:56:47 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f} pods didn't roll. Remaining seconds for stability: 2
2022-03-28 13:56:47 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (242965ms till timeout)
2022-03-28 13:56:47 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1273870ms till timeout)
2022-03-28 13:56:48 [ForkJoinPool-1-worker-1] INFO  [PodUtils:189] Message All topics created found in create-admin-my-cluster-8f6c28cb-kafka-clients-gtqvg log
2022-03-28 13:56:48 [ForkJoinPool-1-worker-1] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment create-admin-my-cluster-8f6c28cb-kafka-clients deletion
2022-03-28 13:56:48 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for ReplicaSet create-admin-my-cluster-8f6c28cb-kafka-clients to be deleted
2022-03-28 13:56:48 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] ReplicaSet create-admin-my-cluster-8f6c28cb-kafka-clients to be deleted not ready, will try again in 5000 ms (179897ms till timeout)
2022-03-28 13:56:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-309
2022-03-28 13:56:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:56:48 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-31
2022-03-28 13:56:48 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:49 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-31
2022-03-28 13:56:49 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:56:49 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-310
2022-03-28 13:56:49 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:49 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:49 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-8af61d52-kafka-0 hasn't rolled
2022-03-28 13:56:49 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f} pods didn't roll. Remaining seconds for stability: 1
2022-03-28 13:56:49 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (241772ms till timeout)
2022-03-28 13:56:49 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1272678ms till timeout)
2022-03-28 13:56:49 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-844045c3-kafka-0=7a441eef-cc28-4155-958e-c8c61cc69438, my-cluster-844045c3-kafka-1=8a676e65-d331-46fd-b04b-b47fb87d1932, my-cluster-844045c3-kafka-2=009a7082-9aa0-4208-b610-501b2bffd9cd}
2022-03-28 13:56:49 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-844045c3-kafka-0=e24c016e-3d31-4327-92e5-6a4d2019dc61, my-cluster-844045c3-kafka-1=8a676e65-d331-46fd-b04b-b47fb87d1932, my-cluster-844045c3-kafka-2=c477a607-0ed7-44a0-a864-53d364b682a1}
2022-03-28 13:56:49 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-844045c3-kafka-0=e24c016e-3d31-4327-92e5-6a4d2019dc61, my-cluster-844045c3-kafka-1=8a676e65-d331-46fd-b04b-b47fb87d1932, my-cluster-844045c3-kafka-2=c477a607-0ed7-44a0-a864-53d364b682a1}
2022-03-28 13:56:49 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:50] At least my-cluster-844045c3-kafka-1 hasn't rolled
2022-03-28 13:56:49 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] component with name my-cluster-844045c3-kafka rolling update not ready, will try again in 5000 ms (1717117ms till timeout)
2022-03-28 13:56:50 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:50 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:50 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36}
2022-03-28 13:56:50 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-8af61d52-kafka-0 hasn't rolled
2022-03-28 13:56:50 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-8af61d52-kafka-0=c9c1aba4-0055-4f7f-89dc-49233e193259, my-cluster-8af61d52-kafka-2=f475199c-adae-480e-a620-250fd4534d36, my-cluster-8af61d52-kafka-1=196c76e7-7148-48db-a153-032dc96c8d3f} pods didn't roll. Remaining seconds for stability: 0
2022-03-28 13:56:50 [ForkJoinPool-1-worker-3] INFO  [CruiseControlConfigurationST:174] Verifying new configuration in the Kafka CR
2022-03-28 13:56:50 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1271485ms till timeout)
2022-03-28 13:56:50 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Verify that kafka contains cruise control topics with related configuration.
2022-03-28 13:56:50 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Verify that kafka contains cruise control topics with related configuration.
2022-03-28 13:56:50 [ForkJoinPool-1-worker-3] DEBUG [AbstractST:674] ============================================================================
2022-03-28 13:56:50 [ForkJoinPool-1-worker-3] DEBUG [AbstractST:675] [cruisecontrol.CruiseControlConfigurationST - After Each] - Clean up after test
2022-03-28 13:56:50 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:344] ############################################################################
2022-03-28 13:56:50 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:348] Delete all resources for testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods
2022-03-28 13:56:50 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:241] Delete of Kafka my-cluster-8af61d52 in namespace namespace-2
2022-03-28 13:56:50 [ForkJoinPool-1-worker-3] INFO  [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-2, for cruise control Kafka cluster my-cluster-8af61d52
2022-03-28 13:56:51 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1270352ms till timeout)
2022-03-28 13:56:51 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-8af61d52
2022-03-28 13:56:51 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster-8af61d52 not ready, will try again in 10000 ms (839896ms till timeout)
2022-03-28 13:56:51 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testTlsExternalUser is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 13:56:52 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1269249ms till timeout)
2022-03-28 13:56:53 [ForkJoinPool-1-worker-1] DEBUG [JobUtils:40] Job create-admin-my-cluster-8f6c28cb-kafka-clients was deleted
2022-03-28 13:56:53 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:155] Create/Update Job alter-admin-my-cluster-8f6c28cb-kafka-clients in namespace throttling-quota-st
2022-03-28 13:56:53 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:alter-admin-my-cluster-8f6c28cb-kafka-clients
2022-03-28 13:56:53 [ForkJoinPool-1-worker-1] INFO  [JobUtils:81] Waiting for job: alter-admin-my-cluster-8f6c28cb-kafka-clients will be in active state
2022-03-28 13:56:53 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 13:56:53 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1268074ms till timeout)
2022-03-28 13:56:53 [ForkJoinPool-1-worker-1] INFO  [PodUtils:186] Waiting for message will be in the log
2022-03-28 13:56:53 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Waiting for message will be in the log
2022-03-28 13:56:53 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (299899ms till timeout)
2022-03-28 13:56:54 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-844045c3-kafka-0=7a441eef-cc28-4155-958e-c8c61cc69438, my-cluster-844045c3-kafka-1=8a676e65-d331-46fd-b04b-b47fb87d1932, my-cluster-844045c3-kafka-2=009a7082-9aa0-4208-b610-501b2bffd9cd}
2022-03-28 13:56:54 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-844045c3-kafka-0=e24c016e-3d31-4327-92e5-6a4d2019dc61, my-cluster-844045c3-kafka-1=4e2bb7a9-1ab6-4428-b334-4497615c4bd2, my-cluster-844045c3-kafka-2=c477a607-0ed7-44a0-a864-53d364b682a1}
2022-03-28 13:56:54 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-844045c3-kafka-0=e24c016e-3d31-4327-92e5-6a4d2019dc61, my-cluster-844045c3-kafka-1=4e2bb7a9-1ab6-4428-b334-4497615c4bd2, my-cluster-844045c3-kafka-2=c477a607-0ed7-44a0-a864-53d364b682a1}
2022-03-28 13:56:54 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:55] All pods seem to have rolled
2022-03-28 13:56:54 [ForkJoinPool-1-worker-7] INFO  [RollingUpdateUtils:86] Component with name: my-cluster-844045c3-kafka has been successfully rolled
2022-03-28 13:56:54 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:87] Component matching LabelSelector: LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-844045c3-kafka, strimzi.io/cluster=my-cluster-844045c3, strimzi.io/kind=Kafka}, additionalProperties={}) successfully rolled
2022-03-28 13:56:54 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-4 exec my-cluster-0444229e-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-889283546-691691584 --describe --bootstrap-server my-cluster-0444229e-kafka-bootstrap:9092
2022-03-28 13:56:54 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 13:56:54 [ForkJoinPool-1-worker-15] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 7 polls
2022-03-28 13:56:54 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (99049ms till timeout)
2022-03-28 13:56:54 [ForkJoinPool-1-worker-7] INFO  [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-844045c3-kafka to be ready
2022-03-28 13:56:54 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-844045c3-kafka, strimzi.io/cluster=my-cluster-844045c3, strimzi.io/kind=Kafka}, additionalProperties={})to be ready
2022-03-28 13:56:54 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-0 not ready: kafka)
2022-03-28 13:56:54 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-844045c3-kafka-1)
2022-03-28 13:56:54 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-844045c3-kafka, strimzi.io/cluster=my-cluster-844045c3, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1799901ms till timeout)
2022-03-28 13:56:54 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1266963ms till timeout)
2022-03-28 13:56:55 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (298795ms till timeout)
2022-03-28 13:56:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-310
2022-03-28 13:56:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:56:55 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-311
2022-03-28 13:56:55 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-0444229e-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-889283546-691691584 --describe --bootstrap-server my-cluster-0444229e-kafka-bootstrap:9092
2022-03-28 13:56:55 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-0 not ready: kafka)
2022-03-28 13:56:55 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-844045c3-kafka-1)
2022-03-28 13:56:55 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-844045c3-kafka, strimzi.io/cluster=my-cluster-844045c3, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1798708ms till timeout)
2022-03-28 13:56:55 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1265852ms till timeout)
2022-03-28 13:56:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-311
2022-03-28 13:56:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:56:55 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-312
2022-03-28 13:56:56 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (297638ms till timeout)
2022-03-28 13:56:56 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testTlsExternalUser is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 13:56:57 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-0 not ready: kafka)
2022-03-28 13:56:57 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-844045c3-kafka-1)
2022-03-28 13:56:57 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-844045c3-kafka, strimzi.io/cluster=my-cluster-844045c3, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1797515ms till timeout)
2022-03-28 13:56:57 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1264670ms till timeout)
2022-03-28 13:56:57 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (296536ms till timeout)
2022-03-28 13:56:58 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-0 not ready: kafka)
2022-03-28 13:56:58 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-844045c3-kafka-1)
2022-03-28 13:56:58 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-844045c3-kafka, strimzi.io/cluster=my-cluster-844045c3, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1796320ms till timeout)
2022-03-28 13:56:58 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1263475ms till timeout)
2022-03-28 13:56:58 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (295403ms till timeout)
2022-03-28 13:56:59 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-0 not ready: kafka)
2022-03-28 13:56:59 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-844045c3-kafka-1)
2022-03-28 13:56:59 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-844045c3-kafka, strimzi.io/cluster=my-cluster-844045c3, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1795125ms till timeout)
2022-03-28 13:56:59 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1262280ms till timeout)
2022-03-28 13:56:59 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (294216ms till timeout)
2022-03-28 13:57:00 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-0 not ready: kafka)
2022-03-28 13:57:00 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-844045c3-kafka-1)
2022-03-28 13:57:00 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-844045c3-kafka, strimzi.io/cluster=my-cluster-844045c3, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1793931ms till timeout)
2022-03-28 13:57:00 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1261084ms till timeout)
2022-03-28 13:57:00 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (293021ms till timeout)
2022-03-28 13:57:01 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-312
2022-03-28 13:57:01 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:01 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-313
2022-03-28 13:57:01 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-0 not ready: kafka)
2022-03-28 13:57:01 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:369] ############################################################################
2022-03-28 13:57:01 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-844045c3-kafka-1)
2022-03-28 13:57:01 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-844045c3-kafka, strimzi.io/cluster=my-cluster-844045c3, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1792699ms till timeout)
2022-03-28 13:57:01 [ForkJoinPool-1-worker-3] INFO  [TestSuiteNamespaceManager:200] Deleting namespace:namespace-2 for test case:testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods
2022-03-28 13:57:01 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testTlsExternalUser is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 13:57:02 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (291789ms till timeout)
2022-03-28 13:57:02 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1259765ms till timeout)
2022-03-28 13:57:02 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Namespace namespace-2 removal
2022-03-28 13:57:02 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 13:57:02 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-313
2022-03-28 13:57:02 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:02 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-314
2022-03-28 13:57:02 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 13:57:02 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:02 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (479550ms till timeout)
2022-03-28 13:57:02 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-314
2022-03-28 13:57:02 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:02 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-315
2022-03-28 13:57:03 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-0 not ready: kafka)
2022-03-28 13:57:03 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-844045c3-kafka-1)
2022-03-28 13:57:03 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-844045c3-kafka, strimzi.io/cluster=my-cluster-844045c3, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1791507ms till timeout)
2022-03-28 13:57:03 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1258660ms till timeout)
2022-03-28 13:57:03 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (290596ms till timeout)
2022-03-28 13:57:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-315
2022-03-28 13:57:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:03 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-316
2022-03-28 13:57:03 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 13:57:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-316
2022-03-28 13:57:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:03 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-317
2022-03-28 13:57:03 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 13:57:03 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:03 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (478073ms till timeout)
2022-03-28 13:57:04 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-0 not ready: kafka)
2022-03-28 13:57:04 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-844045c3-kafka-1)
2022-03-28 13:57:04 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-844045c3-kafka, strimzi.io/cluster=my-cluster-844045c3, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1790313ms till timeout)
2022-03-28 13:57:04 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1257467ms till timeout)
2022-03-28 13:57:04 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (289402ms till timeout)
2022-03-28 13:57:04 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-317
2022-03-28 13:57:04 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:04 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-318
2022-03-28 13:57:04 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 13:57:05 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-4 exec my-cluster-0444229e-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-889283546-691691584 --describe --bootstrap-server my-cluster-0444229e-kafka-bootstrap:9092
2022-03-28 13:57:05 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 13:57:05 [ForkJoinPool-1-worker-15] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 6 polls
2022-03-28 13:57:05 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (88639ms till timeout)
2022-03-28 13:57:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-318
2022-03-28 13:57:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:05 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-319
2022-03-28 13:57:05 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 13:57:05 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:05 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (476624ms till timeout)
2022-03-28 13:57:05 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-0 not ready: kafka)
2022-03-28 13:57:05 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-844045c3-kafka-1)
2022-03-28 13:57:05 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-844045c3-kafka, strimzi.io/cluster=my-cluster-844045c3, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1789119ms till timeout)
2022-03-28 13:57:05 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (288209ms till timeout)
2022-03-28 13:57:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-319
2022-03-28 13:57:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:05 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1256186ms till timeout)
2022-03-28 13:57:05 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-320
2022-03-28 13:57:06 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-0444229e-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-889283546-691691584 --describe --bootstrap-server my-cluster-0444229e-kafka-bootstrap:9092
2022-03-28 13:57:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-320
2022-03-28 13:57:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:06 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-321
2022-03-28 13:57:06 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 13:57:06 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-0 not ready: kafka)
2022-03-28 13:57:06 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-844045c3-kafka-1)
2022-03-28 13:57:06 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-844045c3-kafka, strimzi.io/cluster=my-cluster-844045c3, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1787923ms till timeout)
2022-03-28 13:57:06 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1255074ms till timeout)
2022-03-28 13:57:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-321
2022-03-28 13:57:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:06 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-322
2022-03-28 13:57:06 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (287011ms till timeout)
2022-03-28 13:57:06 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 13:57:06 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:06 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (475156ms till timeout)
2022-03-28 13:57:06 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testTlsExternalUser is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 13:57:07 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-322
2022-03-28 13:57:07 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:07 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-323
2022-03-28 13:57:07 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 13:57:07 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-0 not ready: kafka)
2022-03-28 13:57:07 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-844045c3-kafka-1)
2022-03-28 13:57:07 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-844045c3-kafka, strimzi.io/cluster=my-cluster-844045c3, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1786730ms till timeout)
2022-03-28 13:57:07 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1253884ms till timeout)
2022-03-28 13:57:08 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (285820ms till timeout)
2022-03-28 13:57:08 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-323
2022-03-28 13:57:08 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:08 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-324
2022-03-28 13:57:08 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 13:57:08 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:08 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (473689ms till timeout)
2022-03-28 13:57:08 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-324
2022-03-28 13:57:08 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:08 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-325
2022-03-28 13:57:09 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-0 not ready: kafka)
2022-03-28 13:57:09 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-844045c3-kafka-1)
2022-03-28 13:57:09 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-844045c3-kafka, strimzi.io/cluster=my-cluster-844045c3, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1785536ms till timeout)
2022-03-28 13:57:09 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1252691ms till timeout)
2022-03-28 13:57:09 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-325
2022-03-28 13:57:09 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:09 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-326
2022-03-28 13:57:09 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (284626ms till timeout)
2022-03-28 13:57:09 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 13:57:09 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-326
2022-03-28 13:57:09 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:09 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-327
2022-03-28 13:57:09 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 13:57:09 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:09 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (472257ms till timeout)
2022-03-28 13:57:09 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-4 exec my-cluster-0444229e-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-889283546-691691584 --describe --bootstrap-server my-cluster-0444229e-kafka-bootstrap:9092
2022-03-28 13:57:09 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 13:57:09 [ForkJoinPool-1-worker-15] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 5 polls
2022-03-28 13:57:09 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (83769ms till timeout)
2022-03-28 13:57:10 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-327
2022-03-28 13:57:10 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:10 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-328
2022-03-28 13:57:10 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-0 not ready: kafka)
2022-03-28 13:57:10 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-844045c3-kafka-1)
2022-03-28 13:57:10 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-844045c3-kafka, strimzi.io/cluster=my-cluster-844045c3, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1784345ms till timeout)
2022-03-28 13:57:10 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1251463ms till timeout)
2022-03-28 13:57:10 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (283434ms till timeout)
2022-03-28 13:57:10 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-328
2022-03-28 13:57:10 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:10 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-329
2022-03-28 13:57:10 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 13:57:10 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-0444229e-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-889283546-691691584 --describe --bootstrap-server my-cluster-0444229e-kafka-bootstrap:9092
2022-03-28 13:57:11 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 13:57:11 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:11 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (470804ms till timeout)
2022-03-28 13:57:11 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-329
2022-03-28 13:57:11 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:11 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-330
2022-03-28 13:57:11 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-0 not ready: kafka)
2022-03-28 13:57:11 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-844045c3-kafka-1)
2022-03-28 13:57:11 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-844045c3-kafka, strimzi.io/cluster=my-cluster-844045c3, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1783154ms till timeout)
2022-03-28 13:57:11 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1250271ms till timeout)
2022-03-28 13:57:11 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (282243ms till timeout)
2022-03-28 13:57:11 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-330
2022-03-28 13:57:11 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:11 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-331
2022-03-28 13:57:11 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testTlsExternalUser is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 13:57:12 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 13:57:12 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-331
2022-03-28 13:57:12 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:12 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-332
2022-03-28 13:57:12 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-0 not ready: kafka)
2022-03-28 13:57:12 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-844045c3-kafka-1)
2022-03-28 13:57:12 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-844045c3-kafka, strimzi.io/cluster=my-cluster-844045c3, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1781962ms till timeout)
2022-03-28 13:57:12 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1249080ms till timeout)
2022-03-28 13:57:12 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 13:57:12 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:12 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (469319ms till timeout)
2022-03-28 13:57:12 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (281052ms till timeout)
2022-03-28 13:57:12 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-332
2022-03-28 13:57:12 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:12 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-333
2022-03-28 13:57:13 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-333
2022-03-28 13:57:13 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:13 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-334
2022-03-28 13:57:13 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 13:57:13 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-0 not ready: kafka)
2022-03-28 13:57:13 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-1 not ready: kafka)
2022-03-28 13:57:13 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-2 not ready: kafka)
2022-03-28 13:57:13 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:106] Pods my-cluster-844045c3-kafka-0, my-cluster-844045c3-kafka-1, my-cluster-844045c3-kafka-2 are ready
2022-03-28 13:57:13 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-844045c3-kafka, strimzi.io/cluster=my-cluster-844045c3, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1780765ms till timeout)
2022-03-28 13:57:13 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1247917ms till timeout)
2022-03-28 13:57:13 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (279854ms till timeout)
2022-03-28 13:57:14 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-334
2022-03-28 13:57:14 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:14 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-335
2022-03-28 13:57:14 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 13:57:14 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:14 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (467858ms till timeout)
2022-03-28 13:57:14 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-335
2022-03-28 13:57:14 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:14 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-336
2022-03-28 13:57:15 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-0 not ready: kafka)
2022-03-28 13:57:15 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-1 not ready: kafka)
2022-03-28 13:57:15 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-2 not ready: kafka)
2022-03-28 13:57:15 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:106] Pods my-cluster-844045c3-kafka-0, my-cluster-844045c3-kafka-1, my-cluster-844045c3-kafka-2 are ready
2022-03-28 13:57:15 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-844045c3-kafka, strimzi.io/cluster=my-cluster-844045c3, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1779569ms till timeout)
2022-03-28 13:57:15 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1246725ms till timeout)
2022-03-28 13:57:15 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-4 exec my-cluster-0444229e-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-889283546-691691584 --describe --bootstrap-server my-cluster-0444229e-kafka-bootstrap:9092
2022-03-28 13:57:15 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 13:57:15 [ForkJoinPool-1-worker-15] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 4 polls
2022-03-28 13:57:15 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (78507ms till timeout)
2022-03-28 13:57:15 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (278660ms till timeout)
2022-03-28 13:57:15 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 13:57:15 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-336
2022-03-28 13:57:15 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:15 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-337
2022-03-28 13:57:15 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 13:57:15 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 1
2022-03-28 13:57:15 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 13:57:15 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Error from server (NotFound): namespaces "namespace-2" not found
2022-03-28 13:57:15 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] ======STDERR END======
2022-03-28 13:57:15 [ForkJoinPool-1-worker-3] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[], io.strimzi.test.logs.CollectorElement@aec142ef=[], io.strimzi.test.logs.CollectorElement@f851b6c3=[namespace-6], io.strimzi.test.logs.CollectorElement@3881d5f2=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[namespace-4], io.strimzi.test.logs.CollectorElement@f4405b3b=[namespace-5], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 13:57:15 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:267] testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods - Notifies waiting test cases:[testCapacityFile, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testThrottlingQuotasCreateAlterPartitions, testSendSimpleMessageTls, testReceiveSimpleMessageTlsScramSha, testSendSimpleMessageTlsScramSha, testReceiveSimpleMessageTls, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testConfigurationFileIsCreated, testPauseReconciliationInKafkaRebalanceAndTopic, testTlsExternalUser] to and randomly select one to start execution
2022-03-28 13:57:15 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:93] [cruisecontrol.CruiseControlConfigurationST] - Removing parallel test: testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods
2022-03-28 13:57:15 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:97] [cruisecontrol.CruiseControlConfigurationST] - Parallel test count: 6
2022-03-28 13:57:15 [ForkJoinPool-1-worker-3] INFO  [TestSeparator:29] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods-FINISHED
2022-03-28 13:57:15 [ForkJoinPool-1-worker-3] INFO  [TestSeparator:30] ############################################################################
2022-03-28 13:57:15 [ForkJoinPool-1-worker-3] INFO  [TestSeparator:23] ############################################################################
2022-03-28 13:57:15 [ForkJoinPool-1-worker-3] INFO  [TestSeparator:24] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testConfigurationReflection-STARTED
2022-03-28 13:57:15 [ForkJoinPool-1-worker-3] DEBUG [AbstractST:658] ============================================================================
2022-03-28 13:57:15 [ForkJoinPool-1-worker-3] DEBUG [AbstractST:659] [cruisecontrol.CruiseControlConfigurationST - Before Each] - Setup test case environment
2022-03-28 13:57:15 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:77] [cruisecontrol.CruiseControlConfigurationST] - Adding parallel test: testConfigurationReflection
2022-03-28 13:57:15 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:81] [cruisecontrol.CruiseControlConfigurationST] - Parallel test count: 7
2022-03-28 13:57:15 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:205] [testConfigurationReflection] moved to the WaitZone, because current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 13:57:15 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testConfigurationReflection is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 13:57:15 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-337
2022-03-28 13:57:15 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:15 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-338
2022-03-28 13:57:16 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-0444229e-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-889283546-691691584 --describe --bootstrap-server my-cluster-0444229e-kafka-bootstrap:9092
2022-03-28 13:57:16 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-0 not ready: kafka)
2022-03-28 13:57:16 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-1 not ready: kafka)
2022-03-28 13:57:16 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-2 not ready: kafka)
2022-03-28 13:57:16 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:106] Pods my-cluster-844045c3-kafka-0, my-cluster-844045c3-kafka-1, my-cluster-844045c3-kafka-2 are ready
2022-03-28 13:57:16 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-844045c3-kafka, strimzi.io/cluster=my-cluster-844045c3, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1778377ms till timeout)
2022-03-28 13:57:16 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1245530ms till timeout)
2022-03-28 13:57:16 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (277466ms till timeout)
2022-03-28 13:57:16 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-338
2022-03-28 13:57:16 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:16 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-339
2022-03-28 13:57:16 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:230] testTlsExternalUser test now can proceed its execution
2022-03-28 13:57:16 [ForkJoinPool-1-worker-13] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 13:57:16 [ForkJoinPool-1-worker-13] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testScramUserWithQuotas=my-cluster-6f653060, testReceiveSimpleMessageTlsScramSha=my-cluster-af298d01, testConfigurationFileIsCreated=my-cluster-3244df0d, testThrottlingQuotasCreateAlterPartitions=my-cluster-8f6c28cb, testSendSimpleMessageTls=my-cluster-fbc845dd, testCreatingUsersWithSecretPrefix=my-cluster-0c65d2dd, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-0444229e, testUserWithNameMoreThan64Chars=my-cluster-02e25de6, testDeployAndUnDeployCruiseControl=my-cluster-844045c3, testTlsUserWithQuotas=my-cluster-9832c9ae, testThrottlingQuotasCreateTopic=my-cluster-ac2cadd6, testCapacityFile=my-cluster-0531d5ce, testReceiveSimpleMessageTls=my-cluster-ace1fa24, testConfigurationPerformanceOptions=my-cluster-4994018d, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-8af61d52, testTlsExternalUser=my-cluster-43f83aef, testSendSimpleMessageTlsScramSha=my-cluster-e353214b}
2022-03-28 13:57:16 [ForkJoinPool-1-worker-13] TRACE [AbstractST:607] USERS_NAME_MAP: {testScramUserWithQuotas=my-user-27563760-1632863338, testReceiveSimpleMessageTlsScramSha=my-user-1597210213-707359243, testConfigurationFileIsCreated=my-user-1799841978-1468805631, testThrottlingQuotasCreateAlterPartitions=my-user-1557959119-1722636050, testSendSimpleMessageTls=my-user-1754673553-2077518027, testCreatingUsersWithSecretPrefix=my-user-144001639-226807213, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-1010649803-214635564, testUserWithNameMoreThan64Chars=my-user-2124392695-340403978, testDeployAndUnDeployCruiseControl=my-user-2117120687-604608920, testTlsUserWithQuotas=my-user-1602931187-443256751, testThrottlingQuotasCreateTopic=my-user-46611685-1079680568, testCapacityFile=my-user-155969448-1270555822, testReceiveSimpleMessageTls=my-user-1418772225-1024845009, testConfigurationPerformanceOptions=my-user-882948629-1821489961, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-1807142870-188660517, testTlsExternalUser=my-user-370513135-1989152728, testSendSimpleMessageTlsScramSha=my-user-182357028-1132873010}
2022-03-28 13:57:16 [ForkJoinPool-1-worker-13] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testScramUserWithQuotas=my-topic-1345794255-226872460, testReceiveSimpleMessageTlsScramSha=my-topic-649423016-1623551209, testConfigurationFileIsCreated=my-topic-1435014906-681104826, testThrottlingQuotasCreateAlterPartitions=my-topic-1449977352-344480794, testSendSimpleMessageTls=my-topic-115477502-2025274778, testCreatingUsersWithSecretPrefix=my-topic-130462979-2061578809, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-889283546-691691584, testUserWithNameMoreThan64Chars=my-topic-1053060011-1199293498, testDeployAndUnDeployCruiseControl=my-topic-1101175281-2107378523, testTlsUserWithQuotas=my-topic-985048738-437665866, testThrottlingQuotasCreateTopic=my-topic-114922478-1506136333, testCapacityFile=my-topic-296605537-1008743104, testReceiveSimpleMessageTls=my-topic-1131768783-120051821, testConfigurationPerformanceOptions=my-topic-627301835-843876360, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-1367124108-1748498395, testTlsExternalUser=my-topic-2081139838-1814296134, testSendSimpleMessageTlsScramSha=my-topic-652506937-1802310709}
2022-03-28 13:57:16 [ForkJoinPool-1-worker-13] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testScramUserWithQuotas=my-cluster-6f653060-kafka-clients, testReceiveSimpleMessageTlsScramSha=my-cluster-af298d01-kafka-clients, testConfigurationFileIsCreated=my-cluster-3244df0d-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-8f6c28cb-kafka-clients, testSendSimpleMessageTls=my-cluster-fbc845dd-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-0c65d2dd-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-0444229e-kafka-clients, testUserWithNameMoreThan64Chars=my-cluster-02e25de6-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-844045c3-kafka-clients, testTlsUserWithQuotas=my-cluster-9832c9ae-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-ac2cadd6-kafka-clients, testCapacityFile=my-cluster-0531d5ce-kafka-clients, testReceiveSimpleMessageTls=my-cluster-ace1fa24-kafka-clients, testConfigurationPerformanceOptions=my-cluster-4994018d-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-8af61d52-kafka-clients, testTlsExternalUser=my-cluster-43f83aef-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-e353214b-kafka-clients}
2022-03-28 13:57:16 [ForkJoinPool-1-worker-13] INFO  [TestSuiteNamespaceManager:163] Creating namespace:namespace-7 for test case:testTlsExternalUser
2022-03-28 13:57:17 [ForkJoinPool-1-worker-13] INFO  [KubeClusterResource:156] Creating Namespace: namespace-7
2022-03-28 13:57:17 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-339
2022-03-28 13:57:17 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:17 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-345
2022-03-28 13:57:17 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Namespace namespace-7
2022-03-28 13:57:17 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace namespace-7 -o json
2022-03-28 13:57:17 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-0 not ready: kafka)
2022-03-28 13:57:17 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-1 not ready: kafka)
2022-03-28 13:57:17 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-2 not ready: kafka)
2022-03-28 13:57:17 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:106] Pods my-cluster-844045c3-kafka-0, my-cluster-844045c3-kafka-1, my-cluster-844045c3-kafka-2 are ready
2022-03-28 13:57:17 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-844045c3-kafka, strimzi.io/cluster=my-cluster-844045c3, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1777168ms till timeout)
2022-03-28 13:57:17 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1244324ms till timeout)
2022-03-28 13:57:17 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (276274ms till timeout)
2022-03-28 13:57:17 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-345
2022-03-28 13:57:17 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:17 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-348
2022-03-28 13:57:17 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace namespace-7 -o json
2022-03-28 13:57:17 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:17 [ForkJoinPool-1-worker-13] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c32,c9",
            "openshift.io/sa.scc.supplemental-groups": "1001010000/10000",
            "openshift.io/sa.scc.uid-range": "1001010000/10000"
        },
        "creationTimestamp": "2022-03-28T13:57:12Z",
        "labels": {
            "kubernetes.io/metadata.name": "namespace-7"
        },
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T13:57:12Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:labels": {
                            ".": {},
                            "f:kubernetes.io/metadata.name": {}
                        }
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T13:57:12Z"
            }
        ],
        "name": "namespace-7",
        "resourceVersion": "92399",
        "uid": "96eaae23-9c5b-44dc-83ed-490833ceb971"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 13:57:17 [ForkJoinPool-1-worker-13] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[], io.strimzi.test.logs.CollectorElement@aec142ef=[], io.strimzi.test.logs.CollectorElement@f851b6c3=[namespace-6], io.strimzi.test.logs.CollectorElement@3881d5f2=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@5c7379cb=[namespace-7], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[namespace-4], io.strimzi.test.logs.CollectorElement@f4405b3b=[namespace-5], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 13:57:17 [ForkJoinPool-1-worker-13] INFO  [KubeClusterResource:82] Client use Namespace: namespace-7
2022-03-28 13:57:17 [ForkJoinPool-1-worker-13] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-7, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 13:57:17 [ForkJoinPool-1-worker-13] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-7
2022-03-28 13:57:17 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:155] Create/Update Kafka my-cluster-43f83aef in namespace namespace-7
2022-03-28 13:57:17 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:164] Using Namespace: namespace-7
2022-03-28 13:57:17 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-43f83aef
2022-03-28 13:57:18 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:433] Wait for Kafka: my-cluster-43f83aef will have desired state: Ready
2022-03-28 13:57:18 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Kafka: my-cluster-43f83aef will have desired state: Ready
2022-03-28 13:57:18 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-43f83aef will have desired state: Ready not ready, will try again in 1000 ms (839902ms till timeout)
2022-03-28 13:57:18 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-348
2022-03-28 13:57:18 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:18 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-350
2022-03-28 13:57:18 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-0 not ready: kafka)
2022-03-28 13:57:18 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-1 not ready: kafka)
2022-03-28 13:57:18 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-2 not ready: kafka)
2022-03-28 13:57:18 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:106] Pods my-cluster-844045c3-kafka-0, my-cluster-844045c3-kafka-1, my-cluster-844045c3-kafka-2 are ready
2022-03-28 13:57:18 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-844045c3-kafka, strimzi.io/cluster=my-cluster-844045c3, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1775975ms till timeout)
2022-03-28 13:57:18 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1243129ms till timeout)
2022-03-28 13:57:18 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (275064ms till timeout)
2022-03-28 13:57:18 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-350
2022-03-28 13:57:18 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:18 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-351
2022-03-28 13:57:19 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-43f83aef will have desired state: Ready not ready, will try again in 1000 ms (838803ms till timeout)
2022-03-28 13:57:19 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-351
2022-03-28 13:57:19 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:19 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-352
2022-03-28 13:57:19 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-0 not ready: kafka)
2022-03-28 13:57:19 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-1 not ready: kafka)
2022-03-28 13:57:19 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-2 not ready: kafka)
2022-03-28 13:57:19 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:106] Pods my-cluster-844045c3-kafka-0, my-cluster-844045c3-kafka-1, my-cluster-844045c3-kafka-2 are ready
2022-03-28 13:57:19 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-844045c3-kafka, strimzi.io/cluster=my-cluster-844045c3, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1774782ms till timeout)
2022-03-28 13:57:19 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1241933ms till timeout)
2022-03-28 13:57:19 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (273872ms till timeout)
2022-03-28 13:57:20 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-352
2022-03-28 13:57:20 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:20 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-353
2022-03-28 13:57:20 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-4 exec my-cluster-0444229e-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-889283546-691691584 --describe --bootstrap-server my-cluster-0444229e-kafka-bootstrap:9092
2022-03-28 13:57:20 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 13:57:20 [ForkJoinPool-1-worker-15] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 3 polls
2022-03-28 13:57:20 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (73566ms till timeout)
2022-03-28 13:57:20 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-43f83aef will have desired state: Ready not ready, will try again in 1000 ms (837701ms till timeout)
2022-03-28 13:57:20 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-353
2022-03-28 13:57:20 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:20 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-354
2022-03-28 13:57:20 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testConfigurationReflection is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 13:57:21 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-0 not ready: kafka)
2022-03-28 13:57:21 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-1 not ready: kafka)
2022-03-28 13:57:21 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-2 not ready: kafka)
2022-03-28 13:57:21 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:106] Pods my-cluster-844045c3-kafka-0, my-cluster-844045c3-kafka-1, my-cluster-844045c3-kafka-2 are ready
2022-03-28 13:57:21 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-844045c3-kafka, strimzi.io/cluster=my-cluster-844045c3, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1773588ms till timeout)
2022-03-28 13:57:21 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1240742ms till timeout)
2022-03-28 13:57:21 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-0444229e-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-889283546-691691584 --describe --bootstrap-server my-cluster-0444229e-kafka-bootstrap:9092
2022-03-28 13:57:21 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (272678ms till timeout)
2022-03-28 13:57:21 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-354
2022-03-28 13:57:21 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:21 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-355
2022-03-28 13:57:21 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-43f83aef will have desired state: Ready not ready, will try again in 1000 ms (836602ms till timeout)
2022-03-28 13:57:21 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-355
2022-03-28 13:57:21 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:21 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-363
2022-03-28 13:57:22 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-0 not ready: kafka)
2022-03-28 13:57:22 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-1 not ready: kafka)
2022-03-28 13:57:22 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-2 not ready: kafka)
2022-03-28 13:57:22 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:106] Pods my-cluster-844045c3-kafka-0, my-cluster-844045c3-kafka-1, my-cluster-844045c3-kafka-2 are ready
2022-03-28 13:57:22 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-844045c3-kafka, strimzi.io/cluster=my-cluster-844045c3, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1772393ms till timeout)
2022-03-28 13:57:22 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1239549ms till timeout)
2022-03-28 13:57:22 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (271483ms till timeout)
2022-03-28 13:57:22 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-363
2022-03-28 13:57:22 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:22 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-365
2022-03-28 13:57:22 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-43f83aef will have desired state: Ready not ready, will try again in 1000 ms (835504ms till timeout)
2022-03-28 13:57:22 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-365
2022-03-28 13:57:22 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:22 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-368
2022-03-28 13:57:23 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-368
2022-03-28 13:57:23 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:23 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-37
2022-03-28 13:57:23 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-0 not ready: kafka)
2022-03-28 13:57:23 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-1 not ready: kafka)
2022-03-28 13:57:23 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-2 not ready: kafka)
2022-03-28 13:57:23 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:106] Pods my-cluster-844045c3-kafka-0, my-cluster-844045c3-kafka-1, my-cluster-844045c3-kafka-2 are ready
2022-03-28 13:57:23 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-844045c3-kafka, strimzi.io/cluster=my-cluster-844045c3, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1771201ms till timeout)
2022-03-28 13:57:23 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1238354ms till timeout)
2022-03-28 13:57:23 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (270291ms till timeout)
2022-03-28 13:57:23 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-43f83aef will have desired state: Ready not ready, will try again in 1000 ms (834404ms till timeout)
2022-03-28 13:57:24 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-37
2022-03-28 13:57:24 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:24 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-372
2022-03-28 13:57:24 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-372
2022-03-28 13:57:24 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:24 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-378
2022-03-28 13:57:24 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-0 not ready: kafka)
2022-03-28 13:57:24 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-1 not ready: kafka)
2022-03-28 13:57:24 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-2 not ready: kafka)
2022-03-28 13:57:24 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:106] Pods my-cluster-844045c3-kafka-0, my-cluster-844045c3-kafka-1, my-cluster-844045c3-kafka-2 are ready
2022-03-28 13:57:24 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-844045c3-kafka, strimzi.io/cluster=my-cluster-844045c3, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1770008ms till timeout)
2022-03-28 13:57:24 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1237127ms till timeout)
2022-03-28 13:57:24 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (269098ms till timeout)
2022-03-28 13:57:24 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-43f83aef will have desired state: Ready not ready, will try again in 1000 ms (833285ms till timeout)
2022-03-28 13:57:25 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-378
2022-03-28 13:57:25 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:25 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-380
2022-03-28 13:57:25 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-380
2022-03-28 13:57:25 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:25 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-382
2022-03-28 13:57:25 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testConfigurationReflection is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 13:57:25 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-0 not ready: kafka)
2022-03-28 13:57:25 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-1 not ready: kafka)
2022-03-28 13:57:25 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-2 not ready: kafka)
2022-03-28 13:57:25 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:106] Pods my-cluster-844045c3-kafka-0, my-cluster-844045c3-kafka-1, my-cluster-844045c3-kafka-2 are ready
2022-03-28 13:57:25 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1235971ms till timeout)
2022-03-28 13:57:25 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (267906ms till timeout)
2022-03-28 13:57:25 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-43f83aef will have desired state: Ready not ready, will try again in 1000 ms (832097ms till timeout)
2022-03-28 13:57:26 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-4 exec my-cluster-0444229e-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-889283546-691691584 --describe --bootstrap-server my-cluster-0444229e-kafka-bootstrap:9092
2022-03-28 13:57:26 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 13:57:26 [ForkJoinPool-1-worker-15] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 2 polls
2022-03-28 13:57:26 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (67626ms till timeout)
2022-03-28 13:57:26 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:433] Wait for Kafka: my-cluster-844045c3 will have desired state: Ready
2022-03-28 13:57:26 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Kafka: my-cluster-844045c3 will have desired state: Ready
2022-03-28 13:57:26 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-382
2022-03-28 13:57:26 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:26 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-383
2022-03-28 13:57:26 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:444] Kafka: my-cluster-844045c3 is in desired state: Ready
2022-03-28 13:57:26 [ForkJoinPool-1-worker-7] INFO  [RollingUpdateUtils:132] Kafka: my-cluster-844045c3 is ready
2022-03-28 13:57:26 [ForkJoinPool-1-worker-7] INFO  [CruiseControlConfigurationST:117] Verifying that in Cruise Control is not present in the Kafka cluster
2022-03-28 13:57:26 [ForkJoinPool-1-worker-7] INFO  [CruiseControlConfigurationST:120] Verifying that my-cluster-844045c3-cruise-control- pod is not present
2022-03-28 13:57:26 [ForkJoinPool-1-worker-7] INFO  [PodUtils:209] Wait until Pod my-cluster-844045c3-cruise-control- will have stable 0 replicas
2022-03-28 13:57:26 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for  Podmy-cluster-844045c3-cruise-control- will have 0 replicas
2022-03-28 13:57:26 [ForkJoinPool-1-worker-7] INFO  [PodUtils:225] Pod replicas gonna be stable in 19 polls
2022-03-28 13:57:26 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176]  Podmy-cluster-844045c3-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (179808ms till timeout)
2022-03-28 13:57:26 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-383
2022-03-28 13:57:26 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:26 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-387
2022-03-28 13:57:26 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1234873ms till timeout)
2022-03-28 13:57:27 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-43f83aef will have desired state: Ready not ready, will try again in 1000 ms (830990ms till timeout)
2022-03-28 13:57:27 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (266797ms till timeout)
2022-03-28 13:57:27 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-0444229e-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-889283546-691691584 --describe --bootstrap-server my-cluster-0444229e-kafka-bootstrap:9092
2022-03-28 13:57:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-387
2022-03-28 13:57:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:27 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-388
2022-03-28 13:57:27 [ForkJoinPool-1-worker-7] INFO  [PodUtils:225] Pod replicas gonna be stable in 18 polls
2022-03-28 13:57:27 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176]  Podmy-cluster-844045c3-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (178614ms till timeout)
2022-03-28 13:57:28 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-388
2022-03-28 13:57:28 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:28 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-389
2022-03-28 13:57:28 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1233772ms till timeout)
2022-03-28 13:57:28 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-43f83aef will have desired state: Ready not ready, will try again in 1000 ms (829889ms till timeout)
2022-03-28 13:57:28 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (265597ms till timeout)
2022-03-28 13:57:28 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-389
2022-03-28 13:57:28 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:28 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-393
2022-03-28 13:57:29 [ForkJoinPool-1-worker-7] INFO  [PodUtils:225] Pod replicas gonna be stable in 17 polls
2022-03-28 13:57:29 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176]  Podmy-cluster-844045c3-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (177420ms till timeout)
2022-03-28 13:57:29 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1232674ms till timeout)
2022-03-28 13:57:29 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-393
2022-03-28 13:57:29 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:29 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-394
2022-03-28 13:57:29 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-43f83aef will have desired state: Ready not ready, will try again in 1000 ms (828792ms till timeout)
2022-03-28 13:57:29 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (264496ms till timeout)
2022-03-28 13:57:29 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-394
2022-03-28 13:57:29 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:29 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-396
2022-03-28 13:57:30 [ForkJoinPool-1-worker-7] INFO  [PodUtils:225] Pod replicas gonna be stable in 16 polls
2022-03-28 13:57:30 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176]  Podmy-cluster-844045c3-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (176227ms till timeout)
2022-03-28 13:57:30 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1231487ms till timeout)
2022-03-28 13:57:30 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-43f83aef will have desired state: Ready not ready, will try again in 1000 ms (827688ms till timeout)
2022-03-28 13:57:30 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-396
2022-03-28 13:57:30 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:30 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-398
2022-03-28 13:57:30 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (263390ms till timeout)
2022-03-28 13:57:30 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testConfigurationReflection is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 13:57:30 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-398
2022-03-28 13:57:30 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:30 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-404
2022-03-28 13:57:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-404
2022-03-28 13:57:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:31 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-405
2022-03-28 13:57:31 [ForkJoinPool-1-worker-7] INFO  [PodUtils:225] Pod replicas gonna be stable in 15 polls
2022-03-28 13:57:31 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176]  Podmy-cluster-844045c3-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (174996ms till timeout)
2022-03-28 13:57:31 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1230295ms till timeout)
2022-03-28 13:57:31 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-43f83aef will have desired state: Ready not ready, will try again in 1000 ms (826504ms till timeout)
2022-03-28 13:57:31 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (262219ms till timeout)
2022-03-28 13:57:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-405
2022-03-28 13:57:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:31 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-406
2022-03-28 13:57:32 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-406
2022-03-28 13:57:32 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:32 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-41
2022-03-28 13:57:32 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-43f83aef will have desired state: Ready not ready, will try again in 1000 ms (825275ms till timeout)
2022-03-28 13:57:32 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1229061ms till timeout)
2022-03-28 13:57:32 [ForkJoinPool-1-worker-7] INFO  [PodUtils:225] Pod replicas gonna be stable in 14 polls
2022-03-28 13:57:32 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176]  Podmy-cluster-844045c3-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (173710ms till timeout)
2022-03-28 13:57:32 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (260993ms till timeout)
2022-03-28 13:57:32 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-4 exec my-cluster-0444229e-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-889283546-691691584 --describe --bootstrap-server my-cluster-0444229e-kafka-bootstrap:9092
2022-03-28 13:57:32 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 13:57:32 [ForkJoinPool-1-worker-15] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 1 polls
2022-03-28 13:57:32 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (60783ms till timeout)
2022-03-28 13:57:33 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-41
2022-03-28 13:57:33 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:33 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-412
2022-03-28 13:57:33 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-412
2022-03-28 13:57:33 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:33 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-420
2022-03-28 13:57:33 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-43f83aef will have desired state: Ready not ready, will try again in 1000 ms (824172ms till timeout)
2022-03-28 13:57:33 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1227958ms till timeout)
2022-03-28 13:57:33 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-0444229e-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-889283546-691691584 --describe --bootstrap-server my-cluster-0444229e-kafka-bootstrap:9092
2022-03-28 13:57:34 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (259703ms till timeout)
2022-03-28 13:57:34 [ForkJoinPool-1-worker-7] INFO  [PodUtils:225] Pod replicas gonna be stable in 13 polls
2022-03-28 13:57:34 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176]  Podmy-cluster-844045c3-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (172329ms till timeout)
2022-03-28 13:57:34 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-420
2022-03-28 13:57:34 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:34 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-421
2022-03-28 13:57:34 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-421
2022-03-28 13:57:34 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:34 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-422
2022-03-28 13:57:34 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1226859ms till timeout)
2022-03-28 13:57:35 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-43f83aef will have desired state: Ready not ready, will try again in 1000 ms (822976ms till timeout)
2022-03-28 13:57:35 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-422
2022-03-28 13:57:35 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:35 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-423
2022-03-28 13:57:35 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (258602ms till timeout)
2022-03-28 13:57:35 [ForkJoinPool-1-worker-7] INFO  [PodUtils:225] Pod replicas gonna be stable in 12 polls
2022-03-28 13:57:35 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176]  Podmy-cluster-844045c3-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (171039ms till timeout)
2022-03-28 13:57:35 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testConfigurationReflection is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 13:57:35 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-423
2022-03-28 13:57:35 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:35 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-424
2022-03-28 13:57:36 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1225760ms till timeout)
2022-03-28 13:57:36 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-43f83aef will have desired state: Ready not ready, will try again in 1000 ms (821879ms till timeout)
2022-03-28 13:57:36 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-424
2022-03-28 13:57:36 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:36 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-425
2022-03-28 13:57:36 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (257502ms till timeout)
2022-03-28 13:57:36 [ForkJoinPool-1-worker-7] INFO  [PodUtils:225] Pod replicas gonna be stable in 11 polls
2022-03-28 13:57:36 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176]  Podmy-cluster-844045c3-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (169756ms till timeout)
2022-03-28 13:57:36 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-425
2022-03-28 13:57:36 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:36 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-426
2022-03-28 13:57:37 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1224661ms till timeout)
2022-03-28 13:57:37 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-43f83aef will have desired state: Ready not ready, will try again in 1000 ms (820778ms till timeout)
2022-03-28 13:57:37 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-426
2022-03-28 13:57:37 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:37 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-427
2022-03-28 13:57:37 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-4 exec my-cluster-0444229e-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-889283546-691691584 --describe --bootstrap-server my-cluster-0444229e-kafka-bootstrap:9092
2022-03-28 13:57:37 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 13:57:37 [ForkJoinPool-1-worker-15] INFO  [KafkaTopicUtils:197] KafkaTopic's spec is stable for 20 polls intervals
2022-03-28 13:57:37 [ForkJoinPool-1-worker-15] INFO  [ReconciliationST:156] Setting annotation to "false", partitions should be scaled to 4
2022-03-28 13:57:37 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (256398ms till timeout)
2022-03-28 13:57:37 [ForkJoinPool-1-worker-15] INFO  [KafkaTopicUtils:124] Waiting for KafkaTopic change my-topic-889283546-691691584
2022-03-28 13:57:37 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for KafkaTopic change my-topic-889283546-691691584
2022-03-28 13:57:37 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:155] Create/Update KafkaRebalance my-cluster-0444229e in namespace namespace-7
2022-03-28 13:57:37 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:164] Using Namespace: namespace-4
2022-03-28 13:57:37 [ForkJoinPool-1-worker-15] WARN  [VersionUsageUtils:60] The client is using resource type 'kafkarebalances' with unstable version 'v1beta2'
2022-03-28 13:57:37 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-427
2022-03-28 13:57:37 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:37 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-428
2022-03-28 13:57:37 [ForkJoinPool-1-worker-7] INFO  [PodUtils:225] Pod replicas gonna be stable in 10 polls
2022-03-28 13:57:37 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176]  Podmy-cluster-844045c3-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (168532ms till timeout)
2022-03-28 13:57:37 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaRebalance:my-cluster-0444229e
2022-03-28 13:57:38 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:433] Wait for KafkaRebalance: my-cluster-0444229e will have desired state: PendingProposal
2022-03-28 13:57:38 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for KafkaRebalance: my-cluster-0444229e will have desired state: PendingProposal
2022-03-28 13:57:38 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: PendingProposal not ready, will try again in 1000 ms (359904ms till timeout)
2022-03-28 13:57:38 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1223485ms till timeout)
2022-03-28 13:57:38 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-428
2022-03-28 13:57:38 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:38 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-429
2022-03-28 13:57:38 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-43f83aef will have desired state: Ready not ready, will try again in 1000 ms (819603ms till timeout)
2022-03-28 13:57:38 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (255297ms till timeout)
2022-03-28 13:57:38 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-429
2022-03-28 13:57:38 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:38 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-43
2022-03-28 13:57:39 [ForkJoinPool-1-worker-7] INFO  [PodUtils:225] Pod replicas gonna be stable in 9 polls
2022-03-28 13:57:39 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176]  Podmy-cluster-844045c3-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (167339ms till timeout)
2022-03-28 13:57:39 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:444] KafkaRebalance: my-cluster-0444229e is in desired state: PendingProposal
2022-03-28 13:57:39 [ForkJoinPool-1-worker-15] INFO  [ReconciliationST:164] Waiting for ProposalReady, then add pause and rebalance annotation, rebalancing should not be triggered
2022-03-28 13:57:39 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1222386ms till timeout)
2022-03-28 13:57:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-43
2022-03-28 13:57:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:39 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-430
2022-03-28 13:57:39 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:433] Wait for KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady
2022-03-28 13:57:39 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady
2022-03-28 13:57:39 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-43f83aef will have desired state: Ready not ready, will try again in 1000 ms (818504ms till timeout)
2022-03-28 13:57:39 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (599903ms till timeout)
2022-03-28 13:57:39 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (254117ms till timeout)
2022-03-28 13:57:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-430
2022-03-28 13:57:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:39 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-431
2022-03-28 13:57:40 [ForkJoinPool-1-worker-7] INFO  [PodUtils:225] Pod replicas gonna be stable in 8 polls
2022-03-28 13:57:40 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176]  Podmy-cluster-844045c3-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (166147ms till timeout)
2022-03-28 13:57:40 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1221287ms till timeout)
2022-03-28 13:57:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-431
2022-03-28 13:57:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:40 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-432
2022-03-28 13:57:40 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-43f83aef will have desired state: Ready not ready, will try again in 1000 ms (817406ms till timeout)
2022-03-28 13:57:40 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (598806ms till timeout)
2022-03-28 13:57:40 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testConfigurationReflection is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 13:57:40 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (253016ms till timeout)
2022-03-28 13:57:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-432
2022-03-28 13:57:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:41 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-433
2022-03-28 13:57:41 [ForkJoinPool-1-worker-7] INFO  [PodUtils:225] Pod replicas gonna be stable in 7 polls
2022-03-28 13:57:41 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176]  Podmy-cluster-844045c3-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (164954ms till timeout)
2022-03-28 13:57:41 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1220189ms till timeout)
2022-03-28 13:57:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-433
2022-03-28 13:57:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:41 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-434
2022-03-28 13:57:41 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-43f83aef will have desired state: Ready not ready, will try again in 1000 ms (816306ms till timeout)
2022-03-28 13:57:41 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (597707ms till timeout)
2022-03-28 13:57:41 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (251912ms till timeout)
2022-03-28 13:57:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-434
2022-03-28 13:57:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:42 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-435
2022-03-28 13:57:42 [ForkJoinPool-1-worker-7] INFO  [PodUtils:225] Pod replicas gonna be stable in 6 polls
2022-03-28 13:57:42 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176]  Podmy-cluster-844045c3-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (163757ms till timeout)
2022-03-28 13:57:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-435
2022-03-28 13:57:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:42 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-436
2022-03-28 13:57:42 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-4994018d will have desired state: Ready not ready, will try again in 1000 ms (1219017ms till timeout)
2022-03-28 13:57:42 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-43f83aef will have desired state: Ready not ready, will try again in 1000 ms (815206ms till timeout)
2022-03-28 13:57:42 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (596606ms till timeout)
2022-03-28 13:57:43 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (250811ms till timeout)
2022-03-28 13:57:43 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-436
2022-03-28 13:57:43 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:43 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-437
2022-03-28 13:57:43 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-437
2022-03-28 13:57:43 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:43 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-438
2022-03-28 13:57:43 [ForkJoinPool-1-worker-7] INFO  [PodUtils:225] Pod replicas gonna be stable in 5 polls
2022-03-28 13:57:43 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176]  Podmy-cluster-844045c3-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (162565ms till timeout)
2022-03-28 13:57:43 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:444] Kafka: my-cluster-4994018d is in desired state: Ready
2022-03-28 13:57:43 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-43f83aef will have desired state: Ready not ready, will try again in 1000 ms (814034ms till timeout)
2022-03-28 13:57:44 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (595438ms till timeout)
2022-03-28 13:57:44 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (249653ms till timeout)
2022-03-28 13:57:44 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-438
2022-03-28 13:57:44 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:44 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-439
2022-03-28 13:57:44 [ForkJoinPool-1-worker-5] INFO  [CruiseControlConfigurationST:271] Changing cruise control performance tuning options
2022-03-28 13:57:44 [ForkJoinPool-1-worker-5] INFO  [CruiseControlConfigurationST:277] Verifying that CC pod is rolling, after changing options
2022-03-28 13:57:44 [ForkJoinPool-1-worker-5] INFO  [DeploymentUtils:136] Waiting for Deployment my-cluster-4994018d-cruise-control rolling update
2022-03-28 13:57:44 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Deployment my-cluster-4994018d-cruise-control rolling update in namespace:namespace-6
2022-03-28 13:57:44 [ForkJoinPool-1-worker-5] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-4994018d-cruise-control-5ddf6dd4db-cp72p=40df3b12-1710-456d-8436-da40af53da0d}
2022-03-28 13:57:44 [ForkJoinPool-1-worker-5] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-4994018d-cruise-control-5ddf6dd4db-cp72p=40df3b12-1710-456d-8436-da40af53da0d}
2022-03-28 13:57:44 [ForkJoinPool-1-worker-5] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-4994018d-cruise-control-5ddf6dd4db-cp72p=40df3b12-1710-456d-8436-da40af53da0d}
2022-03-28 13:57:44 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Deployment my-cluster-4994018d-cruise-control rolling update in namespace:namespace-6 not ready, will try again in 5000 ms (599801ms till timeout)
2022-03-28 13:57:44 [ForkJoinPool-1-worker-7] INFO  [PodUtils:225] Pod replicas gonna be stable in 4 polls
2022-03-28 13:57:44 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176]  Podmy-cluster-844045c3-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (161465ms till timeout)
2022-03-28 13:57:45 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-439
2022-03-28 13:57:45 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:45 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-440
2022-03-28 13:57:45 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-43f83aef will have desired state: Ready not ready, will try again in 1000 ms (812935ms till timeout)
2022-03-28 13:57:45 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (594335ms till timeout)
2022-03-28 13:57:45 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (248549ms till timeout)
2022-03-28 13:57:45 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-440
2022-03-28 13:57:45 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:45 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-441
2022-03-28 13:57:45 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testConfigurationReflection is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 13:57:46 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-441
2022-03-28 13:57:46 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:46 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-442
2022-03-28 13:57:46 [ForkJoinPool-1-worker-7] INFO  [PodUtils:225] Pod replicas gonna be stable in 3 polls
2022-03-28 13:57:46 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176]  Podmy-cluster-844045c3-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (160270ms till timeout)
2022-03-28 13:57:46 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-43f83aef will have desired state: Ready not ready, will try again in 1000 ms (811832ms till timeout)
2022-03-28 13:57:46 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (593236ms till timeout)
2022-03-28 13:57:46 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (247448ms till timeout)
2022-03-28 13:57:46 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-442
2022-03-28 13:57:46 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:46 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-443
2022-03-28 13:57:47 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-443
2022-03-28 13:57:47 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:47 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-444
2022-03-28 13:57:47 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-43f83aef will have desired state: Ready not ready, will try again in 1000 ms (810549ms till timeout)
2022-03-28 13:57:47 [ForkJoinPool-1-worker-7] INFO  [PodUtils:225] Pod replicas gonna be stable in 2 polls
2022-03-28 13:57:47 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176]  Podmy-cluster-844045c3-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (158985ms till timeout)
2022-03-28 13:57:47 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (592044ms till timeout)
2022-03-28 13:57:47 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (246268ms till timeout)
2022-03-28 13:57:47 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-444
2022-03-28 13:57:47 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:47 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-445
2022-03-28 13:57:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-445
2022-03-28 13:57:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:48 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-446
2022-03-28 13:57:48 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-43f83aef will have desired state: Ready not ready, will try again in 1000 ms (809449ms till timeout)
2022-03-28 13:57:48 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (244970ms till timeout)
2022-03-28 13:57:48 [ForkJoinPool-1-worker-7] INFO  [PodUtils:225] Pod replicas gonna be stable in 1 polls
2022-03-28 13:57:48 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176]  Podmy-cluster-844045c3-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (157595ms till timeout)
2022-03-28 13:57:48 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (590651ms till timeout)
2022-03-28 13:57:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-446
2022-03-28 13:57:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:48 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-447
2022-03-28 13:57:49 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-447
2022-03-28 13:57:49 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:49 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-448
2022-03-28 13:57:49 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-43f83aef will have desired state: Ready not ready, will try again in 1000 ms (808349ms till timeout)
2022-03-28 13:57:49 [ForkJoinPool-1-worker-5] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-4994018d-cruise-control-5ddf6dd4db-cp72p=40df3b12-1710-456d-8436-da40af53da0d}
2022-03-28 13:57:49 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (589538ms till timeout)
2022-03-28 13:57:50 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (243758ms till timeout)
2022-03-28 13:57:50 [ForkJoinPool-1-worker-7] INFO  [PodUtils:217] Pod replicas are stable for 20 polls intervals
2022-03-28 13:57:50 [ForkJoinPool-1-worker-7] INFO  [PodUtils:228] Pod my-cluster-844045c3-cruise-control- has 0 replicas
2022-03-28 13:57:50 [ForkJoinPool-1-worker-7] INFO  [CruiseControlConfigurationST:123] Verifying that in Kafka config map there is no configuration to cruise control metric reporter
2022-03-28 13:57:50 [ForkJoinPool-1-worker-5] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-4994018d-cruise-control-5ddf6dd4db-cp72p=40df3b12-1710-456d-8436-da40af53da0d, my-cluster-4994018d-cruise-control-77fcd86cd4-z485d=9a41e3a8-fa9a-4520-8665-84fd7ba063ff}
2022-03-28 13:57:50 [ForkJoinPool-1-worker-5] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-4994018d-cruise-control-5ddf6dd4db-cp72p=40df3b12-1710-456d-8436-da40af53da0d}
2022-03-28 13:57:50 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Deployment my-cluster-4994018d-cruise-control rolling update in namespace:namespace-6 not ready, will try again in 5000 ms (594489ms till timeout)
2022-03-28 13:57:50 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-448
2022-03-28 13:57:50 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:50 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-449
2022-03-28 13:57:50 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties
2022-03-28 13:57:50 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (120000ms till timeout)
2022-03-28 13:57:50 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-449
2022-03-28 13:57:50 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:50 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-45
2022-03-28 13:57:50 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testConfigurationReflection is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 13:57:50 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-43f83aef will have desired state: Ready not ready, will try again in 1000 ms (807245ms till timeout)
2022-03-28 13:57:51 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (588441ms till timeout)
2022-03-28 13:57:51 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (242654ms till timeout)
2022-03-28 13:57:51 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (119000ms till timeout)
2022-03-28 13:57:51 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-45
2022-03-28 13:57:51 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:51 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-450
2022-03-28 13:57:51 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-450
2022-03-28 13:57:51 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:51 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-451
2022-03-28 13:57:51 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-43f83aef will have desired state: Ready not ready, will try again in 1000 ms (806145ms till timeout)
2022-03-28 13:57:52 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (587342ms till timeout)
2022-03-28 13:57:52 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (117999ms till timeout)
2022-03-28 13:57:52 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (241548ms till timeout)
2022-03-28 13:57:52 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-451
2022-03-28 13:57:52 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:52 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-452
2022-03-28 13:57:52 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-43f83aef will have desired state: Ready not ready, will try again in 1000 ms (805045ms till timeout)
2022-03-28 13:57:53 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-452
2022-03-28 13:57:53 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:53 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-453
2022-03-28 13:57:53 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (116998ms till timeout)
2022-03-28 13:57:53 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (586244ms till timeout)
2022-03-28 13:57:53 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (240443ms till timeout)
2022-03-28 13:57:53 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-453
2022-03-28 13:57:53 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:53 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-454
2022-03-28 13:57:54 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-43f83aef will have desired state: Ready not ready, will try again in 1000 ms (803947ms till timeout)
2022-03-28 13:57:54 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-454
2022-03-28 13:57:54 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:54 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-455
2022-03-28 13:57:54 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (115998ms till timeout)
2022-03-28 13:57:54 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (585146ms till timeout)
2022-03-28 13:57:54 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (239342ms till timeout)
2022-03-28 13:57:54 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-455
2022-03-28 13:57:54 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:54 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-456
2022-03-28 13:57:55 [ForkJoinPool-1-worker-5] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-4994018d-cruise-control-5ddf6dd4db-cp72p=40df3b12-1710-456d-8436-da40af53da0d}
2022-03-28 13:57:55 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-43f83aef will have desired state: Ready not ready, will try again in 1000 ms (802847ms till timeout)
2022-03-28 13:57:55 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (114997ms till timeout)
2022-03-28 13:57:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-456
2022-03-28 13:57:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:55 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-457
2022-03-28 13:57:55 [ForkJoinPool-1-worker-5] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-4994018d-cruise-control-5ddf6dd4db-cp72p=40df3b12-1710-456d-8436-da40af53da0d, my-cluster-4994018d-cruise-control-77fcd86cd4-z485d=9a41e3a8-fa9a-4520-8665-84fd7ba063ff}
2022-03-28 13:57:55 [ForkJoinPool-1-worker-5] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-4994018d-cruise-control-5ddf6dd4db-cp72p=40df3b12-1710-456d-8436-da40af53da0d}
2022-03-28 13:57:55 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Deployment my-cluster-4994018d-cruise-control rolling update in namespace:namespace-6 not ready, will try again in 5000 ms (589032ms till timeout)
2022-03-28 13:57:55 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (583961ms till timeout)
2022-03-28 13:57:55 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (238178ms till timeout)
2022-03-28 13:57:55 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testConfigurationReflection is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 13:57:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-457
2022-03-28 13:57:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:57:55 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-458
2022-03-28 13:57:56 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (113996ms till timeout)
2022-03-28 13:57:56 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-43f83aef will have desired state: Ready not ready, will try again in 1000 ms (801749ms till timeout)
2022-03-28 13:57:56 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (582864ms till timeout)
2022-03-28 13:57:56 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (237078ms till timeout)
2022-03-28 13:57:57 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (112996ms till timeout)
2022-03-28 13:57:57 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-43f83aef will have desired state: Ready not ready, will try again in 1000 ms (800652ms till timeout)
2022-03-28 13:57:57 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (581766ms till timeout)
2022-03-28 13:57:57 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (235978ms till timeout)
2022-03-28 13:57:58 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (111995ms till timeout)
2022-03-28 13:57:58 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-43f83aef will have desired state: Ready not ready, will try again in 1000 ms (799553ms till timeout)
2022-03-28 13:57:58 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (580668ms till timeout)
2022-03-28 13:57:58 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (234877ms till timeout)
2022-03-28 13:57:59 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (110994ms till timeout)
2022-03-28 13:57:59 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-43f83aef will have desired state: Ready not ready, will try again in 1000 ms (798443ms till timeout)
2022-03-28 13:57:59 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (579571ms till timeout)
2022-03-28 13:58:00 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (233774ms till timeout)
2022-03-28 13:58:00 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (109994ms till timeout)
2022-03-28 13:58:00 [ForkJoinPool-1-worker-5] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-4994018d-cruise-control-5ddf6dd4db-cp72p=40df3b12-1710-456d-8436-da40af53da0d}
2022-03-28 13:58:00 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testConfigurationReflection is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 13:58:00 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-43f83aef will have desired state: Ready not ready, will try again in 1000 ms (797268ms till timeout)
2022-03-28 13:58:00 [ForkJoinPool-1-worker-5] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-4994018d-cruise-control-5ddf6dd4db-cp72p=40df3b12-1710-456d-8436-da40af53da0d, my-cluster-4994018d-cruise-control-77fcd86cd4-z485d=9a41e3a8-fa9a-4520-8665-84fd7ba063ff}
2022-03-28 13:58:00 [ForkJoinPool-1-worker-5] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-4994018d-cruise-control-5ddf6dd4db-cp72p=40df3b12-1710-456d-8436-da40af53da0d}
2022-03-28 13:58:00 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Deployment my-cluster-4994018d-cruise-control rolling update in namespace:namespace-6 not ready, will try again in 5000 ms (583642ms till timeout)
2022-03-28 13:58:01 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (578474ms till timeout)
2022-03-28 13:58:01 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (232674ms till timeout)
2022-03-28 13:58:01 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (108993ms till timeout)
2022-03-28 13:58:01 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-458
2022-03-28 13:58:01 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:58:01 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-459
2022-03-28 13:58:01 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-43f83aef will have desired state: Ready not ready, will try again in 1000 ms (796171ms till timeout)
2022-03-28 13:58:02 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-459
2022-03-28 13:58:02 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:58:02 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-46
2022-03-28 13:58:02 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (577375ms till timeout)
2022-03-28 13:58:02 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (107993ms till timeout)
2022-03-28 13:58:02 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (231574ms till timeout)
2022-03-28 13:58:02 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-43f83aef will have desired state: Ready not ready, will try again in 1000 ms (795072ms till timeout)
2022-03-28 13:58:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-46
2022-03-28 13:58:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:58:03 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-466
2022-03-28 13:58:03 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (106992ms till timeout)
2022-03-28 13:58:03 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (576278ms till timeout)
2022-03-28 13:58:03 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (230472ms till timeout)
2022-03-28 13:58:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-466
2022-03-28 13:58:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:58:03 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-468
2022-03-28 13:58:04 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-43f83aef will have desired state: Ready not ready, will try again in 1000 ms (793972ms till timeout)
2022-03-28 13:58:04 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (105992ms till timeout)
2022-03-28 13:58:04 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (575177ms till timeout)
2022-03-28 13:58:04 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-468
2022-03-28 13:58:04 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:58:04 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-469
2022-03-28 13:58:04 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (229371ms till timeout)
2022-03-28 13:58:05 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-43f83aef will have desired state: Ready not ready, will try again in 1000 ms (792873ms till timeout)
2022-03-28 13:58:05 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (104991ms till timeout)
2022-03-28 13:58:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-469
2022-03-28 13:58:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:58:05 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-48
2022-03-28 13:58:05 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (574079ms till timeout)
2022-03-28 13:58:05 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (228269ms till timeout)
2022-03-28 13:58:05 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testConfigurationReflection is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 13:58:05 [ForkJoinPool-1-worker-5] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-4994018d-cruise-control-5ddf6dd4db-cp72p=40df3b12-1710-456d-8436-da40af53da0d}
2022-03-28 13:58:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-48
2022-03-28 13:58:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:58:05 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-488
2022-03-28 13:58:06 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (103991ms till timeout)
2022-03-28 13:58:06 [ForkJoinPool-1-worker-5] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-4994018d-cruise-control-5ddf6dd4db-cp72p=40df3b12-1710-456d-8436-da40af53da0d, my-cluster-4994018d-cruise-control-77fcd86cd4-z485d=9a41e3a8-fa9a-4520-8665-84fd7ba063ff}
2022-03-28 13:58:06 [ForkJoinPool-1-worker-5] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-4994018d-cruise-control-5ddf6dd4db-cp72p=40df3b12-1710-456d-8436-da40af53da0d}
2022-03-28 13:58:06 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Deployment my-cluster-4994018d-cruise-control rolling update in namespace:namespace-6 not ready, will try again in 5000 ms (578249ms till timeout)
2022-03-28 13:58:06 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-43f83aef will have desired state: Ready not ready, will try again in 1000 ms (791683ms till timeout)
2022-03-28 13:58:06 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (572982ms till timeout)
2022-03-28 13:58:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-488
2022-03-28 13:58:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:58:06 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-493
2022-03-28 13:58:06 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (227169ms till timeout)
2022-03-28 13:58:07 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (102990ms till timeout)
2022-03-28 13:58:07 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-43f83aef will have desired state: Ready not ready, will try again in 1000 ms (790583ms till timeout)
2022-03-28 13:58:07 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (571885ms till timeout)
2022-03-28 13:58:07 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (226068ms till timeout)
2022-03-28 13:58:07 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-493
2022-03-28 13:58:07 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:58:07 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-495
2022-03-28 13:58:08 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (101989ms till timeout)
2022-03-28 13:58:08 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-43f83aef will have desired state: Ready not ready, will try again in 1000 ms (789485ms till timeout)
2022-03-28 13:58:08 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (570787ms till timeout)
2022-03-28 13:58:08 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (224961ms till timeout)
2022-03-28 13:58:09 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (100988ms till timeout)
2022-03-28 13:58:09 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-43f83aef will have desired state: Ready not ready, will try again in 1000 ms (788385ms till timeout)
2022-03-28 13:58:09 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (569690ms till timeout)
2022-03-28 13:58:09 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (223861ms till timeout)
2022-03-28 13:58:10 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (99988ms till timeout)
2022-03-28 13:58:10 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testConfigurationReflection is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 13:58:10 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-43f83aef will have desired state: Ready not ready, will try again in 1000 ms (787283ms till timeout)
2022-03-28 13:58:10 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (568590ms till timeout)
2022-03-28 13:58:11 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (222761ms till timeout)
2022-03-28 13:58:11 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (98987ms till timeout)
2022-03-28 13:58:11 [ForkJoinPool-1-worker-5] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-4994018d-cruise-control-5ddf6dd4db-cp72p=40df3b12-1710-456d-8436-da40af53da0d}
2022-03-28 13:58:11 [ForkJoinPool-1-worker-5] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-4994018d-cruise-control-5ddf6dd4db-cp72p=40df3b12-1710-456d-8436-da40af53da0d, my-cluster-4994018d-cruise-control-77fcd86cd4-z485d=9a41e3a8-fa9a-4520-8665-84fd7ba063ff}
2022-03-28 13:58:11 [ForkJoinPool-1-worker-5] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-4994018d-cruise-control-5ddf6dd4db-cp72p=40df3b12-1710-456d-8436-da40af53da0d}
2022-03-28 13:58:11 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Deployment my-cluster-4994018d-cruise-control rolling update in namespace:namespace-6 not ready, will try again in 5000 ms (572867ms till timeout)
2022-03-28 13:58:11 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-43f83aef will have desired state: Ready not ready, will try again in 1000 ms (786185ms till timeout)
2022-03-28 13:58:12 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (567492ms till timeout)
2022-03-28 13:58:12 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (221660ms till timeout)
2022-03-28 13:58:12 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (97986ms till timeout)
2022-03-28 13:58:12 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-43f83aef will have desired state: Ready not ready, will try again in 1000 ms (785085ms till timeout)
2022-03-28 13:58:13 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (566395ms till timeout)
2022-03-28 13:58:13 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (96986ms till timeout)
2022-03-28 13:58:13 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (220559ms till timeout)
2022-03-28 13:58:13 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-495
2022-03-28 13:58:13 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:58:13 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-496
2022-03-28 13:58:14 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-496
2022-03-28 13:58:14 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:58:14 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-498
2022-03-28 13:58:14 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-43f83aef will have desired state: Ready not ready, will try again in 1000 ms (783987ms till timeout)
2022-03-28 13:58:14 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (95985ms till timeout)
2022-03-28 13:58:14 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (565298ms till timeout)
2022-03-28 13:58:14 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (219458ms till timeout)
2022-03-28 13:58:15 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-43f83aef will have desired state: Ready not ready, will try again in 1000 ms (782888ms till timeout)
2022-03-28 13:58:15 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (94984ms till timeout)
2022-03-28 13:58:15 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (564200ms till timeout)
2022-03-28 13:58:15 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (218357ms till timeout)
2022-03-28 13:58:15 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testConfigurationReflection is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 13:58:16 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (93983ms till timeout)
2022-03-28 13:58:16 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-43f83aef will have desired state: Ready not ready, will try again in 1000 ms (781788ms till timeout)
2022-03-28 13:58:16 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (563102ms till timeout)
2022-03-28 13:58:16 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (217257ms till timeout)
2022-03-28 13:58:16 [ForkJoinPool-1-worker-5] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-4994018d-cruise-control-5ddf6dd4db-cp72p=40df3b12-1710-456d-8436-da40af53da0d}
2022-03-28 13:58:16 [ForkJoinPool-1-worker-5] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-4994018d-cruise-control-5ddf6dd4db-cp72p=40df3b12-1710-456d-8436-da40af53da0d, my-cluster-4994018d-cruise-control-77fcd86cd4-z485d=9a41e3a8-fa9a-4520-8665-84fd7ba063ff}
2022-03-28 13:58:16 [ForkJoinPool-1-worker-5] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-4994018d-cruise-control-5ddf6dd4db-cp72p=40df3b12-1710-456d-8436-da40af53da0d}
2022-03-28 13:58:16 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Deployment my-cluster-4994018d-cruise-control rolling update in namespace:namespace-6 not ready, will try again in 5000 ms (567668ms till timeout)
2022-03-28 13:58:17 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (92983ms till timeout)
2022-03-28 13:58:17 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-43f83aef will have desired state: Ready not ready, will try again in 1000 ms (780690ms till timeout)
2022-03-28 13:58:17 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (562004ms till timeout)
2022-03-28 13:58:17 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (216155ms till timeout)
2022-03-28 13:58:18 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (91982ms till timeout)
2022-03-28 13:58:18 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-43f83aef will have desired state: Ready not ready, will try again in 1000 ms (779591ms till timeout)
2022-03-28 13:58:18 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (560905ms till timeout)
2022-03-28 13:58:18 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (215040ms till timeout)
2022-03-28 13:58:19 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (90981ms till timeout)
2022-03-28 13:58:19 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-43f83aef will have desired state: Ready not ready, will try again in 1000 ms (778492ms till timeout)
2022-03-28 13:58:19 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-498
2022-03-28 13:58:19 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:58:19 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-499
2022-03-28 13:58:19 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (559807ms till timeout)
2022-03-28 13:58:19 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (213939ms till timeout)
2022-03-28 13:58:20 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (89981ms till timeout)
2022-03-28 13:58:20 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-43f83aef will have desired state: Ready not ready, will try again in 1000 ms (777391ms till timeout)
2022-03-28 13:58:20 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testConfigurationReflection is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 13:58:20 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (558709ms till timeout)
2022-03-28 13:58:20 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (212838ms till timeout)
2022-03-28 13:58:21 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (88980ms till timeout)
2022-03-28 13:58:21 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-43f83aef will have desired state: Ready not ready, will try again in 1000 ms (776294ms till timeout)
2022-03-28 13:58:21 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (557611ms till timeout)
2022-03-28 13:58:21 [ForkJoinPool-1-worker-5] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-4994018d-cruise-control-5ddf6dd4db-cp72p=40df3b12-1710-456d-8436-da40af53da0d}
2022-03-28 13:58:22 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (211714ms till timeout)
2022-03-28 13:58:22 [ForkJoinPool-1-worker-5] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-4994018d-cruise-control-5ddf6dd4db-cp72p=40df3b12-1710-456d-8436-da40af53da0d, my-cluster-4994018d-cruise-control-77fcd86cd4-z485d=9a41e3a8-fa9a-4520-8665-84fd7ba063ff}
2022-03-28 13:58:22 [ForkJoinPool-1-worker-5] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-4994018d-cruise-control-5ddf6dd4db-cp72p=40df3b12-1710-456d-8436-da40af53da0d}
2022-03-28 13:58:22 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Deployment my-cluster-4994018d-cruise-control rolling update in namespace:namespace-6 not ready, will try again in 5000 ms (562373ms till timeout)
2022-03-28 13:58:22 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (87979ms till timeout)
2022-03-28 13:58:22 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-43f83aef will have desired state: Ready not ready, will try again in 1000 ms (775195ms till timeout)
2022-03-28 13:58:22 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (556514ms till timeout)
2022-03-28 13:58:23 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (210613ms till timeout)
2022-03-28 13:58:23 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (86979ms till timeout)
2022-03-28 13:58:23 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-43f83aef will have desired state: Ready not ready, will try again in 1000 ms (774096ms till timeout)
2022-03-28 13:58:24 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (555416ms till timeout)
2022-03-28 13:58:24 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (85978ms till timeout)
2022-03-28 13:58:24 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (209513ms till timeout)
2022-03-28 13:58:25 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-43f83aef will have desired state: Ready not ready, will try again in 1000 ms (772997ms till timeout)
2022-03-28 13:58:25 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (554319ms till timeout)
2022-03-28 13:58:25 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (84978ms till timeout)
2022-03-28 13:58:25 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (208412ms till timeout)
2022-03-28 13:58:25 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-499
2022-03-28 13:58:25 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:58:25 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-5
2022-03-28 13:58:25 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testConfigurationReflection is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 13:58:26 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:444] Kafka: my-cluster-43f83aef is in desired state: Ready
2022-03-28 13:58:26 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:155] Create/Update KafkaUser my-user-370513135-1989152728 in namespace namespace-7
2022-03-28 13:58:26 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:164] Using Namespace: namespace-7
2022-03-28 13:58:26 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (83977ms till timeout)
2022-03-28 13:58:26 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (553221ms till timeout)
2022-03-28 13:58:26 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-370513135-1989152728
2022-03-28 13:58:26 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:433] Wait for KafkaUser: my-user-370513135-1989152728 will have desired state: Ready
2022-03-28 13:58:26 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for KafkaUser: my-user-370513135-1989152728 will have desired state: Ready
2022-03-28 13:58:26 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (207232ms till timeout)
2022-03-28 13:58:26 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] KafkaUser: my-user-370513135-1989152728 will have desired state: Ready not ready, will try again in 1000 ms (179807ms till timeout)
2022-03-28 13:58:27 [ForkJoinPool-1-worker-5] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-4994018d-cruise-control-5ddf6dd4db-cp72p=40df3b12-1710-456d-8436-da40af53da0d}
2022-03-28 13:58:27 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (82976ms till timeout)
2022-03-28 13:58:27 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (552105ms till timeout)
2022-03-28 13:58:27 [ForkJoinPool-1-worker-5] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-4994018d-cruise-control-77fcd86cd4-z485d=9a41e3a8-fa9a-4520-8665-84fd7ba063ff}
2022-03-28 13:58:27 [ForkJoinPool-1-worker-5] DEBUG [DeploymentUtils:119] All pods seem to have rolled
2022-03-28 13:58:27 [ForkJoinPool-1-worker-5] INFO  [DeploymentUtils:161] Wait for Deployment: my-cluster-4994018d-cruise-control will be ready
2022-03-28 13:58:27 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Wait for Deployment: my-cluster-4994018d-cruise-control will be ready
2022-03-28 13:58:27 [ForkJoinPool-1-worker-5] INFO  [DeploymentUtils:168] Deployment: my-cluster-4994018d-cruise-control is ready
2022-03-28 13:58:27 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (206125ms till timeout)
2022-03-28 13:58:27 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:444] KafkaUser: my-user-370513135-1989152728 is in desired state: Ready
2022-03-28 13:58:27 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-4994018d, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-4994018d-cruise-control}, additionalProperties={})to be ready
2022-03-28 13:58:27 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:433] Wait for KafkaUser: my-user-370513135-1989152728 will have desired state: Ready
2022-03-28 13:58:27 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for KafkaUser: my-user-370513135-1989152728 will have desired state: Ready
2022-03-28 13:58:27 [ForkJoinPool-1-worker-5] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-4994018d-cruise-control-77fcd86cd4-z485d not ready: cruise-control)
2022-03-28 13:58:27 [ForkJoinPool-1-worker-5] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-4994018d-cruise-control-77fcd86cd4-z485d not ready: tls-sidecar)
2022-03-28 13:58:27 [ForkJoinPool-1-worker-5] DEBUG [PodUtils:106] Pods my-cluster-4994018d-cruise-control-77fcd86cd4-z485d are ready
2022-03-28 13:58:27 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-4994018d, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-4994018d-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599804ms till timeout)
2022-03-28 13:58:28 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:444] KafkaUser: my-user-370513135-1989152728 is in desired state: Ready
2022-03-28 13:58:28 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (81976ms till timeout)
2022-03-28 13:58:28 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:674] ============================================================================
2022-03-28 13:58:28 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:675] [operators.user.UserST - After Each] - Clean up after test
2022-03-28 13:58:28 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:344] ############################################################################
2022-03-28 13:58:28 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:348] Delete all resources for testTlsExternalUser
2022-03-28 13:58:28 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of KafkaUser my-user-370513135-1989152728 in namespace namespace-7
2022-03-28 13:58:28 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-370513135-1989152728
2022-03-28 13:58:28 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of Kafka my-cluster-43f83aef in namespace namespace-7
2022-03-28 13:58:28 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (550931ms till timeout)
2022-03-28 13:58:28 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-43f83aef
2022-03-28 13:58:28 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster-43f83aef not ready, will try again in 10000 ms (839896ms till timeout)
2022-03-28 13:58:28 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (204918ms till timeout)
2022-03-28 13:58:29 [ForkJoinPool-1-worker-5] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-4994018d-cruise-control-77fcd86cd4-z485d not ready: cruise-control)
2022-03-28 13:58:29 [ForkJoinPool-1-worker-5] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-4994018d-cruise-control-77fcd86cd4-z485d not ready: tls-sidecar)
2022-03-28 13:58:29 [ForkJoinPool-1-worker-5] DEBUG [PodUtils:106] Pods my-cluster-4994018d-cruise-control-77fcd86cd4-z485d are ready
2022-03-28 13:58:29 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-4994018d, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-4994018d-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598700ms till timeout)
2022-03-28 13:58:29 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (80975ms till timeout)
2022-03-28 13:58:29 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (549833ms till timeout)
2022-03-28 13:58:30 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (203811ms till timeout)
2022-03-28 13:58:30 [ForkJoinPool-1-worker-5] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-4994018d-cruise-control-77fcd86cd4-z485d not ready: cruise-control)
2022-03-28 13:58:30 [ForkJoinPool-1-worker-5] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-4994018d-cruise-control-77fcd86cd4-z485d not ready: tls-sidecar)
2022-03-28 13:58:30 [ForkJoinPool-1-worker-5] DEBUG [PodUtils:106] Pods my-cluster-4994018d-cruise-control-77fcd86cd4-z485d are ready
2022-03-28 13:58:30 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-4994018d, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-4994018d-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597599ms till timeout)
2022-03-28 13:58:30 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (79975ms till timeout)
2022-03-28 13:58:30 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testConfigurationReflection is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 13:58:30 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (548735ms till timeout)
2022-03-28 13:58:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-5
2022-03-28 13:58:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:58:31 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-52
2022-03-28 13:58:31 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (202711ms till timeout)
2022-03-28 13:58:31 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (78974ms till timeout)
2022-03-28 13:58:31 [ForkJoinPool-1-worker-5] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-4994018d-cruise-control-77fcd86cd4-z485d not ready: cruise-control)
2022-03-28 13:58:31 [ForkJoinPool-1-worker-5] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-4994018d-cruise-control-77fcd86cd4-z485d not ready: tls-sidecar)
2022-03-28 13:58:31 [ForkJoinPool-1-worker-5] DEBUG [PodUtils:106] Pods my-cluster-4994018d-cruise-control-77fcd86cd4-z485d are ready
2022-03-28 13:58:31 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-4994018d, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-4994018d-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596500ms till timeout)
2022-03-28 13:58:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-52
2022-03-28 13:58:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:58:31 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-58
2022-03-28 13:58:31 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (547637ms till timeout)
2022-03-28 13:58:32 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (201609ms till timeout)
2022-03-28 13:58:32 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (77973ms till timeout)
2022-03-28 13:58:32 [ForkJoinPool-1-worker-5] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-4994018d-cruise-control-77fcd86cd4-z485d not ready: cruise-control)
2022-03-28 13:58:32 [ForkJoinPool-1-worker-5] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-4994018d-cruise-control-77fcd86cd4-z485d not ready: tls-sidecar)
2022-03-28 13:58:32 [ForkJoinPool-1-worker-5] DEBUG [PodUtils:106] Pods my-cluster-4994018d-cruise-control-77fcd86cd4-z485d are ready
2022-03-28 13:58:32 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-4994018d, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-4994018d-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595396ms till timeout)
2022-03-28 13:58:32 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-58
2022-03-28 13:58:32 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:58:32 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-59
2022-03-28 13:58:32 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (546539ms till timeout)
2022-03-28 13:58:33 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (76973ms till timeout)
2022-03-28 13:58:33 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (200508ms till timeout)
2022-03-28 13:58:33 [ForkJoinPool-1-worker-5] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-4994018d-cruise-control-77fcd86cd4-z485d not ready: cruise-control)
2022-03-28 13:58:33 [ForkJoinPool-1-worker-5] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-4994018d-cruise-control-77fcd86cd4-z485d not ready: tls-sidecar)
2022-03-28 13:58:33 [ForkJoinPool-1-worker-5] DEBUG [PodUtils:106] Pods my-cluster-4994018d-cruise-control-77fcd86cd4-z485d are ready
2022-03-28 13:58:33 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-4994018d, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-4994018d-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594296ms till timeout)
2022-03-28 13:58:34 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (545441ms till timeout)
2022-03-28 13:58:34 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (75972ms till timeout)
2022-03-28 13:58:34 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (199408ms till timeout)
2022-03-28 13:58:34 [ForkJoinPool-1-worker-5] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-4994018d-cruise-control-77fcd86cd4-z485d not ready: cruise-control)
2022-03-28 13:58:34 [ForkJoinPool-1-worker-5] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-4994018d-cruise-control-77fcd86cd4-z485d not ready: tls-sidecar)
2022-03-28 13:58:34 [ForkJoinPool-1-worker-5] DEBUG [PodUtils:106] Pods my-cluster-4994018d-cruise-control-77fcd86cd4-z485d are ready
2022-03-28 13:58:34 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-4994018d, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-4994018d-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593197ms till timeout)
2022-03-28 13:58:35 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (544342ms till timeout)
2022-03-28 13:58:35 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (74971ms till timeout)
2022-03-28 13:58:35 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (198307ms till timeout)
2022-03-28 13:58:35 [ForkJoinPool-1-worker-5] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-4994018d-cruise-control-77fcd86cd4-z485d not ready: cruise-control)
2022-03-28 13:58:35 [ForkJoinPool-1-worker-5] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-4994018d-cruise-control-77fcd86cd4-z485d not ready: tls-sidecar)
2022-03-28 13:58:35 [ForkJoinPool-1-worker-5] DEBUG [PodUtils:106] Pods my-cluster-4994018d-cruise-control-77fcd86cd4-z485d are ready
2022-03-28 13:58:35 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-4994018d, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-4994018d-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592097ms till timeout)
2022-03-28 13:58:35 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testConfigurationReflection is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 13:58:36 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (73971ms till timeout)
2022-03-28 13:58:36 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (543244ms till timeout)
2022-03-28 13:58:36 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (197207ms till timeout)
2022-03-28 13:58:36 [ForkJoinPool-1-worker-5] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-4994018d-cruise-control-77fcd86cd4-z485d not ready: cruise-control)
2022-03-28 13:58:36 [ForkJoinPool-1-worker-5] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-4994018d-cruise-control-77fcd86cd4-z485d not ready: tls-sidecar)
2022-03-28 13:58:36 [ForkJoinPool-1-worker-5] DEBUG [PodUtils:106] Pods my-cluster-4994018d-cruise-control-77fcd86cd4-z485d are ready
2022-03-28 13:58:36 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-4994018d, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-4994018d-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590997ms till timeout)
2022-03-28 13:58:37 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (72970ms till timeout)
2022-03-28 13:58:37 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (542145ms till timeout)
2022-03-28 13:58:37 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (196104ms till timeout)
2022-03-28 13:58:37 [ForkJoinPool-1-worker-5] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-4994018d-cruise-control-77fcd86cd4-z485d not ready: cruise-control)
2022-03-28 13:58:37 [ForkJoinPool-1-worker-5] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-4994018d-cruise-control-77fcd86cd4-z485d not ready: tls-sidecar)
2022-03-28 13:58:37 [ForkJoinPool-1-worker-5] DEBUG [PodUtils:106] Pods my-cluster-4994018d-cruise-control-77fcd86cd4-z485d are ready
2022-03-28 13:58:37 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-4994018d, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-4994018d-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (589898ms till timeout)
2022-03-28 13:58:38 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (71969ms till timeout)
2022-03-28 13:58:38 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-59
2022-03-28 13:58:38 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:58:38 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-6
2022-03-28 13:58:38 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (541047ms till timeout)
2022-03-28 13:58:38 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (195003ms till timeout)
2022-03-28 13:58:38 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:369] ############################################################################
2022-03-28 13:58:38 [ForkJoinPool-1-worker-13] INFO  [TestSuiteNamespaceManager:200] Deleting namespace:namespace-7 for test case:testTlsExternalUser
2022-03-28 13:58:39 [ForkJoinPool-1-worker-5] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-4994018d-cruise-control-77fcd86cd4-z485d not ready: cruise-control)
2022-03-28 13:58:39 [ForkJoinPool-1-worker-5] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-4994018d-cruise-control-77fcd86cd4-z485d not ready: tls-sidecar)
2022-03-28 13:58:39 [ForkJoinPool-1-worker-5] DEBUG [PodUtils:106] Pods my-cluster-4994018d-cruise-control-77fcd86cd4-z485d are ready
2022-03-28 13:58:39 [ForkJoinPool-1-worker-5] INFO  [DeploymentUtils:141] Deployment my-cluster-4994018d-cruise-control rolling update finished
2022-03-28 13:58:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-6
2022-03-28 13:58:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:58:39 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-61
2022-03-28 13:58:39 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Namespace namespace-7 removal
2022-03-28 13:58:39 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-28 13:58:39 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (70969ms till timeout)
2022-03-28 13:58:39 [ForkJoinPool-1-worker-5] INFO  [CruiseControlConfigurationST:280] Verifying that Kafka pods did not roll
2022-03-28 13:58:39 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Waiting for stability of rolling update will be not triggered
2022-03-28 13:58:39 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:58:39 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:58:39 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:58:39 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-4994018d-kafka-2 hasn't rolled
2022-03-28 13:58:39 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f} pods didn't roll. Remaining seconds for stability: 50
2022-03-28 13:58:39 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (299896ms till timeout)
2022-03-28 13:58:39 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (539948ms till timeout)
2022-03-28 13:58:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-61
2022-03-28 13:58:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:58:39 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-63
2022-03-28 13:58:39 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-28 13:58:39 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 13:58:39 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (479503ms till timeout)
2022-03-28 13:58:39 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (193903ms till timeout)
2022-03-28 13:58:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-63
2022-03-28 13:58:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:58:40 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-64
2022-03-28 13:58:40 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (69968ms till timeout)
2022-03-28 13:58:40 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:58:40 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:58:40 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:58:40 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-4994018d-kafka-2 hasn't rolled
2022-03-28 13:58:40 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f} pods didn't roll. Remaining seconds for stability: 49
2022-03-28 13:58:40 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (298702ms till timeout)
2022-03-28 13:58:40 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-28 13:58:40 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (538851ms till timeout)
2022-03-28 13:58:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-64
2022-03-28 13:58:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:58:40 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-65
2022-03-28 13:58:40 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testConfigurationReflection is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 13:58:41 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (192801ms till timeout)
2022-03-28 13:58:41 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-28 13:58:41 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 13:58:41 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (478032ms till timeout)
2022-03-28 13:58:41 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (68968ms till timeout)
2022-03-28 13:58:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-65
2022-03-28 13:58:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:58:41 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-67
2022-03-28 13:58:41 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:58:41 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:58:41 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:58:41 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-4994018d-kafka-2 hasn't rolled
2022-03-28 13:58:41 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f} pods didn't roll. Remaining seconds for stability: 48
2022-03-28 13:58:41 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (297508ms till timeout)
2022-03-28 13:58:41 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (537713ms till timeout)
2022-03-28 13:58:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-67
2022-03-28 13:58:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:58:41 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-68
2022-03-28 13:58:42 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-28 13:58:42 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (191700ms till timeout)
2022-03-28 13:58:42 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (67967ms till timeout)
2022-03-28 13:58:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-68
2022-03-28 13:58:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:58:42 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-7
2022-03-28 13:58:42 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-28 13:58:42 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 13:58:42 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (476562ms till timeout)
2022-03-28 13:58:42 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:58:42 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (536615ms till timeout)
2022-03-28 13:58:43 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-7
2022-03-28 13:58:43 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:58:43 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-75
2022-03-28 13:58:43 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:58:43 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:58:43 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-4994018d-kafka-2 hasn't rolled
2022-03-28 13:58:43 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f} pods didn't roll. Remaining seconds for stability: 47
2022-03-28 13:58:43 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (296219ms till timeout)
2022-03-28 13:58:43 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (66966ms till timeout)
2022-03-28 13:58:43 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (190592ms till timeout)
2022-03-28 13:58:43 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-28 13:58:43 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-75
2022-03-28 13:58:43 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:58:43 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-8
2022-03-28 13:58:43 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (535516ms till timeout)
2022-03-28 13:58:44 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-28 13:58:44 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 13:58:44 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (475090ms till timeout)
2022-03-28 13:58:44 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:58:44 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-8
2022-03-28 13:58:44 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:58:44 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-80
2022-03-28 13:58:44 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (65966ms till timeout)
2022-03-28 13:58:44 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:58:44 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:58:44 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-4994018d-kafka-2 hasn't rolled
2022-03-28 13:58:44 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f} pods didn't roll. Remaining seconds for stability: 46
2022-03-28 13:58:44 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (295027ms till timeout)
2022-03-28 13:58:44 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (189456ms till timeout)
2022-03-28 13:58:44 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-80
2022-03-28 13:58:44 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:58:44 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-81
2022-03-28 13:58:45 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-28 13:58:45 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (534415ms till timeout)
2022-03-28 13:58:45 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (64965ms till timeout)
2022-03-28 13:58:45 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-81
2022-03-28 13:58:45 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:58:45 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-82
2022-03-28 13:58:45 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:58:45 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:58:45 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:58:45 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-4994018d-kafka-2 hasn't rolled
2022-03-28 13:58:45 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f} pods didn't roll. Remaining seconds for stability: 45
2022-03-28 13:58:45 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (293834ms till timeout)
2022-03-28 13:58:45 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-28 13:58:45 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 13:58:45 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (473617ms till timeout)
2022-03-28 13:58:45 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (188262ms till timeout)
2022-03-28 13:58:45 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testConfigurationReflection is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 13:58:45 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-82
2022-03-28 13:58:45 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:58:45 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-87
2022-03-28 13:58:46 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (533316ms till timeout)
2022-03-28 13:58:46 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (63965ms till timeout)
2022-03-28 13:58:46 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-87
2022-03-28 13:58:46 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:58:46 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-89
2022-03-28 13:58:46 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:58:46 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-28 13:58:46 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:58:46 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:58:46 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-4994018d-kafka-2 hasn't rolled
2022-03-28 13:58:46 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f} pods didn't roll. Remaining seconds for stability: 44
2022-03-28 13:58:46 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (292640ms till timeout)
2022-03-28 13:58:46 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (187070ms till timeout)
2022-03-28 13:58:46 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-28 13:58:46 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 13:58:46 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (472165ms till timeout)
2022-03-28 13:58:46 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-89
2022-03-28 13:58:46 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:58:46 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-9
2022-03-28 13:58:47 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (62964ms till timeout)
2022-03-28 13:58:47 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (532217ms till timeout)
2022-03-28 13:58:47 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-9
2022-03-28 13:58:47 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:58:47 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-90
2022-03-28 13:58:47 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:58:47 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:58:47 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:58:47 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-4994018d-kafka-2 hasn't rolled
2022-03-28 13:58:47 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f} pods didn't roll. Remaining seconds for stability: 43
2022-03-28 13:58:47 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (291446ms till timeout)
2022-03-28 13:58:47 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (185876ms till timeout)
2022-03-28 13:58:47 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-28 13:58:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-90
2022-03-28 13:58:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:58:48 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-92
2022-03-28 13:58:48 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (61963ms till timeout)
2022-03-28 13:58:48 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (531120ms till timeout)
2022-03-28 13:58:48 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-28 13:58:48 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 13:58:48 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (470715ms till timeout)
2022-03-28 13:58:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-92
2022-03-28 13:58:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:58:48 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-93
2022-03-28 13:58:48 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:58:49 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:58:49 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:58:49 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-4994018d-kafka-2 hasn't rolled
2022-03-28 13:58:49 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f} pods didn't roll. Remaining seconds for stability: 42
2022-03-28 13:58:49 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (290252ms till timeout)
2022-03-28 13:58:49 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (184682ms till timeout)
2022-03-28 13:58:49 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (60963ms till timeout)
2022-03-28 13:58:49 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-93
2022-03-28 13:58:49 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:58:49 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-95
2022-03-28 13:58:49 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-28 13:58:49 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (530022ms till timeout)
2022-03-28 13:58:49 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-28 13:58:49 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 13:58:49 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (469257ms till timeout)
2022-03-28 13:58:49 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-95
2022-03-28 13:58:49 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:58:49 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-96
2022-03-28 13:58:50 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:58:50 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (59962ms till timeout)
2022-03-28 13:58:50 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:58:50 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:58:50 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-4994018d-kafka-2 hasn't rolled
2022-03-28 13:58:50 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f} pods didn't roll. Remaining seconds for stability: 41
2022-03-28 13:58:50 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (289057ms till timeout)
2022-03-28 13:58:50 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (183486ms till timeout)
2022-03-28 13:58:50 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-96
2022-03-28 13:58:50 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:58:50 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-99
2022-03-28 13:58:50 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (528924ms till timeout)
2022-03-28 13:58:50 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testConfigurationReflection is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 13:58:50 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-28 13:58:50 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-99
2022-03-28 13:58:50 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 13:58:50 [ForkJoinPool-1-worker-9] DEBUG [AbstractST:674] ============================================================================
2022-03-28 13:58:50 [ForkJoinPool-1-worker-9] DEBUG [AbstractST:675] [operators.topic.ThrottlingQuotaST - After Each] - Clean up after test
2022-03-28 13:58:50 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:344] ############################################################################
2022-03-28 13:58:50 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:348] Delete all resources for testThrottlingQuotasCreateTopic
2022-03-28 13:58:50 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:241] Delete of Job create-admin-my-cluster-ac2cadd6-kafka-clients in namespace throttling-quota-st
2022-03-28 13:58:51 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:create-admin-my-cluster-ac2cadd6-kafka-clients
2022-03-28 13:58:51 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:241] Delete of KafkaUser my-user-46611685-1079680568 in namespace throttling-quota-st
2022-03-28 13:58:51 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (58961ms till timeout)
2022-03-28 13:58:51 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:58:51 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-46611685-1079680568
2022-03-28 13:58:51 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-28 13:58:51 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 1
2022-03-28 13:58:51 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 13:58:51 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Error from server (NotFound): namespaces "namespace-7" not found
2022-03-28 13:58:51 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] ======STDERR END======
2022-03-28 13:58:51 [ForkJoinPool-1-worker-13] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[], io.strimzi.test.logs.CollectorElement@aec142ef=[], io.strimzi.test.logs.CollectorElement@f851b6c3=[namespace-6], io.strimzi.test.logs.CollectorElement@3881d5f2=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@5c7379cb=[], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[namespace-4], io.strimzi.test.logs.CollectorElement@f4405b3b=[namespace-5], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 13:58:51 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:267] testTlsExternalUser - Notifies waiting test cases:[testCapacityFile, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testThrottlingQuotasCreateAlterPartitions, testSendSimpleMessageTls, testReceiveSimpleMessageTlsScramSha, testSendSimpleMessageTlsScramSha, testReceiveSimpleMessageTls, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testConfigurationFileIsCreated, testPauseReconciliationInKafkaRebalanceAndTopic, testConfigurationReflection] to and randomly select one to start execution
2022-03-28 13:58:51 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:93] [operators.user.UserST] - Removing parallel test: testTlsExternalUser
2022-03-28 13:58:51 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:97] [operators.user.UserST] - Parallel test count: 6
2022-03-28 13:58:51 [ForkJoinPool-1-worker-13] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.user.UserST.testTlsExternalUser-FINISHED
2022-03-28 13:58:51 [ForkJoinPool-1-worker-13] INFO  [TestSeparator:30] ############################################################################
2022-03-28 13:58:51 [ForkJoinPool-1-worker-13] INFO  [TestSeparator:23] ############################################################################
2022-03-28 13:58:51 [ForkJoinPool-1-worker-13] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.user.UserST.testUpdateUser-STARTED
2022-03-28 13:58:51 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:658] ============================================================================
2022-03-28 13:58:51 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:659] [operators.user.UserST - Before Each] - Setup test case environment
2022-03-28 13:58:51 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:77] [operators.user.UserST] - Adding parallel test: testUpdateUser
2022-03-28 13:58:51 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:81] [operators.user.UserST] - Parallel test count: 7
2022-03-28 13:58:51 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:205] [testUpdateUser] moved to the WaitZone, because current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 13:58:51 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testUpdateUser is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 13:58:51 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:58:51 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:58:51 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-4994018d-kafka-2 hasn't rolled
2022-03-28 13:58:51 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f} pods didn't roll. Remaining seconds for stability: 40
2022-03-28 13:58:51 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (287828ms till timeout)
2022-03-28 13:58:51 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:369] ############################################################################
2022-03-28 13:58:51 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:267] testThrottlingQuotasCreateTopic - Notifies waiting test cases:[testCapacityFile, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testThrottlingQuotasCreateAlterPartitions, testSendSimpleMessageTls, testReceiveSimpleMessageTlsScramSha, testSendSimpleMessageTlsScramSha, testReceiveSimpleMessageTls, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testConfigurationFileIsCreated, testPauseReconciliationInKafkaRebalanceAndTopic, testConfigurationReflection, testUpdateUser] to and randomly select one to start execution
2022-03-28 13:58:51 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:93] [operators.topic.ThrottlingQuotaST] - Removing parallel test: testThrottlingQuotasCreateTopic
2022-03-28 13:58:51 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:97] [operators.topic.ThrottlingQuotaST] - Parallel test count: 6
2022-03-28 13:58:51 [ForkJoinPool-1-worker-9] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.topic.ThrottlingQuotaST.testThrottlingQuotasCreateTopic-FINISHED
2022-03-28 13:58:51 [ForkJoinPool-1-worker-9] INFO  [TestSeparator:30] ############################################################################
2022-03-28 13:58:51 [ForkJoinPool-1-worker-9] INFO  [TestSeparator:23] ############################################################################
2022-03-28 13:58:51 [ForkJoinPool-1-worker-9] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.topic.ThrottlingQuotaST.testThrottlingQuotasDeleteTopic-STARTED
2022-03-28 13:58:51 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (182342ms till timeout)
2022-03-28 13:58:51 [ForkJoinPool-1-worker-9] DEBUG [AbstractST:658] ============================================================================
2022-03-28 13:58:51 [ForkJoinPool-1-worker-9] DEBUG [AbstractST:659] [operators.topic.ThrottlingQuotaST - Before Each] - Setup test case environment
2022-03-28 13:58:51 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:77] [operators.topic.ThrottlingQuotaST] - Adding parallel test: testThrottlingQuotasDeleteTopic
2022-03-28 13:58:51 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:81] [operators.topic.ThrottlingQuotaST] - Parallel test count: 7
2022-03-28 13:58:51 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:205] [testThrottlingQuotasDeleteTopic] moved to the WaitZone, because current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 13:58:51 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:210] testThrottlingQuotasDeleteTopic is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 13:58:51 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (527826ms till timeout)
2022-03-28 13:58:52 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (57961ms till timeout)
2022-03-28 13:58:52 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:58:52 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:58:52 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:58:52 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-4994018d-kafka-2 hasn't rolled
2022-03-28 13:58:52 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f} pods didn't roll. Remaining seconds for stability: 39
2022-03-28 13:58:52 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (286632ms till timeout)
2022-03-28 13:58:52 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (181063ms till timeout)
2022-03-28 13:58:52 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (526729ms till timeout)
2022-03-28 13:58:53 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (56960ms till timeout)
2022-03-28 13:58:53 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:58:53 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:58:53 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:58:53 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-4994018d-kafka-2 hasn't rolled
2022-03-28 13:58:53 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f} pods didn't roll. Remaining seconds for stability: 38
2022-03-28 13:58:53 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (285437ms till timeout)
2022-03-28 13:58:53 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (179867ms till timeout)
2022-03-28 13:58:53 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (525546ms till timeout)
2022-03-28 13:58:54 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (55959ms till timeout)
2022-03-28 13:58:54 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:58:55 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:58:55 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:58:55 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-4994018d-kafka-2 hasn't rolled
2022-03-28 13:58:55 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f} pods didn't roll. Remaining seconds for stability: 37
2022-03-28 13:58:55 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (284245ms till timeout)
2022-03-28 13:58:55 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (524449ms till timeout)
2022-03-28 13:58:55 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (178673ms till timeout)
2022-03-28 13:58:55 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (54959ms till timeout)
2022-03-28 13:58:55 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:230] testConfigurationReflection test now can proceed its execution
2022-03-28 13:58:55 [ForkJoinPool-1-worker-3] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 13:58:55 [ForkJoinPool-1-worker-3] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testScramUserWithQuotas=my-cluster-6f653060, testReceiveSimpleMessageTlsScramSha=my-cluster-af298d01, testConfigurationFileIsCreated=my-cluster-3244df0d, testThrottlingQuotasCreateAlterPartitions=my-cluster-8f6c28cb, testSendSimpleMessageTls=my-cluster-fbc845dd, testCreatingUsersWithSecretPrefix=my-cluster-0c65d2dd, testConfigurationReflection=my-cluster-5137fff1, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-0444229e, testUserWithNameMoreThan64Chars=my-cluster-02e25de6, testDeployAndUnDeployCruiseControl=my-cluster-844045c3, testTlsUserWithQuotas=my-cluster-9832c9ae, testThrottlingQuotasCreateTopic=my-cluster-ac2cadd6, testCapacityFile=my-cluster-0531d5ce, testReceiveSimpleMessageTls=my-cluster-ace1fa24, testConfigurationPerformanceOptions=my-cluster-4994018d, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-8af61d52, testTlsExternalUser=my-cluster-43f83aef, testSendSimpleMessageTlsScramSha=my-cluster-e353214b}
2022-03-28 13:58:55 [ForkJoinPool-1-worker-3] TRACE [AbstractST:607] USERS_NAME_MAP: {testScramUserWithQuotas=my-user-27563760-1632863338, testReceiveSimpleMessageTlsScramSha=my-user-1597210213-707359243, testConfigurationFileIsCreated=my-user-1799841978-1468805631, testThrottlingQuotasCreateAlterPartitions=my-user-1557959119-1722636050, testSendSimpleMessageTls=my-user-1754673553-2077518027, testCreatingUsersWithSecretPrefix=my-user-144001639-226807213, testConfigurationReflection=my-user-2029261898-1375854235, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-1010649803-214635564, testUserWithNameMoreThan64Chars=my-user-2124392695-340403978, testDeployAndUnDeployCruiseControl=my-user-2117120687-604608920, testTlsUserWithQuotas=my-user-1602931187-443256751, testThrottlingQuotasCreateTopic=my-user-46611685-1079680568, testCapacityFile=my-user-155969448-1270555822, testReceiveSimpleMessageTls=my-user-1418772225-1024845009, testConfigurationPerformanceOptions=my-user-882948629-1821489961, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-1807142870-188660517, testTlsExternalUser=my-user-370513135-1989152728, testSendSimpleMessageTlsScramSha=my-user-182357028-1132873010}
2022-03-28 13:58:55 [ForkJoinPool-1-worker-3] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testScramUserWithQuotas=my-topic-1345794255-226872460, testReceiveSimpleMessageTlsScramSha=my-topic-649423016-1623551209, testConfigurationFileIsCreated=my-topic-1435014906-681104826, testThrottlingQuotasCreateAlterPartitions=my-topic-1449977352-344480794, testSendSimpleMessageTls=my-topic-115477502-2025274778, testCreatingUsersWithSecretPrefix=my-topic-130462979-2061578809, testConfigurationReflection=my-topic-1621095669-1276202830, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-889283546-691691584, testUserWithNameMoreThan64Chars=my-topic-1053060011-1199293498, testDeployAndUnDeployCruiseControl=my-topic-1101175281-2107378523, testTlsUserWithQuotas=my-topic-985048738-437665866, testThrottlingQuotasCreateTopic=my-topic-114922478-1506136333, testCapacityFile=my-topic-296605537-1008743104, testReceiveSimpleMessageTls=my-topic-1131768783-120051821, testConfigurationPerformanceOptions=my-topic-627301835-843876360, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-1367124108-1748498395, testTlsExternalUser=my-topic-2081139838-1814296134, testSendSimpleMessageTlsScramSha=my-topic-652506937-1802310709}
2022-03-28 13:58:55 [ForkJoinPool-1-worker-3] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testScramUserWithQuotas=my-cluster-6f653060-kafka-clients, testReceiveSimpleMessageTlsScramSha=my-cluster-af298d01-kafka-clients, testConfigurationFileIsCreated=my-cluster-3244df0d-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-8f6c28cb-kafka-clients, testSendSimpleMessageTls=my-cluster-fbc845dd-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-0c65d2dd-kafka-clients, testConfigurationReflection=my-cluster-5137fff1-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-0444229e-kafka-clients, testUserWithNameMoreThan64Chars=my-cluster-02e25de6-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-844045c3-kafka-clients, testTlsUserWithQuotas=my-cluster-9832c9ae-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-ac2cadd6-kafka-clients, testCapacityFile=my-cluster-0531d5ce-kafka-clients, testReceiveSimpleMessageTls=my-cluster-ace1fa24-kafka-clients, testConfigurationPerformanceOptions=my-cluster-4994018d-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-8af61d52-kafka-clients, testTlsExternalUser=my-cluster-43f83aef-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-e353214b-kafka-clients}
2022-03-28 13:58:55 [ForkJoinPool-1-worker-3] INFO  [TestSuiteNamespaceManager:163] Creating namespace:namespace-8 for test case:testConfigurationReflection
2022-03-28 13:58:55 [ForkJoinPool-1-worker-3] INFO  [KubeClusterResource:156] Creating Namespace: namespace-8
2022-03-28 13:58:55 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Namespace namespace-8
2022-03-28 13:58:55 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-8 -o json
2022-03-28 13:58:56 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:58:56 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (523327ms till timeout)
2022-03-28 13:58:56 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (53958ms till timeout)
2022-03-28 13:58:56 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:58:56 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:58:56 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-4994018d-kafka-2 hasn't rolled
2022-03-28 13:58:56 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f} pods didn't roll. Remaining seconds for stability: 36
2022-03-28 13:58:56 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (283027ms till timeout)
2022-03-28 13:58:56 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testUpdateUser is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 13:58:56 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (177456ms till timeout)
2022-03-28 13:58:56 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-8 -o json
2022-03-28 13:58:56 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 13:58:56 [ForkJoinPool-1-worker-3] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c32,c14",
            "openshift.io/sa.scc.supplemental-groups": "1001020000/10000",
            "openshift.io/sa.scc.uid-range": "1001020000/10000"
        },
        "creationTimestamp": "2022-03-28T13:58:51Z",
        "labels": {
            "kubernetes.io/metadata.name": "namespace-8"
        },
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T13:58:51Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:labels": {
                            ".": {},
                            "f:kubernetes.io/metadata.name": {}
                        }
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T13:58:51Z"
            }
        ],
        "name": "namespace-8",
        "resourceVersion": "93684",
        "uid": "61da6237-debd-4b30-b04d-98182821287e"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 13:58:56 [ForkJoinPool-1-worker-3] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[], io.strimzi.test.logs.CollectorElement@c4c0ea0=[namespace-8], io.strimzi.test.logs.CollectorElement@aec142ef=[], io.strimzi.test.logs.CollectorElement@f851b6c3=[namespace-6], io.strimzi.test.logs.CollectorElement@3881d5f2=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@5c7379cb=[], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[namespace-4], io.strimzi.test.logs.CollectorElement@f4405b3b=[namespace-5], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 13:58:56 [ForkJoinPool-1-worker-3] INFO  [KubeClusterResource:82] Client use Namespace: namespace-8
2022-03-28 13:58:56 [ForkJoinPool-1-worker-3] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-8, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 13:58:56 [ForkJoinPool-1-worker-3] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-8
2022-03-28 13:58:56 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:155] Create/Update Kafka my-cluster-5137fff1 in namespace namespace-8
2022-03-28 13:58:56 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:164] Using Namespace: namespace-8
2022-03-28 13:58:56 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:210] testThrottlingQuotasDeleteTopic is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 13:58:56 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-5137fff1
2022-03-28 13:58:56 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:433] Wait for Kafka: my-cluster-5137fff1 will have desired state: Ready
2022-03-28 13:58:56 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Kafka: my-cluster-5137fff1 will have desired state: Ready
2022-03-28 13:58:56 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1319904ms till timeout)
2022-03-28 13:58:57 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (52958ms till timeout)
2022-03-28 13:58:57 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:58:57 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (522230ms till timeout)
2022-03-28 13:58:57 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:58:57 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:58:57 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-4994018d-kafka-2 hasn't rolled
2022-03-28 13:58:57 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f} pods didn't roll. Remaining seconds for stability: 35
2022-03-28 13:58:57 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (281834ms till timeout)
2022-03-28 13:58:57 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (176345ms till timeout)
2022-03-28 13:58:57 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1318805ms till timeout)
2022-03-28 13:58:58 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (51957ms till timeout)
2022-03-28 13:58:58 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (521131ms till timeout)
2022-03-28 13:58:58 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:58:58 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:58:58 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:58:58 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-4994018d-kafka-2 hasn't rolled
2022-03-28 13:58:58 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f} pods didn't roll. Remaining seconds for stability: 34
2022-03-28 13:58:58 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (280643ms till timeout)
2022-03-28 13:58:58 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (175122ms till timeout)
2022-03-28 13:58:58 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1317708ms till timeout)
2022-03-28 13:58:59 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (50956ms till timeout)
2022-03-28 13:58:59 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (520033ms till timeout)
2022-03-28 13:58:59 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:58:59 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:58:59 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:58:59 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-4994018d-kafka-2 hasn't rolled
2022-03-28 13:58:59 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f} pods didn't roll. Remaining seconds for stability: 33
2022-03-28 13:58:59 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (279451ms till timeout)
2022-03-28 13:58:59 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (173965ms till timeout)
2022-03-28 13:58:59 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1316609ms till timeout)
2022-03-28 13:59:00 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (49956ms till timeout)
2022-03-28 13:59:00 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (518935ms till timeout)
2022-03-28 13:59:00 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:01 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:01 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:01 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-4994018d-kafka-2 hasn't rolled
2022-03-28 13:59:01 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f} pods didn't roll. Remaining seconds for stability: 32
2022-03-28 13:59:01 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (278258ms till timeout)
2022-03-28 13:59:01 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (172771ms till timeout)
2022-03-28 13:59:01 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1315465ms till timeout)
2022-03-28 13:59:01 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (48955ms till timeout)
2022-03-28 13:59:01 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testUpdateUser is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 13:59:01 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:210] testThrottlingQuotasDeleteTopic is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 13:59:01 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (517837ms till timeout)
2022-03-28 13:59:02 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:02 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (47955ms till timeout)
2022-03-28 13:59:02 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:02 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:02 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-4994018d-kafka-2 hasn't rolled
2022-03-28 13:59:02 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f} pods didn't roll. Remaining seconds for stability: 31
2022-03-28 13:59:02 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (277064ms till timeout)
2022-03-28 13:59:02 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1314364ms till timeout)
2022-03-28 13:59:02 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (171493ms till timeout)
2022-03-28 13:59:02 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (516726ms till timeout)
2022-03-28 13:59:03 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (46954ms till timeout)
2022-03-28 13:59:03 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:03 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:03 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:03 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-4994018d-kafka-2 hasn't rolled
2022-03-28 13:59:03 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f} pods didn't roll. Remaining seconds for stability: 30
2022-03-28 13:59:03 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (275871ms till timeout)
2022-03-28 13:59:03 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1313166ms till timeout)
2022-03-28 13:59:03 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (170300ms till timeout)
2022-03-28 13:59:03 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (515629ms till timeout)
2022-03-28 13:59:04 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (45953ms till timeout)
2022-03-28 13:59:04 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:04 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:04 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:04 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-4994018d-kafka-2 hasn't rolled
2022-03-28 13:59:04 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f} pods didn't roll. Remaining seconds for stability: 29
2022-03-28 13:59:04 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (274677ms till timeout)
2022-03-28 13:59:04 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1311977ms till timeout)
2022-03-28 13:59:04 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (169106ms till timeout)
2022-03-28 13:59:05 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (514499ms till timeout)
2022-03-28 13:59:05 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (44953ms till timeout)
2022-03-28 13:59:05 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:05 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:05 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:05 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-4994018d-kafka-2 hasn't rolled
2022-03-28 13:59:05 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f} pods didn't roll. Remaining seconds for stability: 28
2022-03-28 13:59:05 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (273484ms till timeout)
2022-03-28 13:59:05 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1310779ms till timeout)
2022-03-28 13:59:05 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (167912ms till timeout)
2022-03-28 13:59:06 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (513401ms till timeout)
2022-03-28 13:59:06 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (43952ms till timeout)
2022-03-28 13:59:06 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testUpdateUser is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 13:59:06 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:210] testThrottlingQuotasDeleteTopic is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 13:59:06 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:07 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:07 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:07 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-4994018d-kafka-2 hasn't rolled
2022-03-28 13:59:07 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f} pods didn't roll. Remaining seconds for stability: 27
2022-03-28 13:59:07 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (272290ms till timeout)
2022-03-28 13:59:07 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1309591ms till timeout)
2022-03-28 13:59:07 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (166719ms till timeout)
2022-03-28 13:59:07 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (512303ms till timeout)
2022-03-28 13:59:07 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (42952ms till timeout)
2022-03-28 13:59:08 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:08 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:08 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:08 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-4994018d-kafka-2 hasn't rolled
2022-03-28 13:59:08 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f} pods didn't roll. Remaining seconds for stability: 26
2022-03-28 13:59:08 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (271095ms till timeout)
2022-03-28 13:59:08 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1308396ms till timeout)
2022-03-28 13:59:08 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (41951ms till timeout)
2022-03-28 13:59:08 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (165524ms till timeout)
2022-03-28 13:59:08 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (511206ms till timeout)
2022-03-28 13:59:09 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:09 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (40950ms till timeout)
2022-03-28 13:59:09 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:09 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:09 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-4994018d-kafka-2 hasn't rolled
2022-03-28 13:59:09 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f} pods didn't roll. Remaining seconds for stability: 25
2022-03-28 13:59:09 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (269899ms till timeout)
2022-03-28 13:59:09 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (510100ms till timeout)
2022-03-28 13:59:09 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (164329ms till timeout)
2022-03-28 13:59:09 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1307111ms till timeout)
2022-03-28 13:59:10 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (39950ms till timeout)
2022-03-28 13:59:10 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:10 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:10 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:10 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-4994018d-kafka-2 hasn't rolled
2022-03-28 13:59:10 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f} pods didn't roll. Remaining seconds for stability: 24
2022-03-28 13:59:10 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (268701ms till timeout)
2022-03-28 13:59:10 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (508870ms till timeout)
2022-03-28 13:59:10 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (163130ms till timeout)
2022-03-28 13:59:10 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1305913ms till timeout)
2022-03-28 13:59:11 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (38949ms till timeout)
2022-03-28 13:59:11 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testUpdateUser is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 13:59:11 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:210] testThrottlingQuotasDeleteTopic is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 13:59:11 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:11 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:11 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:11 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-4994018d-kafka-2 hasn't rolled
2022-03-28 13:59:11 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f} pods didn't roll. Remaining seconds for stability: 23
2022-03-28 13:59:11 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (267506ms till timeout)
2022-03-28 13:59:11 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1304807ms till timeout)
2022-03-28 13:59:11 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (161935ms till timeout)
2022-03-28 13:59:11 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (507622ms till timeout)
2022-03-28 13:59:12 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (37949ms till timeout)
2022-03-28 13:59:12 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:12 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:12 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:12 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-4994018d-kafka-2 hasn't rolled
2022-03-28 13:59:12 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f} pods didn't roll. Remaining seconds for stability: 22
2022-03-28 13:59:12 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (266309ms till timeout)
2022-03-28 13:59:12 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (506514ms till timeout)
2022-03-28 13:59:13 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1303521ms till timeout)
2022-03-28 13:59:13 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (160738ms till timeout)
2022-03-28 13:59:13 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (36948ms till timeout)
2022-03-28 13:59:13 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:14 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:14 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:14 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-4994018d-kafka-2 hasn't rolled
2022-03-28 13:59:14 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f} pods didn't roll. Remaining seconds for stability: 21
2022-03-28 13:59:14 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (265112ms till timeout)
2022-03-28 13:59:14 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (505317ms till timeout)
2022-03-28 13:59:14 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (35947ms till timeout)
2022-03-28 13:59:14 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (159542ms till timeout)
2022-03-28 13:59:14 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1302325ms till timeout)
2022-03-28 13:59:15 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:15 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (34947ms till timeout)
2022-03-28 13:59:15 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:15 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:15 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-4994018d-kafka-2 hasn't rolled
2022-03-28 13:59:15 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f} pods didn't roll. Remaining seconds for stability: 20
2022-03-28 13:59:15 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (263912ms till timeout)
2022-03-28 13:59:15 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (504115ms till timeout)
2022-03-28 13:59:15 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (158342ms till timeout)
2022-03-28 13:59:15 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1301124ms till timeout)
2022-03-28 13:59:16 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (33946ms till timeout)
2022-03-28 13:59:16 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testUpdateUser is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 13:59:16 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:16 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:210] testThrottlingQuotasDeleteTopic is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 13:59:16 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:16 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:16 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-4994018d-kafka-2 hasn't rolled
2022-03-28 13:59:16 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f} pods didn't roll. Remaining seconds for stability: 19
2022-03-28 13:59:16 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (262715ms till timeout)
2022-03-28 13:59:16 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1300017ms till timeout)
2022-03-28 13:59:16 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (157145ms till timeout)
2022-03-28 13:59:16 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (502831ms till timeout)
2022-03-28 13:59:17 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (32946ms till timeout)
2022-03-28 13:59:17 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:17 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1298916ms till timeout)
2022-03-28 13:59:17 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (501724ms till timeout)
2022-03-28 13:59:17 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:17 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:17 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-4994018d-kafka-2 hasn't rolled
2022-03-28 13:59:17 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f} pods didn't roll. Remaining seconds for stability: 18
2022-03-28 13:59:17 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (261422ms till timeout)
2022-03-28 13:59:17 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (155851ms till timeout)
2022-03-28 13:59:18 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (31945ms till timeout)
2022-03-28 13:59:18 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1297817ms till timeout)
2022-03-28 13:59:18 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:18 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (500625ms till timeout)
2022-03-28 13:59:19 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:19 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:19 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-4994018d-kafka-2 hasn't rolled
2022-03-28 13:59:19 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f} pods didn't roll. Remaining seconds for stability: 17
2022-03-28 13:59:19 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (260229ms till timeout)
2022-03-28 13:59:19 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (154658ms till timeout)
2022-03-28 13:59:19 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (30944ms till timeout)
2022-03-28 13:59:19 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1296717ms till timeout)
2022-03-28 13:59:19 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (499526ms till timeout)
2022-03-28 13:59:20 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:20 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (29944ms till timeout)
2022-03-28 13:59:20 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:20 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:20 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-4994018d-kafka-2 hasn't rolled
2022-03-28 13:59:20 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f} pods didn't roll. Remaining seconds for stability: 16
2022-03-28 13:59:20 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (259035ms till timeout)
2022-03-28 13:59:20 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (153465ms till timeout)
2022-03-28 13:59:20 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1295619ms till timeout)
2022-03-28 13:59:21 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (498425ms till timeout)
2022-03-28 13:59:21 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (28943ms till timeout)
2022-03-28 13:59:21 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:21 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testUpdateUser is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 13:59:21 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:21 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:21 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-4994018d-kafka-2 hasn't rolled
2022-03-28 13:59:21 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f} pods didn't roll. Remaining seconds for stability: 15
2022-03-28 13:59:21 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (257841ms till timeout)
2022-03-28 13:59:21 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:210] testThrottlingQuotasDeleteTopic is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 13:59:21 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (152270ms till timeout)
2022-03-28 13:59:22 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1294520ms till timeout)
2022-03-28 13:59:22 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (497328ms till timeout)
2022-03-28 13:59:22 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (27942ms till timeout)
2022-03-28 13:59:22 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:22 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:22 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:22 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-4994018d-kafka-2 hasn't rolled
2022-03-28 13:59:22 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f} pods didn't roll. Remaining seconds for stability: 14
2022-03-28 13:59:22 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (256647ms till timeout)
2022-03-28 13:59:22 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (151076ms till timeout)
2022-03-28 13:59:23 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1293422ms till timeout)
2022-03-28 13:59:23 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (26942ms till timeout)
2022-03-28 13:59:23 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (496230ms till timeout)
2022-03-28 13:59:23 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:23 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:23 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:23 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-4994018d-kafka-2 hasn't rolled
2022-03-28 13:59:23 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f} pods didn't roll. Remaining seconds for stability: 13
2022-03-28 13:59:23 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (255453ms till timeout)
2022-03-28 13:59:23 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (149882ms till timeout)
2022-03-28 13:59:24 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (25941ms till timeout)
2022-03-28 13:59:24 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1292321ms till timeout)
2022-03-28 13:59:24 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (495124ms till timeout)
2022-03-28 13:59:24 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:25 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:25 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:25 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-4994018d-kafka-2 hasn't rolled
2022-03-28 13:59:25 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f} pods didn't roll. Remaining seconds for stability: 12
2022-03-28 13:59:25 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (254260ms till timeout)
2022-03-28 13:59:25 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (148689ms till timeout)
2022-03-28 13:59:25 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (24940ms till timeout)
2022-03-28 13:59:25 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1291222ms till timeout)
2022-03-28 13:59:25 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (494016ms till timeout)
2022-03-28 13:59:26 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:26 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (23939ms till timeout)
2022-03-28 13:59:26 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:26 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:26 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-4994018d-kafka-2 hasn't rolled
2022-03-28 13:59:26 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f} pods didn't roll. Remaining seconds for stability: 11
2022-03-28 13:59:26 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (253023ms till timeout)
2022-03-28 13:59:26 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testUpdateUser is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 13:59:26 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (147451ms till timeout)
2022-03-28 13:59:26 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1290124ms till timeout)
2022-03-28 13:59:26 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:210] testThrottlingQuotasDeleteTopic is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 13:59:26 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (492919ms till timeout)
2022-03-28 13:59:27 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (22937ms till timeout)
2022-03-28 13:59:27 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:27 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:27 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:27 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-4994018d-kafka-2 hasn't rolled
2022-03-28 13:59:27 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f} pods didn't roll. Remaining seconds for stability: 10
2022-03-28 13:59:27 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (251830ms till timeout)
2022-03-28 13:59:27 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (146259ms till timeout)
2022-03-28 13:59:27 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1289026ms till timeout)
2022-03-28 13:59:27 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (491821ms till timeout)
2022-03-28 13:59:28 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (21936ms till timeout)
2022-03-28 13:59:28 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:28 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:28 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:28 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-4994018d-kafka-2 hasn't rolled
2022-03-28 13:59:28 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f} pods didn't roll. Remaining seconds for stability: 9
2022-03-28 13:59:28 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (250636ms till timeout)
2022-03-28 13:59:28 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (145065ms till timeout)
2022-03-28 13:59:28 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1287843ms till timeout)
2022-03-28 13:59:28 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (490656ms till timeout)
2022-03-28 13:59:29 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (20936ms till timeout)
2022-03-28 13:59:29 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:29 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:29 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:29 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-4994018d-kafka-2 hasn't rolled
2022-03-28 13:59:29 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f} pods didn't roll. Remaining seconds for stability: 8
2022-03-28 13:59:29 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (249443ms till timeout)
2022-03-28 13:59:29 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1286742ms till timeout)
2022-03-28 13:59:29 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (143871ms till timeout)
2022-03-28 13:59:29 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (489554ms till timeout)
2022-03-28 13:59:30 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (19935ms till timeout)
2022-03-28 13:59:30 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:31 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:31 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:31 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-4994018d-kafka-2 hasn't rolled
2022-03-28 13:59:31 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f} pods didn't roll. Remaining seconds for stability: 7
2022-03-28 13:59:31 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (248248ms till timeout)
2022-03-28 13:59:31 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (488415ms till timeout)
2022-03-28 13:59:31 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (142677ms till timeout)
2022-03-28 13:59:31 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1285459ms till timeout)
2022-03-28 13:59:31 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (18934ms till timeout)
2022-03-28 13:59:31 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testUpdateUser is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 13:59:31 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:210] testThrottlingQuotasDeleteTopic is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 13:59:32 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:32 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:32 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:32 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-4994018d-kafka-2 hasn't rolled
2022-03-28 13:59:32 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f} pods didn't roll. Remaining seconds for stability: 6
2022-03-28 13:59:32 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (247054ms till timeout)
2022-03-28 13:59:32 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (487257ms till timeout)
2022-03-28 13:59:32 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (17934ms till timeout)
2022-03-28 13:59:32 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1284265ms till timeout)
2022-03-28 13:59:32 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (141482ms till timeout)
2022-03-28 13:59:33 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:33 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (16933ms till timeout)
2022-03-28 13:59:33 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:33 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:33 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-4994018d-kafka-2 hasn't rolled
2022-03-28 13:59:33 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f} pods didn't roll. Remaining seconds for stability: 5
2022-03-28 13:59:33 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (245859ms till timeout)
2022-03-28 13:59:33 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (486025ms till timeout)
2022-03-28 13:59:33 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (140287ms till timeout)
2022-03-28 13:59:33 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1283070ms till timeout)
2022-03-28 13:59:34 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (15933ms till timeout)
2022-03-28 13:59:34 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:34 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:34 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:34 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-4994018d-kafka-2 hasn't rolled
2022-03-28 13:59:34 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f} pods didn't roll. Remaining seconds for stability: 4
2022-03-28 13:59:34 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (244663ms till timeout)
2022-03-28 13:59:34 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1281963ms till timeout)
2022-03-28 13:59:34 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (139091ms till timeout)
2022-03-28 13:59:34 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (484778ms till timeout)
2022-03-28 13:59:35 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (14932ms till timeout)
2022-03-28 13:59:35 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:35 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:35 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:35 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-4994018d-kafka-2 hasn't rolled
2022-03-28 13:59:35 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f} pods didn't roll. Remaining seconds for stability: 3
2022-03-28 13:59:35 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (243468ms till timeout)
2022-03-28 13:59:35 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (483672ms till timeout)
2022-03-28 13:59:35 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (137897ms till timeout)
2022-03-28 13:59:35 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1280679ms till timeout)
2022-03-28 13:59:36 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (13932ms till timeout)
2022-03-28 13:59:36 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testUpdateUser is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 13:59:36 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:210] testThrottlingQuotasDeleteTopic is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 13:59:36 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:37 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:37 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:37 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-4994018d-kafka-2 hasn't rolled
2022-03-28 13:59:37 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f} pods didn't roll. Remaining seconds for stability: 2
2022-03-28 13:59:37 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (242273ms till timeout)
2022-03-28 13:59:37 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (482477ms till timeout)
2022-03-28 13:59:37 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (136693ms till timeout)
2022-03-28 13:59:37 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1279475ms till timeout)
2022-03-28 13:59:37 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (12931ms till timeout)
2022-03-28 13:59:38 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:38 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:38 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:38 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-4994018d-kafka-2 hasn't rolled
2022-03-28 13:59:38 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f} pods didn't roll. Remaining seconds for stability: 1
2022-03-28 13:59:38 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (241079ms till timeout)
2022-03-28 13:59:38 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (481284ms till timeout)
2022-03-28 13:59:38 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (11931ms till timeout)
2022-03-28 13:59:38 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1278286ms till timeout)
2022-03-28 13:59:38 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (135501ms till timeout)
2022-03-28 13:59:39 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:39 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (10930ms till timeout)
2022-03-28 13:59:39 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:39 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9}
2022-03-28 13:59:39 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-4994018d-kafka-2 hasn't rolled
2022-03-28 13:59:39 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-4994018d-kafka-2=7987f4ce-373d-41b6-b4cf-3d2aeaeec5c9, my-cluster-4994018d-kafka-1=367bee6b-96b8-4b43-a24b-9f79a1ef98ea, my-cluster-4994018d-kafka-0=6c6858ab-c697-4e5f-9dbd-89bbc263910f} pods didn't roll. Remaining seconds for stability: 0
2022-03-28 13:59:39 [ForkJoinPool-1-worker-5] INFO  [CruiseControlConfigurationST:283] Verifying new configuration in the Kafka CR
2022-03-28 13:59:39 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (480087ms till timeout)
2022-03-28 13:59:39 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (134313ms till timeout)
2022-03-28 13:59:39 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1277096ms till timeout)
2022-03-28 13:59:39 [ForkJoinPool-1-worker-5] INFO  [CruiseControlConfigurationST:300] Verifying Cruise control performance options are set in Kafka CR
2022-03-28 13:59:39 [ForkJoinPool-1-worker-5] DEBUG [AbstractST:674] ============================================================================
2022-03-28 13:59:39 [ForkJoinPool-1-worker-5] DEBUG [AbstractST:675] [cruisecontrol.CruiseControlConfigurationST - After Each] - Clean up after test
2022-03-28 13:59:39 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:344] ############################################################################
2022-03-28 13:59:39 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:348] Delete all resources for testConfigurationPerformanceOptions
2022-03-28 13:59:39 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:241] Delete of Kafka my-cluster-4994018d in namespace namespace-6
2022-03-28 13:59:39 [ForkJoinPool-1-worker-5] INFO  [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-6, for cruise control Kafka cluster my-cluster-4994018d
2022-03-28 13:59:40 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (9930ms till timeout)
2022-03-28 13:59:40 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-4994018d
2022-03-28 13:59:40 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (478981ms till timeout)
2022-03-28 13:59:40 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:369] ############################################################################
2022-03-28 13:59:40 [ForkJoinPool-1-worker-5] INFO  [TestSuiteNamespaceManager:200] Deleting namespace:namespace-6 for test case:testConfigurationPerformanceOptions
2022-03-28 13:59:40 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1275955ms till timeout)
2022-03-28 13:59:40 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (133168ms till timeout)
2022-03-28 13:59:40 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Namespace namespace-6 removal
2022-03-28 13:59:40 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-6 -o yaml
2022-03-28 13:59:41 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-6 -o yaml
2022-03-28 13:59:41 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 13:59:41 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (479551ms till timeout)
2022-03-28 13:59:41 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (8929ms till timeout)
2022-03-28 13:59:41 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testUpdateUser is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 13:59:41 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:210] testThrottlingQuotasDeleteTopic is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 13:59:41 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (477884ms till timeout)
2022-03-28 13:59:41 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1274858ms till timeout)
2022-03-28 13:59:41 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (131975ms till timeout)
2022-03-28 13:59:42 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-6 -o yaml
2022-03-28 13:59:42 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (7929ms till timeout)
2022-03-28 13:59:42 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-6 -o yaml
2022-03-28 13:59:42 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 13:59:42 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (478069ms till timeout)
2022-03-28 13:59:42 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (476785ms till timeout)
2022-03-28 13:59:42 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1273760ms till timeout)
2022-03-28 13:59:42 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (130874ms till timeout)
2022-03-28 13:59:43 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (6928ms till timeout)
2022-03-28 13:59:43 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-6 -o yaml
2022-03-28 13:59:43 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (475686ms till timeout)
2022-03-28 13:59:43 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1272663ms till timeout)
2022-03-28 13:59:44 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (129773ms till timeout)
2022-03-28 13:59:44 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-6 -o yaml
2022-03-28 13:59:44 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 13:59:44 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (476557ms till timeout)
2022-03-28 13:59:44 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (5927ms till timeout)
2022-03-28 13:59:44 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (474588ms till timeout)
2022-03-28 13:59:45 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1271565ms till timeout)
2022-03-28 13:59:45 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (128659ms till timeout)
2022-03-28 13:59:45 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-6 -o yaml
2022-03-28 13:59:45 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (4927ms till timeout)
2022-03-28 13:59:45 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-6 -o yaml
2022-03-28 13:59:45 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 13:59:45 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (475083ms till timeout)
2022-03-28 13:59:46 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (473486ms till timeout)
2022-03-28 13:59:46 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1270467ms till timeout)
2022-03-28 13:59:46 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (127558ms till timeout)
2022-03-28 13:59:46 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (3926ms till timeout)
2022-03-28 13:59:46 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testUpdateUser is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 13:59:46 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:210] testThrottlingQuotasDeleteTopic is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 13:59:46 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-6 -o yaml
2022-03-28 13:59:47 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (472389ms till timeout)
2022-03-28 13:59:47 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-6 -o yaml
2022-03-28 13:59:47 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 13:59:47 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (473628ms till timeout)
2022-03-28 13:59:47 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1269369ms till timeout)
2022-03-28 13:59:47 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (2926ms till timeout)
2022-03-28 13:59:47 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (126451ms till timeout)
2022-03-28 13:59:48 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-6 -o yaml
2022-03-28 13:59:48 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (471290ms till timeout)
2022-03-28 13:59:48 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (1925ms till timeout)
2022-03-28 13:59:48 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1268271ms till timeout)
2022-03-28 13:59:48 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (125351ms till timeout)
2022-03-28 13:59:48 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-6 -o yaml
2022-03-28 13:59:48 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 13:59:48 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (472170ms till timeout)
2022-03-28 13:59:49 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties not ready, will try again in 925 ms (925ms till timeout)
2022-03-28 13:59:49 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (470192ms till timeout)
2022-03-28 13:59:49 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1267174ms till timeout)
2022-03-28 13:59:49 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (124248ms till timeout)
2022-03-28 13:59:49 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-6 -o yaml
2022-03-28 13:59:50 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-6 -o yaml
2022-03-28 13:59:50 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 13:59:50 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (470716ms till timeout)
2022-03-28 13:59:50 [ForkJoinPool-1-worker-7] ERROR [TestUtils:162] Exception waiting for Verify that kafka configuration {cluster-name=my-cluster-844045c3} has correct cruise control metric reporter properties, null
2022-03-28 13:59:50 [ForkJoinPool-1-worker-7] INFO  [CruiseControlConfigurationST:126] Cruise Control topics will not be deleted and will stay in the Kafka cluster
2022-03-28 13:59:50 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Verify that kafka contains cruise control topics with related configuration.
2022-03-28 13:59:50 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Verify that kafka contains cruise control topics with related configuration.
2022-03-28 13:59:50 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (469056ms till timeout)
2022-03-28 13:59:50 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1266056ms till timeout)
2022-03-28 13:59:50 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (123073ms till timeout)
2022-03-28 13:59:50 [ForkJoinPool-1-worker-7] INFO  [CruiseControlConfigurationST:130] Adding Cruise Control to the classic Kafka.
2022-03-28 13:59:50 [ForkJoinPool-1-worker-7] INFO  [RollingUpdateUtils:73] Waiting for component with name: my-cluster-844045c3-kafka rolling update
2022-03-28 13:59:50 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:74] Waiting for rolling update of component matching LabelSelector: LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-844045c3-kafka, strimzi.io/cluster=my-cluster-844045c3, strimzi.io/kind=Kafka}, additionalProperties={})
2022-03-28 13:59:50 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for component with name my-cluster-844045c3-kafka rolling update
2022-03-28 13:59:50 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-844045c3-kafka-0=e24c016e-3d31-4327-92e5-6a4d2019dc61, my-cluster-844045c3-kafka-1=4e2bb7a9-1ab6-4428-b334-4497615c4bd2, my-cluster-844045c3-kafka-2=c477a607-0ed7-44a0-a864-53d364b682a1}
2022-03-28 13:59:51 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-6 -o yaml
2022-03-28 13:59:51 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-844045c3-kafka-0=e24c016e-3d31-4327-92e5-6a4d2019dc61, my-cluster-844045c3-kafka-1=4e2bb7a9-1ab6-4428-b334-4497615c4bd2, my-cluster-844045c3-kafka-2=c477a607-0ed7-44a0-a864-53d364b682a1}
2022-03-28 13:59:51 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-844045c3-kafka-0=e24c016e-3d31-4327-92e5-6a4d2019dc61, my-cluster-844045c3-kafka-1=4e2bb7a9-1ab6-4428-b334-4497615c4bd2, my-cluster-844045c3-kafka-2=c477a607-0ed7-44a0-a864-53d364b682a1}
2022-03-28 13:59:51 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:50] At least my-cluster-844045c3-kafka-1 hasn't rolled
2022-03-28 13:59:51 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] component with name my-cluster-844045c3-kafka rolling update not ready, will try again in 5000 ms (1799806ms till timeout)
2022-03-28 13:59:51 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testUpdateUser is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 13:59:51 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:210] testThrottlingQuotasDeleteTopic is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 13:59:51 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-6 -o yaml
2022-03-28 13:59:51 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 13:59:51 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (469250ms till timeout)
2022-03-28 13:59:51 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (467957ms till timeout)
2022-03-28 13:59:51 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1264954ms till timeout)
2022-03-28 13:59:51 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (121973ms till timeout)
2022-03-28 13:59:52 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-6 -o yaml
2022-03-28 13:59:52 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (466858ms till timeout)
2022-03-28 13:59:52 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1263857ms till timeout)
2022-03-28 13:59:52 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (120870ms till timeout)
2022-03-28 13:59:52 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-6 -o yaml
2022-03-28 13:59:52 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 13:59:52 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (467778ms till timeout)
2022-03-28 13:59:53 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (465759ms till timeout)
2022-03-28 13:59:53 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1262759ms till timeout)
2022-03-28 13:59:53 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-6 -o yaml
2022-03-28 13:59:54 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (119770ms till timeout)
2022-03-28 13:59:54 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-6 -o yaml
2022-03-28 13:59:54 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 13:59:54 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (466332ms till timeout)
2022-03-28 13:59:54 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (464659ms till timeout)
2022-03-28 13:59:54 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1261659ms till timeout)
2022-03-28 13:59:55 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (118670ms till timeout)
2022-03-28 13:59:55 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-6 -o yaml
2022-03-28 13:59:55 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-6 -o yaml
2022-03-28 13:59:55 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 13:59:55 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (464889ms till timeout)
2022-03-28 13:59:55 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (463561ms till timeout)
2022-03-28 13:59:56 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1260561ms till timeout)
2022-03-28 13:59:56 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-844045c3-kafka-0=e24c016e-3d31-4327-92e5-6a4d2019dc61, my-cluster-844045c3-kafka-1=4e2bb7a9-1ab6-4428-b334-4497615c4bd2, my-cluster-844045c3-kafka-2=c477a607-0ed7-44a0-a864-53d364b682a1}
2022-03-28 13:59:56 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testUpdateUser is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 13:59:56 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-844045c3-kafka-0=e24c016e-3d31-4327-92e5-6a4d2019dc61, my-cluster-844045c3-kafka-1=4e2bb7a9-1ab6-4428-b334-4497615c4bd2, my-cluster-844045c3-kafka-2=c477a607-0ed7-44a0-a864-53d364b682a1}
2022-03-28 13:59:56 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-844045c3-kafka-0=e24c016e-3d31-4327-92e5-6a4d2019dc61, my-cluster-844045c3-kafka-1=4e2bb7a9-1ab6-4428-b334-4497615c4bd2, my-cluster-844045c3-kafka-2=c477a607-0ed7-44a0-a864-53d364b682a1}
2022-03-28 13:59:56 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:50] At least my-cluster-844045c3-kafka-1 hasn't rolled
2022-03-28 13:59:56 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] component with name my-cluster-844045c3-kafka rolling update not ready, will try again in 5000 ms (1794606ms till timeout)
2022-03-28 13:59:56 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (117384ms till timeout)
2022-03-28 13:59:56 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:210] testThrottlingQuotasDeleteTopic is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 13:59:56 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-6 -o yaml
2022-03-28 13:59:57 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (462461ms till timeout)
2022-03-28 13:59:57 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1259460ms till timeout)
2022-03-28 13:59:57 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-6 -o yaml
2022-03-28 13:59:57 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 13:59:57 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (463433ms till timeout)
2022-03-28 13:59:57 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (116280ms till timeout)
2022-03-28 13:59:58 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (461360ms till timeout)
2022-03-28 13:59:58 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1258362ms till timeout)
2022-03-28 13:59:58 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-6 -o yaml
2022-03-28 13:59:58 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (115175ms till timeout)
2022-03-28 13:59:58 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-6 -o yaml
2022-03-28 13:59:58 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 13:59:58 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (461899ms till timeout)
2022-03-28 13:59:59 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (460259ms till timeout)
2022-03-28 13:59:59 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1257254ms till timeout)
2022-03-28 13:59:59 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (114075ms till timeout)
2022-03-28 13:59:59 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-6 -o yaml
2022-03-28 14:00:00 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-6 -o yaml
2022-03-28 14:00:00 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 14:00:00 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (460425ms till timeout)
2022-03-28 14:00:00 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (459160ms till timeout)
2022-03-28 14:00:00 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1256156ms till timeout)
2022-03-28 14:00:00 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (112974ms till timeout)
2022-03-28 14:00:01 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-6 -o yaml
2022-03-28 14:00:01 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testUpdateUser is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 14:00:01 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-844045c3-kafka-0=e24c016e-3d31-4327-92e5-6a4d2019dc61, my-cluster-844045c3-kafka-1=4e2bb7a9-1ab6-4428-b334-4497615c4bd2, my-cluster-844045c3-kafka-2=c477a607-0ed7-44a0-a864-53d364b682a1}
2022-03-28 14:00:01 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:210] testThrottlingQuotasDeleteTopic is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 14:00:01 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-844045c3-kafka-0=e24c016e-3d31-4327-92e5-6a4d2019dc61, my-cluster-844045c3-kafka-1=04b14cf3-1574-4f41-92bb-05ea39e5908e, my-cluster-844045c3-kafka-2=c477a607-0ed7-44a0-a864-53d364b682a1}
2022-03-28 14:00:01 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-844045c3-kafka-0=e24c016e-3d31-4327-92e5-6a4d2019dc61, my-cluster-844045c3-kafka-1=04b14cf3-1574-4f41-92bb-05ea39e5908e, my-cluster-844045c3-kafka-2=c477a607-0ed7-44a0-a864-53d364b682a1}
2022-03-28 14:00:01 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:50] At least my-cluster-844045c3-kafka-2 hasn't rolled
2022-03-28 14:00:01 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] component with name my-cluster-844045c3-kafka rolling update not ready, will try again in 5000 ms (1789413ms till timeout)
2022-03-28 14:00:01 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (457963ms till timeout)
2022-03-28 14:00:01 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1254969ms till timeout)
2022-03-28 14:00:01 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-6 -o yaml
2022-03-28 14:00:01 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 14:00:01 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (458956ms till timeout)
2022-03-28 14:00:01 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (111874ms till timeout)
2022-03-28 14:00:02 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (456863ms till timeout)
2022-03-28 14:00:02 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1253864ms till timeout)
2022-03-28 14:00:02 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-6 -o yaml
2022-03-28 14:00:03 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (110773ms till timeout)
2022-03-28 14:00:03 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-6 -o yaml
2022-03-28 14:00:03 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 14:00:03 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (457479ms till timeout)
2022-03-28 14:00:03 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (455765ms till timeout)
2022-03-28 14:00:03 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1252765ms till timeout)
2022-03-28 14:00:04 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (109671ms till timeout)
2022-03-28 14:00:04 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-6 -o yaml
2022-03-28 14:00:04 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-6 -o yaml
2022-03-28 14:00:04 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 14:00:04 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (456001ms till timeout)
2022-03-28 14:00:04 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (454665ms till timeout)
2022-03-28 14:00:04 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1251665ms till timeout)
2022-03-28 14:00:05 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (108568ms till timeout)
2022-03-28 14:00:05 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-6 -o yaml
2022-03-28 14:00:05 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (453567ms till timeout)
2022-03-28 14:00:06 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1250562ms till timeout)
2022-03-28 14:00:06 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-6 -o yaml
2022-03-28 14:00:06 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 14:00:06 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (454519ms till timeout)
2022-03-28 14:00:06 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testUpdateUser is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 14:00:06 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (107467ms till timeout)
2022-03-28 14:00:06 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:210] testThrottlingQuotasDeleteTopic is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 14:00:06 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-844045c3-kafka-0=e24c016e-3d31-4327-92e5-6a4d2019dc61, my-cluster-844045c3-kafka-1=4e2bb7a9-1ab6-4428-b334-4497615c4bd2, my-cluster-844045c3-kafka-2=c477a607-0ed7-44a0-a864-53d364b682a1}
2022-03-28 14:00:06 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-844045c3-kafka-0=e24c016e-3d31-4327-92e5-6a4d2019dc61, my-cluster-844045c3-kafka-1=04b14cf3-1574-4f41-92bb-05ea39e5908e, my-cluster-844045c3-kafka-2=c477a607-0ed7-44a0-a864-53d364b682a1}
2022-03-28 14:00:06 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-844045c3-kafka-0=e24c016e-3d31-4327-92e5-6a4d2019dc61, my-cluster-844045c3-kafka-1=04b14cf3-1574-4f41-92bb-05ea39e5908e, my-cluster-844045c3-kafka-2=c477a607-0ed7-44a0-a864-53d364b682a1}
2022-03-28 14:00:06 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:50] At least my-cluster-844045c3-kafka-2 hasn't rolled
2022-03-28 14:00:06 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] component with name my-cluster-844045c3-kafka rolling update not ready, will try again in 5000 ms (1784218ms till timeout)
2022-03-28 14:00:07 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (452469ms till timeout)
2022-03-28 14:00:07 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1249464ms till timeout)
2022-03-28 14:00:07 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-6 -o yaml
2022-03-28 14:00:07 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (106366ms till timeout)
2022-03-28 14:00:07 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-6 -o yaml
2022-03-28 14:00:07 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 14:00:07 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (453041ms till timeout)
2022-03-28 14:00:08 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (451369ms till timeout)
2022-03-28 14:00:08 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1248366ms till timeout)
2022-03-28 14:00:08 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (105265ms till timeout)
2022-03-28 14:00:08 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-6 -o yaml
2022-03-28 14:00:09 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-6 -o yaml
2022-03-28 14:00:09 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 14:00:09 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (451541ms till timeout)
2022-03-28 14:00:09 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (450270ms till timeout)
2022-03-28 14:00:09 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1247265ms till timeout)
2022-03-28 14:00:09 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (104163ms till timeout)
2022-03-28 14:00:10 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-6 -o yaml
2022-03-28 14:00:10 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (449169ms till timeout)
2022-03-28 14:00:10 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1246167ms till timeout)
2022-03-28 14:00:10 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-6 -o yaml
2022-03-28 14:00:10 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 14:00:10 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (450063ms till timeout)
2022-03-28 14:00:10 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (103063ms till timeout)
2022-03-28 14:00:11 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testUpdateUser is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 14:00:11 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (448070ms till timeout)
2022-03-28 14:00:11 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:210] testThrottlingQuotasDeleteTopic is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 14:00:11 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1245068ms till timeout)
2022-03-28 14:00:11 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-6 -o yaml
2022-03-28 14:00:11 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-844045c3-kafka-0=e24c016e-3d31-4327-92e5-6a4d2019dc61, my-cluster-844045c3-kafka-1=4e2bb7a9-1ab6-4428-b334-4497615c4bd2, my-cluster-844045c3-kafka-2=c477a607-0ed7-44a0-a864-53d364b682a1}
2022-03-28 14:00:11 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-844045c3-kafka-0=e24c016e-3d31-4327-92e5-6a4d2019dc61, my-cluster-844045c3-kafka-1=04b14cf3-1574-4f41-92bb-05ea39e5908e, my-cluster-844045c3-kafka-2=c477a607-0ed7-44a0-a864-53d364b682a1}
2022-03-28 14:00:11 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-844045c3-kafka-0=e24c016e-3d31-4327-92e5-6a4d2019dc61, my-cluster-844045c3-kafka-1=04b14cf3-1574-4f41-92bb-05ea39e5908e, my-cluster-844045c3-kafka-2=c477a607-0ed7-44a0-a864-53d364b682a1}
2022-03-28 14:00:11 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:50] At least my-cluster-844045c3-kafka-2 hasn't rolled
2022-03-28 14:00:11 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] component with name my-cluster-844045c3-kafka rolling update not ready, will try again in 5000 ms (1779023ms till timeout)
2022-03-28 14:00:12 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (101802ms till timeout)
2022-03-28 14:00:12 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-6 -o yaml
2022-03-28 14:00:12 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 14:00:12 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (448574ms till timeout)
2022-03-28 14:00:12 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (446970ms till timeout)
2022-03-28 14:00:12 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1243971ms till timeout)
2022-03-28 14:00:13 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (100702ms till timeout)
2022-03-28 14:00:13 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-6 -o yaml
2022-03-28 14:00:13 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (445871ms till timeout)
2022-03-28 14:00:13 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-6 -o yaml
2022-03-28 14:00:13 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 14:00:13 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (447110ms till timeout)
2022-03-28 14:00:13 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1242871ms till timeout)
2022-03-28 14:00:14 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (99600ms till timeout)
2022-03-28 14:00:14 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-6 -o yaml
2022-03-28 14:00:14 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (444771ms till timeout)
2022-03-28 14:00:14 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1241772ms till timeout)
2022-03-28 14:00:15 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-6 -o yaml
2022-03-28 14:00:15 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 14:00:15 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (445560ms till timeout)
2022-03-28 14:00:15 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (98496ms till timeout)
2022-03-28 14:00:15 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (443672ms till timeout)
2022-03-28 14:00:15 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1240672ms till timeout)
2022-03-28 14:00:16 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-6 -o yaml
2022-03-28 14:00:16 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testUpdateUser is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 14:00:16 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (97395ms till timeout)
2022-03-28 14:00:16 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:210] testThrottlingQuotasDeleteTopic is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 14:00:16 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-8 get Namespace namespace-6 -o yaml
2022-03-28 14:00:16 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 1
2022-03-28 14:00:16 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 14:00:16 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Error from server (NotFound): namespaces "namespace-6" not found
2022-03-28 14:00:16 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] ======STDERR END======
2022-03-28 14:00:16 [ForkJoinPool-1-worker-5] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[], io.strimzi.test.logs.CollectorElement@c4c0ea0=[namespace-8], io.strimzi.test.logs.CollectorElement@aec142ef=[], io.strimzi.test.logs.CollectorElement@f851b6c3=[], io.strimzi.test.logs.CollectorElement@3881d5f2=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@5c7379cb=[], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[namespace-4], io.strimzi.test.logs.CollectorElement@f4405b3b=[namespace-5], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 14:00:16 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:267] testConfigurationPerformanceOptions - Notifies waiting test cases:[testCapacityFile, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testThrottlingQuotasCreateAlterPartitions, testSendSimpleMessageTls, testReceiveSimpleMessageTlsScramSha, testSendSimpleMessageTlsScramSha, testReceiveSimpleMessageTls, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testConfigurationFileIsCreated, testPauseReconciliationInKafkaRebalanceAndTopic, testUpdateUser, testThrottlingQuotasDeleteTopic] to and randomly select one to start execution
2022-03-28 14:00:16 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:93] [cruisecontrol.CruiseControlConfigurationST] - Removing parallel test: testConfigurationPerformanceOptions
2022-03-28 14:00:16 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:97] [cruisecontrol.CruiseControlConfigurationST] - Parallel test count: 6
2022-03-28 14:00:16 [ForkJoinPool-1-worker-5] INFO  [TestSeparator:29] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testConfigurationPerformanceOptions-FINISHED
2022-03-28 14:00:16 [ForkJoinPool-1-worker-5] INFO  [TestSeparator:30] ############################################################################
2022-03-28 14:00:16 [ForkJoinPool-1-worker-5] INFO  [TestSeparator:23] ############################################################################
2022-03-28 14:00:16 [ForkJoinPool-1-worker-5] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.topic.ThrottlingQuotaST.testKafkaAdminTopicOperations-STARTED
2022-03-28 14:00:16 [ForkJoinPool-1-worker-5] DEBUG [AbstractST:658] ============================================================================
2022-03-28 14:00:16 [ForkJoinPool-1-worker-5] DEBUG [AbstractST:659] [operators.topic.ThrottlingQuotaST - Before Each] - Setup test case environment
2022-03-28 14:00:16 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:77] [operators.topic.ThrottlingQuotaST] - Adding parallel test: testKafkaAdminTopicOperations
2022-03-28 14:00:16 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:81] [operators.topic.ThrottlingQuotaST] - Parallel test count: 7
2022-03-28 14:00:16 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:205] [testKafkaAdminTopicOperations] moved to the WaitZone, because current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 14:00:16 [ForkJoinPool-1-worker-5] TRACE [SuiteThreadController:210] testKafkaAdminTopicOperations is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 14:00:16 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-844045c3-kafka-0=e24c016e-3d31-4327-92e5-6a4d2019dc61, my-cluster-844045c3-kafka-1=4e2bb7a9-1ab6-4428-b334-4497615c4bd2, my-cluster-844045c3-kafka-2=c477a607-0ed7-44a0-a864-53d364b682a1}
2022-03-28 14:00:16 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (442570ms till timeout)
2022-03-28 14:00:17 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1239570ms till timeout)
2022-03-28 14:00:17 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-844045c3-kafka-0=e24c016e-3d31-4327-92e5-6a4d2019dc61, my-cluster-844045c3-kafka-1=04b14cf3-1574-4f41-92bb-05ea39e5908e, my-cluster-844045c3-kafka-2=c477a607-0ed7-44a0-a864-53d364b682a1}
2022-03-28 14:00:17 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-844045c3-kafka-0=e24c016e-3d31-4327-92e5-6a4d2019dc61, my-cluster-844045c3-kafka-1=04b14cf3-1574-4f41-92bb-05ea39e5908e, my-cluster-844045c3-kafka-2=c477a607-0ed7-44a0-a864-53d364b682a1}
2022-03-28 14:00:17 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:50] At least my-cluster-844045c3-kafka-2 hasn't rolled
2022-03-28 14:00:17 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] component with name my-cluster-844045c3-kafka rolling update not ready, will try again in 5000 ms (1773825ms till timeout)
2022-03-28 14:00:17 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (96294ms till timeout)
2022-03-28 14:00:18 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (441471ms till timeout)
2022-03-28 14:00:18 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1238471ms till timeout)
2022-03-28 14:00:18 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (95191ms till timeout)
2022-03-28 14:00:19 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (440372ms till timeout)
2022-03-28 14:00:19 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1237372ms till timeout)
2022-03-28 14:00:19 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (94090ms till timeout)
2022-03-28 14:00:20 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (439274ms till timeout)
2022-03-28 14:00:20 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1236274ms till timeout)
2022-03-28 14:00:20 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (92989ms till timeout)
2022-03-28 14:00:21 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (438172ms till timeout)
2022-03-28 14:00:21 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:230] testUpdateUser test now can proceed its execution
2022-03-28 14:00:21 [ForkJoinPool-1-worker-13] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 14:00:21 [ForkJoinPool-1-worker-13] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testScramUserWithQuotas=my-cluster-6f653060, testReceiveSimpleMessageTlsScramSha=my-cluster-af298d01, testConfigurationFileIsCreated=my-cluster-3244df0d, testThrottlingQuotasCreateAlterPartitions=my-cluster-8f6c28cb, testSendSimpleMessageTls=my-cluster-fbc845dd, testCreatingUsersWithSecretPrefix=my-cluster-0c65d2dd, testUpdateUser=my-cluster-c2c4d05a, testConfigurationReflection=my-cluster-5137fff1, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-0444229e, testUserWithNameMoreThan64Chars=my-cluster-02e25de6, testDeployAndUnDeployCruiseControl=my-cluster-844045c3, testTlsUserWithQuotas=my-cluster-9832c9ae, testThrottlingQuotasCreateTopic=my-cluster-ac2cadd6, testCapacityFile=my-cluster-0531d5ce, testReceiveSimpleMessageTls=my-cluster-ace1fa24, testConfigurationPerformanceOptions=my-cluster-4994018d, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-8af61d52, testTlsExternalUser=my-cluster-43f83aef, testSendSimpleMessageTlsScramSha=my-cluster-e353214b}
2022-03-28 14:00:21 [ForkJoinPool-1-worker-13] TRACE [AbstractST:607] USERS_NAME_MAP: {testScramUserWithQuotas=my-user-27563760-1632863338, testReceiveSimpleMessageTlsScramSha=my-user-1597210213-707359243, testConfigurationFileIsCreated=my-user-1799841978-1468805631, testThrottlingQuotasCreateAlterPartitions=my-user-1557959119-1722636050, testSendSimpleMessageTls=my-user-1754673553-2077518027, testCreatingUsersWithSecretPrefix=my-user-144001639-226807213, testUpdateUser=my-user-1870578302-1216914412, testConfigurationReflection=my-user-2029261898-1375854235, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-1010649803-214635564, testUserWithNameMoreThan64Chars=my-user-2124392695-340403978, testDeployAndUnDeployCruiseControl=my-user-2117120687-604608920, testTlsUserWithQuotas=my-user-1602931187-443256751, testThrottlingQuotasCreateTopic=my-user-46611685-1079680568, testCapacityFile=my-user-155969448-1270555822, testReceiveSimpleMessageTls=my-user-1418772225-1024845009, testConfigurationPerformanceOptions=my-user-882948629-1821489961, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-1807142870-188660517, testTlsExternalUser=my-user-370513135-1989152728, testSendSimpleMessageTlsScramSha=my-user-182357028-1132873010}
2022-03-28 14:00:21 [ForkJoinPool-1-worker-13] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testScramUserWithQuotas=my-topic-1345794255-226872460, testReceiveSimpleMessageTlsScramSha=my-topic-649423016-1623551209, testConfigurationFileIsCreated=my-topic-1435014906-681104826, testThrottlingQuotasCreateAlterPartitions=my-topic-1449977352-344480794, testSendSimpleMessageTls=my-topic-115477502-2025274778, testCreatingUsersWithSecretPrefix=my-topic-130462979-2061578809, testUpdateUser=my-topic-829325799-540543436, testConfigurationReflection=my-topic-1621095669-1276202830, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-889283546-691691584, testUserWithNameMoreThan64Chars=my-topic-1053060011-1199293498, testDeployAndUnDeployCruiseControl=my-topic-1101175281-2107378523, testTlsUserWithQuotas=my-topic-985048738-437665866, testThrottlingQuotasCreateTopic=my-topic-114922478-1506136333, testCapacityFile=my-topic-296605537-1008743104, testReceiveSimpleMessageTls=my-topic-1131768783-120051821, testConfigurationPerformanceOptions=my-topic-627301835-843876360, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-1367124108-1748498395, testTlsExternalUser=my-topic-2081139838-1814296134, testSendSimpleMessageTlsScramSha=my-topic-652506937-1802310709}
2022-03-28 14:00:21 [ForkJoinPool-1-worker-13] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testScramUserWithQuotas=my-cluster-6f653060-kafka-clients, testReceiveSimpleMessageTlsScramSha=my-cluster-af298d01-kafka-clients, testConfigurationFileIsCreated=my-cluster-3244df0d-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-8f6c28cb-kafka-clients, testSendSimpleMessageTls=my-cluster-fbc845dd-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-0c65d2dd-kafka-clients, testUpdateUser=my-cluster-c2c4d05a-kafka-clients, testConfigurationReflection=my-cluster-5137fff1-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-0444229e-kafka-clients, testUserWithNameMoreThan64Chars=my-cluster-02e25de6-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-844045c3-kafka-clients, testTlsUserWithQuotas=my-cluster-9832c9ae-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-ac2cadd6-kafka-clients, testCapacityFile=my-cluster-0531d5ce-kafka-clients, testReceiveSimpleMessageTls=my-cluster-ace1fa24-kafka-clients, testConfigurationPerformanceOptions=my-cluster-4994018d-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-8af61d52-kafka-clients, testTlsExternalUser=my-cluster-43f83aef-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-e353214b-kafka-clients}
2022-03-28 14:00:21 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:155] Create/Update KafkaUser my-user-1870578302-1216914412 in namespace user-st
2022-03-28 14:00:21 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1235172ms till timeout)
2022-03-28 14:00:21 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:210] testThrottlingQuotasDeleteTopic is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 14:00:21 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-1870578302-1216914412
2022-03-28 14:00:21 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:433] Wait for KafkaUser: my-user-1870578302-1216914412 will have desired state: Ready
2022-03-28 14:00:21 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for KafkaUser: my-user-1870578302-1216914412 will have desired state: Ready
2022-03-28 14:00:21 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:444] KafkaUser: my-user-1870578302-1216914412 is in desired state: Ready
2022-03-28 14:00:21 [ForkJoinPool-1-worker-5] TRACE [SuiteThreadController:210] testKafkaAdminTopicOperations is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 14:00:21 [ForkJoinPool-1-worker-13] DEBUG [CompiledPath:93] Evaluating path: $['data']['ca.crt']
2022-03-28 14:00:21 [ForkJoinPool-1-worker-13] DEBUG [CompiledPath:93] Evaluating path: $['data']['user.crt']
2022-03-28 14:00:21 [ForkJoinPool-1-worker-13] DEBUG [CompiledPath:93] Evaluating path: $['data']['user.key']
2022-03-28 14:00:21 [ForkJoinPool-1-worker-13] DEBUG [CompiledPath:93] Evaluating path: $['metadata']['name']
2022-03-28 14:00:21 [ForkJoinPool-1-worker-13] DEBUG [CompiledPath:93] Evaluating path: $['metadata']['namespace']
2022-03-28 14:00:22 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (91765ms till timeout)
2022-03-28 14:00:22 [ForkJoinPool-1-worker-13] DEBUG [CompiledPath:93] Evaluating path: $['metadata']['name']
2022-03-28 14:00:22 [ForkJoinPool-1-worker-13] DEBUG [CompiledPath:93] Evaluating path: $['metadata']['namespace']
2022-03-28 14:00:22 [ForkJoinPool-1-worker-13] DEBUG [CompiledPath:93] Evaluating path: $['spec']['authentication']['type']
2022-03-28 14:00:22 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-844045c3-kafka-0=e24c016e-3d31-4327-92e5-6a4d2019dc61, my-cluster-844045c3-kafka-1=4e2bb7a9-1ab6-4428-b334-4497615c4bd2, my-cluster-844045c3-kafka-2=c477a607-0ed7-44a0-a864-53d364b682a1}
2022-03-28 14:00:22 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-844045c3-kafka-0=e24c016e-3d31-4327-92e5-6a4d2019dc61, my-cluster-844045c3-kafka-1=04b14cf3-1574-4f41-92bb-05ea39e5908e, my-cluster-844045c3-kafka-2=c477a607-0ed7-44a0-a864-53d364b682a1}
2022-03-28 14:00:22 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-844045c3-kafka-0=e24c016e-3d31-4327-92e5-6a4d2019dc61, my-cluster-844045c3-kafka-1=04b14cf3-1574-4f41-92bb-05ea39e5908e, my-cluster-844045c3-kafka-2=c477a607-0ed7-44a0-a864-53d364b682a1}
2022-03-28 14:00:22 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:50] At least my-cluster-844045c3-kafka-2 hasn't rolled
2022-03-28 14:00:22 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] component with name my-cluster-844045c3-kafka rolling update not ready, will try again in 5000 ms (1768693ms till timeout)
2022-03-28 14:00:22 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (437058ms till timeout)
2022-03-28 14:00:22 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1234058ms till timeout)
2022-03-28 14:00:22 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for increase observation generation from 1 for user my-user-1870578302-1216914412
2022-03-28 14:00:22 [ForkJoinPool-1-worker-13] INFO  [SecretUtils:46] Waiting for Secret my-user-1870578302-1216914412
2022-03-28 14:00:22 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Expected secret my-user-1870578302-1216914412 exists
2022-03-28 14:00:22 [ForkJoinPool-1-worker-13] INFO  [SecretUtils:50] Secret my-user-1870578302-1216914412 created
2022-03-28 14:00:22 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:433] Wait for KafkaUser: my-user-1870578302-1216914412 will have desired state: Ready
2022-03-28 14:00:22 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for KafkaUser: my-user-1870578302-1216914412 will have desired state: Ready
2022-03-28 14:00:23 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:444] KafkaUser: my-user-1870578302-1216914412 is in desired state: Ready
2022-03-28 14:00:23 [ForkJoinPool-1-worker-13] DEBUG [CompiledPath:93] Evaluating path: $['data']['password']
2022-03-28 14:00:23 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (90567ms till timeout)
2022-03-28 14:00:23 [ForkJoinPool-1-worker-13] DEBUG [CompiledPath:93] Evaluating path: $['metadata']['name']
2022-03-28 14:00:23 [ForkJoinPool-1-worker-13] DEBUG [CompiledPath:93] Evaluating path: $['metadata']['namespace']
2022-03-28 14:00:23 [ForkJoinPool-1-worker-13] DEBUG [CompiledPath:93] Evaluating path: $['spec']['authentication']['type']
2022-03-28 14:00:23 [ForkJoinPool-1-worker-13] INFO  [KafkaUserUtils:62] Waiting for KafkaUser deletion my-user-1870578302-1216914412
2022-03-28 14:00:23 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for KafkaUser deletion my-user-1870578302-1216914412
2022-03-28 14:00:23 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady not ready, will try again in 1000 ms (435958ms till timeout)
2022-03-28 14:00:23 [ForkJoinPool-1-worker-13] INFO  [KafkaUserUtils:75] KafkaUser my-user-1870578302-1216914412 deleted
2022-03-28 14:00:23 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:674] ============================================================================
2022-03-28 14:00:23 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:675] [operators.user.UserST - After Each] - Clean up after test
2022-03-28 14:00:23 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:344] ############################################################################
2022-03-28 14:00:23 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:348] Delete all resources for testUpdateUser
2022-03-28 14:00:23 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1232954ms till timeout)
2022-03-28 14:00:23 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of KafkaUser my-user-1870578302-1216914412 in namespace user-st
2022-03-28 14:00:23 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-1870578302-1216914412
2022-03-28 14:00:23 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:369] ############################################################################
2022-03-28 14:00:23 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:267] testUpdateUser - Notifies waiting test cases:[testCapacityFile, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testThrottlingQuotasCreateAlterPartitions, testSendSimpleMessageTls, testReceiveSimpleMessageTlsScramSha, testSendSimpleMessageTlsScramSha, testReceiveSimpleMessageTls, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testConfigurationFileIsCreated, testPauseReconciliationInKafkaRebalanceAndTopic, testThrottlingQuotasDeleteTopic, testKafkaAdminTopicOperations] to and randomly select one to start execution
2022-03-28 14:00:23 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:93] [operators.user.UserST] - Removing parallel test: testUpdateUser
2022-03-28 14:00:23 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:97] [operators.user.UserST] - Parallel test count: 6
2022-03-28 14:00:23 [ForkJoinPool-1-worker-13] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.user.UserST.testUpdateUser-FINISHED
2022-03-28 14:00:23 [ForkJoinPool-1-worker-13] INFO  [TestSeparator:30] ############################################################################
2022-03-28 14:00:23 [ForkJoinPool-1-worker-13] INFO  [TestSeparator:23] ############################################################################
2022-03-28 14:00:23 [ForkJoinPool-1-worker-13] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.user.UserST.testUserTemplate-STARTED
2022-03-28 14:00:23 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:658] ============================================================================
2022-03-28 14:00:23 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:659] [operators.user.UserST - Before Each] - Setup test case environment
2022-03-28 14:00:23 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:77] [operators.user.UserST] - Adding parallel test: testUserTemplate
2022-03-28 14:00:23 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:81] [operators.user.UserST] - Parallel test count: 7
2022-03-28 14:00:23 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:205] [testUserTemplate] moved to the WaitZone, because current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 14:00:23 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testUserTemplate is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 14:00:24 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (89464ms till timeout)
2022-03-28 14:00:24 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:444] KafkaRebalance: my-cluster-0444229e is in desired state: ProposalReady
2022-03-28 14:00:24 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1231856ms till timeout)
2022-03-28 14:00:25 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:433] Wait for KafkaRebalance: my-cluster-0444229e will have desired state: ReconciliationPaused
2022-03-28 14:00:25 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for KafkaRebalance: my-cluster-0444229e will have desired state: ReconciliationPaused
2022-03-28 14:00:25 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:444] KafkaRebalance: my-cluster-0444229e is in desired state: ReconciliationPaused
2022-03-28 14:00:25 [ForkJoinPool-1-worker-15] INFO  [KafkaRebalanceUtils:63] Reconciliation #1(test) KafkaRebalance(namespace-4/my-cluster-0444229e): Annotating KafkaRebalance:my-cluster-0444229e with annotation approve
2022-03-28 14:00:25 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 annotate kafkarebalance my-cluster-0444229e strimzi.io/rebalance=approve
2022-03-28 14:00:25 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (88362ms till timeout)
2022-03-28 14:00:25 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-4 annotate kafkarebalance my-cluster-0444229e strimzi.io/rebalance=approve
2022-03-28 14:00:25 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 14:00:25 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1230757ms till timeout)
2022-03-28 14:00:25 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for KafkaRebalance status will be stable
2022-03-28 14:00:26 [ForkJoinPool-1-worker-15] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-4/my-cluster-0444229e): KafkaRebalance status gonna be stable in 19 polls
2022-03-28 14:00:26 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (179902ms till timeout)
2022-03-28 14:00:26 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:230] testThrottlingQuotasDeleteTopic test now can proceed its execution
2022-03-28 14:00:26 [ForkJoinPool-1-worker-9] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 14:00:26 [ForkJoinPool-1-worker-9] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testScramUserWithQuotas=my-cluster-6f653060, testReceiveSimpleMessageTlsScramSha=my-cluster-af298d01, testConfigurationFileIsCreated=my-cluster-3244df0d, testThrottlingQuotasCreateAlterPartitions=my-cluster-8f6c28cb, testSendSimpleMessageTls=my-cluster-fbc845dd, testCreatingUsersWithSecretPrefix=my-cluster-0c65d2dd, testUpdateUser=my-cluster-c2c4d05a, testThrottlingQuotasDeleteTopic=my-cluster-419d5c31, testConfigurationReflection=my-cluster-5137fff1, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-0444229e, testUserWithNameMoreThan64Chars=my-cluster-02e25de6, testDeployAndUnDeployCruiseControl=my-cluster-844045c3, testTlsUserWithQuotas=my-cluster-9832c9ae, testThrottlingQuotasCreateTopic=my-cluster-ac2cadd6, testCapacityFile=my-cluster-0531d5ce, testReceiveSimpleMessageTls=my-cluster-ace1fa24, testConfigurationPerformanceOptions=my-cluster-4994018d, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-8af61d52, testTlsExternalUser=my-cluster-43f83aef, testSendSimpleMessageTlsScramSha=my-cluster-e353214b}
2022-03-28 14:00:26 [ForkJoinPool-1-worker-9] TRACE [AbstractST:607] USERS_NAME_MAP: {testScramUserWithQuotas=my-user-27563760-1632863338, testReceiveSimpleMessageTlsScramSha=my-user-1597210213-707359243, testConfigurationFileIsCreated=my-user-1799841978-1468805631, testThrottlingQuotasCreateAlterPartitions=my-user-1557959119-1722636050, testSendSimpleMessageTls=my-user-1754673553-2077518027, testCreatingUsersWithSecretPrefix=my-user-144001639-226807213, testUpdateUser=my-user-1870578302-1216914412, testThrottlingQuotasDeleteTopic=my-user-1782685497-1980640589, testConfigurationReflection=my-user-2029261898-1375854235, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-1010649803-214635564, testUserWithNameMoreThan64Chars=my-user-2124392695-340403978, testDeployAndUnDeployCruiseControl=my-user-2117120687-604608920, testTlsUserWithQuotas=my-user-1602931187-443256751, testThrottlingQuotasCreateTopic=my-user-46611685-1079680568, testCapacityFile=my-user-155969448-1270555822, testReceiveSimpleMessageTls=my-user-1418772225-1024845009, testConfigurationPerformanceOptions=my-user-882948629-1821489961, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-1807142870-188660517, testTlsExternalUser=my-user-370513135-1989152728, testSendSimpleMessageTlsScramSha=my-user-182357028-1132873010}
2022-03-28 14:00:26 [ForkJoinPool-1-worker-9] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testScramUserWithQuotas=my-topic-1345794255-226872460, testReceiveSimpleMessageTlsScramSha=my-topic-649423016-1623551209, testConfigurationFileIsCreated=my-topic-1435014906-681104826, testThrottlingQuotasCreateAlterPartitions=my-topic-1449977352-344480794, testSendSimpleMessageTls=my-topic-115477502-2025274778, testCreatingUsersWithSecretPrefix=my-topic-130462979-2061578809, testUpdateUser=my-topic-829325799-540543436, testThrottlingQuotasDeleteTopic=my-topic-1797350406-813887294, testConfigurationReflection=my-topic-1621095669-1276202830, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-889283546-691691584, testUserWithNameMoreThan64Chars=my-topic-1053060011-1199293498, testDeployAndUnDeployCruiseControl=my-topic-1101175281-2107378523, testTlsUserWithQuotas=my-topic-985048738-437665866, testThrottlingQuotasCreateTopic=my-topic-114922478-1506136333, testCapacityFile=my-topic-296605537-1008743104, testReceiveSimpleMessageTls=my-topic-1131768783-120051821, testConfigurationPerformanceOptions=my-topic-627301835-843876360, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-1367124108-1748498395, testTlsExternalUser=my-topic-2081139838-1814296134, testSendSimpleMessageTlsScramSha=my-topic-652506937-1802310709}
2022-03-28 14:00:26 [ForkJoinPool-1-worker-9] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testScramUserWithQuotas=my-cluster-6f653060-kafka-clients, testReceiveSimpleMessageTlsScramSha=my-cluster-af298d01-kafka-clients, testConfigurationFileIsCreated=my-cluster-3244df0d-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-8f6c28cb-kafka-clients, testSendSimpleMessageTls=my-cluster-fbc845dd-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-0c65d2dd-kafka-clients, testUpdateUser=my-cluster-c2c4d05a-kafka-clients, testThrottlingQuotasDeleteTopic=my-cluster-419d5c31-kafka-clients, testConfigurationReflection=my-cluster-5137fff1-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-0444229e-kafka-clients, testUserWithNameMoreThan64Chars=my-cluster-02e25de6-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-844045c3-kafka-clients, testTlsUserWithQuotas=my-cluster-9832c9ae-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-ac2cadd6-kafka-clients, testCapacityFile=my-cluster-0531d5ce-kafka-clients, testReceiveSimpleMessageTls=my-cluster-ace1fa24-kafka-clients, testConfigurationPerformanceOptions=my-cluster-4994018d-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-8af61d52-kafka-clients, testTlsExternalUser=my-cluster-43f83aef-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-e353214b-kafka-clients}
2022-03-28 14:00:26 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:155] Create/Update KafkaUser my-user-1782685497-1980640589 in namespace throttling-quota-st
2022-03-28 14:00:26 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (87257ms till timeout)
2022-03-28 14:00:26 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-1782685497-1980640589
2022-03-28 14:00:26 [ForkJoinPool-1-worker-5] TRACE [SuiteThreadController:210] testKafkaAdminTopicOperations is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 14:00:26 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:433] Wait for KafkaUser: my-user-1782685497-1980640589 will have desired state: Ready
2022-03-28 14:00:26 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for KafkaUser: my-user-1782685497-1980640589 will have desired state: Ready
2022-03-28 14:00:26 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:444] KafkaUser: my-user-1782685497-1980640589 is in desired state: Ready
2022-03-28 14:00:26 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1229645ms till timeout)
2022-03-28 14:00:27 [ForkJoinPool-1-worker-9] INFO  [ThrottlingQuotaST:112] Executing 1/5 iteration.
2022-03-28 14:00:27 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:155] Create/Update Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st
2022-03-28 14:00:27 [ForkJoinPool-1-worker-15] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-4/my-cluster-0444229e): KafkaRebalance status gonna be stable in 18 polls
2022-03-28 14:00:27 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (178792ms till timeout)
2022-03-28 14:00:27 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:create-admin-my-cluster-419d5c31-kafka-clients
2022-03-28 14:00:27 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-844045c3-kafka-0=e24c016e-3d31-4327-92e5-6a4d2019dc61, my-cluster-844045c3-kafka-1=4e2bb7a9-1ab6-4428-b334-4497615c4bd2, my-cluster-844045c3-kafka-2=c477a607-0ed7-44a0-a864-53d364b682a1}
2022-03-28 14:00:27 [ForkJoinPool-1-worker-9] INFO  [JobUtils:81] Waiting for job: create-admin-my-cluster-419d5c31-kafka-clients will be in active state
2022-03-28 14:00:27 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 14:00:27 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-844045c3-kafka-0=e24c016e-3d31-4327-92e5-6a4d2019dc61, my-cluster-844045c3-kafka-1=04b14cf3-1574-4f41-92bb-05ea39e5908e, my-cluster-844045c3-kafka-2=90f9b83e-a8e9-408c-ac3e-6b21f611ef07}
2022-03-28 14:00:27 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-844045c3-kafka-0=e24c016e-3d31-4327-92e5-6a4d2019dc61, my-cluster-844045c3-kafka-1=04b14cf3-1574-4f41-92bb-05ea39e5908e, my-cluster-844045c3-kafka-2=90f9b83e-a8e9-408c-ac3e-6b21f611ef07}
2022-03-28 14:00:27 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:50] At least my-cluster-844045c3-kafka-0 hasn't rolled
2022-03-28 14:00:27 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] component with name my-cluster-844045c3-kafka rolling update not ready, will try again in 5000 ms (1763408ms till timeout)
2022-03-28 14:00:27 [ForkJoinPool-1-worker-9] INFO  [ClientUtils:76] Waiting for producer/consumer:create-admin-my-cluster-419d5c31-kafka-clients to finished
2022-03-28 14:00:27 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for job finished
2022-03-28 14:00:27 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:00:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:00:27 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (86075ms till timeout)
2022-03-28 14:00:27 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (219707ms till timeout)
2022-03-28 14:00:28 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1228547ms till timeout)
2022-03-28 14:00:28 [ForkJoinPool-1-worker-15] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-4/my-cluster-0444229e): KafkaRebalance status gonna be stable in 17 polls
2022-03-28 14:00:28 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (177694ms till timeout)
2022-03-28 14:00:28 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (84969ms till timeout)
2022-03-28 14:00:28 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testUserTemplate is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 14:00:28 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:00:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:00:29 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (218499ms till timeout)
2022-03-28 14:00:29 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1227449ms till timeout)
2022-03-28 14:00:29 [ForkJoinPool-1-worker-15] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-4/my-cluster-0444229e): KafkaRebalance status gonna be stable in 16 polls
2022-03-28 14:00:29 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (176596ms till timeout)
2022-03-28 14:00:29 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (83859ms till timeout)
2022-03-28 14:00:30 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:00:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:00:30 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (217303ms till timeout)
2022-03-28 14:00:30 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-5137fff1 will have desired state: Ready not ready, will try again in 1000 ms (1226264ms till timeout)
2022-03-28 14:00:30 [ForkJoinPool-1-worker-15] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-4/my-cluster-0444229e): KafkaRebalance status gonna be stable in 15 polls
2022-03-28 14:00:30 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (175497ms till timeout)
2022-03-28 14:00:31 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (82757ms till timeout)
2022-03-28 14:00:31 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:00:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:00:31 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:444] Kafka: my-cluster-5137fff1 is in desired state: Ready
2022-03-28 14:00:31 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (216012ms till timeout)
2022-03-28 14:00:31 [ForkJoinPool-1-worker-15] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-4/my-cluster-0444229e): KafkaRebalance status gonna be stable in 14 polls
2022-03-28 14:00:31 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (174314ms till timeout)
2022-03-28 14:00:31 [ForkJoinPool-1-worker-5] TRACE [SuiteThreadController:210] testKafkaAdminTopicOperations is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 14:00:31 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-8 exec my-cluster-5137fff1-cruise-control-66c5b4759f-g7qbv -- /bin/bash -c cat /tmp/cruisecontrol.properties
2022-03-28 14:00:32 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (81656ms till timeout)
2022-03-28 14:00:32 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-844045c3-kafka-0=e24c016e-3d31-4327-92e5-6a4d2019dc61, my-cluster-844045c3-kafka-1=4e2bb7a9-1ab6-4428-b334-4497615c4bd2, my-cluster-844045c3-kafka-2=c477a607-0ed7-44a0-a864-53d364b682a1}
2022-03-28 14:00:32 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:00:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:00:32 [ForkJoinPool-1-worker-15] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-4/my-cluster-0444229e): KafkaRebalance status gonna be stable in 13 polls
2022-03-28 14:00:32 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (173212ms till timeout)
2022-03-28 14:00:32 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-844045c3-kafka-0=e24c016e-3d31-4327-92e5-6a4d2019dc61, my-cluster-844045c3-kafka-1=04b14cf3-1574-4f41-92bb-05ea39e5908e, my-cluster-844045c3-kafka-2=90f9b83e-a8e9-408c-ac3e-6b21f611ef07}
2022-03-28 14:00:32 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-844045c3-kafka-0=e24c016e-3d31-4327-92e5-6a4d2019dc61, my-cluster-844045c3-kafka-1=04b14cf3-1574-4f41-92bb-05ea39e5908e, my-cluster-844045c3-kafka-2=90f9b83e-a8e9-408c-ac3e-6b21f611ef07}
2022-03-28 14:00:32 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:50] At least my-cluster-844045c3-kafka-0 hasn't rolled
2022-03-28 14:00:32 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] component with name my-cluster-844045c3-kafka rolling update not ready, will try again in 5000 ms (1758125ms till timeout)
2022-03-28 14:00:32 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (214718ms till timeout)
2022-03-28 14:00:33 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Command: oc --namespace namespace-8 exec my-cluster-5137fff1-cruise-control-66c5b4759f-g7qbv -- /bin/bash -c cat /tmp/cruisecontrol.properties
2022-03-28 14:00:33 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 14:00:33 [ForkJoinPool-1-worker-3] INFO  [CruiseControlConfigurationST:221] Verifying that all configuration in the cruise control container matching the cruise control file /tmp/cruisecontrol.properties properties
2022-03-28 14:00:33 [ForkJoinPool-1-worker-3] DEBUG [AbstractST:674] ============================================================================
2022-03-28 14:00:33 [ForkJoinPool-1-worker-3] DEBUG [AbstractST:675] [cruisecontrol.CruiseControlConfigurationST - After Each] - Clean up after test
2022-03-28 14:00:33 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:344] ############################################################################
2022-03-28 14:00:33 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:348] Delete all resources for testConfigurationReflection
2022-03-28 14:00:33 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:241] Delete of Kafka my-cluster-5137fff1 in namespace namespace-8
2022-03-28 14:00:33 [ForkJoinPool-1-worker-3] INFO  [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-8, for cruise control Kafka cluster my-cluster-5137fff1
2022-03-28 14:00:33 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (80469ms till timeout)
2022-03-28 14:00:33 [ForkJoinPool-1-worker-15] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-4/my-cluster-0444229e): KafkaRebalance status gonna be stable in 12 polls
2022-03-28 14:00:33 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (172101ms till timeout)
2022-03-28 14:00:33 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testUserTemplate is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 14:00:33 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:00:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:00:34 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (213510ms till timeout)
2022-03-28 14:00:34 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-5137fff1
2022-03-28 14:00:34 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:369] ############################################################################
2022-03-28 14:00:34 [ForkJoinPool-1-worker-3] INFO  [TestSuiteNamespaceManager:200] Deleting namespace:namespace-8 for test case:testConfigurationReflection
2022-03-28 14:00:34 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (79344ms till timeout)
2022-03-28 14:00:34 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Namespace namespace-8 removal
2022-03-28 14:00:34 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-28 14:00:34 [ForkJoinPool-1-worker-15] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-4/my-cluster-0444229e): KafkaRebalance status gonna be stable in 11 polls
2022-03-28 14:00:34 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (171004ms till timeout)
2022-03-28 14:00:35 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-28 14:00:35 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 14:00:35 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (479558ms till timeout)
2022-03-28 14:00:35 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:00:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:00:35 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (212316ms till timeout)
2022-03-28 14:00:35 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (78228ms till timeout)
2022-03-28 14:00:36 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-28 14:00:36 [ForkJoinPool-1-worker-15] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-4/my-cluster-0444229e): KafkaRebalance status gonna be stable in 10 polls
2022-03-28 14:00:36 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (169904ms till timeout)
2022-03-28 14:00:36 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:00:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:00:36 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (211121ms till timeout)
2022-03-28 14:00:36 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-28 14:00:36 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 14:00:36 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (478091ms till timeout)
2022-03-28 14:00:36 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (77123ms till timeout)
2022-03-28 14:00:36 [ForkJoinPool-1-worker-5] TRACE [SuiteThreadController:210] testKafkaAdminTopicOperations is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 14:00:37 [ForkJoinPool-1-worker-15] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-4/my-cluster-0444229e): KafkaRebalance status gonna be stable in 9 polls
2022-03-28 14:00:37 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (168805ms till timeout)
2022-03-28 14:00:37 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-28 14:00:37 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:00:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:00:37 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (209927ms till timeout)
2022-03-28 14:00:37 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (76023ms till timeout)
2022-03-28 14:00:37 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-844045c3-kafka-0=e24c016e-3d31-4327-92e5-6a4d2019dc61, my-cluster-844045c3-kafka-1=4e2bb7a9-1ab6-4428-b334-4497615c4bd2, my-cluster-844045c3-kafka-2=c477a607-0ed7-44a0-a864-53d364b682a1}
2022-03-28 14:00:37 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-28 14:00:37 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 14:00:37 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (476638ms till timeout)
2022-03-28 14:00:38 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-844045c3-kafka-0=e24c016e-3d31-4327-92e5-6a4d2019dc61, my-cluster-844045c3-kafka-1=04b14cf3-1574-4f41-92bb-05ea39e5908e, my-cluster-844045c3-kafka-2=90f9b83e-a8e9-408c-ac3e-6b21f611ef07}
2022-03-28 14:00:38 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-844045c3-kafka-0=e24c016e-3d31-4327-92e5-6a4d2019dc61, my-cluster-844045c3-kafka-1=04b14cf3-1574-4f41-92bb-05ea39e5908e, my-cluster-844045c3-kafka-2=90f9b83e-a8e9-408c-ac3e-6b21f611ef07}
2022-03-28 14:00:38 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:50] At least my-cluster-844045c3-kafka-0 hasn't rolled
2022-03-28 14:00:38 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] component with name my-cluster-844045c3-kafka rolling update not ready, will try again in 5000 ms (1752933ms till timeout)
2022-03-28 14:00:38 [ForkJoinPool-1-worker-15] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-4/my-cluster-0444229e): KafkaRebalance status gonna be stable in 8 polls
2022-03-28 14:00:38 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (167707ms till timeout)
2022-03-28 14:00:38 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:00:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:00:38 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (208731ms till timeout)
2022-03-28 14:00:38 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testUserTemplate is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 14:00:38 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (74907ms till timeout)
2022-03-28 14:00:38 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-28 14:00:39 [ForkJoinPool-1-worker-15] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-4/my-cluster-0444229e): KafkaRebalance status gonna be stable in 7 polls
2022-03-28 14:00:39 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (166607ms till timeout)
2022-03-28 14:00:39 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-28 14:00:39 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 14:00:39 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (475175ms till timeout)
2022-03-28 14:00:39 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:00:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:00:40 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (207536ms till timeout)
2022-03-28 14:00:40 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (73711ms till timeout)
2022-03-28 14:00:40 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-28 14:00:40 [ForkJoinPool-1-worker-15] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-4/my-cluster-0444229e): KafkaRebalance status gonna be stable in 6 polls
2022-03-28 14:00:40 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (165509ms till timeout)
2022-03-28 14:00:40 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-28 14:00:40 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 14:00:40 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (473718ms till timeout)
2022-03-28 14:00:41 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:00:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:00:41 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (206342ms till timeout)
2022-03-28 14:00:41 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (72517ms till timeout)
2022-03-28 14:00:41 [ForkJoinPool-1-worker-15] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-4/my-cluster-0444229e): KafkaRebalance status gonna be stable in 5 polls
2022-03-28 14:00:41 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (164408ms till timeout)
2022-03-28 14:00:41 [ForkJoinPool-1-worker-5] TRACE [SuiteThreadController:210] testKafkaAdminTopicOperations is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 14:00:41 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-28 14:00:42 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:00:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:00:42 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-28 14:00:42 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 14:00:42 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (472245ms till timeout)
2022-03-28 14:00:42 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (205146ms till timeout)
2022-03-28 14:00:42 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (71321ms till timeout)
2022-03-28 14:00:42 [ForkJoinPool-1-worker-15] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-4/my-cluster-0444229e): KafkaRebalance status gonna be stable in 4 polls
2022-03-28 14:00:42 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (163309ms till timeout)
2022-03-28 14:00:43 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-844045c3-kafka-0=e24c016e-3d31-4327-92e5-6a4d2019dc61, my-cluster-844045c3-kafka-1=4e2bb7a9-1ab6-4428-b334-4497615c4bd2, my-cluster-844045c3-kafka-2=c477a607-0ed7-44a0-a864-53d364b682a1}
2022-03-28 14:00:43 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-844045c3-kafka-0=e24c016e-3d31-4327-92e5-6a4d2019dc61, my-cluster-844045c3-kafka-1=04b14cf3-1574-4f41-92bb-05ea39e5908e, my-cluster-844045c3-kafka-2=90f9b83e-a8e9-408c-ac3e-6b21f611ef07}
2022-03-28 14:00:43 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-844045c3-kafka-0=e24c016e-3d31-4327-92e5-6a4d2019dc61, my-cluster-844045c3-kafka-1=04b14cf3-1574-4f41-92bb-05ea39e5908e, my-cluster-844045c3-kafka-2=90f9b83e-a8e9-408c-ac3e-6b21f611ef07}
2022-03-28 14:00:43 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:50] At least my-cluster-844045c3-kafka-0 hasn't rolled
2022-03-28 14:00:43 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] component with name my-cluster-844045c3-kafka rolling update not ready, will try again in 5000 ms (1747740ms till timeout)
2022-03-28 14:00:43 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-28 14:00:43 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:00:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:00:43 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (70220ms till timeout)
2022-03-28 14:00:43 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (203851ms till timeout)
2022-03-28 14:00:43 [ForkJoinPool-1-worker-15] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-4/my-cluster-0444229e): KafkaRebalance status gonna be stable in 3 polls
2022-03-28 14:00:43 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (162153ms till timeout)
2022-03-28 14:00:43 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-28 14:00:43 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 14:00:43 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (470741ms till timeout)
2022-03-28 14:00:43 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testUserTemplate is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 14:00:44 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (69115ms till timeout)
2022-03-28 14:00:44 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:00:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:00:44 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-28 14:00:44 [ForkJoinPool-1-worker-15] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-4/my-cluster-0444229e): KafkaRebalance status gonna be stable in 2 polls
2022-03-28 14:00:44 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (161049ms till timeout)
2022-03-28 14:00:44 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (202555ms till timeout)
2022-03-28 14:00:45 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-28 14:00:45 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 14:00:45 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (469261ms till timeout)
2022-03-28 14:00:45 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (68012ms till timeout)
2022-03-28 14:00:45 [ForkJoinPool-1-worker-15] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-4/my-cluster-0444229e): KafkaRebalance status gonna be stable in 1 polls
2022-03-28 14:00:45 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (159948ms till timeout)
2022-03-28 14:00:46 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:00:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:00:46 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (201357ms till timeout)
2022-03-28 14:00:46 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-28 14:00:46 [ForkJoinPool-1-worker-5] TRACE [SuiteThreadController:210] testKafkaAdminTopicOperations is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 14:00:46 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-28 14:00:46 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 14:00:46 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (467786ms till timeout)
2022-03-28 14:00:46 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (66838ms till timeout)
2022-03-28 14:00:47 [ForkJoinPool-1-worker-15] INFO  [KafkaRebalanceUtils:118] Reconciliation #2(test) KafkaRebalance(namespace-4/my-cluster-0444229e): KafkaRebalance status is stable for 20 polls intervals
2022-03-28 14:00:47 [ForkJoinPool-1-worker-15] INFO  [ReconciliationST:178] Setting annotation to "false" and waiting for KafkaRebalance to be in Ready state
2022-03-28 14:00:47 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:00:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:00:47 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (200058ms till timeout)
2022-03-28 14:00:47 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:433] Wait for KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady
2022-03-28 14:00:47 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for KafkaRebalance: my-cluster-0444229e will have desired state: ProposalReady
2022-03-28 14:00:47 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:444] KafkaRebalance: my-cluster-0444229e is in desired state: ProposalReady
2022-03-28 14:00:47 [ForkJoinPool-1-worker-15] INFO  [KafkaRebalanceUtils:63] Reconciliation #3(test) KafkaRebalance(namespace-4/my-cluster-0444229e): Annotating KafkaRebalance:my-cluster-0444229e with annotation approve
2022-03-28 14:00:47 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 annotate kafkarebalance my-cluster-0444229e strimzi.io/rebalance=approve
2022-03-28 14:00:47 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-28 14:00:48 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (65729ms till timeout)
2022-03-28 14:00:48 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-844045c3-kafka-0=e24c016e-3d31-4327-92e5-6a4d2019dc61, my-cluster-844045c3-kafka-1=4e2bb7a9-1ab6-4428-b334-4497615c4bd2, my-cluster-844045c3-kafka-2=c477a607-0ed7-44a0-a864-53d364b682a1}
2022-03-28 14:00:48 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-4 annotate kafkarebalance my-cluster-0444229e strimzi.io/rebalance=approve
2022-03-28 14:00:48 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 14:00:48 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-28 14:00:48 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 14:00:48 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (466286ms till timeout)
2022-03-28 14:00:48 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-844045c3-kafka-0=e24c016e-3d31-4327-92e5-6a4d2019dc61, my-cluster-844045c3-kafka-1=04b14cf3-1574-4f41-92bb-05ea39e5908e, my-cluster-844045c3-kafka-2=90f9b83e-a8e9-408c-ac3e-6b21f611ef07}
2022-03-28 14:00:48 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-844045c3-kafka-0=e24c016e-3d31-4327-92e5-6a4d2019dc61, my-cluster-844045c3-kafka-1=04b14cf3-1574-4f41-92bb-05ea39e5908e, my-cluster-844045c3-kafka-2=90f9b83e-a8e9-408c-ac3e-6b21f611ef07}
2022-03-28 14:00:48 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:50] At least my-cluster-844045c3-kafka-0 hasn't rolled
2022-03-28 14:00:48 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] component with name my-cluster-844045c3-kafka rolling update not ready, will try again in 5000 ms (1742547ms till timeout)
2022-03-28 14:00:48 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:433] Wait for KafkaRebalance: my-cluster-0444229e will have desired state: Ready
2022-03-28 14:00:48 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for KafkaRebalance: my-cluster-0444229e will have desired state: Ready
2022-03-28 14:00:48 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (599901ms till timeout)
2022-03-28 14:00:48 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:00:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:00:48 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (198845ms till timeout)
2022-03-28 14:00:48 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testUserTemplate is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 14:00:49 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (64628ms till timeout)
2022-03-28 14:00:49 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-28 14:00:49 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (598802ms till timeout)
2022-03-28 14:00:49 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-28 14:00:49 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 14:00:49 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (464823ms till timeout)
2022-03-28 14:00:49 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:00:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:00:49 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (197649ms till timeout)
2022-03-28 14:00:50 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (63527ms till timeout)
2022-03-28 14:00:50 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (597703ms till timeout)
2022-03-28 14:00:50 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-28 14:00:51 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:00:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:00:51 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (196391ms till timeout)
2022-03-28 14:00:51 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-28 14:00:51 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 14:00:51 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (463307ms till timeout)
2022-03-28 14:00:51 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (62426ms till timeout)
2022-03-28 14:00:51 [ForkJoinPool-1-worker-5] TRACE [SuiteThreadController:210] testKafkaAdminTopicOperations is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 14:00:51 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (596604ms till timeout)
2022-03-28 14:00:52 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:00:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:00:52 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-28 14:00:52 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (195195ms till timeout)
2022-03-28 14:00:52 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (61236ms till timeout)
2022-03-28 14:00:52 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-28 14:00:52 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 14:00:52 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (461703ms till timeout)
2022-03-28 14:00:52 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (595506ms till timeout)
2022-03-28 14:00:53 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-844045c3-kafka-0=e24c016e-3d31-4327-92e5-6a4d2019dc61, my-cluster-844045c3-kafka-1=4e2bb7a9-1ab6-4428-b334-4497615c4bd2, my-cluster-844045c3-kafka-2=c477a607-0ed7-44a0-a864-53d364b682a1}
2022-03-28 14:00:53 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:00:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:00:53 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-844045c3-kafka-0=44168e9b-60b2-4f32-9a79-5e8aa89530ef, my-cluster-844045c3-kafka-1=04b14cf3-1574-4f41-92bb-05ea39e5908e, my-cluster-844045c3-kafka-2=90f9b83e-a8e9-408c-ac3e-6b21f611ef07}
2022-03-28 14:00:53 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-844045c3-kafka-0=44168e9b-60b2-4f32-9a79-5e8aa89530ef, my-cluster-844045c3-kafka-1=04b14cf3-1574-4f41-92bb-05ea39e5908e, my-cluster-844045c3-kafka-2=90f9b83e-a8e9-408c-ac3e-6b21f611ef07}
2022-03-28 14:00:53 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:55] All pods seem to have rolled
2022-03-28 14:00:53 [ForkJoinPool-1-worker-7] INFO  [RollingUpdateUtils:86] Component with name: my-cluster-844045c3-kafka has been successfully rolled
2022-03-28 14:00:53 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:87] Component matching LabelSelector: LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-844045c3-kafka, strimzi.io/cluster=my-cluster-844045c3, strimzi.io/kind=Kafka}, additionalProperties={}) successfully rolled
2022-03-28 14:00:53 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (193904ms till timeout)
2022-03-28 14:00:53 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (60080ms till timeout)
2022-03-28 14:00:53 [ForkJoinPool-1-worker-7] INFO  [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-844045c3-kafka to be ready
2022-03-28 14:00:53 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-844045c3-kafka, strimzi.io/cluster=my-cluster-844045c3, strimzi.io/kind=Kafka}, additionalProperties={})to be ready
2022-03-28 14:00:53 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testUserTemplate is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 14:00:53 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-28 14:00:53 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-844045c3-kafka-0)
2022-03-28 14:00:53 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-844045c3-kafka, strimzi.io/cluster=my-cluster-844045c3, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1799900ms till timeout)
2022-03-28 14:00:54 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (594376ms till timeout)
2022-03-28 14:00:54 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-28 14:00:54 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 14:00:54 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (460219ms till timeout)
2022-03-28 14:00:54 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:00:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:00:54 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (58978ms till timeout)
2022-03-28 14:00:54 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (192610ms till timeout)
2022-03-28 14:00:55 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-844045c3-kafka-0)
2022-03-28 14:00:55 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-844045c3-kafka, strimzi.io/cluster=my-cluster-844045c3, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1798709ms till timeout)
2022-03-28 14:00:55 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (593278ms till timeout)
2022-03-28 14:00:55 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-28 14:00:55 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-28 14:00:55 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 14:00:55 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (458754ms till timeout)
2022-03-28 14:00:55 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (57876ms till timeout)
2022-03-28 14:00:56 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:00:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:00:56 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (191314ms till timeout)
2022-03-28 14:00:56 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (592172ms till timeout)
2022-03-28 14:00:56 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-844045c3-kafka-0)
2022-03-28 14:00:56 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-844045c3-kafka, strimzi.io/cluster=my-cluster-844045c3, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1797509ms till timeout)
2022-03-28 14:00:56 [ForkJoinPool-1-worker-5] TRACE [SuiteThreadController:210] testKafkaAdminTopicOperations is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 14:00:56 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-28 14:00:57 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (56775ms till timeout)
2022-03-28 14:00:57 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-28 14:00:57 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 14:00:57 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (457262ms till timeout)
2022-03-28 14:00:57 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:00:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:00:57 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (590981ms till timeout)
2022-03-28 14:00:57 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (190025ms till timeout)
2022-03-28 14:00:57 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-844045c3-kafka-0)
2022-03-28 14:00:57 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-844045c3-kafka, strimzi.io/cluster=my-cluster-844045c3, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1796216ms till timeout)
2022-03-28 14:00:58 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (55674ms till timeout)
2022-03-28 14:00:58 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-28 14:00:58 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (589881ms till timeout)
2022-03-28 14:00:58 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:00:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:00:58 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (188827ms till timeout)
2022-03-28 14:00:58 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testUserTemplate is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 14:00:58 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-28 14:00:58 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 14:00:58 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (455687ms till timeout)
2022-03-28 14:00:58 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-844045c3-kafka-0)
2022-03-28 14:00:58 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-844045c3-kafka, strimzi.io/cluster=my-cluster-844045c3, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1794925ms till timeout)
2022-03-28 14:00:59 [ForkJoinPool-1-worker-1] INFO  [PodUtils:189] Message org.apache.kafka.common.errors.ThrottlingQuotaExceededException: The throttling quota has been exceeded. found in alter-admin-my-cluster-8f6c28cb-kafka-clients-d52d9 log
2022-03-28 14:00:59 [ForkJoinPool-1-worker-1] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment alter-admin-my-cluster-8f6c28cb-kafka-clients deletion
2022-03-28 14:00:59 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for ReplicaSet alter-admin-my-cluster-8f6c28cb-kafka-clients to be deleted
2022-03-28 14:00:59 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] ReplicaSet alter-admin-my-cluster-8f6c28cb-kafka-clients to be deleted not ready, will try again in 5000 ms (179903ms till timeout)
2022-03-28 14:00:59 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (588782ms till timeout)
2022-03-28 14:00:59 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:00:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:00:59 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-28 14:00:59 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (187631ms till timeout)
2022-03-28 14:01:00 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-844045c3-kafka-0)
2022-03-28 14:01:00 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-844045c3-kafka, strimzi.io/cluster=my-cluster-844045c3, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1793732ms till timeout)
2022-03-28 14:01:00 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-28 14:01:00 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 14:01:00 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (454195ms till timeout)
2022-03-28 14:01:00 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (587681ms till timeout)
2022-03-28 14:01:01 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:00:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:01 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (186432ms till timeout)
2022-03-28 14:01:01 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-844045c3-kafka-0)
2022-03-28 14:01:01 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-844045c3-kafka, strimzi.io/cluster=my-cluster-844045c3, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1792532ms till timeout)
2022-03-28 14:01:01 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-28 14:01:01 [ForkJoinPool-1-worker-5] TRACE [SuiteThreadController:210] testKafkaAdminTopicOperations is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 14:01:01 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (586582ms till timeout)
2022-03-28 14:01:01 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-28 14:01:01 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 14:01:01 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (452730ms till timeout)
2022-03-28 14:01:02 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:00:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:02 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (185234ms till timeout)
2022-03-28 14:01:02 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-844045c3-kafka-0)
2022-03-28 14:01:02 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-844045c3-kafka, strimzi.io/cluster=my-cluster-844045c3, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1791335ms till timeout)
2022-03-28 14:01:02 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-28 14:01:02 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (585484ms till timeout)
2022-03-28 14:01:03 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-28 14:01:03 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 14:01:03 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (451268ms till timeout)
2022-03-28 14:01:03 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:00:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:03 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (184038ms till timeout)
2022-03-28 14:01:03 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-844045c3-kafka-0)
2022-03-28 14:01:03 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-844045c3-kafka, strimzi.io/cluster=my-cluster-844045c3, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1790138ms till timeout)
2022-03-28 14:01:03 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testUserTemplate is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 14:01:04 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (584385ms till timeout)
2022-03-28 14:01:04 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-28 14:01:04 [ForkJoinPool-1-worker-1] DEBUG [JobUtils:40] Job alter-admin-my-cluster-8f6c28cb-kafka-clients was deleted
2022-03-28 14:01:04 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:155] Create/Update Job teardown-delete in namespace throttling-quota-st
2022-03-28 14:01:04 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:00:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:04 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:teardown-delete
2022-03-28 14:01:04 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-28 14:01:04 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 14:01:04 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (449788ms till timeout)
2022-03-28 14:01:04 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (182708ms till timeout)
2022-03-28 14:01:04 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-844045c3-kafka-0)
2022-03-28 14:01:04 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-844045c3-kafka, strimzi.io/cluster=my-cluster-844045c3, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1788905ms till timeout)
2022-03-28 14:01:04 [ForkJoinPool-1-worker-1] INFO  [JobUtils:81] Waiting for job: teardown-delete will be in active state
2022-03-28 14:01:04 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 14:01:05 [ForkJoinPool-1-worker-1] INFO  [ClientUtils:76] Waiting for producer/consumer:teardown-delete to finished
2022-03-28 14:01:05 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for job finished
2022-03-28 14:01:05 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (583261ms till timeout)
2022-03-28 14:01:05 [ForkJoinPool-1-worker-1] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:00Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:05 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (129710ms till timeout)
2022-03-28 14:01:05 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-28 14:01:05 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:00:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:06 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-844045c3-kafka-0)
2022-03-28 14:01:06 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-844045c3-kafka, strimzi.io/cluster=my-cluster-844045c3, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1787707ms till timeout)
2022-03-28 14:01:06 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (181379ms till timeout)
2022-03-28 14:01:06 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (582163ms till timeout)
2022-03-28 14:01:06 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-28 14:01:06 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 14:01:06 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (448326ms till timeout)
2022-03-28 14:01:06 [ForkJoinPool-1-worker-1] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:00Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:06 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (128515ms till timeout)
2022-03-28 14:01:06 [ForkJoinPool-1-worker-5] TRACE [SuiteThreadController:210] testKafkaAdminTopicOperations is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 14:01:07 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-28 14:01:07 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:00:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:07 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-844045c3-kafka-0)
2022-03-28 14:01:07 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-844045c3-kafka, strimzi.io/cluster=my-cluster-844045c3, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1786474ms till timeout)
2022-03-28 14:01:07 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (580955ms till timeout)
2022-03-28 14:01:07 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (180088ms till timeout)
2022-03-28 14:01:07 [ForkJoinPool-1-worker-1] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:00Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:07 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (127319ms till timeout)
2022-03-28 14:01:07 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-28 14:01:07 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 14:01:07 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (446855ms till timeout)
2022-03-28 14:01:08 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:00:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:08 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-844045c3-kafka-0)
2022-03-28 14:01:08 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-844045c3-kafka, strimzi.io/cluster=my-cluster-844045c3, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1785279ms till timeout)
2022-03-28 14:01:08 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (579760ms till timeout)
2022-03-28 14:01:08 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (178892ms till timeout)
2022-03-28 14:01:08 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-28 14:01:08 [ForkJoinPool-1-worker-1] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:00Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:08 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testUserTemplate is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 14:01:08 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (126125ms till timeout)
2022-03-28 14:01:09 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-28 14:01:09 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 14:01:09 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (445373ms till timeout)
2022-03-28 14:01:09 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-0 not ready: kafka)
2022-03-28 14:01:09 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-1 not ready: kafka)
2022-03-28 14:01:09 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-2 not ready: kafka)
2022-03-28 14:01:09 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:106] Pods my-cluster-844045c3-kafka-0, my-cluster-844045c3-kafka-1, my-cluster-844045c3-kafka-2 are ready
2022-03-28 14:01:09 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-844045c3-kafka, strimzi.io/cluster=my-cluster-844045c3, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1784086ms till timeout)
2022-03-28 14:01:09 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (578656ms till timeout)
2022-03-28 14:01:09 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:00:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:09 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (177608ms till timeout)
2022-03-28 14:01:10 [ForkJoinPool-1-worker-1] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:00Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:10 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (124899ms till timeout)
2022-03-28 14:01:10 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-28 14:01:10 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-28 14:01:10 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 1
2022-03-28 14:01:10 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 14:01:10 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Error from server (NotFound): namespaces "namespace-8" not found
2022-03-28 14:01:10 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] ======STDERR END======
2022-03-28 14:01:10 [ForkJoinPool-1-worker-3] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[], io.strimzi.test.logs.CollectorElement@c4c0ea0=[], io.strimzi.test.logs.CollectorElement@aec142ef=[], io.strimzi.test.logs.CollectorElement@f851b6c3=[], io.strimzi.test.logs.CollectorElement@3881d5f2=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@5c7379cb=[], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[namespace-4], io.strimzi.test.logs.CollectorElement@f4405b3b=[namespace-5], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 14:01:10 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:267] testConfigurationReflection - Notifies waiting test cases:[testCapacityFile, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testThrottlingQuotasCreateAlterPartitions, testSendSimpleMessageTls, testReceiveSimpleMessageTlsScramSha, testSendSimpleMessageTlsScramSha, testReceiveSimpleMessageTls, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testConfigurationFileIsCreated, testPauseReconciliationInKafkaRebalanceAndTopic, testKafkaAdminTopicOperations, testUserTemplate] to and randomly select one to start execution
2022-03-28 14:01:10 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:93] [cruisecontrol.CruiseControlConfigurationST] - Removing parallel test: testConfigurationReflection
2022-03-28 14:01:10 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:97] [cruisecontrol.CruiseControlConfigurationST] - Parallel test count: 6
2022-03-28 14:01:10 [ForkJoinPool-1-worker-3] INFO  [TestSeparator:29] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testConfigurationReflection-FINISHED
2022-03-28 14:01:10 [ForkJoinPool-1-worker-3] INFO  [TestSeparator:30] ############################################################################
2022-03-28 14:01:10 [ForkJoinPool-1-worker-3] INFO  [TestSeparator:23] ############################################################################
2022-03-28 14:01:10 [ForkJoinPool-1-worker-3] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.user.UserST.testTlsExternalUserWithQuotas-STARTED
2022-03-28 14:01:10 [ForkJoinPool-1-worker-3] DEBUG [AbstractST:658] ============================================================================
2022-03-28 14:01:10 [ForkJoinPool-1-worker-3] DEBUG [AbstractST:659] [operators.user.UserST - Before Each] - Setup test case environment
2022-03-28 14:01:10 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:77] [operators.user.UserST] - Adding parallel test: testTlsExternalUserWithQuotas
2022-03-28 14:01:10 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:81] [operators.user.UserST] - Parallel test count: 7
2022-03-28 14:01:10 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:205] [testTlsExternalUserWithQuotas] moved to the WaitZone, because current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 14:01:10 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testTlsExternalUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 14:01:10 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-0 not ready: kafka)
2022-03-28 14:01:10 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-1 not ready: kafka)
2022-03-28 14:01:10 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-2 not ready: kafka)
2022-03-28 14:01:10 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:106] Pods my-cluster-844045c3-kafka-0, my-cluster-844045c3-kafka-1, my-cluster-844045c3-kafka-2 are ready
2022-03-28 14:01:10 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-844045c3-kafka, strimzi.io/cluster=my-cluster-844045c3, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1782892ms till timeout)
2022-03-28 14:01:10 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (577461ms till timeout)
2022-03-28 14:01:11 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:00:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:11 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (176410ms till timeout)
2022-03-28 14:01:11 [ForkJoinPool-1-worker-1] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:00Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:11 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (123705ms till timeout)
2022-03-28 14:01:11 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:230] testKafkaAdminTopicOperations test now can proceed its execution
2022-03-28 14:01:11 [ForkJoinPool-1-worker-5] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 14:01:11 [ForkJoinPool-1-worker-5] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testScramUserWithQuotas=my-cluster-6f653060, testKafkaAdminTopicOperations=my-cluster-600e8332, testReceiveSimpleMessageTlsScramSha=my-cluster-af298d01, testConfigurationFileIsCreated=my-cluster-3244df0d, testThrottlingQuotasCreateAlterPartitions=my-cluster-8f6c28cb, testSendSimpleMessageTls=my-cluster-fbc845dd, testCreatingUsersWithSecretPrefix=my-cluster-0c65d2dd, testUpdateUser=my-cluster-c2c4d05a, testThrottlingQuotasDeleteTopic=my-cluster-419d5c31, testConfigurationReflection=my-cluster-5137fff1, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-0444229e, testUserWithNameMoreThan64Chars=my-cluster-02e25de6, testDeployAndUnDeployCruiseControl=my-cluster-844045c3, testTlsUserWithQuotas=my-cluster-9832c9ae, testThrottlingQuotasCreateTopic=my-cluster-ac2cadd6, testCapacityFile=my-cluster-0531d5ce, testReceiveSimpleMessageTls=my-cluster-ace1fa24, testConfigurationPerformanceOptions=my-cluster-4994018d, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-8af61d52, testTlsExternalUser=my-cluster-43f83aef, testSendSimpleMessageTlsScramSha=my-cluster-e353214b}
2022-03-28 14:01:11 [ForkJoinPool-1-worker-5] TRACE [AbstractST:607] USERS_NAME_MAP: {testScramUserWithQuotas=my-user-27563760-1632863338, testKafkaAdminTopicOperations=my-user-1735322302-803507072, testReceiveSimpleMessageTlsScramSha=my-user-1597210213-707359243, testConfigurationFileIsCreated=my-user-1799841978-1468805631, testThrottlingQuotasCreateAlterPartitions=my-user-1557959119-1722636050, testSendSimpleMessageTls=my-user-1754673553-2077518027, testCreatingUsersWithSecretPrefix=my-user-144001639-226807213, testUpdateUser=my-user-1870578302-1216914412, testThrottlingQuotasDeleteTopic=my-user-1782685497-1980640589, testConfigurationReflection=my-user-2029261898-1375854235, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-1010649803-214635564, testUserWithNameMoreThan64Chars=my-user-2124392695-340403978, testDeployAndUnDeployCruiseControl=my-user-2117120687-604608920, testTlsUserWithQuotas=my-user-1602931187-443256751, testThrottlingQuotasCreateTopic=my-user-46611685-1079680568, testCapacityFile=my-user-155969448-1270555822, testReceiveSimpleMessageTls=my-user-1418772225-1024845009, testConfigurationPerformanceOptions=my-user-882948629-1821489961, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-1807142870-188660517, testTlsExternalUser=my-user-370513135-1989152728, testSendSimpleMessageTlsScramSha=my-user-182357028-1132873010}
2022-03-28 14:01:11 [ForkJoinPool-1-worker-5] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testScramUserWithQuotas=my-topic-1345794255-226872460, testKafkaAdminTopicOperations=my-topic-1935791607-1758305855, testReceiveSimpleMessageTlsScramSha=my-topic-649423016-1623551209, testConfigurationFileIsCreated=my-topic-1435014906-681104826, testThrottlingQuotasCreateAlterPartitions=my-topic-1449977352-344480794, testSendSimpleMessageTls=my-topic-115477502-2025274778, testCreatingUsersWithSecretPrefix=my-topic-130462979-2061578809, testUpdateUser=my-topic-829325799-540543436, testThrottlingQuotasDeleteTopic=my-topic-1797350406-813887294, testConfigurationReflection=my-topic-1621095669-1276202830, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-889283546-691691584, testUserWithNameMoreThan64Chars=my-topic-1053060011-1199293498, testDeployAndUnDeployCruiseControl=my-topic-1101175281-2107378523, testTlsUserWithQuotas=my-topic-985048738-437665866, testThrottlingQuotasCreateTopic=my-topic-114922478-1506136333, testCapacityFile=my-topic-296605537-1008743104, testReceiveSimpleMessageTls=my-topic-1131768783-120051821, testConfigurationPerformanceOptions=my-topic-627301835-843876360, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-1367124108-1748498395, testTlsExternalUser=my-topic-2081139838-1814296134, testSendSimpleMessageTlsScramSha=my-topic-652506937-1802310709}
2022-03-28 14:01:11 [ForkJoinPool-1-worker-5] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testScramUserWithQuotas=my-cluster-6f653060-kafka-clients, testKafkaAdminTopicOperations=my-cluster-600e8332-kafka-clients, testReceiveSimpleMessageTlsScramSha=my-cluster-af298d01-kafka-clients, testConfigurationFileIsCreated=my-cluster-3244df0d-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-8f6c28cb-kafka-clients, testSendSimpleMessageTls=my-cluster-fbc845dd-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-0c65d2dd-kafka-clients, testUpdateUser=my-cluster-c2c4d05a-kafka-clients, testThrottlingQuotasDeleteTopic=my-cluster-419d5c31-kafka-clients, testConfigurationReflection=my-cluster-5137fff1-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-0444229e-kafka-clients, testUserWithNameMoreThan64Chars=my-cluster-02e25de6-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-844045c3-kafka-clients, testTlsUserWithQuotas=my-cluster-9832c9ae-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-ac2cadd6-kafka-clients, testCapacityFile=my-cluster-0531d5ce-kafka-clients, testReceiveSimpleMessageTls=my-cluster-ace1fa24-kafka-clients, testConfigurationPerformanceOptions=my-cluster-4994018d-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-8af61d52-kafka-clients, testTlsExternalUser=my-cluster-43f83aef-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-e353214b-kafka-clients}
2022-03-28 14:01:11 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:155] Create/Update KafkaUser my-user-1735322302-803507072 in namespace throttling-quota-st
2022-03-28 14:01:11 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-1735322302-803507072
2022-03-28 14:01:11 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:433] Wait for KafkaUser: my-user-1735322302-803507072 will have desired state: Ready
2022-03-28 14:01:11 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for KafkaUser: my-user-1735322302-803507072 will have desired state: Ready
2022-03-28 14:01:12 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (576352ms till timeout)
2022-03-28 14:01:12 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-0 not ready: kafka)
2022-03-28 14:01:12 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-1 not ready: kafka)
2022-03-28 14:01:12 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-2 not ready: kafka)
2022-03-28 14:01:12 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:106] Pods my-cluster-844045c3-kafka-0, my-cluster-844045c3-kafka-1, my-cluster-844045c3-kafka-2 are ready
2022-03-28 14:01:12 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-844045c3-kafka, strimzi.io/cluster=my-cluster-844045c3, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1781687ms till timeout)
2022-03-28 14:01:12 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:444] KafkaUser: my-user-1735322302-803507072 is in desired state: Ready
2022-03-28 14:01:12 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:00:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:12 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:155] Create/Update Job create-admin-my-cluster-600e8332-kafka-clients in namespace throttling-quota-st
2022-03-28 14:01:12 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (175112ms till timeout)
2022-03-28 14:01:12 [ForkJoinPool-1-worker-1] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:00Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:12 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:create-admin-my-cluster-600e8332-kafka-clients
2022-03-28 14:01:12 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (122424ms till timeout)
2022-03-28 14:01:12 [ForkJoinPool-1-worker-5] INFO  [JobUtils:81] Waiting for job: create-admin-my-cluster-600e8332-kafka-clients will be in active state
2022-03-28 14:01:12 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 14:01:12 [ForkJoinPool-1-worker-5] INFO  [PodUtils:186] Waiting for message will be in the log
2022-03-28 14:01:12 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Waiting for message will be in the log
2022-03-28 14:01:13 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (119899ms till timeout)
2022-03-28 14:01:13 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (575255ms till timeout)
2022-03-28 14:01:13 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-0 not ready: kafka)
2022-03-28 14:01:13 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-1 not ready: kafka)
2022-03-28 14:01:13 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-2 not ready: kafka)
2022-03-28 14:01:13 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:106] Pods my-cluster-844045c3-kafka-0, my-cluster-844045c3-kafka-1, my-cluster-844045c3-kafka-2 are ready
2022-03-28 14:01:13 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-844045c3-kafka, strimzi.io/cluster=my-cluster-844045c3, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1780585ms till timeout)
2022-03-28 14:01:13 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:00:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:13 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (173918ms till timeout)
2022-03-28 14:01:13 [ForkJoinPool-1-worker-1] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:00Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:13 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (121230ms till timeout)
2022-03-28 14:01:13 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testUserTemplate is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 14:01:14 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (118799ms till timeout)
2022-03-28 14:01:14 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (574152ms till timeout)
2022-03-28 14:01:14 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-0 not ready: kafka)
2022-03-28 14:01:14 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-1 not ready: kafka)
2022-03-28 14:01:14 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-2 not ready: kafka)
2022-03-28 14:01:14 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:106] Pods my-cluster-844045c3-kafka-0, my-cluster-844045c3-kafka-1, my-cluster-844045c3-kafka-2 are ready
2022-03-28 14:01:14 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-844045c3-kafka, strimzi.io/cluster=my-cluster-844045c3, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1779482ms till timeout)
2022-03-28 14:01:14 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:00:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:14 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (172724ms till timeout)
2022-03-28 14:01:14 [ForkJoinPool-1-worker-1] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:00Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:15 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (120036ms till timeout)
2022-03-28 14:01:15 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (117699ms till timeout)
2022-03-28 14:01:15 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (573054ms till timeout)
2022-03-28 14:01:15 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-0 not ready: kafka)
2022-03-28 14:01:15 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-1 not ready: kafka)
2022-03-28 14:01:15 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-2 not ready: kafka)
2022-03-28 14:01:15 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:106] Pods my-cluster-844045c3-kafka-0, my-cluster-844045c3-kafka-1, my-cluster-844045c3-kafka-2 are ready
2022-03-28 14:01:15 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-844045c3-kafka, strimzi.io/cluster=my-cluster-844045c3, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1778381ms till timeout)
2022-03-28 14:01:15 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testTlsExternalUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 14:01:15 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:00:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:16 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (171529ms till timeout)
2022-03-28 14:01:16 [ForkJoinPool-1-worker-1] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:00Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:16 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (118841ms till timeout)
2022-03-28 14:01:16 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (116595ms till timeout)
2022-03-28 14:01:16 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (571955ms till timeout)
2022-03-28 14:01:16 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-0 not ready: kafka)
2022-03-28 14:01:16 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-1 not ready: kafka)
2022-03-28 14:01:16 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-2 not ready: kafka)
2022-03-28 14:01:16 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:106] Pods my-cluster-844045c3-kafka-0, my-cluster-844045c3-kafka-1, my-cluster-844045c3-kafka-2 are ready
2022-03-28 14:01:16 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-844045c3-kafka, strimzi.io/cluster=my-cluster-844045c3, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1777281ms till timeout)
2022-03-28 14:01:17 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:00:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:17 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (170333ms till timeout)
2022-03-28 14:01:17 [ForkJoinPool-1-worker-1] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:00Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:17 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (117645ms till timeout)
2022-03-28 14:01:17 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (115413ms till timeout)
2022-03-28 14:01:17 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (570810ms till timeout)
2022-03-28 14:01:17 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-0 not ready: kafka)
2022-03-28 14:01:17 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-1 not ready: kafka)
2022-03-28 14:01:17 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-2 not ready: kafka)
2022-03-28 14:01:17 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:106] Pods my-cluster-844045c3-kafka-0, my-cluster-844045c3-kafka-1, my-cluster-844045c3-kafka-2 are ready
2022-03-28 14:01:17 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-844045c3-kafka, strimzi.io/cluster=my-cluster-844045c3, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1776140ms till timeout)
2022-03-28 14:01:18 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:00:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:18 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (169138ms till timeout)
2022-03-28 14:01:18 [ForkJoinPool-1-worker-1] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:00Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:18 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (114220ms till timeout)
2022-03-28 14:01:18 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (116351ms till timeout)
2022-03-28 14:01:18 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (569709ms till timeout)
2022-03-28 14:01:18 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-0 not ready: kafka)
2022-03-28 14:01:18 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-1 not ready: kafka)
2022-03-28 14:01:18 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-2 not ready: kafka)
2022-03-28 14:01:18 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:106] Pods my-cluster-844045c3-kafka-0, my-cluster-844045c3-kafka-1, my-cluster-844045c3-kafka-2 are ready
2022-03-28 14:01:18 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-844045c3-kafka, strimzi.io/cluster=my-cluster-844045c3, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1775040ms till timeout)
2022-03-28 14:01:18 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testUserTemplate is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 14:01:19 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:00:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:19 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (167940ms till timeout)
2022-03-28 14:01:19 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (113028ms till timeout)
2022-03-28 14:01:19 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (568516ms till timeout)
2022-03-28 14:01:19 [ForkJoinPool-1-worker-1] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:00Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:19 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-0 not ready: kafka)
2022-03-28 14:01:19 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-1 not ready: kafka)
2022-03-28 14:01:19 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-2 not ready: kafka)
2022-03-28 14:01:19 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:106] Pods my-cluster-844045c3-kafka-0, my-cluster-844045c3-kafka-1, my-cluster-844045c3-kafka-2 are ready
2022-03-28 14:01:19 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-844045c3-kafka, strimzi.io/cluster=my-cluster-844045c3, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1773850ms till timeout)
2022-03-28 14:01:20 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (114968ms till timeout)
2022-03-28 14:01:20 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testTlsExternalUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 14:01:20 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:00:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:20 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (166741ms till timeout)
2022-03-28 14:01:21 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (111833ms till timeout)
2022-03-28 14:01:21 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (567321ms till timeout)
2022-03-28 14:01:21 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-0 not ready: kafka)
2022-03-28 14:01:21 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-1 not ready: kafka)
2022-03-28 14:01:21 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-844045c3-kafka-2 not ready: kafka)
2022-03-28 14:01:21 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:106] Pods my-cluster-844045c3-kafka-0, my-cluster-844045c3-kafka-1, my-cluster-844045c3-kafka-2 are ready
2022-03-28 14:01:21 [ForkJoinPool-1-worker-1] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:00Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:21 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:433] Wait for Kafka: my-cluster-844045c3 will have desired state: Ready
2022-03-28 14:01:21 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Kafka: my-cluster-844045c3 will have desired state: Ready
2022-03-28 14:01:21 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (113674ms till timeout)
2022-03-28 14:01:21 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:444] Kafka: my-cluster-844045c3 is in desired state: Ready
2022-03-28 14:01:21 [ForkJoinPool-1-worker-7] INFO  [RollingUpdateUtils:132] Kafka: my-cluster-844045c3 is ready
2022-03-28 14:01:21 [ForkJoinPool-1-worker-7] INFO  [CruiseControlConfigurationST:136] Verifying that in Kafka config map there is configuration to cruise control metric reporter
2022-03-28 14:01:21 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Verify that kafka configuration {cruise.control.metrics.reporter.ssl.keystore.location=/tmp/kafka/cluster.keystore.p12, cruise.control.metrics.reporter.ssl.endpoint.identification.algorithm=HTTPS, cruise.control.metrics.reporter.ssl.truststore.password=${CERTS_STORE_PASSWORD}, cruise.control.metrics.reporter.ssl.truststore.type=PKCS12, cruise.control.metrics.reporter.ssl.keystore.password=${CERTS_STORE_PASSWORD}, cruise.control.metrics.topic.replication.factor=3, cluster-name=my-cluster-844045c3, cruise.control.metrics.reporter.security.protocol=SSL, cruise.control.metrics.reporter.ssl.keystore.type=PKCS12, cruise.control.metrics.topic.num.partitions=1, cruise.control.metrics.reporter.ssl.truststore.location=/tmp/kafka/cluster.truststore.p12, cruise.control.metrics.topic=strimzi.cruisecontrol.metrics, cruise.control.metrics.topic.min.insync.replicas=1, cruise.control.metrics.reporter.bootstrap.servers=my-cluster-844045c3-kafka-brokers:9091, cruise.control.metrics.topic.auto.create=true} has correct cruise control metric reporter properties
2022-03-28 14:01:21 [ForkJoinPool-1-worker-7] INFO  [CruiseControlConfigurationST:139] Verifying that Cruise Control topics are created after CC is instantiated.
2022-03-28 14:01:21 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Verify that kafka contains cruise control topics with related configuration.
2022-03-28 14:01:21 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Verify that kafka contains cruise control topics with related configuration.
2022-03-28 14:01:21 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:00:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:22 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:674] ============================================================================
2022-03-28 14:01:22 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:675] [cruisecontrol.CruiseControlConfigurationST - After Each] - Clean up after test
2022-03-28 14:01:22 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:344] ############################################################################
2022-03-28 14:01:22 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:348] Delete all resources for testDeployAndUnDeployCruiseControl
2022-03-28 14:01:22 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:241] Delete of Kafka my-cluster-844045c3 in namespace namespace-5
2022-03-28 14:01:22 [ForkJoinPool-1-worker-7] INFO  [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-5, for cruise control Kafka cluster my-cluster-844045c3
2022-03-28 14:01:22 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (165392ms till timeout)
2022-03-28 14:01:22 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (566156ms till timeout)
2022-03-28 14:01:22 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (110661ms till timeout)
2022-03-28 14:01:22 [ForkJoinPool-1-worker-1] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:00Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:22 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (112315ms till timeout)
2022-03-28 14:01:23 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-844045c3
2022-03-28 14:01:23 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster-844045c3 not ready, will try again in 10000 ms (839901ms till timeout)
2022-03-28 14:01:23 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:00:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:23 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (564954ms till timeout)
2022-03-28 14:01:23 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (109418ms till timeout)
2022-03-28 14:01:23 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (163988ms till timeout)
2022-03-28 14:01:23 [ForkJoinPool-1-worker-1] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:00Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:23 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testUserTemplate is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 14:01:23 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (111120ms till timeout)
2022-03-28 14:01:24 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (563855ms till timeout)
2022-03-28 14:01:24 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (108171ms till timeout)
2022-03-28 14:01:24 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:00:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:24 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (162695ms till timeout)
2022-03-28 14:01:25 [ForkJoinPool-1-worker-1] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:00Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:25 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (109910ms till timeout)
2022-03-28 14:01:25 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (562757ms till timeout)
2022-03-28 14:01:25 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testTlsExternalUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 14:01:25 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (106977ms till timeout)
2022-03-28 14:01:25 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:00:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:26 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (161498ms till timeout)
2022-03-28 14:01:26 [ForkJoinPool-1-worker-1] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:00Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:26 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (108715ms till timeout)
2022-03-28 14:01:26 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (561659ms till timeout)
2022-03-28 14:01:27 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (105783ms till timeout)
2022-03-28 14:01:27 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:00:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:27 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (160302ms till timeout)
2022-03-28 14:01:27 [ForkJoinPool-1-worker-1] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:00Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:27 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (107517ms till timeout)
2022-03-28 14:01:27 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (560559ms till timeout)
2022-03-28 14:01:28 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (104591ms till timeout)
2022-03-28 14:01:28 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:00:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:28 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (159106ms till timeout)
2022-03-28 14:01:28 [ForkJoinPool-1-worker-1] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:00Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:28 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (106322ms till timeout)
2022-03-28 14:01:28 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testUserTemplate is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 14:01:28 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (559461ms till timeout)
2022-03-28 14:01:29 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (103398ms till timeout)
2022-03-28 14:01:29 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:00:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:29 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (157909ms till timeout)
2022-03-28 14:01:29 [ForkJoinPool-1-worker-1] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:00Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:29 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (105126ms till timeout)
2022-03-28 14:01:30 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (558362ms till timeout)
2022-03-28 14:01:30 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testTlsExternalUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 14:01:30 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (102205ms till timeout)
2022-03-28 14:01:30 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:00:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:30 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (156712ms till timeout)
2022-03-28 14:01:31 [ForkJoinPool-1-worker-1] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:00Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:31 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (103928ms till timeout)
2022-03-28 14:01:31 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (557191ms till timeout)
2022-03-28 14:01:31 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (101011ms till timeout)
2022-03-28 14:01:31 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:00:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:32 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (155517ms till timeout)
2022-03-28 14:01:32 [ForkJoinPool-1-worker-1] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:00Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:32 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (556092ms till timeout)
2022-03-28 14:01:32 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (102638ms till timeout)
2022-03-28 14:01:33 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (99819ms till timeout)
2022-03-28 14:01:33 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:00:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:33 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (154322ms till timeout)
2022-03-28 14:01:33 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:369] ############################################################################
2022-03-28 14:01:33 [ForkJoinPool-1-worker-7] INFO  [TestSuiteNamespaceManager:200] Deleting namespace:namespace-5 for test case:testDeployAndUnDeployCruiseControl
2022-03-28 14:01:33 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (554968ms till timeout)
2022-03-28 14:01:33 [ForkJoinPool-1-worker-1] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:00Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:33 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Namespace namespace-5 removal
2022-03-28 14:01:33 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 14:01:33 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (101417ms till timeout)
2022-03-28 14:01:33 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testUserTemplate is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 14:01:34 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (98626ms till timeout)
2022-03-28 14:01:34 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:00:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:34 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (153112ms till timeout)
2022-03-28 14:01:34 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (553854ms till timeout)
2022-03-28 14:01:34 [ForkJoinPool-1-worker-1] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:00Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:34 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (100211ms till timeout)
2022-03-28 14:01:35 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (97432ms till timeout)
2022-03-28 14:01:35 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:00:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:35 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (151915ms till timeout)
2022-03-28 14:01:35 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testTlsExternalUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 14:01:35 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (552681ms till timeout)
2022-03-28 14:01:35 [ForkJoinPool-1-worker-1] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:00Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:36 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (99017ms till timeout)
2022-03-28 14:01:36 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (96239ms till timeout)
2022-03-28 14:01:36 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:00:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:36 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (551582ms till timeout)
2022-03-28 14:01:36 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (150622ms till timeout)
2022-03-28 14:01:37 [ForkJoinPool-1-worker-1] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:00Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:37 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (97819ms till timeout)
2022-03-28 14:01:37 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (95045ms till timeout)
2022-03-28 14:01:37 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (550484ms till timeout)
2022-03-28 14:01:38 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:00:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:38 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (149426ms till timeout)
2022-03-28 14:01:38 [ForkJoinPool-1-worker-1] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:00Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:38 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (96625ms till timeout)
2022-03-28 14:01:38 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testUserTemplate is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 14:01:39 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 14:01:39 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 14:01:39 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (474489ms till timeout)
2022-03-28 14:01:39 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (93853ms till timeout)
2022-03-28 14:01:39 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (549341ms till timeout)
2022-03-28 14:01:39 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:00:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:39 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (148230ms till timeout)
2022-03-28 14:01:39 [ForkJoinPool-1-worker-1] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:00Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:39 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (95430ms till timeout)
2022-03-28 14:01:40 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 14:01:40 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (92659ms till timeout)
2022-03-28 14:01:40 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (548147ms till timeout)
2022-03-28 14:01:40 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:00:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:40 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (147030ms till timeout)
2022-03-28 14:01:40 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 14:01:40 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 14:01:40 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (472915ms till timeout)
2022-03-28 14:01:40 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testTlsExternalUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 14:01:40 [ForkJoinPool-1-worker-1] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:00Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:40 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (94234ms till timeout)
2022-03-28 14:01:41 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (91467ms till timeout)
2022-03-28 14:01:41 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (546954ms till timeout)
2022-03-28 14:01:41 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:00:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:41 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 14:01:41 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (145836ms till timeout)
2022-03-28 14:01:41 [ForkJoinPool-1-worker-1] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:00Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:42 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (93039ms till timeout)
2022-03-28 14:01:42 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (90274ms till timeout)
2022-03-28 14:01:42 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (545763ms till timeout)
2022-03-28 14:01:42 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:00:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:42 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (144641ms till timeout)
2022-03-28 14:01:43 [ForkJoinPool-1-worker-1] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:00Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:43 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (91845ms till timeout)
2022-03-28 14:01:43 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (89079ms till timeout)
2022-03-28 14:01:43 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (544567ms till timeout)
2022-03-28 14:01:43 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testUserTemplate is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 14:01:44 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:00:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:44 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (143446ms till timeout)
2022-03-28 14:01:44 [ForkJoinPool-1-worker-1] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:00Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:44 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (90649ms till timeout)
2022-03-28 14:01:45 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (87888ms till timeout)
2022-03-28 14:01:45 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (543375ms till timeout)
2022-03-28 14:01:45 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:00:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:45 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (142252ms till timeout)
2022-03-28 14:01:45 [ForkJoinPool-1-worker-1] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:00Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:45 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (89453ms till timeout)
2022-03-28 14:01:45 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testTlsExternalUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 14:01:46 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (86696ms till timeout)
2022-03-28 14:01:46 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (542181ms till timeout)
2022-03-28 14:01:46 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:00:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:46 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (141055ms till timeout)
2022-03-28 14:01:46 [ForkJoinPool-1-worker-1] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:00Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:46 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (88257ms till timeout)
2022-03-28 14:01:47 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 14:01:47 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 14:01:47 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (466418ms till timeout)
2022-03-28 14:01:47 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (85505ms till timeout)
2022-03-28 14:01:47 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (540993ms till timeout)
2022-03-28 14:01:47 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:00:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:47 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (139859ms till timeout)
2022-03-28 14:01:47 [ForkJoinPool-1-worker-1] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:00Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:47 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (87061ms till timeout)
2022-03-28 14:01:48 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 14:01:48 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (84312ms till timeout)
2022-03-28 14:01:48 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (539799ms till timeout)
2022-03-28 14:01:48 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:00:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:48 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testUserTemplate is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 14:01:48 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (138664ms till timeout)
2022-03-28 14:01:49 [ForkJoinPool-1-worker-1] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:00Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:49 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (85866ms till timeout)
2022-03-28 14:01:49 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (83119ms till timeout)
2022-03-28 14:01:49 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (538606ms till timeout)
2022-03-28 14:01:49 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:00:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:50 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (137469ms till timeout)
2022-03-28 14:01:50 [ForkJoinPool-1-worker-1] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:00Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:50 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (84671ms till timeout)
2022-03-28 14:01:50 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testTlsExternalUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 14:01:50 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (81925ms till timeout)
2022-03-28 14:01:50 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (537411ms till timeout)
2022-03-28 14:01:51 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:00:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:51 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (136273ms till timeout)
2022-03-28 14:01:51 [ForkJoinPool-1-worker-1] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:00Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:51 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (83477ms till timeout)
2022-03-28 14:01:52 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (80734ms till timeout)
2022-03-28 14:01:52 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (536221ms till timeout)
2022-03-28 14:01:52 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:00:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:52 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (135077ms till timeout)
2022-03-28 14:01:52 [ForkJoinPool-1-worker-1] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:00Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:52 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (82281ms till timeout)
2022-03-28 14:01:53 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (79542ms till timeout)
2022-03-28 14:01:53 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaRebalance: my-cluster-0444229e will have desired state: Ready not ready, will try again in 1000 ms (535027ms till timeout)
2022-03-28 14:01:53 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:00:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:53 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 14:01:53 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 14:01:53 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (459898ms till timeout)
2022-03-28 14:01:53 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (133881ms till timeout)
2022-03-28 14:01:53 [ForkJoinPool-1-worker-1] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:00Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:53 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testUserTemplate is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 14:01:53 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (81086ms till timeout)
2022-03-28 14:01:54 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (78349ms till timeout)
2022-03-28 14:01:54 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:444] KafkaRebalance: my-cluster-0444229e is in desired state: Ready
2022-03-28 14:01:54 [ForkJoinPool-1-worker-15] DEBUG [AbstractST:674] ============================================================================
2022-03-28 14:01:54 [ForkJoinPool-1-worker-15] DEBUG [AbstractST:675] [operators.ReconciliationST - After Each] - Clean up after test
2022-03-28 14:01:54 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:344] ############################################################################
2022-03-28 14:01:54 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:348] Delete all resources for testPauseReconciliationInKafkaRebalanceAndTopic
2022-03-28 14:01:54 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:241] Delete of KafkaTopic my-topic-889283546-691691584 in namespace namespace-4
2022-03-28 14:01:54 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 14:01:54 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-889283546-691691584
2022-03-28 14:01:54 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:00:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:54 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:241] Delete of KafkaRebalance my-cluster-0444229e in namespace namespace-4
2022-03-28 14:01:54 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (132590ms till timeout)
2022-03-28 14:01:55 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaRebalance:my-cluster-0444229e
2022-03-28 14:01:55 [ForkJoinPool-1-worker-1] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:00Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:55 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 14:01:55 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 14:01:55 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (458338ms till timeout)
2022-03-28 14:01:55 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:241] Delete of Kafka my-cluster-0444229e in namespace namespace-4
2022-03-28 14:01:55 [ForkJoinPool-1-worker-15] INFO  [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-4, for cruise control Kafka cluster my-cluster-0444229e
2022-03-28 14:01:55 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (79714ms till timeout)
2022-03-28 14:01:55 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testTlsExternalUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 14:01:55 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (77186ms till timeout)
2022-03-28 14:01:56 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:00:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:56 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 14:01:56 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (131241ms till timeout)
2022-03-28 14:01:56 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-0444229e
2022-03-28 14:01:56 [ForkJoinPool-1-worker-1] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:00Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:56 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:369] ############################################################################
2022-03-28 14:01:56 [ForkJoinPool-1-worker-15] INFO  [TestSuiteNamespaceManager:200] Deleting namespace:namespace-4 for test case:testPauseReconciliationInKafkaRebalanceAndTopic
2022-03-28 14:01:56 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 14:01:56 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 14:01:56 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (456806ms till timeout)
2022-03-28 14:01:56 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (78296ms till timeout)
2022-03-28 14:01:56 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (76024ms till timeout)
2022-03-28 14:01:56 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Namespace namespace-4 removal
2022-03-28 14:01:56 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-4 -o yaml
2022-03-28 14:01:57 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:00:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:57 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (130047ms till timeout)
2022-03-28 14:01:57 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 14:01:57 [ForkJoinPool-1-worker-1] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:00Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:57 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (77102ms till timeout)
2022-03-28 14:01:58 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (74777ms till timeout)
2022-03-28 14:01:58 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 14:01:58 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 14:01:58 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (455241ms till timeout)
2022-03-28 14:01:58 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:00:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:58 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (128853ms till timeout)
2022-03-28 14:01:58 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testUserTemplate is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 14:01:59 [ForkJoinPool-1-worker-1] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:00Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:59 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (75907ms till timeout)
2022-03-28 14:01:59 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 14:01:59 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (73582ms till timeout)
2022-03-28 14:01:59 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:00:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:01:59 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 14:01:59 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 14:01:59 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (453746ms till timeout)
2022-03-28 14:01:59 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (127657ms till timeout)
2022-03-28 14:02:00 [ForkJoinPool-1-worker-1] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:00Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:02:00 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (74710ms till timeout)
2022-03-28 14:02:00 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (72385ms till timeout)
2022-03-28 14:02:00 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testTlsExternalUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 14:02:00 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 14:02:00 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:00:22Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:02:01 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (126462ms till timeout)
2022-03-28 14:02:01 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 14:02:01 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 14:02:01 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (452176ms till timeout)
2022-03-28 14:02:01 [ForkJoinPool-1-worker-1] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:00Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:02:01 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (73513ms till timeout)
2022-03-28 14:02:01 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (71187ms till timeout)
2022-03-28 14:02:02 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=null, completedIndexes=null, completionTime=2022-03-28T14:01:56Z, conditions=[JobCondition(lastProbeTime=2022-03-28T14:01:56Z, lastTransitionTime=2022-03-28T14:01:56Z, message=null, reason=null, status=True, type=Complete, additionalProperties={})], failed=null, ready=null, startTime=2022-03-28T14:00:22Z, succeeded=1, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:02:02 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 14:02:02 [ForkJoinPool-1-worker-9] INFO  [PodUtils:186] Waiting for message will be in the log
2022-03-28 14:02:02 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Waiting for message will be in the log
2022-03-28 14:02:02 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-4 -o yaml
2022-03-28 14:02:02 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 14:02:02 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (474490ms till timeout)
2022-03-28 14:02:02 [ForkJoinPool-1-worker-9] INFO  [PodUtils:189] Message All topics created found in create-admin-my-cluster-419d5c31-kafka-clients-pwz5c log
2022-03-28 14:02:02 [ForkJoinPool-1-worker-9] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment create-admin-my-cluster-419d5c31-kafka-clients deletion
2022-03-28 14:02:02 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for ReplicaSet create-admin-my-cluster-419d5c31-kafka-clients to be deleted
2022-03-28 14:02:02 [ForkJoinPool-1-worker-1] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:00Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:02:02 [ForkJoinPool-1-worker-9] DEBUG [JobUtils:40] Job create-admin-my-cluster-419d5c31-kafka-clients was deleted
2022-03-28 14:02:02 [ForkJoinPool-1-worker-9] INFO  [ThrottlingQuotaST:112] Executing 2/5 iteration.
2022-03-28 14:02:02 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:155] Create/Update Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st
2022-03-28 14:02:02 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 14:02:02 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 14:02:02 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (450696ms till timeout)
2022-03-28 14:02:02 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (72180ms till timeout)
2022-03-28 14:02:02 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (70015ms till timeout)
2022-03-28 14:02:02 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:create-admin-my-cluster-419d5c31-kafka-clients
2022-03-28 14:02:03 [ForkJoinPool-1-worker-9] INFO  [JobUtils:81] Waiting for job: create-admin-my-cluster-419d5c31-kafka-clients will be in active state
2022-03-28 14:02:03 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 14:02:03 [ForkJoinPool-1-worker-9] INFO  [ClientUtils:76] Waiting for producer/consumer:create-admin-my-cluster-419d5c31-kafka-clients to finished
2022-03-28 14:02:03 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for job finished
2022-03-28 14:02:03 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:02:03 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (219799ms till timeout)
2022-03-28 14:02:03 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-4 -o yaml
2022-03-28 14:02:03 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 14:02:03 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-4 -o yaml
2022-03-28 14:02:03 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 14:02:03 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (473048ms till timeout)
2022-03-28 14:02:03 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testUserTemplate is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 14:02:03 [ForkJoinPool-1-worker-1] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:00Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:02:04 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (68759ms till timeout)
2022-03-28 14:02:04 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (70889ms till timeout)
2022-03-28 14:02:04 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 14:02:04 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 1
2022-03-28 14:02:04 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 14:02:04 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Error from server (NotFound): namespaces "namespace-5" not found
2022-03-28 14:02:04 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] ======STDERR END======
2022-03-28 14:02:04 [ForkJoinPool-1-worker-7] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[], io.strimzi.test.logs.CollectorElement@c4c0ea0=[], io.strimzi.test.logs.CollectorElement@aec142ef=[], io.strimzi.test.logs.CollectorElement@f851b6c3=[], io.strimzi.test.logs.CollectorElement@3881d5f2=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@5c7379cb=[], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[namespace-4], io.strimzi.test.logs.CollectorElement@f4405b3b=[], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 14:02:04 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:267] testDeployAndUnDeployCruiseControl - Notifies waiting test cases:[testCapacityFile, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testThrottlingQuotasCreateAlterPartitions, testSendSimpleMessageTls, testReceiveSimpleMessageTlsScramSha, testSendSimpleMessageTlsScramSha, testReceiveSimpleMessageTls, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testConfigurationFileIsCreated, testPauseReconciliationInKafkaRebalanceAndTopic, testUserTemplate, testTlsExternalUserWithQuotas] to and randomly select one to start execution
2022-03-28 14:02:04 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:93] [cruisecontrol.CruiseControlConfigurationST] - Removing parallel test: testDeployAndUnDeployCruiseControl
2022-03-28 14:02:04 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:97] [cruisecontrol.CruiseControlConfigurationST] - Parallel test count: 6
2022-03-28 14:02:04 [ForkJoinPool-1-worker-7] INFO  [TestSeparator:29] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testDeployAndUnDeployCruiseControl-FINISHED
2022-03-28 14:02:04 [ForkJoinPool-1-worker-7] INFO  [TestSeparator:30] ############################################################################
2022-03-28 14:02:04 [ForkJoinPool-1-worker-7] INFO  [TestSeparator:23] ############################################################################
2022-03-28 14:02:04 [ForkJoinPool-1-worker-7] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.topic.TopicST.testTopicModificationOfReplicationFactor-STARTED
2022-03-28 14:02:04 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:658] ============================================================================
2022-03-28 14:02:04 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:659] [operators.topic.TopicST - Before Each] - Setup test case environment
2022-03-28 14:02:04 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:77] [operators.topic.TopicST] - Adding parallel test: testTopicModificationOfReplicationFactor
2022-03-28 14:02:04 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:81] [operators.topic.TopicST] - Parallel test count: 7
2022-03-28 14:02:04 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:205] [testTopicModificationOfReplicationFactor] moved to the WaitZone, because current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 14:02:04 [ForkJoinPool-1-worker-7] TRACE [SuiteThreadController:210] testTopicModificationOfReplicationFactor is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 14:02:04 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:02:04 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (218604ms till timeout)
2022-03-28 14:02:04 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-4 -o yaml
2022-03-28 14:02:05 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-4 -o yaml
2022-03-28 14:02:05 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 14:02:05 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (471556ms till timeout)
2022-03-28 14:02:05 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (67566ms till timeout)
2022-03-28 14:02:05 [ForkJoinPool-1-worker-1] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:00Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:02:05 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (69599ms till timeout)
2022-03-28 14:02:05 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:02:05 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:230] testTlsExternalUserWithQuotas test now can proceed its execution
2022-03-28 14:02:05 [ForkJoinPool-1-worker-3] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 14:02:05 [ForkJoinPool-1-worker-3] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testScramUserWithQuotas=my-cluster-6f653060, testKafkaAdminTopicOperations=my-cluster-600e8332, testReceiveSimpleMessageTlsScramSha=my-cluster-af298d01, testConfigurationFileIsCreated=my-cluster-3244df0d, testThrottlingQuotasCreateAlterPartitions=my-cluster-8f6c28cb, testSendSimpleMessageTls=my-cluster-fbc845dd, testCreatingUsersWithSecretPrefix=my-cluster-0c65d2dd, testUpdateUser=my-cluster-c2c4d05a, testThrottlingQuotasDeleteTopic=my-cluster-419d5c31, testConfigurationReflection=my-cluster-5137fff1, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-0444229e, testTlsExternalUserWithQuotas=my-cluster-08d24b8e, testUserWithNameMoreThan64Chars=my-cluster-02e25de6, testDeployAndUnDeployCruiseControl=my-cluster-844045c3, testTlsUserWithQuotas=my-cluster-9832c9ae, testThrottlingQuotasCreateTopic=my-cluster-ac2cadd6, testCapacityFile=my-cluster-0531d5ce, testReceiveSimpleMessageTls=my-cluster-ace1fa24, testConfigurationPerformanceOptions=my-cluster-4994018d, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-8af61d52, testTlsExternalUser=my-cluster-43f83aef, testSendSimpleMessageTlsScramSha=my-cluster-e353214b}
2022-03-28 14:02:05 [ForkJoinPool-1-worker-3] TRACE [AbstractST:607] USERS_NAME_MAP: {testScramUserWithQuotas=my-user-27563760-1632863338, testKafkaAdminTopicOperations=my-user-1735322302-803507072, testReceiveSimpleMessageTlsScramSha=my-user-1597210213-707359243, testConfigurationFileIsCreated=my-user-1799841978-1468805631, testThrottlingQuotasCreateAlterPartitions=my-user-1557959119-1722636050, testSendSimpleMessageTls=my-user-1754673553-2077518027, testCreatingUsersWithSecretPrefix=my-user-144001639-226807213, testUpdateUser=my-user-1870578302-1216914412, testThrottlingQuotasDeleteTopic=my-user-1782685497-1980640589, testConfigurationReflection=my-user-2029261898-1375854235, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-1010649803-214635564, testTlsExternalUserWithQuotas=my-user-247789923-205827680, testUserWithNameMoreThan64Chars=my-user-2124392695-340403978, testDeployAndUnDeployCruiseControl=my-user-2117120687-604608920, testTlsUserWithQuotas=my-user-1602931187-443256751, testThrottlingQuotasCreateTopic=my-user-46611685-1079680568, testCapacityFile=my-user-155969448-1270555822, testReceiveSimpleMessageTls=my-user-1418772225-1024845009, testConfigurationPerformanceOptions=my-user-882948629-1821489961, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-1807142870-188660517, testTlsExternalUser=my-user-370513135-1989152728, testSendSimpleMessageTlsScramSha=my-user-182357028-1132873010}
2022-03-28 14:02:05 [ForkJoinPool-1-worker-3] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testScramUserWithQuotas=my-topic-1345794255-226872460, testKafkaAdminTopicOperations=my-topic-1935791607-1758305855, testReceiveSimpleMessageTlsScramSha=my-topic-649423016-1623551209, testConfigurationFileIsCreated=my-topic-1435014906-681104826, testThrottlingQuotasCreateAlterPartitions=my-topic-1449977352-344480794, testSendSimpleMessageTls=my-topic-115477502-2025274778, testCreatingUsersWithSecretPrefix=my-topic-130462979-2061578809, testUpdateUser=my-topic-829325799-540543436, testThrottlingQuotasDeleteTopic=my-topic-1797350406-813887294, testConfigurationReflection=my-topic-1621095669-1276202830, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-889283546-691691584, testTlsExternalUserWithQuotas=my-topic-2037225970-59246891, testUserWithNameMoreThan64Chars=my-topic-1053060011-1199293498, testDeployAndUnDeployCruiseControl=my-topic-1101175281-2107378523, testTlsUserWithQuotas=my-topic-985048738-437665866, testThrottlingQuotasCreateTopic=my-topic-114922478-1506136333, testCapacityFile=my-topic-296605537-1008743104, testReceiveSimpleMessageTls=my-topic-1131768783-120051821, testConfigurationPerformanceOptions=my-topic-627301835-843876360, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-1367124108-1748498395, testTlsExternalUser=my-topic-2081139838-1814296134, testSendSimpleMessageTlsScramSha=my-topic-652506937-1802310709}
2022-03-28 14:02:05 [ForkJoinPool-1-worker-3] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testScramUserWithQuotas=my-cluster-6f653060-kafka-clients, testKafkaAdminTopicOperations=my-cluster-600e8332-kafka-clients, testReceiveSimpleMessageTlsScramSha=my-cluster-af298d01-kafka-clients, testConfigurationFileIsCreated=my-cluster-3244df0d-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-8f6c28cb-kafka-clients, testSendSimpleMessageTls=my-cluster-fbc845dd-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-0c65d2dd-kafka-clients, testUpdateUser=my-cluster-c2c4d05a-kafka-clients, testThrottlingQuotasDeleteTopic=my-cluster-419d5c31-kafka-clients, testConfigurationReflection=my-cluster-5137fff1-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-0444229e-kafka-clients, testTlsExternalUserWithQuotas=my-cluster-08d24b8e-kafka-clients, testUserWithNameMoreThan64Chars=my-cluster-02e25de6-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-844045c3-kafka-clients, testTlsUserWithQuotas=my-cluster-9832c9ae-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-ac2cadd6-kafka-clients, testCapacityFile=my-cluster-0531d5ce-kafka-clients, testReceiveSimpleMessageTls=my-cluster-ace1fa24-kafka-clients, testConfigurationPerformanceOptions=my-cluster-4994018d-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-8af61d52-kafka-clients, testTlsExternalUser=my-cluster-43f83aef-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-e353214b-kafka-clients}
2022-03-28 14:02:05 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:155] Create/Update KafkaUser my-user-247789923-205827680 in namespace user-st
2022-03-28 14:02:05 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (217410ms till timeout)
2022-03-28 14:02:05 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-247789923-205827680
2022-03-28 14:02:05 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:433] Wait for KafkaUser: my-user-247789923-205827680 will have desired state: Ready
2022-03-28 14:02:05 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for KafkaUser: my-user-247789923-205827680 will have desired state: Ready
2022-03-28 14:02:06 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:444] KafkaUser: my-user-247789923-205827680 is in desired state: Ready
2022-03-28 14:02:06 [ForkJoinPool-1-worker-3] DEBUG [UserST:274] Command for kafka-configs.sh bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user CN=my-user-247789923-205827680
2022-03-28 14:02:06 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace user-st exec user-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user CN=my-user-247789923-205827680
2022-03-28 14:02:06 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-4 -o yaml
2022-03-28 14:02:06 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (66373ms till timeout)
2022-03-28 14:02:06 [ForkJoinPool-1-worker-1] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:00Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:02:06 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (68404ms till timeout)
2022-03-28 14:02:06 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-4 -o yaml
2022-03-28 14:02:06 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 14:02:06 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (470063ms till timeout)
2022-03-28 14:02:06 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:02:06 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (216215ms till timeout)
2022-03-28 14:02:07 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (65181ms till timeout)
2022-03-28 14:02:07 [ForkJoinPool-1-worker-1] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:00Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:02:07 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-4 -o yaml
2022-03-28 14:02:07 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (67208ms till timeout)
2022-03-28 14:02:08 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:02:08 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (215020ms till timeout)
2022-03-28 14:02:08 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-4 -o yaml
2022-03-28 14:02:08 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 14:02:08 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (468602ms till timeout)
2022-03-28 14:02:08 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testUserTemplate is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 14:02:08 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (63988ms till timeout)
2022-03-28 14:02:08 [ForkJoinPool-1-worker-1] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=null, completedIndexes=null, completionTime=2022-03-28T14:02:03Z, conditions=[JobCondition(lastProbeTime=2022-03-28T14:02:03Z, lastTransitionTime=2022-03-28T14:02:03Z, message=null, reason=null, status=True, type=Complete, additionalProperties={})], failed=null, ready=null, startTime=2022-03-28T14:01:00Z, succeeded=1, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:02:09 [ForkJoinPool-1-worker-1] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment teardown-delete deletion
2022-03-28 14:02:09 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for ReplicaSet teardown-delete to be deleted
2022-03-28 14:02:09 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:02:09 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-4 -o yaml
2022-03-28 14:02:09 [ForkJoinPool-1-worker-7] TRACE [SuiteThreadController:210] testTopicModificationOfReplicationFactor is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 14:02:09 [ForkJoinPool-1-worker-1] DEBUG [JobUtils:40] Job teardown-delete was deleted
2022-03-28 14:02:09 [ForkJoinPool-1-worker-1] DEBUG [AbstractST:674] ============================================================================
2022-03-28 14:02:09 [ForkJoinPool-1-worker-1] DEBUG [AbstractST:675] [operators.topic.ThrottlingQuotaST - After Each] - Clean up after test
2022-03-28 14:02:09 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:344] ############################################################################
2022-03-28 14:02:09 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:348] Delete all resources for testThrottlingQuotasCreateAlterPartitions
2022-03-28 14:02:09 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:241] Delete of Job create-admin-my-cluster-8f6c28cb-kafka-clients in namespace throttling-quota-st
2022-03-28 14:02:09 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (213725ms till timeout)
2022-03-28 14:02:09 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:create-admin-my-cluster-8f6c28cb-kafka-clients
2022-03-28 14:02:09 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:241] Delete of Job teardown-delete in namespace throttling-quota-st
2022-03-28 14:02:09 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:teardown-delete
2022-03-28 14:02:09 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-4 -o yaml
2022-03-28 14:02:09 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 14:02:09 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (467147ms till timeout)
2022-03-28 14:02:09 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Command: oc --namespace user-st exec user-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user CN=my-user-247789923-205827680
2022-03-28 14:02:09 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 14:02:09 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:241] Delete of Job alter-admin-my-cluster-8f6c28cb-kafka-clients in namespace throttling-quota-st
2022-03-28 14:02:09 [ForkJoinPool-1-worker-3] INFO  [KafkaUserUtils:62] Waiting for KafkaUser deletion my-user-247789923-205827680
2022-03-28 14:02:09 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for KafkaUser deletion my-user-247789923-205827680
2022-03-28 14:02:10 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:alter-admin-my-cluster-8f6c28cb-kafka-clients
2022-03-28 14:02:10 [ForkJoinPool-1-worker-3] INFO  [KafkaUserUtils:75] KafkaUser my-user-247789923-205827680 deleted
2022-03-28 14:02:10 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for all KafkaUser CN=my-user-247789923-205827680 attributes will be cleaned
2022-03-28 14:02:10 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace user-st exec user-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user CN=my-user-247789923-205827680
2022-03-28 14:02:10 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (62801ms till timeout)
2022-03-28 14:02:10 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:241] Delete of Job create-admin-my-cluster-8f6c28cb-kafka-clients in namespace throttling-quota-st
2022-03-28 14:02:10 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:create-admin-my-cluster-8f6c28cb-kafka-clients
2022-03-28 14:02:10 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:241] Delete of KafkaUser my-user-1557959119-1722636050 in namespace throttling-quota-st
2022-03-28 14:02:10 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-1557959119-1722636050
2022-03-28 14:02:10 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:02:10 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:369] ############################################################################
2022-03-28 14:02:10 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:267] testThrottlingQuotasCreateAlterPartitions - Notifies waiting test cases:[testCapacityFile, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testThrottlingQuotasCreateAlterPartitions, testSendSimpleMessageTls, testReceiveSimpleMessageTlsScramSha, testSendSimpleMessageTlsScramSha, testReceiveSimpleMessageTls, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testConfigurationFileIsCreated, testPauseReconciliationInKafkaRebalanceAndTopic, testUserTemplate, testTopicModificationOfReplicationFactor] to and randomly select one to start execution
2022-03-28 14:02:10 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:93] [operators.topic.ThrottlingQuotaST] - Removing parallel test: testThrottlingQuotasCreateAlterPartitions
2022-03-28 14:02:10 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:97] [operators.topic.ThrottlingQuotaST] - Parallel test count: 6
2022-03-28 14:02:10 [ForkJoinPool-1-worker-1] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.topic.ThrottlingQuotaST.testThrottlingQuotasCreateAlterPartitions-FINISHED
2022-03-28 14:02:10 [ForkJoinPool-1-worker-1] INFO  [TestSeparator:30] ############################################################################
2022-03-28 14:02:10 [ForkJoinPool-1-worker-1] INFO  [TestSeparator:23] ############################################################################
2022-03-28 14:02:10 [ForkJoinPool-1-worker-1] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.topic.TopicST.testSendingMessagesToNonExistingTopic-STARTED
2022-03-28 14:02:10 [ForkJoinPool-1-worker-1] DEBUG [AbstractST:658] ============================================================================
2022-03-28 14:02:10 [ForkJoinPool-1-worker-1] DEBUG [AbstractST:659] [operators.topic.TopicST - Before Each] - Setup test case environment
2022-03-28 14:02:10 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:77] [operators.topic.TopicST] - Adding parallel test: testSendingMessagesToNonExistingTopic
2022-03-28 14:02:10 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:81] [operators.topic.TopicST] - Parallel test count: 7
2022-03-28 14:02:10 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:205] [testSendingMessagesToNonExistingTopic] moved to the WaitZone, because current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 14:02:10 [ForkJoinPool-1-worker-1] TRACE [SuiteThreadController:210] testSendingMessagesToNonExistingTopic is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 14:02:10 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-4 -o yaml
2022-03-28 14:02:10 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (212385ms till timeout)
2022-03-28 14:02:11 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-4 -o yaml
2022-03-28 14:02:11 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 14:02:11 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (465690ms till timeout)
2022-03-28 14:02:11 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (61701ms till timeout)
2022-03-28 14:02:11 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:02:11 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (211189ms till timeout)
2022-03-28 14:02:12 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-4 -o yaml
2022-03-28 14:02:12 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (60508ms till timeout)
2022-03-28 14:02:12 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-4 -o yaml
2022-03-28 14:02:12 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 14:02:12 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (464236ms till timeout)
2022-03-28 14:02:13 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:02:13 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (209993ms till timeout)
2022-03-28 14:02:13 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (59315ms till timeout)
2022-03-28 14:02:13 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Command: oc --namespace user-st exec user-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user CN=my-user-247789923-205827680
2022-03-28 14:02:13 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 14:02:13 [ForkJoinPool-1-worker-3] DEBUG [AbstractST:674] ============================================================================
2022-03-28 14:02:13 [ForkJoinPool-1-worker-3] DEBUG [AbstractST:675] [operators.user.UserST - After Each] - Clean up after test
2022-03-28 14:02:13 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:344] ############################################################################
2022-03-28 14:02:13 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:348] Delete all resources for testTlsExternalUserWithQuotas
2022-03-28 14:02:13 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:241] Delete of KafkaUser my-user-247789923-205827680 in namespace user-st
2022-03-28 14:02:13 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-4 -o yaml
2022-03-28 14:02:13 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-247789923-205827680
2022-03-28 14:02:13 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:369] ############################################################################
2022-03-28 14:02:13 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:267] testTlsExternalUserWithQuotas - Notifies waiting test cases:[testCapacityFile, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testThrottlingQuotasCreateAlterPartitions, testSendSimpleMessageTls, testReceiveSimpleMessageTlsScramSha, testSendSimpleMessageTlsScramSha, testReceiveSimpleMessageTls, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testConfigurationFileIsCreated, testPauseReconciliationInKafkaRebalanceAndTopic, testUserTemplate, testTopicModificationOfReplicationFactor, testSendingMessagesToNonExistingTopic] to and randomly select one to start execution
2022-03-28 14:02:13 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:93] [operators.user.UserST] - Removing parallel test: testTlsExternalUserWithQuotas
2022-03-28 14:02:13 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:97] [operators.user.UserST] - Parallel test count: 6
2022-03-28 14:02:13 [ForkJoinPool-1-worker-3] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.user.UserST.testTlsExternalUserWithQuotas-FINISHED
2022-03-28 14:02:13 [ForkJoinPool-1-worker-3] INFO  [TestSeparator:30] ############################################################################
2022-03-28 14:02:13 [ForkJoinPool-1-worker-3] DEBUG [AbstractST:689] ============================================================================
2022-03-28 14:02:13 [ForkJoinPool-1-worker-3] DEBUG [AbstractST:690] [cruisecontrol.CruiseControlConfigurationST - After All] - Clean up after test suite
2022-03-28 14:02:13 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:344] ############################################################################
2022-03-28 14:02:13 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:346] In context CruiseControlConfigurationST is everything deleted.
2022-03-28 14:02:13 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:369] ############################################################################
2022-03-28 14:02:13 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:230] testUserTemplate test now can proceed its execution
2022-03-28 14:02:13 [ForkJoinPool-1-worker-13] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 14:02:13 [ForkJoinPool-1-worker-13] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testScramUserWithQuotas=my-cluster-6f653060, testKafkaAdminTopicOperations=my-cluster-600e8332, testReceiveSimpleMessageTlsScramSha=my-cluster-af298d01, testConfigurationFileIsCreated=my-cluster-3244df0d, testThrottlingQuotasCreateAlterPartitions=my-cluster-8f6c28cb, testSendSimpleMessageTls=my-cluster-fbc845dd, testCreatingUsersWithSecretPrefix=my-cluster-0c65d2dd, testUpdateUser=my-cluster-c2c4d05a, testThrottlingQuotasDeleteTopic=my-cluster-419d5c31, testConfigurationReflection=my-cluster-5137fff1, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-0444229e, testTlsExternalUserWithQuotas=my-cluster-08d24b8e, testUserWithNameMoreThan64Chars=my-cluster-02e25de6, testDeployAndUnDeployCruiseControl=my-cluster-844045c3, testTlsUserWithQuotas=my-cluster-9832c9ae, testThrottlingQuotasCreateTopic=my-cluster-ac2cadd6, testCapacityFile=my-cluster-0531d5ce, testReceiveSimpleMessageTls=my-cluster-ace1fa24, testConfigurationPerformanceOptions=my-cluster-4994018d, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-8af61d52, testTlsExternalUser=my-cluster-43f83aef, testSendSimpleMessageTlsScramSha=my-cluster-e353214b, testUserTemplate=my-cluster-e0217138}
2022-03-28 14:02:13 [ForkJoinPool-1-worker-13] TRACE [AbstractST:607] USERS_NAME_MAP: {testScramUserWithQuotas=my-user-27563760-1632863338, testKafkaAdminTopicOperations=my-user-1735322302-803507072, testReceiveSimpleMessageTlsScramSha=my-user-1597210213-707359243, testConfigurationFileIsCreated=my-user-1799841978-1468805631, testThrottlingQuotasCreateAlterPartitions=my-user-1557959119-1722636050, testSendSimpleMessageTls=my-user-1754673553-2077518027, testCreatingUsersWithSecretPrefix=my-user-144001639-226807213, testUpdateUser=my-user-1870578302-1216914412, testThrottlingQuotasDeleteTopic=my-user-1782685497-1980640589, testConfigurationReflection=my-user-2029261898-1375854235, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-1010649803-214635564, testTlsExternalUserWithQuotas=my-user-247789923-205827680, testUserWithNameMoreThan64Chars=my-user-2124392695-340403978, testDeployAndUnDeployCruiseControl=my-user-2117120687-604608920, testTlsUserWithQuotas=my-user-1602931187-443256751, testThrottlingQuotasCreateTopic=my-user-46611685-1079680568, testCapacityFile=my-user-155969448-1270555822, testReceiveSimpleMessageTls=my-user-1418772225-1024845009, testConfigurationPerformanceOptions=my-user-882948629-1821489961, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-1807142870-188660517, testTlsExternalUser=my-user-370513135-1989152728, testSendSimpleMessageTlsScramSha=my-user-182357028-1132873010, testUserTemplate=my-user-896194994-133213923}
2022-03-28 14:02:13 [ForkJoinPool-1-worker-13] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testScramUserWithQuotas=my-topic-1345794255-226872460, testKafkaAdminTopicOperations=my-topic-1935791607-1758305855, testReceiveSimpleMessageTlsScramSha=my-topic-649423016-1623551209, testConfigurationFileIsCreated=my-topic-1435014906-681104826, testThrottlingQuotasCreateAlterPartitions=my-topic-1449977352-344480794, testSendSimpleMessageTls=my-topic-115477502-2025274778, testCreatingUsersWithSecretPrefix=my-topic-130462979-2061578809, testUpdateUser=my-topic-829325799-540543436, testThrottlingQuotasDeleteTopic=my-topic-1797350406-813887294, testConfigurationReflection=my-topic-1621095669-1276202830, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-889283546-691691584, testTlsExternalUserWithQuotas=my-topic-2037225970-59246891, testUserWithNameMoreThan64Chars=my-topic-1053060011-1199293498, testDeployAndUnDeployCruiseControl=my-topic-1101175281-2107378523, testTlsUserWithQuotas=my-topic-985048738-437665866, testThrottlingQuotasCreateTopic=my-topic-114922478-1506136333, testCapacityFile=my-topic-296605537-1008743104, testReceiveSimpleMessageTls=my-topic-1131768783-120051821, testConfigurationPerformanceOptions=my-topic-627301835-843876360, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-1367124108-1748498395, testTlsExternalUser=my-topic-2081139838-1814296134, testSendSimpleMessageTlsScramSha=my-topic-652506937-1802310709, testUserTemplate=my-topic-1175486259-481154370}
2022-03-28 14:02:13 [ForkJoinPool-1-worker-13] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testScramUserWithQuotas=my-cluster-6f653060-kafka-clients, testKafkaAdminTopicOperations=my-cluster-600e8332-kafka-clients, testReceiveSimpleMessageTlsScramSha=my-cluster-af298d01-kafka-clients, testConfigurationFileIsCreated=my-cluster-3244df0d-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-8f6c28cb-kafka-clients, testSendSimpleMessageTls=my-cluster-fbc845dd-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-0c65d2dd-kafka-clients, testUpdateUser=my-cluster-c2c4d05a-kafka-clients, testThrottlingQuotasDeleteTopic=my-cluster-419d5c31-kafka-clients, testConfigurationReflection=my-cluster-5137fff1-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-0444229e-kafka-clients, testTlsExternalUserWithQuotas=my-cluster-08d24b8e-kafka-clients, testUserWithNameMoreThan64Chars=my-cluster-02e25de6-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-844045c3-kafka-clients, testTlsUserWithQuotas=my-cluster-9832c9ae-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-ac2cadd6-kafka-clients, testCapacityFile=my-cluster-0531d5ce-kafka-clients, testReceiveSimpleMessageTls=my-cluster-ace1fa24-kafka-clients, testConfigurationPerformanceOptions=my-cluster-4994018d-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-8af61d52-kafka-clients, testTlsExternalUser=my-cluster-43f83aef-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-e353214b-kafka-clients, testUserTemplate=my-cluster-e0217138-kafka-clients}
2022-03-28 14:02:13 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:155] Create/Update KafkaUser my-user-896194994-133213923 in namespace user-st
2022-03-28 14:02:13 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Namespace cruise-control-configuration-st removal
2022-03-28 14:02:13 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace cruise-control-configuration-st -o yaml
2022-03-28 14:02:14 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-896194994-133213923
2022-03-28 14:02:14 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-4 -o yaml
2022-03-28 14:02:14 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 14:02:14 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (462761ms till timeout)
2022-03-28 14:02:14 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:433] Wait for KafkaUser: my-user-896194994-133213923 will have desired state: Ready
2022-03-28 14:02:14 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for KafkaUser: my-user-896194994-133213923 will have desired state: Ready
2022-03-28 14:02:14 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:444] KafkaUser: my-user-896194994-133213923 is in desired state: Ready
2022-03-28 14:02:14 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:230] testTopicModificationOfReplicationFactor test now can proceed its execution
2022-03-28 14:02:14 [ForkJoinPool-1-worker-7] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 14:02:14 [ForkJoinPool-1-worker-7] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testScramUserWithQuotas=my-cluster-6f653060, testTopicModificationOfReplicationFactor=my-cluster-a1a53143, testKafkaAdminTopicOperations=my-cluster-600e8332, testReceiveSimpleMessageTlsScramSha=my-cluster-af298d01, testConfigurationFileIsCreated=my-cluster-3244df0d, testThrottlingQuotasCreateAlterPartitions=my-cluster-8f6c28cb, testSendSimpleMessageTls=my-cluster-fbc845dd, testCreatingUsersWithSecretPrefix=my-cluster-0c65d2dd, testUpdateUser=my-cluster-c2c4d05a, testThrottlingQuotasDeleteTopic=my-cluster-419d5c31, testConfigurationReflection=my-cluster-5137fff1, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-0444229e, testTlsExternalUserWithQuotas=my-cluster-08d24b8e, testUserWithNameMoreThan64Chars=my-cluster-02e25de6, testDeployAndUnDeployCruiseControl=my-cluster-844045c3, testTlsUserWithQuotas=my-cluster-9832c9ae, testThrottlingQuotasCreateTopic=my-cluster-ac2cadd6, testCapacityFile=my-cluster-0531d5ce, testReceiveSimpleMessageTls=my-cluster-ace1fa24, testConfigurationPerformanceOptions=my-cluster-4994018d, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-8af61d52, testTlsExternalUser=my-cluster-43f83aef, testSendSimpleMessageTlsScramSha=my-cluster-e353214b, testUserTemplate=my-cluster-e0217138}
2022-03-28 14:02:14 [ForkJoinPool-1-worker-7] TRACE [AbstractST:607] USERS_NAME_MAP: {testScramUserWithQuotas=my-user-27563760-1632863338, testTopicModificationOfReplicationFactor=my-user-1764997166-1500934155, testKafkaAdminTopicOperations=my-user-1735322302-803507072, testReceiveSimpleMessageTlsScramSha=my-user-1597210213-707359243, testConfigurationFileIsCreated=my-user-1799841978-1468805631, testThrottlingQuotasCreateAlterPartitions=my-user-1557959119-1722636050, testSendSimpleMessageTls=my-user-1754673553-2077518027, testCreatingUsersWithSecretPrefix=my-user-144001639-226807213, testUpdateUser=my-user-1870578302-1216914412, testThrottlingQuotasDeleteTopic=my-user-1782685497-1980640589, testConfigurationReflection=my-user-2029261898-1375854235, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-1010649803-214635564, testTlsExternalUserWithQuotas=my-user-247789923-205827680, testUserWithNameMoreThan64Chars=my-user-2124392695-340403978, testDeployAndUnDeployCruiseControl=my-user-2117120687-604608920, testTlsUserWithQuotas=my-user-1602931187-443256751, testThrottlingQuotasCreateTopic=my-user-46611685-1079680568, testCapacityFile=my-user-155969448-1270555822, testReceiveSimpleMessageTls=my-user-1418772225-1024845009, testConfigurationPerformanceOptions=my-user-882948629-1821489961, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-1807142870-188660517, testTlsExternalUser=my-user-370513135-1989152728, testSendSimpleMessageTlsScramSha=my-user-182357028-1132873010, testUserTemplate=my-user-896194994-133213923}
2022-03-28 14:02:14 [ForkJoinPool-1-worker-7] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testScramUserWithQuotas=my-topic-1345794255-226872460, testTopicModificationOfReplicationFactor=my-topic-2129693330-635016449, testKafkaAdminTopicOperations=my-topic-1935791607-1758305855, testReceiveSimpleMessageTlsScramSha=my-topic-649423016-1623551209, testConfigurationFileIsCreated=my-topic-1435014906-681104826, testThrottlingQuotasCreateAlterPartitions=my-topic-1449977352-344480794, testSendSimpleMessageTls=my-topic-115477502-2025274778, testCreatingUsersWithSecretPrefix=my-topic-130462979-2061578809, testUpdateUser=my-topic-829325799-540543436, testThrottlingQuotasDeleteTopic=my-topic-1797350406-813887294, testConfigurationReflection=my-topic-1621095669-1276202830, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-889283546-691691584, testTlsExternalUserWithQuotas=my-topic-2037225970-59246891, testUserWithNameMoreThan64Chars=my-topic-1053060011-1199293498, testDeployAndUnDeployCruiseControl=my-topic-1101175281-2107378523, testTlsUserWithQuotas=my-topic-985048738-437665866, testThrottlingQuotasCreateTopic=my-topic-114922478-1506136333, testCapacityFile=my-topic-296605537-1008743104, testReceiveSimpleMessageTls=my-topic-1131768783-120051821, testConfigurationPerformanceOptions=my-topic-627301835-843876360, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-1367124108-1748498395, testTlsExternalUser=my-topic-2081139838-1814296134, testSendSimpleMessageTlsScramSha=my-topic-652506937-1802310709, testUserTemplate=my-topic-1175486259-481154370}
2022-03-28 14:02:14 [ForkJoinPool-1-worker-7] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testScramUserWithQuotas=my-cluster-6f653060-kafka-clients, testTopicModificationOfReplicationFactor=my-cluster-a1a53143-kafka-clients, testKafkaAdminTopicOperations=my-cluster-600e8332-kafka-clients, testReceiveSimpleMessageTlsScramSha=my-cluster-af298d01-kafka-clients, testConfigurationFileIsCreated=my-cluster-3244df0d-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-8f6c28cb-kafka-clients, testSendSimpleMessageTls=my-cluster-fbc845dd-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-0c65d2dd-kafka-clients, testUpdateUser=my-cluster-c2c4d05a-kafka-clients, testThrottlingQuotasDeleteTopic=my-cluster-419d5c31-kafka-clients, testConfigurationReflection=my-cluster-5137fff1-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-0444229e-kafka-clients, testTlsExternalUserWithQuotas=my-cluster-08d24b8e-kafka-clients, testUserWithNameMoreThan64Chars=my-cluster-02e25de6-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-844045c3-kafka-clients, testTlsUserWithQuotas=my-cluster-9832c9ae-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-ac2cadd6-kafka-clients, testCapacityFile=my-cluster-0531d5ce-kafka-clients, testReceiveSimpleMessageTls=my-cluster-ace1fa24-kafka-clients, testConfigurationPerformanceOptions=my-cluster-4994018d-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-8af61d52-kafka-clients, testTlsExternalUser=my-cluster-43f83aef-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-e353214b-kafka-clients, testUserTemplate=my-cluster-e0217138-kafka-clients}
2022-03-28 14:02:14 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update KafkaTopic my-topic-2129693330-635016449 in namespace topic-st
2022-03-28 14:02:14 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:02:14 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace cruise-control-configuration-st -o yaml
2022-03-28 14:02:14 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 14:02:14 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace cruise-control-configuration-st removal not ready, will try again in 1000 ms (479524ms till timeout)
2022-03-28 14:02:14 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:674] ============================================================================
2022-03-28 14:02:14 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:675] [operators.user.UserST - After Each] - Clean up after test
2022-03-28 14:02:14 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:344] ############################################################################
2022-03-28 14:02:14 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:348] Delete all resources for testUserTemplate
2022-03-28 14:02:14 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of KafkaUser my-user-896194994-133213923 in namespace user-st
2022-03-28 14:02:14 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-2129693330-635016449
2022-03-28 14:02:14 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (208649ms till timeout)
2022-03-28 14:02:14 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:433] Wait for KafkaTopic: my-topic-2129693330-635016449 will have desired state: Ready
2022-03-28 14:02:14 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for KafkaTopic: my-topic-2129693330-635016449 will have desired state: Ready
2022-03-28 14:02:14 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-896194994-133213923
2022-03-28 14:02:14 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (58204ms till timeout)
2022-03-28 14:02:14 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:444] KafkaTopic: my-topic-2129693330-635016449 is in desired state: Ready
2022-03-28 14:02:14 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:369] ############################################################################
2022-03-28 14:02:14 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:267] testUserTemplate - Notifies waiting test cases:[testCapacityFile, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testThrottlingQuotasCreateAlterPartitions, testSendSimpleMessageTls, testReceiveSimpleMessageTlsScramSha, testSendSimpleMessageTlsScramSha, testReceiveSimpleMessageTls, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testConfigurationFileIsCreated, testPauseReconciliationInKafkaRebalanceAndTopic, testTopicModificationOfReplicationFactor, testSendingMessagesToNonExistingTopic] to and randomly select one to start execution
2022-03-28 14:02:14 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:93] [operators.user.UserST] - Removing parallel test: testUserTemplate
2022-03-28 14:02:14 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:97] [operators.user.UserST] - Parallel test count: 5
2022-03-28 14:02:14 [ForkJoinPool-1-worker-13] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.user.UserST.testUserTemplate-FINISHED
2022-03-28 14:02:14 [ForkJoinPool-1-worker-13] INFO  [TestSeparator:30] ############################################################################
2022-03-28 14:02:14 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:689] ============================================================================
2022-03-28 14:02:14 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:690] [operators.user.UserST - After All] - Clean up after test suite
2022-03-28 14:02:14 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:344] ############################################################################
2022-03-28 14:02:14 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:348] Delete all resources for UserST
2022-03-28 14:02:14 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of Kafka user-cluster-name in namespace user-st
2022-03-28 14:02:15 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:user-cluster-name
2022-03-28 14:02:15 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:433] Wait for KafkaTopic: my-topic-2129693330-635016449 will have desired state: NotReady
2022-03-28 14:02:15 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for KafkaTopic: my-topic-2129693330-635016449 will have desired state: NotReady
2022-03-28 14:02:15 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-4 -o yaml
2022-03-28 14:02:15 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:369] ############################################################################
2022-03-28 14:02:15 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:444] KafkaTopic: my-topic-2129693330-635016449 is in desired state: NotReady
2022-03-28 14:02:15 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace cruise-control-configuration-st -o yaml
2022-03-28 14:02:15 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Namespace user-st removal
2022-03-28 14:02:15 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace user-st -o yaml
2022-03-28 14:02:15 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-4 -o yaml
2022-03-28 14:02:15 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 14:02:15 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (461296ms till timeout)
2022-03-28 14:02:15 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace topic-st delete kafkatopic my-topic-2129693330-635016449
2022-03-28 14:02:15 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:230] testSendingMessagesToNonExistingTopic test now can proceed its execution
2022-03-28 14:02:15 [ForkJoinPool-1-worker-1] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 14:02:15 [ForkJoinPool-1-worker-1] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-af298d01, testThrottlingQuotasCreateAlterPartitions=my-cluster-8f6c28cb, testSendSimpleMessageTls=my-cluster-fbc845dd, testConfigurationReflection=my-cluster-5137fff1, testUserWithNameMoreThan64Chars=my-cluster-02e25de6, testDeployAndUnDeployCruiseControl=my-cluster-844045c3, testCapacityFile=my-cluster-0531d5ce, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-8af61d52, testTlsExternalUser=my-cluster-43f83aef, testSendSimpleMessageTlsScramSha=my-cluster-e353214b, testScramUserWithQuotas=my-cluster-6f653060, testTopicModificationOfReplicationFactor=my-cluster-a1a53143, testKafkaAdminTopicOperations=my-cluster-600e8332, testConfigurationFileIsCreated=my-cluster-3244df0d, testCreatingUsersWithSecretPrefix=my-cluster-0c65d2dd, testUpdateUser=my-cluster-c2c4d05a, testThrottlingQuotasDeleteTopic=my-cluster-419d5c31, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-0444229e, testTlsExternalUserWithQuotas=my-cluster-08d24b8e, testTlsUserWithQuotas=my-cluster-9832c9ae, testThrottlingQuotasCreateTopic=my-cluster-ac2cadd6, testSendingMessagesToNonExistingTopic=my-cluster-abe3633a, testReceiveSimpleMessageTls=my-cluster-ace1fa24, testConfigurationPerformanceOptions=my-cluster-4994018d, testUserTemplate=my-cluster-e0217138}
2022-03-28 14:02:15 [ForkJoinPool-1-worker-1] TRACE [AbstractST:607] USERS_NAME_MAP: {testReceiveSimpleMessageTlsScramSha=my-user-1597210213-707359243, testThrottlingQuotasCreateAlterPartitions=my-user-1557959119-1722636050, testSendSimpleMessageTls=my-user-1754673553-2077518027, testConfigurationReflection=my-user-2029261898-1375854235, testUserWithNameMoreThan64Chars=my-user-2124392695-340403978, testDeployAndUnDeployCruiseControl=my-user-2117120687-604608920, testCapacityFile=my-user-155969448-1270555822, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-1807142870-188660517, testTlsExternalUser=my-user-370513135-1989152728, testSendSimpleMessageTlsScramSha=my-user-182357028-1132873010, testScramUserWithQuotas=my-user-27563760-1632863338, testTopicModificationOfReplicationFactor=my-user-1764997166-1500934155, testKafkaAdminTopicOperations=my-user-1735322302-803507072, testConfigurationFileIsCreated=my-user-1799841978-1468805631, testCreatingUsersWithSecretPrefix=my-user-144001639-226807213, testUpdateUser=my-user-1870578302-1216914412, testThrottlingQuotasDeleteTopic=my-user-1782685497-1980640589, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-1010649803-214635564, testTlsExternalUserWithQuotas=my-user-247789923-205827680, testTlsUserWithQuotas=my-user-1602931187-443256751, testThrottlingQuotasCreateTopic=my-user-46611685-1079680568, testSendingMessagesToNonExistingTopic=my-user-954109284-2118832263, testReceiveSimpleMessageTls=my-user-1418772225-1024845009, testConfigurationPerformanceOptions=my-user-882948629-1821489961, testUserTemplate=my-user-896194994-133213923}
2022-03-28 14:02:15 [ForkJoinPool-1-worker-1] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-topic-649423016-1623551209, testThrottlingQuotasCreateAlterPartitions=my-topic-1449977352-344480794, testSendSimpleMessageTls=my-topic-115477502-2025274778, testConfigurationReflection=my-topic-1621095669-1276202830, testUserWithNameMoreThan64Chars=my-topic-1053060011-1199293498, testDeployAndUnDeployCruiseControl=my-topic-1101175281-2107378523, testCapacityFile=my-topic-296605537-1008743104, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-1367124108-1748498395, testTlsExternalUser=my-topic-2081139838-1814296134, testSendSimpleMessageTlsScramSha=my-topic-652506937-1802310709, testScramUserWithQuotas=my-topic-1345794255-226872460, testTopicModificationOfReplicationFactor=my-topic-2129693330-635016449, testKafkaAdminTopicOperations=my-topic-1935791607-1758305855, testConfigurationFileIsCreated=my-topic-1435014906-681104826, testCreatingUsersWithSecretPrefix=my-topic-130462979-2061578809, testUpdateUser=my-topic-829325799-540543436, testThrottlingQuotasDeleteTopic=my-topic-1797350406-813887294, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-889283546-691691584, testTlsExternalUserWithQuotas=my-topic-2037225970-59246891, testTlsUserWithQuotas=my-topic-985048738-437665866, testThrottlingQuotasCreateTopic=my-topic-114922478-1506136333, testSendingMessagesToNonExistingTopic=my-topic-441360312-1858428966, testReceiveSimpleMessageTls=my-topic-1131768783-120051821, testConfigurationPerformanceOptions=my-topic-627301835-843876360, testUserTemplate=my-topic-1175486259-481154370}
2022-03-28 14:02:15 [ForkJoinPool-1-worker-1] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-af298d01-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-8f6c28cb-kafka-clients, testSendSimpleMessageTls=my-cluster-fbc845dd-kafka-clients, testConfigurationReflection=my-cluster-5137fff1-kafka-clients, testUserWithNameMoreThan64Chars=my-cluster-02e25de6-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-844045c3-kafka-clients, testCapacityFile=my-cluster-0531d5ce-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-8af61d52-kafka-clients, testTlsExternalUser=my-cluster-43f83aef-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-e353214b-kafka-clients, testScramUserWithQuotas=my-cluster-6f653060-kafka-clients, testTopicModificationOfReplicationFactor=my-cluster-a1a53143-kafka-clients, testKafkaAdminTopicOperations=my-cluster-600e8332-kafka-clients, testConfigurationFileIsCreated=my-cluster-3244df0d-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-0c65d2dd-kafka-clients, testUpdateUser=my-cluster-c2c4d05a-kafka-clients, testThrottlingQuotasDeleteTopic=my-cluster-419d5c31-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-0444229e-kafka-clients, testTlsExternalUserWithQuotas=my-cluster-08d24b8e-kafka-clients, testTlsUserWithQuotas=my-cluster-9832c9ae-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-ac2cadd6-kafka-clients, testSendingMessagesToNonExistingTopic=my-cluster-abe3633a-kafka-clients, testReceiveSimpleMessageTls=my-cluster-ace1fa24-kafka-clients, testConfigurationPerformanceOptions=my-cluster-4994018d-kafka-clients, testUserTemplate=my-cluster-e0217138-kafka-clients}
2022-03-28 14:02:15 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:155] Create/Update Deployment topic-cluster-name-kafka-clients in namespace topic-st
2022-03-28 14:02:15 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:02:15 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:topic-cluster-name-kafka-clients
2022-03-28 14:02:15 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (57092ms till timeout)
2022-03-28 14:02:15 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (207259ms till timeout)
2022-03-28 14:02:15 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace cruise-control-configuration-st -o yaml
2022-03-28 14:02:15 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 14:02:15 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace cruise-control-configuration-st removal not ready, will try again in 1000 ms (478029ms till timeout)
2022-03-28 14:02:15 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace user-st -o yaml
2022-03-28 14:02:15 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 14:02:15 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (479509ms till timeout)
2022-03-28 14:02:16 [ForkJoinPool-1-worker-1] INFO  [DeploymentUtils:161] Wait for Deployment: topic-cluster-name-kafka-clients will be ready
2022-03-28 14:02:16 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Wait for Deployment: topic-cluster-name-kafka-clients will be ready
2022-03-28 14:02:16 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace topic-st delete kafkatopic my-topic-2129693330-635016449
2022-03-28 14:02:16 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 14:02:16 [ForkJoinPool-1-worker-7] INFO  [KafkaTopicUtils:104] Waiting for KafkaTopic my-topic-2129693330-635016449 deletion
2022-03-28 14:02:16 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for KafkaTopic deletion my-topic-2129693330-635016449
2022-03-28 14:02:16 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Wait for Deployment: topic-cluster-name-kafka-clients will be ready not ready, will try again in 1000 ms (479903ms till timeout)
2022-03-28 14:02:16 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:674] ============================================================================
2022-03-28 14:02:16 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:675] [operators.topic.TopicST - After Each] - Clean up after test
2022-03-28 14:02:16 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:344] ############################################################################
2022-03-28 14:02:16 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:348] Delete all resources for testTopicModificationOfReplicationFactor
2022-03-28 14:02:16 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:241] Delete of KafkaTopic my-topic-2129693330-635016449 in namespace topic-st
2022-03-28 14:02:16 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-2129693330-635016449
2022-03-28 14:02:16 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:369] ############################################################################
2022-03-28 14:02:16 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:267] testTopicModificationOfReplicationFactor - Notifies waiting test cases:[testCapacityFile, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testThrottlingQuotasCreateAlterPartitions, testSendSimpleMessageTls, testReceiveSimpleMessageTlsScramSha, testSendSimpleMessageTlsScramSha, testReceiveSimpleMessageTls, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testConfigurationFileIsCreated, testPauseReconciliationInKafkaRebalanceAndTopic, testTopicModificationOfReplicationFactor] to and randomly select one to start execution
2022-03-28 14:02:16 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:93] [operators.topic.TopicST] - Removing parallel test: testTopicModificationOfReplicationFactor
2022-03-28 14:02:16 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:97] [operators.topic.TopicST] - Parallel test count: 4
2022-03-28 14:02:16 [ForkJoinPool-1-worker-7] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.topic.TopicST.testTopicModificationOfReplicationFactor-FINISHED
2022-03-28 14:02:16 [ForkJoinPool-1-worker-7] INFO  [TestSeparator:30] ############################################################################
2022-03-28 14:02:16 [ForkJoinPool-1-worker-7] INFO  [TestSeparator:23] ############################################################################
2022-03-28 14:02:16 [ForkJoinPool-1-worker-7] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.topic.TopicST.testMoreReplicasThanAvailableBrokers-STARTED
2022-03-28 14:02:16 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:658] ============================================================================
2022-03-28 14:02:16 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:659] [operators.topic.TopicST - Before Each] - Setup test case environment
2022-03-28 14:02:16 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:77] [operators.topic.TopicST] - Adding parallel test: testMoreReplicasThanAvailableBrokers
2022-03-28 14:02:16 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:81] [operators.topic.TopicST] - Parallel test count: 5
2022-03-28 14:02:16 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:230] testMoreReplicasThanAvailableBrokers test now can proceed its execution
2022-03-28 14:02:16 [ForkJoinPool-1-worker-7] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 14:02:16 [ForkJoinPool-1-worker-7] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-af298d01, testThrottlingQuotasCreateAlterPartitions=my-cluster-8f6c28cb, testSendSimpleMessageTls=my-cluster-fbc845dd, testConfigurationReflection=my-cluster-5137fff1, testUserWithNameMoreThan64Chars=my-cluster-02e25de6, testDeployAndUnDeployCruiseControl=my-cluster-844045c3, testCapacityFile=my-cluster-0531d5ce, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-8af61d52, testTlsExternalUser=my-cluster-43f83aef, testSendSimpleMessageTlsScramSha=my-cluster-e353214b, testScramUserWithQuotas=my-cluster-6f653060, testTopicModificationOfReplicationFactor=my-cluster-a1a53143, testKafkaAdminTopicOperations=my-cluster-600e8332, testConfigurationFileIsCreated=my-cluster-3244df0d, testCreatingUsersWithSecretPrefix=my-cluster-0c65d2dd, testUpdateUser=my-cluster-c2c4d05a, testThrottlingQuotasDeleteTopic=my-cluster-419d5c31, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-0444229e, testTlsExternalUserWithQuotas=my-cluster-08d24b8e, testTlsUserWithQuotas=my-cluster-9832c9ae, testThrottlingQuotasCreateTopic=my-cluster-ac2cadd6, testSendingMessagesToNonExistingTopic=my-cluster-abe3633a, testReceiveSimpleMessageTls=my-cluster-ace1fa24, testConfigurationPerformanceOptions=my-cluster-4994018d, testUserTemplate=my-cluster-e0217138, testMoreReplicasThanAvailableBrokers=my-cluster-203feca8}
2022-03-28 14:02:16 [ForkJoinPool-1-worker-7] TRACE [AbstractST:607] USERS_NAME_MAP: {testReceiveSimpleMessageTlsScramSha=my-user-1597210213-707359243, testThrottlingQuotasCreateAlterPartitions=my-user-1557959119-1722636050, testSendSimpleMessageTls=my-user-1754673553-2077518027, testConfigurationReflection=my-user-2029261898-1375854235, testUserWithNameMoreThan64Chars=my-user-2124392695-340403978, testDeployAndUnDeployCruiseControl=my-user-2117120687-604608920, testCapacityFile=my-user-155969448-1270555822, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-1807142870-188660517, testTlsExternalUser=my-user-370513135-1989152728, testSendSimpleMessageTlsScramSha=my-user-182357028-1132873010, testScramUserWithQuotas=my-user-27563760-1632863338, testTopicModificationOfReplicationFactor=my-user-1764997166-1500934155, testKafkaAdminTopicOperations=my-user-1735322302-803507072, testConfigurationFileIsCreated=my-user-1799841978-1468805631, testCreatingUsersWithSecretPrefix=my-user-144001639-226807213, testUpdateUser=my-user-1870578302-1216914412, testThrottlingQuotasDeleteTopic=my-user-1782685497-1980640589, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-1010649803-214635564, testTlsExternalUserWithQuotas=my-user-247789923-205827680, testTlsUserWithQuotas=my-user-1602931187-443256751, testThrottlingQuotasCreateTopic=my-user-46611685-1079680568, testSendingMessagesToNonExistingTopic=my-user-954109284-2118832263, testReceiveSimpleMessageTls=my-user-1418772225-1024845009, testConfigurationPerformanceOptions=my-user-882948629-1821489961, testUserTemplate=my-user-896194994-133213923, testMoreReplicasThanAvailableBrokers=my-user-413609357-1989111041}
2022-03-28 14:02:16 [ForkJoinPool-1-worker-7] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-topic-649423016-1623551209, testThrottlingQuotasCreateAlterPartitions=my-topic-1449977352-344480794, testSendSimpleMessageTls=my-topic-115477502-2025274778, testConfigurationReflection=my-topic-1621095669-1276202830, testUserWithNameMoreThan64Chars=my-topic-1053060011-1199293498, testDeployAndUnDeployCruiseControl=my-topic-1101175281-2107378523, testCapacityFile=my-topic-296605537-1008743104, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-1367124108-1748498395, testTlsExternalUser=my-topic-2081139838-1814296134, testSendSimpleMessageTlsScramSha=my-topic-652506937-1802310709, testScramUserWithQuotas=my-topic-1345794255-226872460, testTopicModificationOfReplicationFactor=my-topic-2129693330-635016449, testKafkaAdminTopicOperations=my-topic-1935791607-1758305855, testConfigurationFileIsCreated=my-topic-1435014906-681104826, testCreatingUsersWithSecretPrefix=my-topic-130462979-2061578809, testUpdateUser=my-topic-829325799-540543436, testThrottlingQuotasDeleteTopic=my-topic-1797350406-813887294, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-889283546-691691584, testTlsExternalUserWithQuotas=my-topic-2037225970-59246891, testTlsUserWithQuotas=my-topic-985048738-437665866, testThrottlingQuotasCreateTopic=my-topic-114922478-1506136333, testSendingMessagesToNonExistingTopic=my-topic-441360312-1858428966, testReceiveSimpleMessageTls=my-topic-1131768783-120051821, testConfigurationPerformanceOptions=my-topic-627301835-843876360, testUserTemplate=my-topic-1175486259-481154370, testMoreReplicasThanAvailableBrokers=my-topic-71698317-859778131}
2022-03-28 14:02:16 [ForkJoinPool-1-worker-7] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-af298d01-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-8f6c28cb-kafka-clients, testSendSimpleMessageTls=my-cluster-fbc845dd-kafka-clients, testConfigurationReflection=my-cluster-5137fff1-kafka-clients, testUserWithNameMoreThan64Chars=my-cluster-02e25de6-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-844045c3-kafka-clients, testCapacityFile=my-cluster-0531d5ce-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-8af61d52-kafka-clients, testTlsExternalUser=my-cluster-43f83aef-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-e353214b-kafka-clients, testScramUserWithQuotas=my-cluster-6f653060-kafka-clients, testTopicModificationOfReplicationFactor=my-cluster-a1a53143-kafka-clients, testKafkaAdminTopicOperations=my-cluster-600e8332-kafka-clients, testConfigurationFileIsCreated=my-cluster-3244df0d-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-0c65d2dd-kafka-clients, testUpdateUser=my-cluster-c2c4d05a-kafka-clients, testThrottlingQuotasDeleteTopic=my-cluster-419d5c31-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-0444229e-kafka-clients, testTlsExternalUserWithQuotas=my-cluster-08d24b8e-kafka-clients, testTlsUserWithQuotas=my-cluster-9832c9ae-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-ac2cadd6-kafka-clients, testSendingMessagesToNonExistingTopic=my-cluster-abe3633a-kafka-clients, testReceiveSimpleMessageTls=my-cluster-ace1fa24-kafka-clients, testConfigurationPerformanceOptions=my-cluster-4994018d-kafka-clients, testUserTemplate=my-cluster-e0217138-kafka-clients, testMoreReplicasThanAvailableBrokers=my-cluster-203feca8-kafka-clients}
2022-03-28 14:02:16 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update KafkaTopic my-topic-71698317-859778131 in namespace topic-st
2022-03-28 14:02:16 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-4 -o yaml
2022-03-28 14:02:16 [ForkJoinPool-1-worker-7] INFO  [TopicST:461] Checking in KafkaTopic CR that topic my-topic-71698317-859778131 exists
2022-03-28 14:02:16 [ForkJoinPool-1-worker-7] INFO  [TopicST:456] Checking topic my-topic-71698317-859778131 in Kafka
2022-03-28 14:02:16 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-03-28 14:02:16 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace cruise-control-configuration-st -o yaml
2022-03-28 14:02:16 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace user-st -o yaml
2022-03-28 14:02:16 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (55989ms till timeout)
2022-03-28 14:02:17 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:02:17 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-4 -o yaml
2022-03-28 14:02:17 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 14:02:17 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (459828ms till timeout)
2022-03-28 14:02:17 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (206044ms till timeout)
2022-03-28 14:02:17 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Wait for Deployment: topic-cluster-name-kafka-clients will be ready not ready, will try again in 1000 ms (478806ms till timeout)
2022-03-28 14:02:17 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace cruise-control-configuration-st -o yaml
2022-03-28 14:02:17 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 14:02:17 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace cruise-control-configuration-st removal not ready, will try again in 1000 ms (476545ms till timeout)
2022-03-28 14:02:17 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace user-st -o yaml
2022-03-28 14:02:17 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 14:02:17 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (478013ms till timeout)
2022-03-28 14:02:18 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-4 -o yaml
2022-03-28 14:02:18 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (54794ms till timeout)
2022-03-28 14:02:18 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:02:18 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (204849ms till timeout)
2022-03-28 14:02:18 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace cruise-control-configuration-st -o yaml
2022-03-28 14:02:18 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Wait for Deployment: topic-cluster-name-kafka-clients will be ready not ready, will try again in 1000 ms (477688ms till timeout)
2022-03-28 14:02:18 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace user-st -o yaml
2022-03-28 14:02:18 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-4 -o yaml
2022-03-28 14:02:18 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 14:02:18 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (458344ms till timeout)
2022-03-28 14:02:18 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace cruise-control-configuration-st -o yaml
2022-03-28 14:02:18 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 14:02:18 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace cruise-control-configuration-st removal not ready, will try again in 1000 ms (475022ms till timeout)
2022-03-28 14:02:18 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace user-st -o yaml
2022-03-28 14:02:18 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 14:02:18 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (476508ms till timeout)
2022-03-28 14:02:19 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (53601ms till timeout)
2022-03-28 14:02:19 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:02:19 [ForkJoinPool-1-worker-1] INFO  [DeploymentUtils:168] Deployment: topic-cluster-name-kafka-clients is ready
2022-03-28 14:02:19 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-4 -o yaml
2022-03-28 14:02:19 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (203561ms till timeout)
2022-03-28 14:02:19 [ForkJoinPool-1-worker-1] INFO  [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-03-28 14:02:19 [ForkJoinPool-1-worker-1] INFO  [TopicST:320] Checking if my-topic-441360312-1858428966 is on topic list
2022-03-28 14:02:19 [ForkJoinPool-1-worker-1] INFO  [TopicST:456] Checking topic my-topic-441360312-1858428966 in Kafka
2022-03-28 14:02:19 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-03-28 14:02:19 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace cruise-control-configuration-st -o yaml
2022-03-28 14:02:19 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace user-st -o yaml
2022-03-28 14:02:19 [ForkJoinPool-1-worker-7] INFO  [Exec:417] Command: oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-03-28 14:02:19 [ForkJoinPool-1-worker-7] INFO  [Exec:417] Return code: 0
2022-03-28 14:02:19 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:433] Wait for KafkaTopic: my-topic-71698317-859778131 will have desired state: NotReady
2022-03-28 14:02:19 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for KafkaTopic: my-topic-71698317-859778131 will have desired state: NotReady
2022-03-28 14:02:20 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-4 -o yaml
2022-03-28 14:02:20 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 14:02:20 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (456835ms till timeout)
2022-03-28 14:02:20 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:444] KafkaTopic: my-topic-71698317-859778131 is in desired state: NotReady
2022-03-28 14:02:20 [ForkJoinPool-1-worker-7] INFO  [TopicST:90] Delete topic my-topic-71698317-859778131
2022-03-28 14:02:20 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace topic-st delete kafkatopic my-topic-71698317-859778131
2022-03-28 14:02:20 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (52500ms till timeout)
2022-03-28 14:02:20 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace user-st -o yaml
2022-03-28 14:02:20 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 14:02:20 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (474989ms till timeout)
2022-03-28 14:02:20 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-8 get Namespace cruise-control-configuration-st -o yaml
2022-03-28 14:02:20 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 1
2022-03-28 14:02:20 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 14:02:20 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Error from server (NotFound): namespaces "cruise-control-configuration-st" not found
2022-03-28 14:02:20 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] ======STDERR END======
2022-03-28 14:02:20 [ForkJoinPool-1-worker-3] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[], io.strimzi.test.logs.CollectorElement@c4c0ea0=[], io.strimzi.test.logs.CollectorElement@aec142ef=[], io.strimzi.test.logs.CollectorElement@f851b6c3=[], io.strimzi.test.logs.CollectorElement@3881d5f2=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@e3067dd1=[], io.strimzi.test.logs.CollectorElement@5c7379cb=[], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[namespace-4], io.strimzi.test.logs.CollectorElement@f4405b3b=[], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 14:02:20 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:254] CruiseControlConfigurationST - Notifies waiting test suites:[UserST, ThrottlingQuotaST, TopicST, CruiseControlConfigurationST, HttpBridgeScramShaST, HttpBridgeTlsST] to and randomly select one to start execution
2022-03-28 14:02:20 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:85] [cruisecontrol.CruiseControlConfigurationST] - Removing parallel suite: CruiseControlConfigurationST
2022-03-28 14:02:20 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:89] [cruisecontrol.CruiseControlConfigurationST] - Parallel suites count: 4
[INFO] Tests run: 16, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 810.466 s - in io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST
2022-03-28 14:02:20 [ForkJoinPool-1-worker-3] INFO  [TestSeparator:23] ############################################################################
2022-03-28 14:02:20 [ForkJoinPool-1-worker-3] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.topic.TopicST.testCreateTopicViaKafka-STARTED
2022-03-28 14:02:20 [ForkJoinPool-1-worker-3] DEBUG [AbstractST:658] ============================================================================
2022-03-28 14:02:20 [ForkJoinPool-1-worker-3] DEBUG [AbstractST:659] [operators.topic.TopicST - Before Each] - Setup test case environment
2022-03-28 14:02:20 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:77] [operators.topic.TopicST] - Adding parallel test: testCreateTopicViaKafka
2022-03-28 14:02:20 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:81] [operators.topic.TopicST] - Parallel test count: 6
2022-03-28 14:02:20 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:230] testCreateTopicViaKafka test now can proceed its execution
2022-03-28 14:02:20 [ForkJoinPool-1-worker-3] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 14:02:20 [ForkJoinPool-1-worker-3] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-af298d01, testThrottlingQuotasCreateAlterPartitions=my-cluster-8f6c28cb, testSendSimpleMessageTls=my-cluster-fbc845dd, testConfigurationReflection=my-cluster-5137fff1, testUserWithNameMoreThan64Chars=my-cluster-02e25de6, testDeployAndUnDeployCruiseControl=my-cluster-844045c3, testCapacityFile=my-cluster-0531d5ce, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-8af61d52, testTlsExternalUser=my-cluster-43f83aef, testSendSimpleMessageTlsScramSha=my-cluster-e353214b, testScramUserWithQuotas=my-cluster-6f653060, testTopicModificationOfReplicationFactor=my-cluster-a1a53143, testKafkaAdminTopicOperations=my-cluster-600e8332, testConfigurationFileIsCreated=my-cluster-3244df0d, testCreateTopicViaKafka=my-cluster-51bcec77, testCreatingUsersWithSecretPrefix=my-cluster-0c65d2dd, testUpdateUser=my-cluster-c2c4d05a, testThrottlingQuotasDeleteTopic=my-cluster-419d5c31, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-0444229e, testTlsExternalUserWithQuotas=my-cluster-08d24b8e, testTlsUserWithQuotas=my-cluster-9832c9ae, testThrottlingQuotasCreateTopic=my-cluster-ac2cadd6, testSendingMessagesToNonExistingTopic=my-cluster-abe3633a, testReceiveSimpleMessageTls=my-cluster-ace1fa24, testConfigurationPerformanceOptions=my-cluster-4994018d, testUserTemplate=my-cluster-e0217138, testMoreReplicasThanAvailableBrokers=my-cluster-203feca8}
2022-03-28 14:02:20 [ForkJoinPool-1-worker-3] TRACE [AbstractST:607] USERS_NAME_MAP: {testReceiveSimpleMessageTlsScramSha=my-user-1597210213-707359243, testThrottlingQuotasCreateAlterPartitions=my-user-1557959119-1722636050, testSendSimpleMessageTls=my-user-1754673553-2077518027, testConfigurationReflection=my-user-2029261898-1375854235, testUserWithNameMoreThan64Chars=my-user-2124392695-340403978, testDeployAndUnDeployCruiseControl=my-user-2117120687-604608920, testCapacityFile=my-user-155969448-1270555822, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-1807142870-188660517, testTlsExternalUser=my-user-370513135-1989152728, testSendSimpleMessageTlsScramSha=my-user-182357028-1132873010, testScramUserWithQuotas=my-user-27563760-1632863338, testTopicModificationOfReplicationFactor=my-user-1764997166-1500934155, testKafkaAdminTopicOperations=my-user-1735322302-803507072, testConfigurationFileIsCreated=my-user-1799841978-1468805631, testCreateTopicViaKafka=my-user-1501031580-1495561780, testCreatingUsersWithSecretPrefix=my-user-144001639-226807213, testUpdateUser=my-user-1870578302-1216914412, testThrottlingQuotasDeleteTopic=my-user-1782685497-1980640589, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-1010649803-214635564, testTlsExternalUserWithQuotas=my-user-247789923-205827680, testTlsUserWithQuotas=my-user-1602931187-443256751, testThrottlingQuotasCreateTopic=my-user-46611685-1079680568, testSendingMessagesToNonExistingTopic=my-user-954109284-2118832263, testReceiveSimpleMessageTls=my-user-1418772225-1024845009, testConfigurationPerformanceOptions=my-user-882948629-1821489961, testUserTemplate=my-user-896194994-133213923, testMoreReplicasThanAvailableBrokers=my-user-413609357-1989111041}
2022-03-28 14:02:20 [ForkJoinPool-1-worker-3] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-topic-649423016-1623551209, testThrottlingQuotasCreateAlterPartitions=my-topic-1449977352-344480794, testSendSimpleMessageTls=my-topic-115477502-2025274778, testConfigurationReflection=my-topic-1621095669-1276202830, testUserWithNameMoreThan64Chars=my-topic-1053060011-1199293498, testDeployAndUnDeployCruiseControl=my-topic-1101175281-2107378523, testCapacityFile=my-topic-296605537-1008743104, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-1367124108-1748498395, testTlsExternalUser=my-topic-2081139838-1814296134, testSendSimpleMessageTlsScramSha=my-topic-652506937-1802310709, testScramUserWithQuotas=my-topic-1345794255-226872460, testTopicModificationOfReplicationFactor=my-topic-2129693330-635016449, testKafkaAdminTopicOperations=my-topic-1935791607-1758305855, testConfigurationFileIsCreated=my-topic-1435014906-681104826, testCreateTopicViaKafka=my-topic-300455033-999140097, testCreatingUsersWithSecretPrefix=my-topic-130462979-2061578809, testUpdateUser=my-topic-829325799-540543436, testThrottlingQuotasDeleteTopic=my-topic-1797350406-813887294, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-889283546-691691584, testTlsExternalUserWithQuotas=my-topic-2037225970-59246891, testTlsUserWithQuotas=my-topic-985048738-437665866, testThrottlingQuotasCreateTopic=my-topic-114922478-1506136333, testSendingMessagesToNonExistingTopic=my-topic-441360312-1858428966, testReceiveSimpleMessageTls=my-topic-1131768783-120051821, testConfigurationPerformanceOptions=my-topic-627301835-843876360, testUserTemplate=my-topic-1175486259-481154370, testMoreReplicasThanAvailableBrokers=my-topic-71698317-859778131}
2022-03-28 14:02:20 [ForkJoinPool-1-worker-3] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-af298d01-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-8f6c28cb-kafka-clients, testSendSimpleMessageTls=my-cluster-fbc845dd-kafka-clients, testConfigurationReflection=my-cluster-5137fff1-kafka-clients, testUserWithNameMoreThan64Chars=my-cluster-02e25de6-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-844045c3-kafka-clients, testCapacityFile=my-cluster-0531d5ce-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-8af61d52-kafka-clients, testTlsExternalUser=my-cluster-43f83aef-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-e353214b-kafka-clients, testScramUserWithQuotas=my-cluster-6f653060-kafka-clients, testTopicModificationOfReplicationFactor=my-cluster-a1a53143-kafka-clients, testKafkaAdminTopicOperations=my-cluster-600e8332-kafka-clients, testConfigurationFileIsCreated=my-cluster-3244df0d-kafka-clients, testCreateTopicViaKafka=my-cluster-51bcec77-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-0c65d2dd-kafka-clients, testUpdateUser=my-cluster-c2c4d05a-kafka-clients, testThrottlingQuotasDeleteTopic=my-cluster-419d5c31-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-0444229e-kafka-clients, testTlsExternalUserWithQuotas=my-cluster-08d24b8e-kafka-clients, testTlsUserWithQuotas=my-cluster-9832c9ae-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-ac2cadd6-kafka-clients, testSendingMessagesToNonExistingTopic=my-cluster-abe3633a-kafka-clients, testReceiveSimpleMessageTls=my-cluster-ace1fa24-kafka-clients, testConfigurationPerformanceOptions=my-cluster-4994018d-kafka-clients, testUserTemplate=my-cluster-e0217138-kafka-clients, testMoreReplicasThanAvailableBrokers=my-cluster-203feca8-kafka-clients}
2022-03-28 14:02:20 [ForkJoinPool-1-worker-3] DEBUG [TopicST:113] Creating topic my-topic-300455033-999140097 with 3 replicas and 3 partitions
2022-03-28 14:02:20 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --bootstrap-server localhost:9092 --create  --topic my-topic-300455033-999140097 --replication-factor 3 --partitions 3
2022-03-28 14:02:20 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:02:20 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace topic-st delete kafkatopic my-topic-71698317-859778131
2022-03-28 14:02:20 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 14:02:20 [ForkJoinPool-1-worker-7] INFO  [KafkaTopicUtils:104] Waiting for KafkaTopic my-topic-71698317-859778131 deletion
2022-03-28 14:02:20 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for KafkaTopic deletion my-topic-71698317-859778131
2022-03-28 14:02:20 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (202366ms till timeout)
2022-03-28 14:02:20 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update KafkaTopic topic-example-new in namespace topic-st
2022-03-28 14:02:21 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:topic-example-new
2022-03-28 14:02:21 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-4 -o yaml
2022-03-28 14:02:21 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:433] Wait for KafkaTopic: topic-example-new will have desired state: Ready
2022-03-28 14:02:21 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for KafkaTopic: topic-example-new will have desired state: Ready
2022-03-28 14:02:21 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:444] KafkaTopic: topic-example-new is in desired state: Ready
2022-03-28 14:02:21 [ForkJoinPool-1-worker-7] INFO  [TopicST:456] Checking topic topic-example-new in Kafka
2022-03-28 14:02:21 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-03-28 14:02:21 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace user-st -o yaml
2022-03-28 14:02:21 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (51398ms till timeout)
2022-03-28 14:02:21 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-4 -o yaml
2022-03-28 14:02:21 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 14:02:21 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (455339ms till timeout)
2022-03-28 14:02:21 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:02:21 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace user-st -o yaml
2022-03-28 14:02:21 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 14:02:21 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (473503ms till timeout)
2022-03-28 14:02:22 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (201166ms till timeout)
2022-03-28 14:02:22 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-4 -o yaml
2022-03-28 14:02:22 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (50204ms till timeout)
2022-03-28 14:02:22 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace user-st -o yaml
2022-03-28 14:02:23 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-4 -o yaml
2022-03-28 14:02:23 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 14:02:23 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (453867ms till timeout)
2022-03-28 14:02:23 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:02:23 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (199971ms till timeout)
2022-03-28 14:02:23 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace user-st -o yaml
2022-03-28 14:02:23 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 14:02:23 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (472029ms till timeout)
2022-03-28 14:02:23 [ForkJoinPool-1-worker-1] INFO  [Exec:417] Command: oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-03-28 14:02:23 [ForkJoinPool-1-worker-1] INFO  [Exec:417] Return code: 0
2022-03-28 14:02:23 [ForkJoinPool-1-worker-1] INFO  [TopicST:323] Topic with name my-topic-441360312-1858428966 is not created yet
2022-03-28 14:02:23 [ForkJoinPool-1-worker-1] INFO  [TopicST:325] Trying to send messages to non-existing topic my-topic-441360312-1858428966
2022-03-28 14:02:23 [ForkJoinPool-1-worker-1] DEBUG [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@2fd9b24d, which are set.
2022-03-28 14:02:23 [ForkJoinPool-1-worker-1] INFO  [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@8875347, messages=[], arguments=[--topic, my-topic-441360312-1858428966, --bootstrap-server, topic-cluster-name-kafka-bootstrap.topic-st.svc:9092, --max-messages, 100], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='topic-cluster-name-kafka-clients-7c47b4459f-zgszb', podNamespace='topic-st', bootstrapServer='topic-cluster-name-kafka-bootstrap.topic-st.svc:9092', topicName='my-topic-441360312-1858428966', maxMessages=100, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@2fd9b24d}
2022-03-28 14:02:23 [ForkJoinPool-1-worker-1] INFO  [InternalKafkaClient:94] Producing 100 messages to topic-cluster-name-kafka-bootstrap.topic-st.svc:9092:my-topic-441360312-1858428966 from pod topic-cluster-name-kafka-clients-7c47b4459f-zgszb
2022-03-28 14:02:23 [ForkJoinPool-1-worker-1] INFO  [VerifiableClient:192] Client command: oc exec topic-cluster-name-kafka-clients-7c47b4459f-zgszb -n topic-st -- /opt/kafka/producer.sh --topic my-topic-441360312-1858428966 --bootstrap-server topic-cluster-name-kafka-bootstrap.topic-st.svc:9092 --max-messages 100
2022-03-28 14:02:23 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc exec topic-cluster-name-kafka-clients-7c47b4459f-zgszb -n topic-st -- /opt/kafka/producer.sh --topic my-topic-441360312-1858428966 --bootstrap-server topic-cluster-name-kafka-bootstrap.topic-st.svc:9092 --max-messages 100
2022-03-28 14:02:23 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (48942ms till timeout)
2022-03-28 14:02:24 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-4 -o yaml
2022-03-28 14:02:24 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:02:24 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (198777ms till timeout)
2022-03-28 14:02:24 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace user-st -o yaml
2022-03-28 14:02:24 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-4 -o yaml
2022-03-28 14:02:24 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 14:02:24 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (452391ms till timeout)
2022-03-28 14:02:24 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace user-st -o yaml
2022-03-28 14:02:24 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 14:02:24 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (470564ms till timeout)
2022-03-28 14:02:25 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (47749ms till timeout)
2022-03-28 14:02:25 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Command: oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --bootstrap-server localhost:9092 --create  --topic my-topic-300455033-999140097 --replication-factor 3 --partitions 3
2022-03-28 14:02:25 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 14:02:25 [ForkJoinPool-1-worker-3] INFO  [KafkaTopicUtils:78] Waiting for KafkaTopic my-topic-300455033-999140097 creation 
2022-03-28 14:02:25 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for KafkaTopic creation my-topic-300455033-999140097
2022-03-28 14:02:25 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] KafkaTopic creation my-topic-300455033-999140097 not ready, will try again in 1000 ms (179901ms till timeout)
2022-03-28 14:02:25 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-4 -o yaml
2022-03-28 14:02:25 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:02:25 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (197537ms till timeout)
2022-03-28 14:02:25 [ForkJoinPool-1-worker-7] INFO  [Exec:417] Command: oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-03-28 14:02:25 [ForkJoinPool-1-worker-7] INFO  [Exec:417] Return code: 0
2022-03-28 14:02:25 [ForkJoinPool-1-worker-7] INFO  [TopicST:461] Checking in KafkaTopic CR that topic topic-example-new exists
2022-03-28 14:02:25 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:674] ============================================================================
2022-03-28 14:02:25 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:675] [operators.topic.TopicST - After Each] - Clean up after test
2022-03-28 14:02:25 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:344] ############################################################################
2022-03-28 14:02:25 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:348] Delete all resources for testMoreReplicasThanAvailableBrokers
2022-03-28 14:02:25 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:241] Delete of KafkaTopic topic-example-new in namespace topic-st
2022-03-28 14:02:25 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:topic-example-new
2022-03-28 14:02:25 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace user-st -o yaml
2022-03-28 14:02:25 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:241] Delete of KafkaTopic my-topic-71698317-859778131 in namespace topic-st
2022-03-28 14:02:25 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-4 -o yaml
2022-03-28 14:02:25 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 14:02:25 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (450921ms till timeout)
2022-03-28 14:02:26 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-71698317-859778131
2022-03-28 14:02:26 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:369] ############################################################################
2022-03-28 14:02:26 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:267] testMoreReplicasThanAvailableBrokers - Notifies waiting test cases:[testCapacityFile, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testThrottlingQuotasCreateAlterPartitions, testSendSimpleMessageTls, testReceiveSimpleMessageTlsScramSha, testSendSimpleMessageTlsScramSha, testReceiveSimpleMessageTls, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testConfigurationFileIsCreated, testPauseReconciliationInKafkaRebalanceAndTopic, testTopicModificationOfReplicationFactor, testMoreReplicasThanAvailableBrokers, testCreateTopicViaKafka] to and randomly select one to start execution
2022-03-28 14:02:26 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:93] [operators.topic.TopicST] - Removing parallel test: testMoreReplicasThanAvailableBrokers
2022-03-28 14:02:26 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:97] [operators.topic.TopicST] - Parallel test count: 5
2022-03-28 14:02:26 [ForkJoinPool-1-worker-7] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.topic.TopicST.testMoreReplicasThanAvailableBrokers-FINISHED
2022-03-28 14:02:26 [ForkJoinPool-1-worker-7] INFO  [TestSeparator:30] ############################################################################
2022-03-28 14:02:26 [ForkJoinPool-1-worker-7] INFO  [TestSeparator:23] ############################################################################
2022-03-28 14:02:26 [ForkJoinPool-1-worker-7] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.topic.TopicST.testCreateTopicAfterUnsupportedOperation-STARTED
2022-03-28 14:02:26 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:658] ============================================================================
2022-03-28 14:02:26 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:659] [operators.topic.TopicST - Before Each] - Setup test case environment
2022-03-28 14:02:26 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:77] [operators.topic.TopicST] - Adding parallel test: testCreateTopicAfterUnsupportedOperation
2022-03-28 14:02:26 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:81] [operators.topic.TopicST] - Parallel test count: 6
2022-03-28 14:02:26 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:230] testCreateTopicAfterUnsupportedOperation test now can proceed its execution
2022-03-28 14:02:26 [ForkJoinPool-1-worker-7] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 14:02:26 [ForkJoinPool-1-worker-7] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-af298d01, testThrottlingQuotasCreateAlterPartitions=my-cluster-8f6c28cb, testSendSimpleMessageTls=my-cluster-fbc845dd, testConfigurationReflection=my-cluster-5137fff1, testUserWithNameMoreThan64Chars=my-cluster-02e25de6, testDeployAndUnDeployCruiseControl=my-cluster-844045c3, testCapacityFile=my-cluster-0531d5ce, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-8af61d52, testTlsExternalUser=my-cluster-43f83aef, testSendSimpleMessageTlsScramSha=my-cluster-e353214b, testCreateTopicAfterUnsupportedOperation=my-cluster-8e26464c, testScramUserWithQuotas=my-cluster-6f653060, testTopicModificationOfReplicationFactor=my-cluster-a1a53143, testKafkaAdminTopicOperations=my-cluster-600e8332, testConfigurationFileIsCreated=my-cluster-3244df0d, testCreateTopicViaKafka=my-cluster-51bcec77, testCreatingUsersWithSecretPrefix=my-cluster-0c65d2dd, testUpdateUser=my-cluster-c2c4d05a, testThrottlingQuotasDeleteTopic=my-cluster-419d5c31, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-0444229e, testTlsExternalUserWithQuotas=my-cluster-08d24b8e, testTlsUserWithQuotas=my-cluster-9832c9ae, testThrottlingQuotasCreateTopic=my-cluster-ac2cadd6, testSendingMessagesToNonExistingTopic=my-cluster-abe3633a, testReceiveSimpleMessageTls=my-cluster-ace1fa24, testConfigurationPerformanceOptions=my-cluster-4994018d, testUserTemplate=my-cluster-e0217138, testMoreReplicasThanAvailableBrokers=my-cluster-203feca8}
2022-03-28 14:02:26 [ForkJoinPool-1-worker-7] TRACE [AbstractST:607] USERS_NAME_MAP: {testReceiveSimpleMessageTlsScramSha=my-user-1597210213-707359243, testThrottlingQuotasCreateAlterPartitions=my-user-1557959119-1722636050, testSendSimpleMessageTls=my-user-1754673553-2077518027, testConfigurationReflection=my-user-2029261898-1375854235, testUserWithNameMoreThan64Chars=my-user-2124392695-340403978, testDeployAndUnDeployCruiseControl=my-user-2117120687-604608920, testCapacityFile=my-user-155969448-1270555822, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-1807142870-188660517, testTlsExternalUser=my-user-370513135-1989152728, testSendSimpleMessageTlsScramSha=my-user-182357028-1132873010, testCreateTopicAfterUnsupportedOperation=my-user-917806669-1508124143, testScramUserWithQuotas=my-user-27563760-1632863338, testTopicModificationOfReplicationFactor=my-user-1764997166-1500934155, testKafkaAdminTopicOperations=my-user-1735322302-803507072, testConfigurationFileIsCreated=my-user-1799841978-1468805631, testCreateTopicViaKafka=my-user-1501031580-1495561780, testCreatingUsersWithSecretPrefix=my-user-144001639-226807213, testUpdateUser=my-user-1870578302-1216914412, testThrottlingQuotasDeleteTopic=my-user-1782685497-1980640589, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-1010649803-214635564, testTlsExternalUserWithQuotas=my-user-247789923-205827680, testTlsUserWithQuotas=my-user-1602931187-443256751, testThrottlingQuotasCreateTopic=my-user-46611685-1079680568, testSendingMessagesToNonExistingTopic=my-user-954109284-2118832263, testReceiveSimpleMessageTls=my-user-1418772225-1024845009, testConfigurationPerformanceOptions=my-user-882948629-1821489961, testUserTemplate=my-user-896194994-133213923, testMoreReplicasThanAvailableBrokers=my-user-413609357-1989111041}
2022-03-28 14:02:26 [ForkJoinPool-1-worker-7] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-topic-649423016-1623551209, testThrottlingQuotasCreateAlterPartitions=my-topic-1449977352-344480794, testSendSimpleMessageTls=my-topic-115477502-2025274778, testConfigurationReflection=my-topic-1621095669-1276202830, testUserWithNameMoreThan64Chars=my-topic-1053060011-1199293498, testDeployAndUnDeployCruiseControl=my-topic-1101175281-2107378523, testCapacityFile=my-topic-296605537-1008743104, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-1367124108-1748498395, testTlsExternalUser=my-topic-2081139838-1814296134, testSendSimpleMessageTlsScramSha=my-topic-652506937-1802310709, testCreateTopicAfterUnsupportedOperation=my-topic-1639085682-1344322559, testScramUserWithQuotas=my-topic-1345794255-226872460, testTopicModificationOfReplicationFactor=my-topic-2129693330-635016449, testKafkaAdminTopicOperations=my-topic-1935791607-1758305855, testConfigurationFileIsCreated=my-topic-1435014906-681104826, testCreateTopicViaKafka=my-topic-300455033-999140097, testCreatingUsersWithSecretPrefix=my-topic-130462979-2061578809, testUpdateUser=my-topic-829325799-540543436, testThrottlingQuotasDeleteTopic=my-topic-1797350406-813887294, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-889283546-691691584, testTlsExternalUserWithQuotas=my-topic-2037225970-59246891, testTlsUserWithQuotas=my-topic-985048738-437665866, testThrottlingQuotasCreateTopic=my-topic-114922478-1506136333, testSendingMessagesToNonExistingTopic=my-topic-441360312-1858428966, testReceiveSimpleMessageTls=my-topic-1131768783-120051821, testConfigurationPerformanceOptions=my-topic-627301835-843876360, testUserTemplate=my-topic-1175486259-481154370, testMoreReplicasThanAvailableBrokers=my-topic-71698317-859778131}
2022-03-28 14:02:26 [ForkJoinPool-1-worker-7] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-af298d01-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-8f6c28cb-kafka-clients, testSendSimpleMessageTls=my-cluster-fbc845dd-kafka-clients, testConfigurationReflection=my-cluster-5137fff1-kafka-clients, testUserWithNameMoreThan64Chars=my-cluster-02e25de6-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-844045c3-kafka-clients, testCapacityFile=my-cluster-0531d5ce-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-8af61d52-kafka-clients, testTlsExternalUser=my-cluster-43f83aef-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-e353214b-kafka-clients, testCreateTopicAfterUnsupportedOperation=my-cluster-8e26464c-kafka-clients, testScramUserWithQuotas=my-cluster-6f653060-kafka-clients, testTopicModificationOfReplicationFactor=my-cluster-a1a53143-kafka-clients, testKafkaAdminTopicOperations=my-cluster-600e8332-kafka-clients, testConfigurationFileIsCreated=my-cluster-3244df0d-kafka-clients, testCreateTopicViaKafka=my-cluster-51bcec77-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-0c65d2dd-kafka-clients, testUpdateUser=my-cluster-c2c4d05a-kafka-clients, testThrottlingQuotasDeleteTopic=my-cluster-419d5c31-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-0444229e-kafka-clients, testTlsExternalUserWithQuotas=my-cluster-08d24b8e-kafka-clients, testTlsUserWithQuotas=my-cluster-9832c9ae-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-ac2cadd6-kafka-clients, testSendingMessagesToNonExistingTopic=my-cluster-abe3633a-kafka-clients, testReceiveSimpleMessageTls=my-cluster-ace1fa24-kafka-clients, testConfigurationPerformanceOptions=my-cluster-4994018d-kafka-clients, testUserTemplate=my-cluster-e0217138-kafka-clients, testMoreReplicasThanAvailableBrokers=my-cluster-203feca8-kafka-clients}
2022-03-28 14:02:26 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update KafkaTopic topic-with-replication-to-change in namespace topic-st
2022-03-28 14:02:26 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (46630ms till timeout)
2022-03-28 14:02:26 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace user-st -o yaml
2022-03-28 14:02:26 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 14:02:26 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (469100ms till timeout)
2022-03-28 14:02:26 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:topic-with-replication-to-change
2022-03-28 14:02:26 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:433] Wait for KafkaTopic: topic-with-replication-to-change will have desired state: Ready
2022-03-28 14:02:26 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for KafkaTopic: topic-with-replication-to-change will have desired state: Ready
2022-03-28 14:02:26 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] KafkaTopic creation my-topic-300455033-999140097 not ready, will try again in 1000 ms (178753ms till timeout)
2022-03-28 14:02:26 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaTopic: topic-with-replication-to-change will have desired state: Ready not ready, will try again in 1000 ms (179805ms till timeout)
2022-03-28 14:02:26 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:02:26 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (196296ms till timeout)
2022-03-28 14:02:26 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-4 -o yaml
2022-03-28 14:02:27 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace user-st -o yaml
2022-03-28 14:02:27 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (45527ms till timeout)
2022-03-28 14:02:27 [ForkJoinPool-1-worker-1] INFO  [InternalKafkaClient:97] Producer finished correctly: true
2022-03-28 14:02:27 [ForkJoinPool-1-worker-1] INFO  [InternalKafkaClient:101] Producer produced 100 messages
2022-03-28 14:02:27 [ForkJoinPool-1-worker-1] DEBUG [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@1961eab3, which are set.
2022-03-28 14:02:27 [ForkJoinPool-1-worker-1] INFO  [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@a9a9798, messages=[], arguments=[--group-id, my-consumer-group-679268085, --topic, my-topic-441360312-1858428966, --group-instance-id, instance1393685061, --bootstrap-server, topic-cluster-name-kafka-bootstrap.topic-st.svc:9092, --max-messages, 100], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='topic-cluster-name-kafka-clients-7c47b4459f-zgszb', podNamespace='topic-st', bootstrapServer='topic-cluster-name-kafka-bootstrap.topic-st.svc:9092', topicName='my-topic-441360312-1858428966', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-679268085', consumerInstanceId='instance1393685061', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@1961eab3}
2022-03-28 14:02:27 [ForkJoinPool-1-worker-1] INFO  [InternalKafkaClient:157] Consuming 100 messages from topic-cluster-name-kafka-bootstrap.topic-st.svc:9092#my-topic-441360312-1858428966 from pod topic-cluster-name-kafka-clients-7c47b4459f-zgszb
2022-03-28 14:02:27 [ForkJoinPool-1-worker-1] INFO  [VerifiableClient:192] Client command: oc exec topic-cluster-name-kafka-clients-7c47b4459f-zgszb -n topic-st -- /opt/kafka/consumer.sh --group-id my-consumer-group-679268085 --topic my-topic-441360312-1858428966 --group-instance-id instance1393685061 --bootstrap-server topic-cluster-name-kafka-bootstrap.topic-st.svc:9092 --max-messages 100
2022-03-28 14:02:27 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc exec topic-cluster-name-kafka-clients-7c47b4459f-zgszb -n topic-st -- /opt/kafka/consumer.sh --group-id my-consumer-group-679268085 --topic my-topic-441360312-1858428966 --group-instance-id instance1393685061 --bootstrap-server topic-cluster-name-kafka-bootstrap.topic-st.svc:9092 --max-messages 100
2022-03-28 14:02:27 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-8 get Namespace namespace-4 -o yaml
2022-03-28 14:02:27 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 1
2022-03-28 14:02:27 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 14:02:27 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Error from server (NotFound): namespaces "namespace-4" not found
2022-03-28 14:02:27 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] ======STDERR END======
2022-03-28 14:02:27 [ForkJoinPool-1-worker-15] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[], io.strimzi.test.logs.CollectorElement@c4c0ea0=[], io.strimzi.test.logs.CollectorElement@aec142ef=[], io.strimzi.test.logs.CollectorElement@f851b6c3=[], io.strimzi.test.logs.CollectorElement@3881d5f2=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@e3067dd1=[], io.strimzi.test.logs.CollectorElement@5c7379cb=[], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[], io.strimzi.test.logs.CollectorElement@f4405b3b=[], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 14:02:27 [ForkJoinPool-1-worker-15] DEBUG [SuiteThreadController:267] testPauseReconciliationInKafkaRebalanceAndTopic - Notifies waiting test cases:[testCapacityFile, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testThrottlingQuotasCreateAlterPartitions, testSendSimpleMessageTls, testReceiveSimpleMessageTlsScramSha, testSendSimpleMessageTlsScramSha, testReceiveSimpleMessageTls, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testConfigurationFileIsCreated, testPauseReconciliationInKafkaRebalanceAndTopic, testTopicModificationOfReplicationFactor, testMoreReplicasThanAvailableBrokers, testCreateTopicViaKafka, testCreateTopicAfterUnsupportedOperation] to and randomly select one to start execution
2022-03-28 14:02:27 [ForkJoinPool-1-worker-15] DEBUG [SuiteThreadController:93] [operators.ReconciliationST] - Removing parallel test: testPauseReconciliationInKafkaRebalanceAndTopic
2022-03-28 14:02:27 [ForkJoinPool-1-worker-15] DEBUG [SuiteThreadController:97] [operators.ReconciliationST] - Parallel test count: 5
2022-03-28 14:02:27 [ForkJoinPool-1-worker-15] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.ReconciliationST.testPauseReconciliationInKafkaRebalanceAndTopic-FINISHED
2022-03-28 14:02:27 [ForkJoinPool-1-worker-15] INFO  [TestSeparator:30] ############################################################################
2022-03-28 14:02:27 [ForkJoinPool-1-worker-15] INFO  [TestSeparator:23] ############################################################################
2022-03-28 14:02:27 [ForkJoinPool-1-worker-15] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.ReconciliationST.testPauseReconciliationInKafkaAndKafkaConnectWithConnector-STARTED
2022-03-28 14:02:27 [ForkJoinPool-1-worker-15] DEBUG [AbstractST:658] ============================================================================
2022-03-28 14:02:27 [ForkJoinPool-1-worker-15] DEBUG [AbstractST:659] [operators.ReconciliationST - Before Each] - Setup test case environment
2022-03-28 14:02:27 [ForkJoinPool-1-worker-15] DEBUG [SuiteThreadController:77] [operators.ReconciliationST] - Adding parallel test: testPauseReconciliationInKafkaAndKafkaConnectWithConnector
2022-03-28 14:02:27 [ForkJoinPool-1-worker-15] DEBUG [SuiteThreadController:81] [operators.ReconciliationST] - Parallel test count: 6
2022-03-28 14:02:27 [ForkJoinPool-1-worker-15] DEBUG [SuiteThreadController:230] testPauseReconciliationInKafkaAndKafkaConnectWithConnector test now can proceed its execution
2022-03-28 14:02:27 [ForkJoinPool-1-worker-15] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 14:02:27 [ForkJoinPool-1-worker-15] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-af298d01, testThrottlingQuotasCreateAlterPartitions=my-cluster-8f6c28cb, testSendSimpleMessageTls=my-cluster-fbc845dd, testConfigurationReflection=my-cluster-5137fff1, testUserWithNameMoreThan64Chars=my-cluster-02e25de6, testDeployAndUnDeployCruiseControl=my-cluster-844045c3, testCapacityFile=my-cluster-0531d5ce, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-131d7a1d, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-8af61d52, testTlsExternalUser=my-cluster-43f83aef, testSendSimpleMessageTlsScramSha=my-cluster-e353214b, testCreateTopicAfterUnsupportedOperation=my-cluster-8e26464c, testScramUserWithQuotas=my-cluster-6f653060, testTopicModificationOfReplicationFactor=my-cluster-a1a53143, testKafkaAdminTopicOperations=my-cluster-600e8332, testConfigurationFileIsCreated=my-cluster-3244df0d, testCreateTopicViaKafka=my-cluster-51bcec77, testCreatingUsersWithSecretPrefix=my-cluster-0c65d2dd, testUpdateUser=my-cluster-c2c4d05a, testThrottlingQuotasDeleteTopic=my-cluster-419d5c31, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-0444229e, testTlsExternalUserWithQuotas=my-cluster-08d24b8e, testTlsUserWithQuotas=my-cluster-9832c9ae, testThrottlingQuotasCreateTopic=my-cluster-ac2cadd6, testSendingMessagesToNonExistingTopic=my-cluster-abe3633a, testReceiveSimpleMessageTls=my-cluster-ace1fa24, testConfigurationPerformanceOptions=my-cluster-4994018d, testUserTemplate=my-cluster-e0217138, testMoreReplicasThanAvailableBrokers=my-cluster-203feca8}
2022-03-28 14:02:27 [ForkJoinPool-1-worker-15] TRACE [AbstractST:607] USERS_NAME_MAP: {testReceiveSimpleMessageTlsScramSha=my-user-1597210213-707359243, testThrottlingQuotasCreateAlterPartitions=my-user-1557959119-1722636050, testSendSimpleMessageTls=my-user-1754673553-2077518027, testConfigurationReflection=my-user-2029261898-1375854235, testUserWithNameMoreThan64Chars=my-user-2124392695-340403978, testDeployAndUnDeployCruiseControl=my-user-2117120687-604608920, testCapacityFile=my-user-155969448-1270555822, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-user-1962143551-1221981023, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-1807142870-188660517, testTlsExternalUser=my-user-370513135-1989152728, testSendSimpleMessageTlsScramSha=my-user-182357028-1132873010, testCreateTopicAfterUnsupportedOperation=my-user-917806669-1508124143, testScramUserWithQuotas=my-user-27563760-1632863338, testTopicModificationOfReplicationFactor=my-user-1764997166-1500934155, testKafkaAdminTopicOperations=my-user-1735322302-803507072, testConfigurationFileIsCreated=my-user-1799841978-1468805631, testCreateTopicViaKafka=my-user-1501031580-1495561780, testCreatingUsersWithSecretPrefix=my-user-144001639-226807213, testUpdateUser=my-user-1870578302-1216914412, testThrottlingQuotasDeleteTopic=my-user-1782685497-1980640589, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-1010649803-214635564, testTlsExternalUserWithQuotas=my-user-247789923-205827680, testTlsUserWithQuotas=my-user-1602931187-443256751, testThrottlingQuotasCreateTopic=my-user-46611685-1079680568, testSendingMessagesToNonExistingTopic=my-user-954109284-2118832263, testReceiveSimpleMessageTls=my-user-1418772225-1024845009, testConfigurationPerformanceOptions=my-user-882948629-1821489961, testUserTemplate=my-user-896194994-133213923, testMoreReplicasThanAvailableBrokers=my-user-413609357-1989111041}
2022-03-28 14:02:27 [ForkJoinPool-1-worker-15] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-topic-649423016-1623551209, testThrottlingQuotasCreateAlterPartitions=my-topic-1449977352-344480794, testSendSimpleMessageTls=my-topic-115477502-2025274778, testConfigurationReflection=my-topic-1621095669-1276202830, testUserWithNameMoreThan64Chars=my-topic-1053060011-1199293498, testDeployAndUnDeployCruiseControl=my-topic-1101175281-2107378523, testCapacityFile=my-topic-296605537-1008743104, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-topic-553801381-847208295, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-1367124108-1748498395, testTlsExternalUser=my-topic-2081139838-1814296134, testSendSimpleMessageTlsScramSha=my-topic-652506937-1802310709, testCreateTopicAfterUnsupportedOperation=my-topic-1639085682-1344322559, testScramUserWithQuotas=my-topic-1345794255-226872460, testTopicModificationOfReplicationFactor=my-topic-2129693330-635016449, testKafkaAdminTopicOperations=my-topic-1935791607-1758305855, testConfigurationFileIsCreated=my-topic-1435014906-681104826, testCreateTopicViaKafka=my-topic-300455033-999140097, testCreatingUsersWithSecretPrefix=my-topic-130462979-2061578809, testUpdateUser=my-topic-829325799-540543436, testThrottlingQuotasDeleteTopic=my-topic-1797350406-813887294, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-889283546-691691584, testTlsExternalUserWithQuotas=my-topic-2037225970-59246891, testTlsUserWithQuotas=my-topic-985048738-437665866, testThrottlingQuotasCreateTopic=my-topic-114922478-1506136333, testSendingMessagesToNonExistingTopic=my-topic-441360312-1858428966, testReceiveSimpleMessageTls=my-topic-1131768783-120051821, testConfigurationPerformanceOptions=my-topic-627301835-843876360, testUserTemplate=my-topic-1175486259-481154370, testMoreReplicasThanAvailableBrokers=my-topic-71698317-859778131}
2022-03-28 14:02:27 [ForkJoinPool-1-worker-15] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-af298d01-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-8f6c28cb-kafka-clients, testSendSimpleMessageTls=my-cluster-fbc845dd-kafka-clients, testConfigurationReflection=my-cluster-5137fff1-kafka-clients, testUserWithNameMoreThan64Chars=my-cluster-02e25de6-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-844045c3-kafka-clients, testCapacityFile=my-cluster-0531d5ce-kafka-clients, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-131d7a1d-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-8af61d52-kafka-clients, testTlsExternalUser=my-cluster-43f83aef-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-e353214b-kafka-clients, testCreateTopicAfterUnsupportedOperation=my-cluster-8e26464c-kafka-clients, testScramUserWithQuotas=my-cluster-6f653060-kafka-clients, testTopicModificationOfReplicationFactor=my-cluster-a1a53143-kafka-clients, testKafkaAdminTopicOperations=my-cluster-600e8332-kafka-clients, testConfigurationFileIsCreated=my-cluster-3244df0d-kafka-clients, testCreateTopicViaKafka=my-cluster-51bcec77-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-0c65d2dd-kafka-clients, testUpdateUser=my-cluster-c2c4d05a-kafka-clients, testThrottlingQuotasDeleteTopic=my-cluster-419d5c31-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-0444229e-kafka-clients, testTlsExternalUserWithQuotas=my-cluster-08d24b8e-kafka-clients, testTlsUserWithQuotas=my-cluster-9832c9ae-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-ac2cadd6-kafka-clients, testSendingMessagesToNonExistingTopic=my-cluster-abe3633a-kafka-clients, testReceiveSimpleMessageTls=my-cluster-ace1fa24-kafka-clients, testConfigurationPerformanceOptions=my-cluster-4994018d-kafka-clients, testUserTemplate=my-cluster-e0217138-kafka-clients, testMoreReplicasThanAvailableBrokers=my-cluster-203feca8-kafka-clients}
2022-03-28 14:02:27 [ForkJoinPool-1-worker-15] INFO  [TestSuiteNamespaceManager:163] Creating namespace:namespace-9 for test case:testPauseReconciliationInKafkaAndKafkaConnectWithConnector
2022-03-28 14:02:27 [ForkJoinPool-1-worker-15] INFO  [KubeClusterResource:156] Creating Namespace: namespace-9
2022-03-28 14:02:27 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace user-st -o yaml
2022-03-28 14:02:27 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 14:02:27 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (467616ms till timeout)
2022-03-28 14:02:27 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:444] KafkaTopic: topic-with-replication-to-change is in desired state: Ready
2022-03-28 14:02:27 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Namespace namespace-9
2022-03-28 14:02:27 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-9 -o json
2022-03-28 14:02:27 [ForkJoinPool-1-worker-3] INFO  [TopicST:482] Checking in KafkaTopic CR that topic my-topic-300455033-999140097 was created with expected settings
2022-03-28 14:02:27 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-03-28 14:02:28 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:02:28 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (195038ms till timeout)
2022-03-28 14:02:28 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:433] Wait for KafkaTopic: topic-with-replication-to-change will have desired state: NotReady
2022-03-28 14:02:28 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for KafkaTopic: topic-with-replication-to-change will have desired state: NotReady
2022-03-28 14:02:28 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:444] KafkaTopic: topic-with-replication-to-change is in desired state: NotReady
2022-03-28 14:02:28 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-9 -o json
2022-03-28 14:02:28 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 14:02:28 [ForkJoinPool-1-worker-15] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c32,c19",
            "openshift.io/sa.scc.supplemental-groups": "1001030000/10000",
            "openshift.io/sa.scc.uid-range": "1001030000/10000"
        },
        "creationTimestamp": "2022-03-28T14:02:23Z",
        "labels": {
            "kubernetes.io/metadata.name": "namespace-9"
        },
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T14:02:23Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:labels": {
                            ".": {},
                            "f:kubernetes.io/metadata.name": {}
                        }
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T14:02:23Z"
            }
        ],
        "name": "namespace-9",
        "resourceVersion": "97705",
        "uid": "9a08ec25-13e3-4027-a157-92675995b812"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 14:02:28 [ForkJoinPool-1-worker-15] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[], io.strimzi.test.logs.CollectorElement@c4c0ea0=[], io.strimzi.test.logs.CollectorElement@aec142ef=[], io.strimzi.test.logs.CollectorElement@24b97ba9=[namespace-9], io.strimzi.test.logs.CollectorElement@f851b6c3=[], io.strimzi.test.logs.CollectorElement@3881d5f2=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@e3067dd1=[], io.strimzi.test.logs.CollectorElement@5c7379cb=[], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[], io.strimzi.test.logs.CollectorElement@f4405b3b=[], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 14:02:28 [ForkJoinPool-1-worker-15] INFO  [KubeClusterResource:82] Client use Namespace: namespace-9
2022-03-28 14:02:28 [ForkJoinPool-1-worker-15] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-9, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 14:02:28 [ForkJoinPool-1-worker-15] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-9
2022-03-28 14:02:28 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:155] Create/Update Kafka my-cluster-131d7a1d in namespace namespace-9
2022-03-28 14:02:28 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:164] Using Namespace: namespace-9
2022-03-28 14:02:28 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (44398ms till timeout)
2022-03-28 14:02:28 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update KafkaTopic another-topic in namespace topic-st
2022-03-28 14:02:28 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-131d7a1d
2022-03-28 14:02:28 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:another-topic
2022-03-28 14:02:28 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:433] Wait for Kafka: my-cluster-131d7a1d will have desired state: Ready
2022-03-28 14:02:28 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Kafka: my-cluster-131d7a1d will have desired state: Ready
2022-03-28 14:02:28 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace user-st -o yaml
2022-03-28 14:02:28 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:433] Wait for KafkaTopic: another-topic will have desired state: Ready
2022-03-28 14:02:28 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for KafkaTopic: another-topic will have desired state: Ready
2022-03-28 14:02:28 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (839809ms till timeout)
2022-03-28 14:02:29 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:444] KafkaTopic: another-topic is in desired state: Ready
2022-03-28 14:02:29 [ForkJoinPool-1-worker-7] INFO  [TopicST:456] Checking topic topic-with-replication-to-change in Kafka
2022-03-28 14:02:29 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-03-28 14:02:29 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:02:29 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace user-st -o yaml
2022-03-28 14:02:29 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 14:02:29 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (466111ms till timeout)
2022-03-28 14:02:29 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (193843ms till timeout)
2022-03-28 14:02:29 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (43297ms till timeout)
2022-03-28 14:02:30 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (838710ms till timeout)
2022-03-28 14:02:30 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace user-st -o yaml
2022-03-28 14:02:30 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:02:30 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (192648ms till timeout)
2022-03-28 14:02:30 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace user-st -o yaml
2022-03-28 14:02:30 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 14:02:30 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (464674ms till timeout)
2022-03-28 14:02:30 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (42105ms till timeout)
2022-03-28 14:02:31 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (837608ms till timeout)
2022-03-28 14:02:31 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:02:31 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Command: oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-03-28 14:02:31 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 14:02:31 [ForkJoinPool-1-worker-3] INFO  [TopicST:121] Editing topic via Kafka, settings to partitions 5
2022-03-28 14:02:31 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --bootstrap-server localhost:9092 --alter --topic my-topic-300455033-999140097 --partitions 5
2022-03-28 14:02:31 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (191452ms till timeout)
2022-03-28 14:02:31 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace user-st -o yaml
2022-03-28 14:02:32 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (40912ms till timeout)
2022-03-28 14:02:32 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace user-st -o yaml
2022-03-28 14:02:32 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 14:02:32 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (463211ms till timeout)
2022-03-28 14:02:32 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (836503ms till timeout)
2022-03-28 14:02:32 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:02:32 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (190257ms till timeout)
2022-03-28 14:02:33 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (39717ms till timeout)
2022-03-28 14:02:33 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace user-st -o yaml
2022-03-28 14:02:33 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (835396ms till timeout)
2022-03-28 14:02:33 [ForkJoinPool-1-worker-7] INFO  [Exec:417] Command: oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-03-28 14:02:33 [ForkJoinPool-1-worker-7] INFO  [Exec:417] Return code: 0
2022-03-28 14:02:33 [ForkJoinPool-1-worker-7] INFO  [TopicST:461] Checking in KafkaTopic CR that topic topic-with-replication-to-change exists
2022-03-28 14:02:33 [ForkJoinPool-1-worker-7] INFO  [TopicST:456] Checking topic another-topic in Kafka
2022-03-28 14:02:33 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-03-28 14:02:33 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace user-st -o yaml
2022-03-28 14:02:33 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 14:02:33 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (461747ms till timeout)
2022-03-28 14:02:33 [ForkJoinPool-1-worker-1] INFO  [InternalKafkaClient:160] Consumer finished correctly: true
2022-03-28 14:02:33 [ForkJoinPool-1-worker-1] INFO  [InternalKafkaClient:163] Consumer consumed 100 messages
2022-03-28 14:02:33 [ForkJoinPool-1-worker-1] INFO  [TopicST:341] Checking if my-topic-441360312-1858428966 is on topic list
2022-03-28 14:02:33 [ForkJoinPool-1-worker-1] INFO  [TopicST:456] Checking topic my-topic-441360312-1858428966 in Kafka
2022-03-28 14:02:33 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-03-28 14:02:34 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:02:34 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (189061ms till timeout)
2022-03-28 14:02:34 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (38521ms till timeout)
2022-03-28 14:02:34 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (834297ms till timeout)
2022-03-28 14:02:34 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace user-st -o yaml
2022-03-28 14:02:35 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace user-st -o yaml
2022-03-28 14:02:35 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 14:02:35 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (460299ms till timeout)
2022-03-28 14:02:35 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:02:35 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Command: oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --bootstrap-server localhost:9092 --alter --topic my-topic-300455033-999140097 --partitions 5
2022-03-28 14:02:35 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 14:02:35 [ForkJoinPool-1-worker-3] DEBUG [TopicST:124] Topic my-topic-300455033-999140097 updated from 3 to 5 partitions
2022-03-28 14:02:35 [ForkJoinPool-1-worker-3] INFO  [KafkaTopicUtils:124] Waiting for KafkaTopic change my-topic-300455033-999140097
2022-03-28 14:02:35 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for KafkaTopic change my-topic-300455033-999140097
2022-03-28 14:02:35 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (187865ms till timeout)
2022-03-28 14:02:35 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Describing topic my-topic-300455033-999140097 using pod CLI
2022-03-28 14:02:35 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --bootstrap-server localhost:9092 --describe --topic my-topic-300455033-999140097
2022-03-28 14:02:35 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (37327ms till timeout)
2022-03-28 14:02:35 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (833199ms till timeout)
2022-03-28 14:02:36 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace user-st -o yaml
2022-03-28 14:02:36 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:02:36 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (186667ms till timeout)
2022-03-28 14:02:36 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace user-st -o yaml
2022-03-28 14:02:36 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 14:02:36 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (458822ms till timeout)
2022-03-28 14:02:36 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (36134ms till timeout)
2022-03-28 14:02:36 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (832017ms till timeout)
2022-03-28 14:02:37 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:02:37 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace user-st -o yaml
2022-03-28 14:02:37 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (185467ms till timeout)
2022-03-28 14:02:37 [ForkJoinPool-1-worker-7] INFO  [Exec:417] Command: oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-03-28 14:02:37 [ForkJoinPool-1-worker-7] INFO  [Exec:417] Return code: 0
2022-03-28 14:02:37 [ForkJoinPool-1-worker-7] INFO  [TopicST:461] Checking in KafkaTopic CR that topic another-topic exists
2022-03-28 14:02:37 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace topic-st delete kafkatopic topic-with-replication-to-change
2022-03-28 14:02:37 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (34942ms till timeout)
2022-03-28 14:02:37 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (830826ms till timeout)
2022-03-28 14:02:38 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace user-st -o yaml
2022-03-28 14:02:38 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 14:02:38 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (457307ms till timeout)
2022-03-28 14:02:38 [ForkJoinPool-1-worker-1] INFO  [Exec:417] Command: oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-03-28 14:02:38 [ForkJoinPool-1-worker-1] INFO  [Exec:417] Return code: 0
2022-03-28 14:02:38 [ForkJoinPool-1-worker-1] INFO  [KafkaTopicUtils:78] Waiting for KafkaTopic my-topic-441360312-1858428966 creation 
2022-03-28 14:02:38 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for KafkaTopic creation my-topic-441360312-1858428966
2022-03-28 14:02:38 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace topic-st delete kafkatopic topic-with-replication-to-change
2022-03-28 14:02:38 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 14:02:38 [ForkJoinPool-1-worker-7] INFO  [KafkaTopicUtils:104] Waiting for KafkaTopic topic-with-replication-to-change deletion
2022-03-28 14:02:38 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for KafkaTopic deletion topic-with-replication-to-change
2022-03-28 14:02:38 [ForkJoinPool-1-worker-1] INFO  [TopicST:353] Topic successfully created
2022-03-28 14:02:38 [ForkJoinPool-1-worker-1] DEBUG [AbstractST:674] ============================================================================
2022-03-28 14:02:38 [ForkJoinPool-1-worker-1] DEBUG [AbstractST:675] [operators.topic.TopicST - After Each] - Clean up after test
2022-03-28 14:02:38 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:344] ############################################################################
2022-03-28 14:02:38 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:348] Delete all resources for testSendingMessagesToNonExistingTopic
2022-03-28 14:02:38 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:241] Delete of Deployment topic-cluster-name-kafka-clients in namespace topic-st
2022-03-28 14:02:38 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace topic-st delete kafkatopic another-topic
2022-03-28 14:02:38 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:topic-cluster-name-kafka-clients
2022-03-28 14:02:38 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:02:38 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:topic-cluster-name-kafka-clients not ready, will try again in 10000 ms (479611ms till timeout)
2022-03-28 14:02:39 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace topic-st delete kafkatopic another-topic
2022-03-28 14:02:39 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 14:02:39 [ForkJoinPool-1-worker-7] INFO  [KafkaTopicUtils:104] Waiting for KafkaTopic another-topic deletion
2022-03-28 14:02:39 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for KafkaTopic deletion another-topic
2022-03-28 14:02:39 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (184141ms till timeout)
2022-03-28 14:02:39 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace user-st -o yaml
2022-03-28 14:02:39 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:674] ============================================================================
2022-03-28 14:02:39 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:675] [operators.topic.TopicST - After Each] - Clean up after test
2022-03-28 14:02:39 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:344] ############################################################################
2022-03-28 14:02:39 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:348] Delete all resources for testCreateTopicAfterUnsupportedOperation
2022-03-28 14:02:39 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:241] Delete of KafkaTopic another-topic in namespace topic-st
2022-03-28 14:02:39 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (829676ms till timeout)
2022-03-28 14:02:39 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (33783ms till timeout)
2022-03-28 14:02:39 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:another-topic
2022-03-28 14:02:39 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:241] Delete of KafkaTopic topic-with-replication-to-change in namespace topic-st
2022-03-28 14:02:39 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:topic-with-replication-to-change
2022-03-28 14:02:39 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Command: oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --bootstrap-server localhost:9092 --describe --topic my-topic-300455033-999140097
2022-03-28 14:02:39 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 14:02:39 [ForkJoinPool-1-worker-3] INFO  [TopicST:470] Checking topic my-topic-300455033-999140097 in Kafka topic-cluster-name
2022-03-28 14:02:39 [ForkJoinPool-1-worker-3] DEBUG [TopicST:471] Topic my-topic-300455033-999140097 info: [Topic:my-topic-300455033-999140097, TopicId:eEM5M0taT2-EGeFXJwtDHw, PartitionCount:5, ReplicationFactor:3, Configs:min.insync.replicas=2,message.format.version=3.0-IV1, Topic:my-topic-300455033-999140097, Partition:0, Leader:0, Replicas:0,1,2, Isr:0,1,2, Topic:my-topic-300455033-999140097, Partition:1, Leader:2, Replicas:2,0,1, Isr:2,0,1, Topic:my-topic-300455033-999140097, Partition:2, Leader:1, Replicas:1,2,0, Isr:1,2,0, Topic:my-topic-300455033-999140097, Partition:3, Leader:0, Replicas:0,2,1, Isr:0,2,1, Topic:my-topic-300455033-999140097, Partition:4, Leader:1, Replicas:1,0,2, Isr:1,0,2]
2022-03-28 14:02:39 [ForkJoinPool-1-worker-3] DEBUG [AbstractST:674] ============================================================================
2022-03-28 14:02:39 [ForkJoinPool-1-worker-3] DEBUG [AbstractST:675] [operators.topic.TopicST - After Each] - Clean up after test
2022-03-28 14:02:39 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:344] ############################################################################
2022-03-28 14:02:39 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:346] In context testCreateTopicViaKafka is everything deleted.
2022-03-28 14:02:39 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:369] ############################################################################
2022-03-28 14:02:39 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:267] testCreateTopicViaKafka - Notifies waiting test cases:[testCapacityFile, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testThrottlingQuotasCreateAlterPartitions, testSendSimpleMessageTls, testReceiveSimpleMessageTlsScramSha, testSendSimpleMessageTlsScramSha, testReceiveSimpleMessageTls, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testConfigurationFileIsCreated, testPauseReconciliationInKafkaRebalanceAndTopic, testTopicModificationOfReplicationFactor, testMoreReplicasThanAvailableBrokers, testCreateTopicViaKafka, testCreateTopicAfterUnsupportedOperation, testPauseReconciliationInKafkaAndKafkaConnectWithConnector] to and randomly select one to start execution
2022-03-28 14:02:39 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:93] [operators.topic.TopicST] - Removing parallel test: testCreateTopicViaKafka
2022-03-28 14:02:39 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:97] [operators.topic.TopicST] - Parallel test count: 5
2022-03-28 14:02:39 [ForkJoinPool-1-worker-3] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.topic.TopicST.testCreateTopicViaKafka-FINISHED
2022-03-28 14:02:39 [ForkJoinPool-1-worker-3] INFO  [TestSeparator:30] ############################################################################
2022-03-28 14:02:39 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:369] ############################################################################
2022-03-28 14:02:39 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:267] testCreateTopicAfterUnsupportedOperation - Notifies waiting test cases:[testCapacityFile, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testThrottlingQuotasCreateAlterPartitions, testSendSimpleMessageTls, testReceiveSimpleMessageTlsScramSha, testSendSimpleMessageTlsScramSha, testReceiveSimpleMessageTls, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testConfigurationFileIsCreated, testPauseReconciliationInKafkaRebalanceAndTopic, testTopicModificationOfReplicationFactor, testMoreReplicasThanAvailableBrokers, testCreateTopicViaKafka, testCreateTopicAfterUnsupportedOperation, testPauseReconciliationInKafkaAndKafkaConnectWithConnector] to and randomly select one to start execution
2022-03-28 14:02:39 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:93] [operators.topic.TopicST] - Removing parallel test: testCreateTopicAfterUnsupportedOperation
2022-03-28 14:02:39 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:97] [operators.topic.TopicST] - Parallel test count: 4
2022-03-28 14:02:39 [ForkJoinPool-1-worker-7] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.topic.TopicST.testCreateTopicAfterUnsupportedOperation-FINISHED
2022-03-28 14:02:39 [ForkJoinPool-1-worker-7] INFO  [TestSeparator:30] ############################################################################
2022-03-28 14:02:39 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace user-st -o yaml
2022-03-28 14:02:39 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 14:02:39 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (455864ms till timeout)
2022-03-28 14:02:40 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:02:40 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (182946ms till timeout)
2022-03-28 14:02:40 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (828486ms till timeout)
2022-03-28 14:02:40 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (32507ms till timeout)
2022-03-28 14:02:40 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace user-st -o yaml
2022-03-28 14:02:41 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace user-st -o yaml
2022-03-28 14:02:41 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 14:02:41 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (454382ms till timeout)
2022-03-28 14:02:41 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:02:41 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (827362ms till timeout)
2022-03-28 14:02:41 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (181630ms till timeout)
2022-03-28 14:02:41 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (31284ms till timeout)
2022-03-28 14:02:42 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace user-st -o yaml
2022-03-28 14:02:42 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace user-st -o yaml
2022-03-28 14:02:42 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 14:02:42 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (452921ms till timeout)
2022-03-28 14:02:42 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (826264ms till timeout)
2022-03-28 14:02:42 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:02:42 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (30087ms till timeout)
2022-03-28 14:02:42 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (180337ms till timeout)
2022-03-28 14:02:43 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace user-st -o yaml
2022-03-28 14:02:43 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (825165ms till timeout)
2022-03-28 14:02:44 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace user-st -o yaml
2022-03-28 14:02:44 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 14:02:44 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (451434ms till timeout)
2022-03-28 14:02:44 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (28895ms till timeout)
2022-03-28 14:02:44 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:02:44 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (179047ms till timeout)
2022-03-28 14:02:44 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (824066ms till timeout)
2022-03-28 14:02:45 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace user-st -o yaml
2022-03-28 14:02:45 [ForkJoinPool-1-worker-5] INFO  [PodUtils:189] Message All topics created found in create-admin-my-cluster-600e8332-kafka-clients-tlvgp log
2022-03-28 14:02:45 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:02:45 [ForkJoinPool-1-worker-5] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment create-admin-my-cluster-600e8332-kafka-clients deletion
2022-03-28 14:02:45 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for ReplicaSet create-admin-my-cluster-600e8332-kafka-clients to be deleted
2022-03-28 14:02:45 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (177749ms till timeout)
2022-03-28 14:02:45 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-8 get Namespace user-st -o yaml
2022-03-28 14:02:45 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 1
2022-03-28 14:02:45 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 14:02:45 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Error from server (NotFound): namespaces "user-st" not found
2022-03-28 14:02:45 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] ======STDERR END======
2022-03-28 14:02:45 [ForkJoinPool-1-worker-13] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[], io.strimzi.test.logs.CollectorElement@c4c0ea0=[], io.strimzi.test.logs.CollectorElement@aec142ef=[], io.strimzi.test.logs.CollectorElement@24b97ba9=[namespace-9], io.strimzi.test.logs.CollectorElement@f851b6c3=[], io.strimzi.test.logs.CollectorElement@3881d5f2=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@e3067dd1=[], io.strimzi.test.logs.CollectorElement@5c7379cb=[], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[], io.strimzi.test.logs.CollectorElement@f4405b3b=[], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 14:02:45 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:254] UserST - Notifies waiting test suites:[UserST, ThrottlingQuotaST, TopicST, CruiseControlConfigurationST, HttpBridgeScramShaST, HttpBridgeTlsST] to and randomly select one to start execution
2022-03-28 14:02:45 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:85] [operators.user.UserST] - Removing parallel suite: UserST
2022-03-28 14:02:45 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:89] [operators.user.UserST] - Parallel suites count: 3
[INFO] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 835.456 s - in io.strimzi.systemtest.operators.user.UserST
2022-03-28 14:02:45 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] ReplicaSet create-admin-my-cluster-600e8332-kafka-clients to be deleted not ready, will try again in 5000 ms (179808ms till timeout)
2022-03-28 14:02:45 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (822967ms till timeout)
2022-03-28 14:02:46 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:02:46 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (176552ms till timeout)
2022-03-28 14:02:46 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (821868ms till timeout)
2022-03-28 14:02:47 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:02:47 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (175348ms till timeout)
2022-03-28 14:02:48 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (820771ms till timeout)
2022-03-28 14:02:48 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:02:49 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (174153ms till timeout)
2022-03-28 14:02:49 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (819594ms till timeout)
2022-03-28 14:02:49 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:topic-cluster-name-kafka-clients not ready, will try again in 10000 ms (469148ms till timeout)
2022-03-28 14:02:50 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:02:50 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (172959ms till timeout)
2022-03-28 14:02:50 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (818496ms till timeout)
2022-03-28 14:02:50 [ForkJoinPool-1-worker-5] DEBUG [JobUtils:40] Job create-admin-my-cluster-600e8332-kafka-clients was deleted
2022-03-28 14:02:50 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:155] Create/Update Job list-admin-my-cluster-600e8332-kafka-clients in namespace throttling-quota-st
2022-03-28 14:02:50 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:list-admin-my-cluster-600e8332-kafka-clients
2022-03-28 14:02:50 [ForkJoinPool-1-worker-5] INFO  [JobUtils:81] Waiting for job: list-admin-my-cluster-600e8332-kafka-clients will be in active state
2022-03-28 14:02:50 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 14:02:51 [ForkJoinPool-1-worker-5] INFO  [PodUtils:186] Waiting for message will be in the log
2022-03-28 14:02:51 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Waiting for message will be in the log
2022-03-28 14:02:51 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (119900ms till timeout)
2022-03-28 14:02:51 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:02:51 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (817399ms till timeout)
2022-03-28 14:02:51 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (171663ms till timeout)
2022-03-28 14:02:52 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (118800ms till timeout)
2022-03-28 14:02:52 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (816300ms till timeout)
2022-03-28 14:02:52 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:02:52 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (170467ms till timeout)
2022-03-28 14:02:53 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (117678ms till timeout)
2022-03-28 14:02:53 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (815200ms till timeout)
2022-03-28 14:02:53 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:02:53 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (169271ms till timeout)
2022-03-28 14:02:54 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (116577ms till timeout)
2022-03-28 14:02:54 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (814102ms till timeout)
2022-03-28 14:02:54 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:02:55 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (168078ms till timeout)
2022-03-28 14:02:55 [ForkJoinPool-1-worker-5] INFO  [PodUtils:189] Message quota-topic-test-simple-99 found in list-admin-my-cluster-600e8332-kafka-clients-qcn7x log
2022-03-28 14:02:55 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (813005ms till timeout)
2022-03-28 14:02:55 [ForkJoinPool-1-worker-5] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment list-admin-my-cluster-600e8332-kafka-clients deletion
2022-03-28 14:02:55 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for ReplicaSet list-admin-my-cluster-600e8332-kafka-clients to be deleted
2022-03-28 14:02:55 [ForkJoinPool-1-worker-5] DEBUG [JobUtils:40] Job list-admin-my-cluster-600e8332-kafka-clients was deleted
2022-03-28 14:02:55 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:155] Create/Update Job delete-admin-my-cluster-600e8332-kafka-clients in namespace throttling-quota-st
2022-03-28 14:02:56 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:delete-admin-my-cluster-600e8332-kafka-clients
2022-03-28 14:02:56 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:02:56 [ForkJoinPool-1-worker-5] INFO  [JobUtils:81] Waiting for job: delete-admin-my-cluster-600e8332-kafka-clients will be in active state
2022-03-28 14:02:56 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 14:02:56 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (166700ms till timeout)
2022-03-28 14:02:56 [ForkJoinPool-1-worker-5] INFO  [PodUtils:186] Waiting for message will be in the log
2022-03-28 14:02:56 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Waiting for message will be in the log
2022-03-28 14:02:56 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (119900ms till timeout)
2022-03-28 14:02:56 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (811906ms till timeout)
2022-03-28 14:02:57 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:02:57 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (165501ms till timeout)
2022-03-28 14:02:57 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (118801ms till timeout)
2022-03-28 14:02:58 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (810805ms till timeout)
2022-03-28 14:02:58 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:02:58 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (164304ms till timeout)
2022-03-28 14:02:58 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (117698ms till timeout)
2022-03-28 14:02:59 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (809706ms till timeout)
2022-03-28 14:02:59 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:topic-cluster-name-kafka-clients not ready, will try again in 10000 ms (458762ms till timeout)
2022-03-28 14:02:59 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:03:00 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (163109ms till timeout)
2022-03-28 14:03:00 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (116504ms till timeout)
2022-03-28 14:03:00 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (808551ms till timeout)
2022-03-28 14:03:01 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:03:01 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (161907ms till timeout)
2022-03-28 14:03:01 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (807445ms till timeout)
2022-03-28 14:03:01 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (115209ms till timeout)
2022-03-28 14:03:02 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:03:02 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (806345ms till timeout)
2022-03-28 14:03:02 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (160612ms till timeout)
2022-03-28 14:03:02 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (114008ms till timeout)
2022-03-28 14:03:03 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (805245ms till timeout)
2022-03-28 14:03:03 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:03:03 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (159416ms till timeout)
2022-03-28 14:03:03 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (112719ms till timeout)
2022-03-28 14:03:04 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (804145ms till timeout)
2022-03-28 14:03:04 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:03:04 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (158222ms till timeout)
2022-03-28 14:03:05 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (111525ms till timeout)
2022-03-28 14:03:05 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (803046ms till timeout)
2022-03-28 14:03:06 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:03:06 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (157014ms till timeout)
2022-03-28 14:03:06 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (110319ms till timeout)
2022-03-28 14:03:06 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (801947ms till timeout)
2022-03-28 14:03:07 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:03:07 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (155820ms till timeout)
2022-03-28 14:03:07 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (109124ms till timeout)
2022-03-28 14:03:07 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (800849ms till timeout)
2022-03-28 14:03:08 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:03:08 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (154621ms till timeout)
2022-03-28 14:03:08 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (107925ms till timeout)
2022-03-28 14:03:09 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (799714ms till timeout)
2022-03-28 14:03:09 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:03:09 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (153427ms till timeout)
2022-03-28 14:03:09 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (106731ms till timeout)
2022-03-28 14:03:10 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (798588ms till timeout)
2022-03-28 14:03:10 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:369] ############################################################################
2022-03-28 14:03:10 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:267] testSendingMessagesToNonExistingTopic - Notifies waiting test cases:[testCapacityFile, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testThrottlingQuotasCreateAlterPartitions, testSendSimpleMessageTls, testReceiveSimpleMessageTlsScramSha, testSendSimpleMessageTlsScramSha, testReceiveSimpleMessageTls, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testConfigurationFileIsCreated, testPauseReconciliationInKafkaRebalanceAndTopic, testTopicModificationOfReplicationFactor, testMoreReplicasThanAvailableBrokers, testCreateTopicViaKafka, testCreateTopicAfterUnsupportedOperation, testPauseReconciliationInKafkaAndKafkaConnectWithConnector] to and randomly select one to start execution
2022-03-28 14:03:10 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:93] [operators.topic.TopicST] - Removing parallel test: testSendingMessagesToNonExistingTopic
2022-03-28 14:03:10 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:97] [operators.topic.TopicST] - Parallel test count: 3
2022-03-28 14:03:10 [ForkJoinPool-1-worker-1] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.topic.TopicST.testSendingMessagesToNonExistingTopic-FINISHED
2022-03-28 14:03:10 [ForkJoinPool-1-worker-1] INFO  [TestSeparator:30] ############################################################################
2022-03-28 14:03:10 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:03:10 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (152233ms till timeout)
2022-03-28 14:03:11 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (105627ms till timeout)
2022-03-28 14:03:11 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (797490ms till timeout)
2022-03-28 14:03:12 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:03:12 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (104429ms till timeout)
2022-03-28 14:03:12 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (150931ms till timeout)
2022-03-28 14:03:12 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (796393ms till timeout)
2022-03-28 14:03:13 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (103237ms till timeout)
2022-03-28 14:03:13 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:03:13 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (795283ms till timeout)
2022-03-28 14:03:13 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (149551ms till timeout)
2022-03-28 14:03:14 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (102045ms till timeout)
2022-03-28 14:03:14 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (794181ms till timeout)
2022-03-28 14:03:14 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:03:14 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (148357ms till timeout)
2022-03-28 14:03:15 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (100851ms till timeout)
2022-03-28 14:03:15 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (792987ms till timeout)
2022-03-28 14:03:15 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:03:16 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (147163ms till timeout)
2022-03-28 14:03:17 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (99657ms till timeout)
2022-03-28 14:03:17 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (791794ms till timeout)
2022-03-28 14:03:17 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:03:17 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (145968ms till timeout)
2022-03-28 14:03:18 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (98465ms till timeout)
2022-03-28 14:03:18 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (790602ms till timeout)
2022-03-28 14:03:18 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:03:18 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (144775ms till timeout)
2022-03-28 14:03:19 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (97272ms till timeout)
2022-03-28 14:03:19 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (789407ms till timeout)
2022-03-28 14:03:19 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:03:19 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (143577ms till timeout)
2022-03-28 14:03:20 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (96079ms till timeout)
2022-03-28 14:03:20 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (788214ms till timeout)
2022-03-28 14:03:20 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:03:20 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (142383ms till timeout)
2022-03-28 14:03:21 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (94886ms till timeout)
2022-03-28 14:03:21 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (787024ms till timeout)
2022-03-28 14:03:21 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:03:21 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (141189ms till timeout)
2022-03-28 14:03:22 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (93693ms till timeout)
2022-03-28 14:03:23 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (785635ms till timeout)
2022-03-28 14:03:23 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:03:23 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (139903ms till timeout)
2022-03-28 14:03:24 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (92501ms till timeout)
2022-03-28 14:03:24 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (784536ms till timeout)
2022-03-28 14:03:24 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:03:24 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (138705ms till timeout)
2022-03-28 14:03:25 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (91309ms till timeout)
2022-03-28 14:03:25 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (783436ms till timeout)
2022-03-28 14:03:25 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:03:25 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (137511ms till timeout)
2022-03-28 14:03:26 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (90112ms till timeout)
2022-03-28 14:03:26 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (782249ms till timeout)
2022-03-28 14:03:26 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:03:26 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (136315ms till timeout)
2022-03-28 14:03:27 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (88917ms till timeout)
2022-03-28 14:03:27 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (781055ms till timeout)
2022-03-28 14:03:27 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:03:28 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (135120ms till timeout)
2022-03-28 14:03:28 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (87724ms till timeout)
2022-03-28 14:03:28 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (779860ms till timeout)
2022-03-28 14:03:29 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:03:29 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (133926ms till timeout)
2022-03-28 14:03:30 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (86531ms till timeout)
2022-03-28 14:03:30 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (778666ms till timeout)
2022-03-28 14:03:30 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:03:30 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (132729ms till timeout)
2022-03-28 14:03:31 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (85338ms till timeout)
2022-03-28 14:03:31 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (777474ms till timeout)
2022-03-28 14:03:31 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:03:31 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (131533ms till timeout)
2022-03-28 14:03:32 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (84144ms till timeout)
2022-03-28 14:03:32 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (776274ms till timeout)
2022-03-28 14:03:32 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:03:32 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (130338ms till timeout)
2022-03-28 14:03:33 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (82952ms till timeout)
2022-03-28 14:03:33 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (775087ms till timeout)
2022-03-28 14:03:33 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:03:34 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (129143ms till timeout)
2022-03-28 14:03:34 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (81758ms till timeout)
2022-03-28 14:03:34 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (773894ms till timeout)
2022-03-28 14:03:35 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:03:35 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (127949ms till timeout)
2022-03-28 14:03:36 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (80566ms till timeout)
2022-03-28 14:03:36 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (772702ms till timeout)
2022-03-28 14:03:36 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:03:36 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (126752ms till timeout)
2022-03-28 14:03:37 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (79373ms till timeout)
2022-03-28 14:03:37 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (771510ms till timeout)
2022-03-28 14:03:37 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:03:37 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (125559ms till timeout)
2022-03-28 14:03:38 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (78182ms till timeout)
2022-03-28 14:03:38 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (770318ms till timeout)
2022-03-28 14:03:38 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:03:38 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (124364ms till timeout)
2022-03-28 14:03:39 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (76989ms till timeout)
2022-03-28 14:03:39 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (769125ms till timeout)
2022-03-28 14:03:39 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:03:40 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (123170ms till timeout)
2022-03-28 14:03:40 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (75796ms till timeout)
2022-03-28 14:03:40 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (767933ms till timeout)
2022-03-28 14:03:41 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:03:41 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (121975ms till timeout)
2022-03-28 14:03:42 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (74603ms till timeout)
2022-03-28 14:03:42 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (766739ms till timeout)
2022-03-28 14:03:42 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=null, completedIndexes=null, completionTime=2022-03-28T14:03:37Z, conditions=[JobCondition(lastProbeTime=2022-03-28T14:03:37Z, lastTransitionTime=2022-03-28T14:03:37Z, message=null, reason=null, status=True, type=Complete, additionalProperties={})], failed=null, ready=null, startTime=2022-03-28T14:01:58Z, succeeded=1, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:03:42 [ForkJoinPool-1-worker-9] INFO  [PodUtils:186] Waiting for message will be in the log
2022-03-28 14:03:42 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Waiting for message will be in the log
2022-03-28 14:03:42 [ForkJoinPool-1-worker-9] INFO  [PodUtils:189] Message All topics created found in create-admin-my-cluster-419d5c31-kafka-clients-xj7ph log
2022-03-28 14:03:42 [ForkJoinPool-1-worker-9] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment create-admin-my-cluster-419d5c31-kafka-clients deletion
2022-03-28 14:03:42 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for ReplicaSet create-admin-my-cluster-419d5c31-kafka-clients to be deleted
2022-03-28 14:03:42 [ForkJoinPool-1-worker-9] DEBUG [JobUtils:40] Job create-admin-my-cluster-419d5c31-kafka-clients was deleted
2022-03-28 14:03:42 [ForkJoinPool-1-worker-9] INFO  [ThrottlingQuotaST:112] Executing 3/5 iteration.
2022-03-28 14:03:42 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:155] Create/Update Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st
2022-03-28 14:03:42 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:create-admin-my-cluster-419d5c31-kafka-clients
2022-03-28 14:03:42 [ForkJoinPool-1-worker-9] INFO  [JobUtils:81] Waiting for job: create-admin-my-cluster-419d5c31-kafka-clients will be in active state
2022-03-28 14:03:42 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 14:03:43 [ForkJoinPool-1-worker-9] INFO  [ClientUtils:76] Waiting for producer/consumer:create-admin-my-cluster-419d5c31-kafka-clients to finished
2022-03-28 14:03:43 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for job finished
2022-03-28 14:03:43 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (765613ms till timeout)
2022-03-28 14:03:43 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (73380ms till timeout)
2022-03-28 14:03:43 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:03:43 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (219711ms till timeout)
2022-03-28 14:03:44 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (764512ms till timeout)
2022-03-28 14:03:44 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (72181ms till timeout)
2022-03-28 14:03:44 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:03:44 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (218512ms till timeout)
2022-03-28 14:03:45 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (763413ms till timeout)
2022-03-28 14:03:45 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (70988ms till timeout)
2022-03-28 14:03:45 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:03:45 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (217317ms till timeout)
2022-03-28 14:03:46 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (762313ms till timeout)
2022-03-28 14:03:46 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (69787ms till timeout)
2022-03-28 14:03:46 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:03:46 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (216119ms till timeout)
2022-03-28 14:03:47 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (761213ms till timeout)
2022-03-28 14:03:48 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (68592ms till timeout)
2022-03-28 14:03:48 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:03:48 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (214923ms till timeout)
2022-03-28 14:03:48 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (760114ms till timeout)
2022-03-28 14:03:49 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (67400ms till timeout)
2022-03-28 14:03:49 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:03:49 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (213728ms till timeout)
2022-03-28 14:03:49 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (759015ms till timeout)
2022-03-28 14:03:50 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (66205ms till timeout)
2022-03-28 14:03:50 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:03:50 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (212532ms till timeout)
2022-03-28 14:03:50 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (757917ms till timeout)
2022-03-28 14:03:51 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (65013ms till timeout)
2022-03-28 14:03:51 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:03:51 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (211331ms till timeout)
2022-03-28 14:03:51 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (756819ms till timeout)
2022-03-28 14:03:52 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (63814ms till timeout)
2022-03-28 14:03:52 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:03:52 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (210136ms till timeout)
2022-03-28 14:03:53 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (755721ms till timeout)
2022-03-28 14:03:54 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (62621ms till timeout)
2022-03-28 14:03:54 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:03:54 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (208940ms till timeout)
2022-03-28 14:03:54 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (754555ms till timeout)
2022-03-28 14:03:55 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (61428ms till timeout)
2022-03-28 14:03:55 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:03:55 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (753455ms till timeout)
2022-03-28 14:03:55 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (207649ms till timeout)
2022-03-28 14:03:56 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (60224ms till timeout)
2022-03-28 14:03:56 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (752349ms till timeout)
2022-03-28 14:03:56 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:03:56 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (206445ms till timeout)
2022-03-28 14:03:57 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (59031ms till timeout)
2022-03-28 14:03:57 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (751167ms till timeout)
2022-03-28 14:03:57 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:03:57 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (205251ms till timeout)
2022-03-28 14:03:58 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (57839ms till timeout)
2022-03-28 14:03:58 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (749974ms till timeout)
2022-03-28 14:03:58 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:03:59 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (204058ms till timeout)
2022-03-28 14:04:00 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (56647ms till timeout)
2022-03-28 14:04:00 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (748783ms till timeout)
2022-03-28 14:04:00 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:04:00 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (202864ms till timeout)
2022-03-28 14:04:01 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (55455ms till timeout)
2022-03-28 14:04:01 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (747591ms till timeout)
2022-03-28 14:04:01 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:04:01 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (201669ms till timeout)
2022-03-28 14:04:02 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (54261ms till timeout)
2022-03-28 14:04:02 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (746397ms till timeout)
2022-03-28 14:04:02 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:04:02 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (200474ms till timeout)
2022-03-28 14:04:03 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (53068ms till timeout)
2022-03-28 14:04:03 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (745204ms till timeout)
2022-03-28 14:04:03 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:04:03 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (199280ms till timeout)
2022-03-28 14:04:04 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (51875ms till timeout)
2022-03-28 14:04:04 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (744011ms till timeout)
2022-03-28 14:04:04 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:04:05 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (198085ms till timeout)
2022-03-28 14:04:05 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (50681ms till timeout)
2022-03-28 14:04:05 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (742818ms till timeout)
2022-03-28 14:04:06 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:04:06 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (196891ms till timeout)
2022-03-28 14:04:07 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (49488ms till timeout)
2022-03-28 14:04:07 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (741624ms till timeout)
2022-03-28 14:04:07 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:04:07 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (195697ms till timeout)
2022-03-28 14:04:08 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (48296ms till timeout)
2022-03-28 14:04:08 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (740433ms till timeout)
2022-03-28 14:04:08 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:04:08 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (194502ms till timeout)
2022-03-28 14:04:09 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (47097ms till timeout)
2022-03-28 14:04:09 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (739233ms till timeout)
2022-03-28 14:04:09 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:04:09 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (193307ms till timeout)
2022-03-28 14:04:10 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (45903ms till timeout)
2022-03-28 14:04:10 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:444] Kafka: my-cluster-131d7a1d is in desired state: Ready
2022-03-28 14:04:10 [ForkJoinPool-1-worker-15] INFO  [ReconciliationST:80] Adding pause annotation into Kafka resource and also scaling replicas to 4, new pod should not appear
2022-03-28 14:04:10 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:04:11 [ForkJoinPool-1-worker-15] INFO  [ReconciliationST:86] Kafka should contain status with ReconciliationPaused
2022-03-28 14:04:11 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (191945ms till timeout)
2022-03-28 14:04:11 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:433] Wait for Kafka: my-cluster-131d7a1d will have desired state: ReconciliationPaused
2022-03-28 14:04:11 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Kafka: my-cluster-131d7a1d will have desired state: ReconciliationPaused
2022-03-28 14:04:11 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: ReconciliationPaused not ready, will try again in 1000 ms (839902ms till timeout)
2022-03-28 14:04:11 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (44711ms till timeout)
2022-03-28 14:04:12 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:04:12 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (190750ms till timeout)
2022-03-28 14:04:12 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-131d7a1d will have desired state: ReconciliationPaused not ready, will try again in 1000 ms (838804ms till timeout)
2022-03-28 14:04:13 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (43519ms till timeout)
2022-03-28 14:04:13 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:04:13 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:444] Kafka: my-cluster-131d7a1d is in desired state: ReconciliationPaused
2022-03-28 14:04:13 [ForkJoinPool-1-worker-15] INFO  [PodUtils:209] Wait until Pod my-cluster-131d7a1d-kafka will have stable 3 replicas
2022-03-28 14:04:13 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for  Podmy-cluster-131d7a1d-kafka will have 3 replicas
2022-03-28 14:04:13 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (189460ms till timeout)
2022-03-28 14:04:14 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 19 polls
2022-03-28 14:04:14 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-131d7a1d-kafka will have 3 replicas not ready, will try again in 1000 ms (179532ms till timeout)
2022-03-28 14:04:14 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (42419ms till timeout)
2022-03-28 14:04:14 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:04:14 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (188264ms till timeout)
2022-03-28 14:04:15 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 18 polls
2022-03-28 14:04:15 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-131d7a1d-kafka will have 3 replicas not ready, will try again in 1000 ms (178246ms till timeout)
2022-03-28 14:04:15 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (41280ms till timeout)
2022-03-28 14:04:15 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:04:16 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (187070ms till timeout)
2022-03-28 14:04:16 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 17 polls
2022-03-28 14:04:16 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-131d7a1d-kafka will have 3 replicas not ready, will try again in 1000 ms (176961ms till timeout)
2022-03-28 14:04:16 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (39996ms till timeout)
2022-03-28 14:04:17 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:04:17 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (185874ms till timeout)
2022-03-28 14:04:17 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 16 polls
2022-03-28 14:04:17 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-131d7a1d-kafka will have 3 replicas not ready, will try again in 1000 ms (175674ms till timeout)
2022-03-28 14:04:17 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (38707ms till timeout)
2022-03-28 14:04:18 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:04:18 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (184679ms till timeout)
2022-03-28 14:04:19 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 15 polls
2022-03-28 14:04:19 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-131d7a1d-kafka will have 3 replicas not ready, will try again in 1000 ms (174389ms till timeout)
2022-03-28 14:04:19 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (37423ms till timeout)
2022-03-28 14:04:19 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:04:19 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (183484ms till timeout)
2022-03-28 14:04:20 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 14 polls
2022-03-28 14:04:20 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-131d7a1d-kafka will have 3 replicas not ready, will try again in 1000 ms (173104ms till timeout)
2022-03-28 14:04:20 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (36137ms till timeout)
2022-03-28 14:04:20 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:04:20 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (182289ms till timeout)
2022-03-28 14:04:21 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (34852ms till timeout)
2022-03-28 14:04:21 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 13 polls
2022-03-28 14:04:21 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-131d7a1d-kafka will have 3 replicas not ready, will try again in 1000 ms (171728ms till timeout)
2022-03-28 14:04:21 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:04:22 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (181093ms till timeout)
2022-03-28 14:04:23 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (33661ms till timeout)
2022-03-28 14:04:23 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 12 polls
2022-03-28 14:04:23 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-131d7a1d-kafka will have 3 replicas not ready, will try again in 1000 ms (170353ms till timeout)
2022-03-28 14:04:23 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:04:23 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (179802ms till timeout)
2022-03-28 14:04:24 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (32469ms till timeout)
2022-03-28 14:04:24 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:04:24 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 11 polls
2022-03-28 14:04:24 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-131d7a1d-kafka will have 3 replicas not ready, will try again in 1000 ms (169067ms till timeout)
2022-03-28 14:04:24 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (178525ms till timeout)
2022-03-28 14:04:25 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (31277ms till timeout)
2022-03-28 14:04:25 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 10 polls
2022-03-28 14:04:25 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-131d7a1d-kafka will have 3 replicas not ready, will try again in 1000 ms (167874ms till timeout)
2022-03-28 14:04:25 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:04:25 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (177330ms till timeout)
2022-03-28 14:04:26 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (30083ms till timeout)
2022-03-28 14:04:26 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 9 polls
2022-03-28 14:04:26 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-131d7a1d-kafka will have 3 replicas not ready, will try again in 1000 ms (166681ms till timeout)
2022-03-28 14:04:26 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:04:27 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (176047ms till timeout)
2022-03-28 14:04:27 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (28891ms till timeout)
2022-03-28 14:04:28 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 8 polls
2022-03-28 14:04:28 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-131d7a1d-kafka will have 3 replicas not ready, will try again in 1000 ms (165488ms till timeout)
2022-03-28 14:04:28 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:04:28 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (174854ms till timeout)
2022-03-28 14:04:28 [ForkJoinPool-1-worker-5] INFO  [PodUtils:189] Message Successfully removed all 100 found in delete-admin-my-cluster-600e8332-kafka-clients-czbtl log
2022-03-28 14:04:29 [ForkJoinPool-1-worker-5] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment delete-admin-my-cluster-600e8332-kafka-clients deletion
2022-03-28 14:04:29 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for ReplicaSet delete-admin-my-cluster-600e8332-kafka-clients to be deleted
2022-03-28 14:04:29 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 7 polls
2022-03-28 14:04:29 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-131d7a1d-kafka will have 3 replicas not ready, will try again in 1000 ms (164288ms till timeout)
2022-03-28 14:04:29 [ForkJoinPool-1-worker-5] DEBUG [JobUtils:40] Job delete-admin-my-cluster-600e8332-kafka-clients was deleted
2022-03-28 14:04:29 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:155] Create/Update Job list-admin-my-cluster-600e8332-kafka-clients in namespace throttling-quota-st
2022-03-28 14:04:29 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:04:29 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:list-admin-my-cluster-600e8332-kafka-clients
2022-03-28 14:04:29 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (173562ms till timeout)
2022-03-28 14:04:29 [ForkJoinPool-1-worker-5] INFO  [JobUtils:81] Waiting for job: list-admin-my-cluster-600e8332-kafka-clients will be in active state
2022-03-28 14:04:29 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 14:04:29 [ForkJoinPool-1-worker-5] INFO  [ClientUtils:76] Waiting for producer/consumer:list-admin-my-cluster-600e8332-kafka-clients to finished
2022-03-28 14:04:29 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for job finished
2022-03-28 14:04:29 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job list-admin-my-cluster-600e8332-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:04:24Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:04:29 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (119807ms till timeout)
2022-03-28 14:04:30 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 6 polls
2022-03-28 14:04:30 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-131d7a1d-kafka will have 3 replicas not ready, will try again in 1000 ms (163096ms till timeout)
2022-03-28 14:04:30 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:04:30 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (172368ms till timeout)
2022-03-28 14:04:31 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job list-admin-my-cluster-600e8332-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:04:24Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:04:31 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (118612ms till timeout)
2022-03-28 14:04:31 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 5 polls
2022-03-28 14:04:31 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-131d7a1d-kafka will have 3 replicas not ready, will try again in 1000 ms (161903ms till timeout)
2022-03-28 14:04:31 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:04:31 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (171175ms till timeout)
2022-03-28 14:04:32 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job list-admin-my-cluster-600e8332-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:04:24Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:04:32 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (117417ms till timeout)
2022-03-28 14:04:32 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 4 polls
2022-03-28 14:04:32 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-131d7a1d-kafka will have 3 replicas not ready, will try again in 1000 ms (160711ms till timeout)
2022-03-28 14:04:33 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:04:33 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (169981ms till timeout)
2022-03-28 14:04:33 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job list-admin-my-cluster-600e8332-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:04:24Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:04:33 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (116218ms till timeout)
2022-03-28 14:04:34 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 3 polls
2022-03-28 14:04:34 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-131d7a1d-kafka will have 3 replicas not ready, will try again in 1000 ms (159518ms till timeout)
2022-03-28 14:04:34 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:04:34 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (168787ms till timeout)
2022-03-28 14:04:34 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job list-admin-my-cluster-600e8332-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:04:24Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:04:34 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (115023ms till timeout)
2022-03-28 14:04:35 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 2 polls
2022-03-28 14:04:35 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-131d7a1d-kafka will have 3 replicas not ready, will try again in 1000 ms (158233ms till timeout)
2022-03-28 14:04:35 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:04:35 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (167592ms till timeout)
2022-03-28 14:04:35 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job list-admin-my-cluster-600e8332-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=null, completedIndexes=null, completionTime=2022-03-28T14:04:30Z, conditions=[JobCondition(lastProbeTime=2022-03-28T14:04:30Z, lastTransitionTime=2022-03-28T14:04:30Z, message=null, reason=null, status=True, type=Complete, additionalProperties={})], failed=null, ready=null, startTime=2022-03-28T14:04:24Z, succeeded=1, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:04:36 [ForkJoinPool-1-worker-5] DEBUG [AbstractST:674] ============================================================================
2022-03-28 14:04:36 [ForkJoinPool-1-worker-5] DEBUG [AbstractST:675] [operators.topic.ThrottlingQuotaST - After Each] - Clean up after test
2022-03-28 14:04:36 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:344] ############################################################################
2022-03-28 14:04:36 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:348] Delete all resources for testKafkaAdminTopicOperations
2022-03-28 14:04:36 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:241] Delete of Job list-admin-my-cluster-600e8332-kafka-clients in namespace throttling-quota-st
2022-03-28 14:04:36 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:241] Delete of Job create-admin-my-cluster-600e8332-kafka-clients in namespace throttling-quota-st
2022-03-28 14:04:36 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:241] Delete of Job delete-admin-my-cluster-600e8332-kafka-clients in namespace throttling-quota-st
2022-03-28 14:04:36 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of Job list-admin-my-cluster-600e8332-kafka-clients in namespace throttling-quota-st
2022-03-28 14:04:36 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:list-admin-my-cluster-600e8332-kafka-clients
2022-03-28 14:04:36 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:create-admin-my-cluster-600e8332-kafka-clients
2022-03-28 14:04:36 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:list-admin-my-cluster-600e8332-kafka-clients
2022-03-28 14:04:36 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:delete-admin-my-cluster-600e8332-kafka-clients
2022-03-28 14:04:36 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:241] Delete of KafkaUser my-user-1735322302-803507072 in namespace throttling-quota-st
2022-03-28 14:04:36 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 1 polls
2022-03-28 14:04:36 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-131d7a1d-kafka will have 3 replicas not ready, will try again in 1000 ms (156969ms till timeout)
2022-03-28 14:04:36 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-1735322302-803507072
2022-03-28 14:04:36 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:04:36 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:369] ############################################################################
2022-03-28 14:04:36 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:267] testKafkaAdminTopicOperations - Notifies waiting test cases:[testCapacityFile, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testThrottlingQuotasCreateAlterPartitions, testSendSimpleMessageTls, testReceiveSimpleMessageTlsScramSha, testSendSimpleMessageTlsScramSha, testReceiveSimpleMessageTls, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testConfigurationFileIsCreated, testPauseReconciliationInKafkaRebalanceAndTopic, testTopicModificationOfReplicationFactor, testMoreReplicasThanAvailableBrokers, testCreateTopicViaKafka, testCreateTopicAfterUnsupportedOperation, testPauseReconciliationInKafkaAndKafkaConnectWithConnector] to and randomly select one to start execution
2022-03-28 14:04:36 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:93] [operators.topic.ThrottlingQuotaST] - Removing parallel test: testKafkaAdminTopicOperations
2022-03-28 14:04:36 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:97] [operators.topic.ThrottlingQuotaST] - Parallel test count: 2
2022-03-28 14:04:36 [ForkJoinPool-1-worker-5] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.topic.ThrottlingQuotaST.testKafkaAdminTopicOperations-FINISHED
2022-03-28 14:04:36 [ForkJoinPool-1-worker-5] INFO  [TestSeparator:30] ############################################################################
2022-03-28 14:04:36 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (166242ms till timeout)
2022-03-28 14:04:37 [ForkJoinPool-1-worker-15] INFO  [PodUtils:217] Pod replicas are stable for 20 polls intervals
2022-03-28 14:04:37 [ForkJoinPool-1-worker-15] INFO  [PodUtils:228] Pod my-cluster-131d7a1d-kafka has 3 replicas
2022-03-28 14:04:37 [ForkJoinPool-1-worker-15] INFO  [ReconciliationST:90] Setting annotation to "false", Kafka should be scaled to 4
2022-03-28 14:04:37 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:04:38 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (164954ms till timeout)
2022-03-28 14:04:38 [ForkJoinPool-1-worker-15] INFO  [RollingUpdateUtils:127] Waiting for 4 Pod(s) of my-cluster-131d7a1d-kafka to be ready
2022-03-28 14:04:38 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready
2022-03-28 14:04:38 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:04:38 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2399902ms till timeout)
2022-03-28 14:04:39 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:04:39 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (163759ms till timeout)
2022-03-28 14:04:39 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:04:39 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2398711ms till timeout)
2022-03-28 14:04:40 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:04:40 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (162558ms till timeout)
2022-03-28 14:04:40 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:04:40 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2397515ms till timeout)
2022-03-28 14:04:41 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:04:41 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (161363ms till timeout)
2022-03-28 14:04:41 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:04:41 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2396317ms till timeout)
2022-03-28 14:04:42 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:04:42 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (160167ms till timeout)
2022-03-28 14:04:43 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:04:43 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2395125ms till timeout)
2022-03-28 14:04:44 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:04:44 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (158972ms till timeout)
2022-03-28 14:04:44 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:04:44 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2393929ms till timeout)
2022-03-28 14:04:45 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:04:45 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (157777ms till timeout)
2022-03-28 14:04:45 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:04:45 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2392734ms till timeout)
2022-03-28 14:04:46 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:04:46 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (156581ms till timeout)
2022-03-28 14:04:46 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:04:46 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2391537ms till timeout)
2022-03-28 14:04:47 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:04:47 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (155385ms till timeout)
2022-03-28 14:04:47 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:04:47 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2390342ms till timeout)
2022-03-28 14:04:48 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:04:48 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (154190ms till timeout)
2022-03-28 14:04:49 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:04:49 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2389147ms till timeout)
2022-03-28 14:04:50 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:04:50 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (152993ms till timeout)
2022-03-28 14:04:50 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:04:50 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2387950ms till timeout)
2022-03-28 14:04:51 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:04:51 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (151796ms till timeout)
2022-03-28 14:04:51 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:04:51 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2386752ms till timeout)
2022-03-28 14:04:52 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:04:52 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (150598ms till timeout)
2022-03-28 14:04:52 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:04:52 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2385554ms till timeout)
2022-03-28 14:04:53 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:04:53 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (149399ms till timeout)
2022-03-28 14:04:53 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:04:53 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2384353ms till timeout)
2022-03-28 14:04:54 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:04:54 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (148204ms till timeout)
2022-03-28 14:04:55 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:04:55 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2383161ms till timeout)
2022-03-28 14:04:55 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:04:56 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (146915ms till timeout)
2022-03-28 14:04:56 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:04:56 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2381877ms till timeout)
2022-03-28 14:04:57 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:04:57 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (145718ms till timeout)
2022-03-28 14:04:57 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:04:57 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2380675ms till timeout)
2022-03-28 14:04:58 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:04:58 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (144524ms till timeout)
2022-03-28 14:04:58 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:04:58 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2379480ms till timeout)
2022-03-28 14:04:59 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:04:59 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (143328ms till timeout)
2022-03-28 14:04:59 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:04:59 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2378286ms till timeout)
2022-03-28 14:05:00 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:05:00 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (142133ms till timeout)
2022-03-28 14:05:01 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:05:01 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2377090ms till timeout)
2022-03-28 14:05:02 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:05:02 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (140937ms till timeout)
2022-03-28 14:05:02 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:05:02 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2375894ms till timeout)
2022-03-28 14:05:03 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:05:03 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (139741ms till timeout)
2022-03-28 14:05:03 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:05:03 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2374697ms till timeout)
2022-03-28 14:05:04 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:05:04 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (138545ms till timeout)
2022-03-28 14:05:04 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:05:04 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2373502ms till timeout)
2022-03-28 14:05:05 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:05:05 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (137342ms till timeout)
2022-03-28 14:05:05 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:05:05 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2372298ms till timeout)
2022-03-28 14:05:06 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:05:06 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (136144ms till timeout)
2022-03-28 14:05:07 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:05:07 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2371100ms till timeout)
2022-03-28 14:05:08 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:05:08 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (134950ms till timeout)
2022-03-28 14:05:08 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:05:08 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2369908ms till timeout)
2022-03-28 14:05:09 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:05:09 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (133753ms till timeout)
2022-03-28 14:05:09 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:05:09 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2368710ms till timeout)
2022-03-28 14:05:10 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:05:10 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (132557ms till timeout)
2022-03-28 14:05:10 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:05:10 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2367515ms till timeout)
2022-03-28 14:05:11 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:05:11 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (131361ms till timeout)
2022-03-28 14:05:11 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:05:11 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2366319ms till timeout)
2022-03-28 14:05:12 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:05:12 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (130166ms till timeout)
2022-03-28 14:05:13 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:05:13 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2365125ms till timeout)
2022-03-28 14:05:14 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:05:14 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (128927ms till timeout)
2022-03-28 14:05:14 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:05:14 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2363888ms till timeout)
2022-03-28 14:05:15 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:05:15 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (127731ms till timeout)
2022-03-28 14:05:15 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:05:15 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2362689ms till timeout)
2022-03-28 14:05:16 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:05:16 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (126534ms till timeout)
2022-03-28 14:05:16 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:05:16 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2361492ms till timeout)
2022-03-28 14:05:17 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:05:17 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (125338ms till timeout)
2022-03-28 14:05:17 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:05:17 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2360295ms till timeout)
2022-03-28 14:05:18 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:05:18 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (124142ms till timeout)
2022-03-28 14:05:19 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:05:19 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2359099ms till timeout)
2022-03-28 14:05:20 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:05:20 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (122947ms till timeout)
2022-03-28 14:05:20 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:05:20 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2357902ms till timeout)
2022-03-28 14:05:21 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:05:21 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (121750ms till timeout)
2022-03-28 14:05:21 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:05:21 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2356706ms till timeout)
2022-03-28 14:05:22 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=null, completedIndexes=null, completionTime=2022-03-28T14:05:17Z, conditions=[JobCondition(lastProbeTime=2022-03-28T14:05:17Z, lastTransitionTime=2022-03-28T14:05:17Z, message=null, reason=null, status=True, type=Complete, additionalProperties={})], failed=null, ready=null, startTime=2022-03-28T14:03:38Z, succeeded=1, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:05:22 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:05:22 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2355511ms till timeout)
2022-03-28 14:05:22 [ForkJoinPool-1-worker-9] INFO  [PodUtils:186] Waiting for message will be in the log
2022-03-28 14:05:22 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Waiting for message will be in the log
2022-03-28 14:05:22 [ForkJoinPool-1-worker-9] INFO  [PodUtils:189] Message All topics created found in create-admin-my-cluster-419d5c31-kafka-clients-qkdkt log
2022-03-28 14:05:22 [ForkJoinPool-1-worker-9] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment create-admin-my-cluster-419d5c31-kafka-clients deletion
2022-03-28 14:05:22 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for ReplicaSet create-admin-my-cluster-419d5c31-kafka-clients to be deleted
2022-03-28 14:05:23 [ForkJoinPool-1-worker-9] DEBUG [JobUtils:40] Job create-admin-my-cluster-419d5c31-kafka-clients was deleted
2022-03-28 14:05:23 [ForkJoinPool-1-worker-9] INFO  [ThrottlingQuotaST:112] Executing 4/5 iteration.
2022-03-28 14:05:23 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:155] Create/Update Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st
2022-03-28 14:05:23 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:create-admin-my-cluster-419d5c31-kafka-clients
2022-03-28 14:05:23 [ForkJoinPool-1-worker-9] INFO  [JobUtils:81] Waiting for job: create-admin-my-cluster-419d5c31-kafka-clients will be in active state
2022-03-28 14:05:23 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 14:05:23 [ForkJoinPool-1-worker-9] INFO  [ClientUtils:76] Waiting for producer/consumer:create-admin-my-cluster-419d5c31-kafka-clients to finished
2022-03-28 14:05:23 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for job finished
2022-03-28 14:05:23 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:05:23 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (219806ms till timeout)
2022-03-28 14:05:23 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:05:23 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2354316ms till timeout)
2022-03-28 14:05:24 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:05:24 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (218610ms till timeout)
2022-03-28 14:05:25 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:05:25 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2353121ms till timeout)
2022-03-28 14:05:25 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:05:25 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (217417ms till timeout)
2022-03-28 14:05:26 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:05:26 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2351928ms till timeout)
2022-03-28 14:05:27 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:05:27 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (216217ms till timeout)
2022-03-28 14:05:27 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:05:27 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2350737ms till timeout)
2022-03-28 14:05:28 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:05:28 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (215021ms till timeout)
2022-03-28 14:05:28 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:05:28 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2349544ms till timeout)
2022-03-28 14:05:29 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:05:29 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (213825ms till timeout)
2022-03-28 14:05:29 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:05:29 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2348351ms till timeout)
2022-03-28 14:05:30 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:05:30 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (212631ms till timeout)
2022-03-28 14:05:31 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:05:31 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2347159ms till timeout)
2022-03-28 14:05:31 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:05:31 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (211436ms till timeout)
2022-03-28 14:05:32 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:05:32 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2345967ms till timeout)
2022-03-28 14:05:32 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:05:33 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (210239ms till timeout)
2022-03-28 14:05:33 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:05:33 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2344773ms till timeout)
2022-03-28 14:05:34 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:05:34 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (209033ms till timeout)
2022-03-28 14:05:34 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:05:34 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2343581ms till timeout)
2022-03-28 14:05:35 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:05:35 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (207837ms till timeout)
2022-03-28 14:05:35 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:05:35 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2342389ms till timeout)
2022-03-28 14:05:36 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:05:36 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (206639ms till timeout)
2022-03-28 14:05:37 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:05:37 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2341196ms till timeout)
2022-03-28 14:05:37 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:05:37 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (205442ms till timeout)
2022-03-28 14:05:38 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:05:38 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2340004ms till timeout)
2022-03-28 14:05:38 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:05:39 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (204246ms till timeout)
2022-03-28 14:05:39 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:05:39 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2338812ms till timeout)
2022-03-28 14:05:40 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:05:40 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (203050ms till timeout)
2022-03-28 14:05:40 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:05:40 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2337620ms till timeout)
2022-03-28 14:05:41 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:05:41 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (201855ms till timeout)
2022-03-28 14:05:41 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:05:41 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2336429ms till timeout)
2022-03-28 14:05:42 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:05:42 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (200660ms till timeout)
2022-03-28 14:05:43 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:05:43 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2335236ms till timeout)
2022-03-28 14:05:43 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:05:43 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (199465ms till timeout)
2022-03-28 14:05:44 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:05:44 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2334045ms till timeout)
2022-03-28 14:05:44 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:05:45 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (198270ms till timeout)
2022-03-28 14:05:45 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:05:45 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2332851ms till timeout)
2022-03-28 14:05:46 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:05:46 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (197074ms till timeout)
2022-03-28 14:05:46 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:05:46 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2331657ms till timeout)
2022-03-28 14:05:47 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:05:47 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (195876ms till timeout)
2022-03-28 14:05:47 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:05:47 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2330465ms till timeout)
2022-03-28 14:05:48 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:05:48 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (194681ms till timeout)
2022-03-28 14:05:48 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:05:48 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2329273ms till timeout)
2022-03-28 14:05:49 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:05:49 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (193485ms till timeout)
2022-03-28 14:05:50 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:05:50 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2328079ms till timeout)
2022-03-28 14:05:50 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:05:51 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (192290ms till timeout)
2022-03-28 14:05:51 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:05:51 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2326887ms till timeout)
2022-03-28 14:05:52 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:05:52 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (191092ms till timeout)
2022-03-28 14:05:52 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:05:52 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2325696ms till timeout)
2022-03-28 14:05:53 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:05:53 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (189897ms till timeout)
2022-03-28 14:05:53 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:05:53 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2324500ms till timeout)
2022-03-28 14:05:54 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:05:54 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (188702ms till timeout)
2022-03-28 14:05:54 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:05:54 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2323306ms till timeout)
2022-03-28 14:05:55 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:05:55 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (187509ms till timeout)
2022-03-28 14:05:56 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:05:56 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2322115ms till timeout)
2022-03-28 14:05:56 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:05:57 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (186314ms till timeout)
2022-03-28 14:05:57 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:05:57 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2320924ms till timeout)
2022-03-28 14:05:58 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:05:58 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (185120ms till timeout)
2022-03-28 14:05:58 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:05:58 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2319728ms till timeout)
2022-03-28 14:05:59 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:05:59 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (183925ms till timeout)
2022-03-28 14:05:59 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:05:59 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2318532ms till timeout)
2022-03-28 14:06:00 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:06:00 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (182731ms till timeout)
2022-03-28 14:06:00 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:06:00 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2317341ms till timeout)
2022-03-28 14:06:01 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:06:01 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (181536ms till timeout)
2022-03-28 14:06:02 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:06:02 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2316146ms till timeout)
2022-03-28 14:06:02 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:06:02 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (180340ms till timeout)
2022-03-28 14:06:03 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:06:03 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2314955ms till timeout)
2022-03-28 14:06:04 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:06:04 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (179145ms till timeout)
2022-03-28 14:06:04 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:06:04 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2313765ms till timeout)
2022-03-28 14:06:05 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:06:05 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (177950ms till timeout)
2022-03-28 14:06:05 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:06:05 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2312572ms till timeout)
2022-03-28 14:06:06 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:06:06 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (176754ms till timeout)
2022-03-28 14:06:06 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:06:06 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2311381ms till timeout)
2022-03-28 14:06:07 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:06:07 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (175559ms till timeout)
2022-03-28 14:06:08 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:06:08 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2310185ms till timeout)
2022-03-28 14:06:08 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:06:08 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (174363ms till timeout)
2022-03-28 14:06:09 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:06:09 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2308993ms till timeout)
2022-03-28 14:06:10 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:06:10 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (173167ms till timeout)
2022-03-28 14:06:10 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:06:10 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2307799ms till timeout)
2022-03-28 14:06:11 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:06:11 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (171971ms till timeout)
2022-03-28 14:06:11 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:06:11 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2306608ms till timeout)
2022-03-28 14:06:12 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:06:12 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (170775ms till timeout)
2022-03-28 14:06:12 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:06:12 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2305417ms till timeout)
2022-03-28 14:06:13 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:06:13 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (169580ms till timeout)
2022-03-28 14:06:14 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:06:14 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2304224ms till timeout)
2022-03-28 14:06:14 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:06:14 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (168358ms till timeout)
2022-03-28 14:06:15 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:06:15 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2303032ms till timeout)
2022-03-28 14:06:16 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:06:16 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (167161ms till timeout)
2022-03-28 14:06:16 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:06:16 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2301838ms till timeout)
2022-03-28 14:06:17 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:06:17 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (165965ms till timeout)
2022-03-28 14:06:17 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:06:17 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2300643ms till timeout)
2022-03-28 14:06:18 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:06:18 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (164770ms till timeout)
2022-03-28 14:06:18 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:06:18 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2299452ms till timeout)
2022-03-28 14:06:19 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:06:19 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (163575ms till timeout)
2022-03-28 14:06:19 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:06:19 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2298260ms till timeout)
2022-03-28 14:06:20 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:06:20 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (162380ms till timeout)
2022-03-28 14:06:21 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:06:21 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2297069ms till timeout)
2022-03-28 14:06:22 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:06:22 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (161175ms till timeout)
2022-03-28 14:06:22 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:06:22 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2295875ms till timeout)
2022-03-28 14:06:23 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:06:23 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (159972ms till timeout)
2022-03-28 14:06:23 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:06:23 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2294684ms till timeout)
2022-03-28 14:06:24 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:06:24 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (158777ms till timeout)
2022-03-28 14:06:24 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:06:24 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2293493ms till timeout)
2022-03-28 14:06:25 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:06:25 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (157584ms till timeout)
2022-03-28 14:06:25 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:06:25 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2292301ms till timeout)
2022-03-28 14:06:26 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:06:26 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (156386ms till timeout)
2022-03-28 14:06:27 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-0 not ready: kafka)
2022-03-28 14:06:27 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-1 not ready: kafka)
2022-03-28 14:06:27 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-2 not ready: kafka)
2022-03-28 14:06:27 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-131d7a1d-kafka-3)
2022-03-28 14:06:27 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2291014ms till timeout)
2022-03-28 14:06:28 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:06:28 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (155192ms till timeout)
2022-03-28 14:06:28 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-0 not ready: kafka)
2022-03-28 14:06:28 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-1 not ready: kafka)
2022-03-28 14:06:28 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-2 not ready: kafka)
2022-03-28 14:06:28 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-131d7a1d-kafka-3)
2022-03-28 14:06:28 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2289731ms till timeout)
2022-03-28 14:06:29 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:06:29 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (153997ms till timeout)
2022-03-28 14:06:29 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-0 not ready: kafka)
2022-03-28 14:06:29 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-1 not ready: kafka)
2022-03-28 14:06:29 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-2 not ready: kafka)
2022-03-28 14:06:29 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-131d7a1d-kafka-3)
2022-03-28 14:06:29 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2288448ms till timeout)
2022-03-28 14:06:30 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:06:30 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (152796ms till timeout)
2022-03-28 14:06:31 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-0 not ready: kafka)
2022-03-28 14:06:31 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-1 not ready: kafka)
2022-03-28 14:06:31 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-2 not ready: kafka)
2022-03-28 14:06:31 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-131d7a1d-kafka-3)
2022-03-28 14:06:31 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2287165ms till timeout)
2022-03-28 14:06:31 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:06:31 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (151601ms till timeout)
2022-03-28 14:06:32 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-0 not ready: kafka)
2022-03-28 14:06:32 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-1 not ready: kafka)
2022-03-28 14:06:32 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-2 not ready: kafka)
2022-03-28 14:06:32 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-131d7a1d-kafka-3)
2022-03-28 14:06:32 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2285881ms till timeout)
2022-03-28 14:06:32 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:06:32 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (150407ms till timeout)
2022-03-28 14:06:33 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-0 not ready: kafka)
2022-03-28 14:06:33 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-1 not ready: kafka)
2022-03-28 14:06:33 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-2 not ready: kafka)
2022-03-28 14:06:33 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-131d7a1d-kafka-3)
2022-03-28 14:06:33 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2284590ms till timeout)
2022-03-28 14:06:34 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:06:34 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (149213ms till timeout)
2022-03-28 14:06:34 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-0 not ready: kafka)
2022-03-28 14:06:34 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-1 not ready: kafka)
2022-03-28 14:06:34 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-2 not ready: kafka)
2022-03-28 14:06:34 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-131d7a1d-kafka-3)
2022-03-28 14:06:34 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2283305ms till timeout)
2022-03-28 14:06:35 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:06:35 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (148017ms till timeout)
2022-03-28 14:06:36 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-0 not ready: kafka)
2022-03-28 14:06:36 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-1 not ready: kafka)
2022-03-28 14:06:36 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-2 not ready: kafka)
2022-03-28 14:06:36 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-131d7a1d-kafka-3)
2022-03-28 14:06:36 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2282021ms till timeout)
2022-03-28 14:06:36 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:06:36 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (146821ms till timeout)
2022-03-28 14:06:37 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-0 not ready: kafka)
2022-03-28 14:06:37 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-1 not ready: kafka)
2022-03-28 14:06:37 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-2 not ready: kafka)
2022-03-28 14:06:37 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-131d7a1d-kafka-3)
2022-03-28 14:06:37 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2280737ms till timeout)
2022-03-28 14:06:37 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:06:37 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (145625ms till timeout)
2022-03-28 14:06:38 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-0 not ready: kafka)
2022-03-28 14:06:38 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-1 not ready: kafka)
2022-03-28 14:06:38 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-2 not ready: kafka)
2022-03-28 14:06:38 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-131d7a1d-kafka-3)
2022-03-28 14:06:38 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2279452ms till timeout)
2022-03-28 14:06:38 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:06:38 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (144431ms till timeout)
2022-03-28 14:06:40 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:06:40 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-0 not ready: kafka)
2022-03-28 14:06:40 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-1 not ready: kafka)
2022-03-28 14:06:40 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-2 not ready: kafka)
2022-03-28 14:06:40 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-131d7a1d-kafka-3)
2022-03-28 14:06:40 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2278168ms till timeout)
2022-03-28 14:06:40 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (143155ms till timeout)
2022-03-28 14:06:41 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:06:41 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-0 not ready: kafka)
2022-03-28 14:06:41 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-1 not ready: kafka)
2022-03-28 14:06:41 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-2 not ready: kafka)
2022-03-28 14:06:41 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-131d7a1d-kafka-3)
2022-03-28 14:06:41 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2276883ms till timeout)
2022-03-28 14:06:41 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (141877ms till timeout)
2022-03-28 14:06:42 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:06:42 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-0 not ready: kafka)
2022-03-28 14:06:42 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-1 not ready: kafka)
2022-03-28 14:06:42 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-2 not ready: kafka)
2022-03-28 14:06:42 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-131d7a1d-kafka-3)
2022-03-28 14:06:42 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2275596ms till timeout)
2022-03-28 14:06:42 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (140590ms till timeout)
2022-03-28 14:06:43 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-0 not ready: kafka)
2022-03-28 14:06:43 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-1 not ready: kafka)
2022-03-28 14:06:43 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-2 not ready: kafka)
2022-03-28 14:06:43 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-131d7a1d-kafka-3)
2022-03-28 14:06:43 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2274309ms till timeout)
2022-03-28 14:06:43 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:06:44 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (139302ms till timeout)
2022-03-28 14:06:45 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:06:45 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-0 not ready: kafka)
2022-03-28 14:06:45 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-1 not ready: kafka)
2022-03-28 14:06:45 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-2 not ready: kafka)
2022-03-28 14:06:45 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-131d7a1d-kafka-3)
2022-03-28 14:06:45 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2273024ms till timeout)
2022-03-28 14:06:45 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (138018ms till timeout)
2022-03-28 14:06:46 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:06:46 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-0 not ready: kafka)
2022-03-28 14:06:46 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-1 not ready: kafka)
2022-03-28 14:06:46 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-2 not ready: kafka)
2022-03-28 14:06:46 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-131d7a1d-kafka-3)
2022-03-28 14:06:46 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2271739ms till timeout)
2022-03-28 14:06:46 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (136732ms till timeout)
2022-03-28 14:06:47 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:06:47 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-0 not ready: kafka)
2022-03-28 14:06:47 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-1 not ready: kafka)
2022-03-28 14:06:47 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-2 not ready: kafka)
2022-03-28 14:06:47 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-131d7a1d-kafka-3)
2022-03-28 14:06:47 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2270452ms till timeout)
2022-03-28 14:06:47 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (135446ms till timeout)
2022-03-28 14:06:49 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-0 not ready: kafka)
2022-03-28 14:06:49 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:06:49 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-1 not ready: kafka)
2022-03-28 14:06:49 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-2 not ready: kafka)
2022-03-28 14:06:49 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-131d7a1d-kafka-3)
2022-03-28 14:06:49 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2269167ms till timeout)
2022-03-28 14:06:49 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (134159ms till timeout)
2022-03-28 14:06:50 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:06:50 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-0 not ready: kafka)
2022-03-28 14:06:50 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-1 not ready: kafka)
2022-03-28 14:06:50 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-2 not ready: kafka)
2022-03-28 14:06:50 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-131d7a1d-kafka-3)
2022-03-28 14:06:50 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2267881ms till timeout)
2022-03-28 14:06:50 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (132873ms till timeout)
2022-03-28 14:06:51 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:06:51 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-0 not ready: kafka)
2022-03-28 14:06:51 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-1 not ready: kafka)
2022-03-28 14:06:51 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-2 not ready: kafka)
2022-03-28 14:06:51 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-131d7a1d-kafka-3)
2022-03-28 14:06:51 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2266597ms till timeout)
2022-03-28 14:06:51 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (131590ms till timeout)
2022-03-28 14:06:52 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:06:52 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-0 not ready: kafka)
2022-03-28 14:06:52 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-1 not ready: kafka)
2022-03-28 14:06:52 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-2 not ready: kafka)
2022-03-28 14:06:52 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-131d7a1d-kafka-3)
2022-03-28 14:06:52 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2265312ms till timeout)
2022-03-28 14:06:53 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (130306ms till timeout)
2022-03-28 14:06:54 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:06:54 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-0 not ready: kafka)
2022-03-28 14:06:54 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-1 not ready: kafka)
2022-03-28 14:06:54 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-2 not ready: kafka)
2022-03-28 14:06:54 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-131d7a1d-kafka-3)
2022-03-28 14:06:54 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2264026ms till timeout)
2022-03-28 14:06:54 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (129020ms till timeout)
2022-03-28 14:06:55 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:06:55 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-0 not ready: kafka)
2022-03-28 14:06:55 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-1 not ready: kafka)
2022-03-28 14:06:55 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-2 not ready: kafka)
2022-03-28 14:06:55 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-131d7a1d-kafka-3)
2022-03-28 14:06:55 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2262742ms till timeout)
2022-03-28 14:06:55 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (127735ms till timeout)
2022-03-28 14:06:56 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:06:56 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-0 not ready: kafka)
2022-03-28 14:06:56 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-1 not ready: kafka)
2022-03-28 14:06:56 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-2 not ready: kafka)
2022-03-28 14:06:56 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-131d7a1d-kafka-3)
2022-03-28 14:06:56 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2261458ms till timeout)
2022-03-28 14:06:56 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (126451ms till timeout)
2022-03-28 14:06:58 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:06:58 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-0 not ready: kafka)
2022-03-28 14:06:58 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-1 not ready: kafka)
2022-03-28 14:06:58 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-2 not ready: kafka)
2022-03-28 14:06:58 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-131d7a1d-kafka-3)
2022-03-28 14:06:58 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2260173ms till timeout)
2022-03-28 14:06:58 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (125167ms till timeout)
2022-03-28 14:06:59 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:06:59 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-0 not ready: kafka)
2022-03-28 14:06:59 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-1 not ready: kafka)
2022-03-28 14:06:59 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-2 not ready: kafka)
2022-03-28 14:06:59 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-131d7a1d-kafka-3)
2022-03-28 14:06:59 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2258888ms till timeout)
2022-03-28 14:06:59 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (123881ms till timeout)
2022-03-28 14:07:00 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:07:00 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-0 not ready: kafka)
2022-03-28 14:07:00 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-1 not ready: kafka)
2022-03-28 14:07:00 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-2 not ready: kafka)
2022-03-28 14:07:00 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-131d7a1d-kafka-3)
2022-03-28 14:07:00 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2257604ms till timeout)
2022-03-28 14:07:00 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (122598ms till timeout)
2022-03-28 14:07:01 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:07:01 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-0 not ready: kafka)
2022-03-28 14:07:01 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-1 not ready: kafka)
2022-03-28 14:07:01 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-2 not ready: kafka)
2022-03-28 14:07:01 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-131d7a1d-kafka-3)
2022-03-28 14:07:01 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2256320ms till timeout)
2022-03-28 14:07:02 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (121311ms till timeout)
2022-03-28 14:07:03 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=null, completedIndexes=null, completionTime=2022-03-28T14:06:57Z, conditions=[JobCondition(lastProbeTime=2022-03-28T14:06:57Z, lastTransitionTime=2022-03-28T14:06:57Z, message=null, reason=null, status=True, type=Complete, additionalProperties={})], failed=null, ready=null, startTime=2022-03-28T14:05:18Z, succeeded=1, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:07:03 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-0 not ready: kafka)
2022-03-28 14:07:03 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-1 not ready: kafka)
2022-03-28 14:07:03 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-2 not ready: kafka)
2022-03-28 14:07:03 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-3 not ready: kafka)
2022-03-28 14:07:03 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-131d7a1d-kafka-0, my-cluster-131d7a1d-kafka-1, my-cluster-131d7a1d-kafka-2, my-cluster-131d7a1d-kafka-3 are ready
2022-03-28 14:07:03 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2255036ms till timeout)
2022-03-28 14:07:03 [ForkJoinPool-1-worker-9] INFO  [PodUtils:186] Waiting for message will be in the log
2022-03-28 14:07:03 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Waiting for message will be in the log
2022-03-28 14:07:03 [ForkJoinPool-1-worker-9] INFO  [PodUtils:189] Message All topics created found in create-admin-my-cluster-419d5c31-kafka-clients-j5lbf log
2022-03-28 14:07:03 [ForkJoinPool-1-worker-9] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment create-admin-my-cluster-419d5c31-kafka-clients deletion
2022-03-28 14:07:03 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for ReplicaSet create-admin-my-cluster-419d5c31-kafka-clients to be deleted
2022-03-28 14:07:03 [ForkJoinPool-1-worker-9] DEBUG [JobUtils:40] Job create-admin-my-cluster-419d5c31-kafka-clients was deleted
2022-03-28 14:07:03 [ForkJoinPool-1-worker-9] INFO  [ThrottlingQuotaST:112] Executing 5/5 iteration.
2022-03-28 14:07:03 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:155] Create/Update Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st
2022-03-28 14:07:03 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:create-admin-my-cluster-419d5c31-kafka-clients
2022-03-28 14:07:03 [ForkJoinPool-1-worker-9] INFO  [JobUtils:81] Waiting for job: create-admin-my-cluster-419d5c31-kafka-clients will be in active state
2022-03-28 14:07:03 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 14:07:04 [ForkJoinPool-1-worker-9] INFO  [ClientUtils:76] Waiting for producer/consumer:create-admin-my-cluster-419d5c31-kafka-clients to finished
2022-03-28 14:07:04 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for job finished
2022-03-28 14:07:04 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:06:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:07:04 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (219808ms till timeout)
2022-03-28 14:07:04 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-0 not ready: kafka)
2022-03-28 14:07:04 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-1 not ready: kafka)
2022-03-28 14:07:04 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-2 not ready: kafka)
2022-03-28 14:07:04 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-3 not ready: kafka)
2022-03-28 14:07:04 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-131d7a1d-kafka-0, my-cluster-131d7a1d-kafka-1, my-cluster-131d7a1d-kafka-2, my-cluster-131d7a1d-kafka-3 are ready
2022-03-28 14:07:04 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2253933ms till timeout)
2022-03-28 14:07:05 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:06:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:07:05 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (218612ms till timeout)
2022-03-28 14:07:05 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-0 not ready: kafka)
2022-03-28 14:07:05 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-1 not ready: kafka)
2022-03-28 14:07:05 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-2 not ready: kafka)
2022-03-28 14:07:05 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-3 not ready: kafka)
2022-03-28 14:07:05 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-131d7a1d-kafka-0, my-cluster-131d7a1d-kafka-1, my-cluster-131d7a1d-kafka-2, my-cluster-131d7a1d-kafka-3 are ready
2022-03-28 14:07:05 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2252568ms till timeout)
2022-03-28 14:07:06 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:06:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:07:06 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (217417ms till timeout)
2022-03-28 14:07:06 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-0 not ready: kafka)
2022-03-28 14:07:06 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-1 not ready: kafka)
2022-03-28 14:07:06 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-2 not ready: kafka)
2022-03-28 14:07:06 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-3 not ready: kafka)
2022-03-28 14:07:06 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-131d7a1d-kafka-0, my-cluster-131d7a1d-kafka-1, my-cluster-131d7a1d-kafka-2, my-cluster-131d7a1d-kafka-3 are ready
2022-03-28 14:07:06 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2251285ms till timeout)
2022-03-28 14:07:07 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:06:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:07:07 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (216222ms till timeout)
2022-03-28 14:07:08 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-0 not ready: kafka)
2022-03-28 14:07:08 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-1 not ready: kafka)
2022-03-28 14:07:08 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-2 not ready: kafka)
2022-03-28 14:07:08 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-3 not ready: kafka)
2022-03-28 14:07:08 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-131d7a1d-kafka-0, my-cluster-131d7a1d-kafka-1, my-cluster-131d7a1d-kafka-2, my-cluster-131d7a1d-kafka-3 are ready
2022-03-28 14:07:08 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2250001ms till timeout)
2022-03-28 14:07:08 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:06:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:07:08 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (215026ms till timeout)
2022-03-28 14:07:09 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-0 not ready: kafka)
2022-03-28 14:07:09 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-1 not ready: kafka)
2022-03-28 14:07:09 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-2 not ready: kafka)
2022-03-28 14:07:09 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-3 not ready: kafka)
2022-03-28 14:07:09 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-131d7a1d-kafka-0, my-cluster-131d7a1d-kafka-1, my-cluster-131d7a1d-kafka-2, my-cluster-131d7a1d-kafka-3 are ready
2022-03-28 14:07:09 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2248717ms till timeout)
2022-03-28 14:07:10 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:06:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:07:10 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (213831ms till timeout)
2022-03-28 14:07:10 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-0 not ready: kafka)
2022-03-28 14:07:10 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-1 not ready: kafka)
2022-03-28 14:07:10 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-2 not ready: kafka)
2022-03-28 14:07:10 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-3 not ready: kafka)
2022-03-28 14:07:10 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-131d7a1d-kafka-0, my-cluster-131d7a1d-kafka-1, my-cluster-131d7a1d-kafka-2, my-cluster-131d7a1d-kafka-3 are ready
2022-03-28 14:07:10 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2247434ms till timeout)
2022-03-28 14:07:11 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:06:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:07:11 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (212636ms till timeout)
2022-03-28 14:07:12 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-0 not ready: kafka)
2022-03-28 14:07:12 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-1 not ready: kafka)
2022-03-28 14:07:12 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-2 not ready: kafka)
2022-03-28 14:07:12 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-3 not ready: kafka)
2022-03-28 14:07:12 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-131d7a1d-kafka-0, my-cluster-131d7a1d-kafka-1, my-cluster-131d7a1d-kafka-2, my-cluster-131d7a1d-kafka-3 are ready
2022-03-28 14:07:12 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2246151ms till timeout)
2022-03-28 14:07:12 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:06:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:07:12 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (211441ms till timeout)
2022-03-28 14:07:13 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-0 not ready: kafka)
2022-03-28 14:07:13 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-1 not ready: kafka)
2022-03-28 14:07:13 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-2 not ready: kafka)
2022-03-28 14:07:13 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-3 not ready: kafka)
2022-03-28 14:07:13 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-131d7a1d-kafka-0, my-cluster-131d7a1d-kafka-1, my-cluster-131d7a1d-kafka-2, my-cluster-131d7a1d-kafka-3 are ready
2022-03-28 14:07:13 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2244866ms till timeout)
2022-03-28 14:07:13 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:06:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:07:13 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (210246ms till timeout)
2022-03-28 14:07:14 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-0 not ready: kafka)
2022-03-28 14:07:14 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-1 not ready: kafka)
2022-03-28 14:07:14 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-2 not ready: kafka)
2022-03-28 14:07:14 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-3 not ready: kafka)
2022-03-28 14:07:14 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-131d7a1d-kafka-0, my-cluster-131d7a1d-kafka-1, my-cluster-131d7a1d-kafka-2, my-cluster-131d7a1d-kafka-3 are ready
2022-03-28 14:07:14 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-131d7a1d-kafka, strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2243581ms till timeout)
2022-03-28 14:07:14 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:06:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:07:14 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (209047ms till timeout)
2022-03-28 14:07:15 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-0 not ready: kafka)
2022-03-28 14:07:15 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-1 not ready: kafka)
2022-03-28 14:07:15 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-2 not ready: kafka)
2022-03-28 14:07:15 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-kafka-3 not ready: kafka)
2022-03-28 14:07:15 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-131d7a1d-kafka-0, my-cluster-131d7a1d-kafka-1, my-cluster-131d7a1d-kafka-2, my-cluster-131d7a1d-kafka-3 are ready
2022-03-28 14:07:16 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:433] Wait for Kafka: my-cluster-131d7a1d will have desired state: Ready
2022-03-28 14:07:16 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Kafka: my-cluster-131d7a1d will have desired state: Ready
2022-03-28 14:07:16 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:06:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:07:16 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:444] Kafka: my-cluster-131d7a1d is in desired state: Ready
2022-03-28 14:07:16 [ForkJoinPool-1-worker-15] INFO  [RollingUpdateUtils:132] Kafka: my-cluster-131d7a1d is ready
2022-03-28 14:07:16 [ForkJoinPool-1-worker-15] INFO  [ReconciliationST:94] Deploying KafkaConnect with pause annotation from the start, no pods should appear
2022-03-28 14:07:16 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:155] Create/Update Deployment my-cluster-131d7a1d-kafka-clients in namespace namespace-9
2022-03-28 14:07:16 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:164] Using Namespace: namespace-9
2022-03-28 14:07:16 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (207677ms till timeout)
2022-03-28 14:07:16 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:my-cluster-131d7a1d-kafka-clients
2022-03-28 14:07:16 [ForkJoinPool-1-worker-15] INFO  [DeploymentUtils:161] Wait for Deployment: my-cluster-131d7a1d-kafka-clients will be ready
2022-03-28 14:07:16 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Wait for Deployment: my-cluster-131d7a1d-kafka-clients will be ready
2022-03-28 14:07:16 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-131d7a1d-kafka-clients will be ready not ready, will try again in 1000 ms (479903ms till timeout)
2022-03-28 14:07:17 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:06:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:07:17 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (206481ms till timeout)
2022-03-28 14:07:17 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-131d7a1d-kafka-clients will be ready not ready, will try again in 1000 ms (478806ms till timeout)
2022-03-28 14:07:18 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:06:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:07:18 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (205286ms till timeout)
2022-03-28 14:07:18 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-131d7a1d-kafka-clients will be ready not ready, will try again in 1000 ms (477709ms till timeout)
2022-03-28 14:07:19 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:06:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:07:19 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (204090ms till timeout)
2022-03-28 14:07:20 [ForkJoinPool-1-worker-15] INFO  [DeploymentUtils:168] Deployment: my-cluster-131d7a1d-kafka-clients is ready
2022-03-28 14:07:20 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:155] Create/Update Deployment my-cluster-131d7a1d-scraper in namespace namespace-9
2022-03-28 14:07:20 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:164] Using Namespace: namespace-9
2022-03-28 14:07:20 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:my-cluster-131d7a1d-scraper
2022-03-28 14:07:20 [ForkJoinPool-1-worker-15] INFO  [DeploymentUtils:161] Wait for Deployment: my-cluster-131d7a1d-scraper will be ready
2022-03-28 14:07:20 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Wait for Deployment: my-cluster-131d7a1d-scraper will be ready
2022-03-28 14:07:20 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-131d7a1d-scraper will be ready not ready, will try again in 1000 ms (479904ms till timeout)
2022-03-28 14:07:21 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:06:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:07:21 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (202858ms till timeout)
2022-03-28 14:07:21 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-131d7a1d-scraper will be ready not ready, will try again in 1000 ms (478806ms till timeout)
2022-03-28 14:07:22 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:06:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:07:22 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (201661ms till timeout)
2022-03-28 14:07:22 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-131d7a1d-scraper will be ready not ready, will try again in 1000 ms (477707ms till timeout)
2022-03-28 14:07:23 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:06:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:07:23 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (200467ms till timeout)
2022-03-28 14:07:23 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-131d7a1d-scraper will be ready not ready, will try again in 1000 ms (476610ms till timeout)
2022-03-28 14:07:24 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:06:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:07:24 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (199271ms till timeout)
2022-03-28 14:07:24 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-131d7a1d-scraper will be ready not ready, will try again in 1000 ms (475436ms till timeout)
2022-03-28 14:07:25 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:06:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:07:25 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-131d7a1d-scraper will be ready not ready, will try again in 1000 ms (474338ms till timeout)
2022-03-28 14:07:26 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (197982ms till timeout)
2022-03-28 14:07:27 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-131d7a1d-scraper will be ready not ready, will try again in 1000 ms (473239ms till timeout)
2022-03-28 14:07:27 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:06:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:07:27 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (196785ms till timeout)
2022-03-28 14:07:28 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-131d7a1d-scraper will be ready not ready, will try again in 1000 ms (472140ms till timeout)
2022-03-28 14:07:28 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:06:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:07:28 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (195591ms till timeout)
2022-03-28 14:07:29 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-131d7a1d-scraper will be ready not ready, will try again in 1000 ms (471039ms till timeout)
2022-03-28 14:07:29 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:06:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:07:29 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (194395ms till timeout)
2022-03-28 14:07:30 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-131d7a1d-scraper will be ready not ready, will try again in 1000 ms (469941ms till timeout)
2022-03-28 14:07:30 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:06:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:07:30 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (193179ms till timeout)
2022-03-28 14:07:31 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-131d7a1d-scraper will be ready not ready, will try again in 1000 ms (468843ms till timeout)
2022-03-28 14:07:31 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:06:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:07:32 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (191984ms till timeout)
2022-03-28 14:07:32 [ForkJoinPool-1-worker-15] INFO  [DeploymentUtils:168] Deployment: my-cluster-131d7a1d-scraper is ready
2022-03-28 14:07:32 [ForkJoinPool-1-worker-15] INFO  [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-131d7a1d-scraper to be ready
2022-03-28 14:07:32 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-131d7a1d-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready
2022-03-28 14:07:32 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-scraper-5c8ffbc57b-2lpf8 not ready: my-cluster-131d7a1d-scraper)
2022-03-28 14:07:32 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-131d7a1d-scraper-5c8ffbc57b-2lpf8 are ready
2022-03-28 14:07:32 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-131d7a1d-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599902ms till timeout)
2022-03-28 14:07:33 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:06:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:07:33 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (190790ms till timeout)
2022-03-28 14:07:33 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-scraper-5c8ffbc57b-2lpf8 not ready: my-cluster-131d7a1d-scraper)
2022-03-28 14:07:33 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-131d7a1d-scraper-5c8ffbc57b-2lpf8 are ready
2022-03-28 14:07:33 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-131d7a1d-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598803ms till timeout)
2022-03-28 14:07:34 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:06:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:07:34 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (189594ms till timeout)
2022-03-28 14:07:34 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-scraper-5c8ffbc57b-2lpf8 not ready: my-cluster-131d7a1d-scraper)
2022-03-28 14:07:34 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-131d7a1d-scraper-5c8ffbc57b-2lpf8 are ready
2022-03-28 14:07:34 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-131d7a1d-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597703ms till timeout)
2022-03-28 14:07:35 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:06:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:07:35 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (188400ms till timeout)
2022-03-28 14:07:36 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-scraper-5c8ffbc57b-2lpf8 not ready: my-cluster-131d7a1d-scraper)
2022-03-28 14:07:36 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-131d7a1d-scraper-5c8ffbc57b-2lpf8 are ready
2022-03-28 14:07:36 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-131d7a1d-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596604ms till timeout)
2022-03-28 14:07:36 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:06:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:07:36 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (187205ms till timeout)
2022-03-28 14:07:37 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-scraper-5c8ffbc57b-2lpf8 not ready: my-cluster-131d7a1d-scraper)
2022-03-28 14:07:37 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-131d7a1d-scraper-5c8ffbc57b-2lpf8 are ready
2022-03-28 14:07:37 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-131d7a1d-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595506ms till timeout)
2022-03-28 14:07:37 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:06:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:07:37 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (186010ms till timeout)
2022-03-28 14:07:38 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-scraper-5c8ffbc57b-2lpf8 not ready: my-cluster-131d7a1d-scraper)
2022-03-28 14:07:38 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-131d7a1d-scraper-5c8ffbc57b-2lpf8 are ready
2022-03-28 14:07:38 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-131d7a1d-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594407ms till timeout)
2022-03-28 14:07:39 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:06:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:07:39 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (184814ms till timeout)
2022-03-28 14:07:39 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-scraper-5c8ffbc57b-2lpf8 not ready: my-cluster-131d7a1d-scraper)
2022-03-28 14:07:39 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-131d7a1d-scraper-5c8ffbc57b-2lpf8 are ready
2022-03-28 14:07:39 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-131d7a1d-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593308ms till timeout)
2022-03-28 14:07:40 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:06:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:07:40 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (183620ms till timeout)
2022-03-28 14:07:40 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-scraper-5c8ffbc57b-2lpf8 not ready: my-cluster-131d7a1d-scraper)
2022-03-28 14:07:40 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-131d7a1d-scraper-5c8ffbc57b-2lpf8 are ready
2022-03-28 14:07:40 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-131d7a1d-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592144ms till timeout)
2022-03-28 14:07:41 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:06:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:07:41 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-scraper-5c8ffbc57b-2lpf8 not ready: my-cluster-131d7a1d-scraper)
2022-03-28 14:07:41 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-131d7a1d-scraper-5c8ffbc57b-2lpf8 are ready
2022-03-28 14:07:41 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-131d7a1d-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591045ms till timeout)
2022-03-28 14:07:41 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (182330ms till timeout)
2022-03-28 14:07:42 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-scraper-5c8ffbc57b-2lpf8 not ready: my-cluster-131d7a1d-scraper)
2022-03-28 14:07:42 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-131d7a1d-scraper-5c8ffbc57b-2lpf8 are ready
2022-03-28 14:07:42 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-131d7a1d-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (589946ms till timeout)
2022-03-28 14:07:42 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:06:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:07:42 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (181134ms till timeout)
2022-03-28 14:07:43 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-scraper-5c8ffbc57b-2lpf8 not ready: my-cluster-131d7a1d-scraper)
2022-03-28 14:07:43 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-131d7a1d-scraper-5c8ffbc57b-2lpf8 are ready
2022-03-28 14:07:43 [ForkJoinPool-1-worker-15] INFO  [DeploymentUtils:197] Deployment my-cluster-131d7a1d-scraper is ready
2022-03-28 14:07:43 [ForkJoinPool-1-worker-15] INFO  [NetworkPolicyResource:187] Apply NetworkPolicy access to my-cluster-131d7a1d-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={})
2022-03-28 14:07:43 [ForkJoinPool-1-worker-15] DEBUG [NetworkPolicyResource:227] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=my-cluster-131d7a1d-allow, namespace=namespace-9, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[NetworkPolicyIngressRule(from=[NetworkPolicyPeer(ipBlock=null, namespaceSelector=null, podSelector=LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}), additionalProperties={})], ports=[NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=8083, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={}), NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=9404, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={}), NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=8080, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={}), NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=9999, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={})], additionalProperties={})], podSelector=LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=KafkaConnect, strimzi.io/name=my-cluster-131d7a1d-connect}, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 14:07:43 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:155] Create/Update NetworkPolicy my-cluster-131d7a1d-allow in namespace namespace-9
2022-03-28 14:07:43 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:164] Using Namespace: namespace-9
2022-03-28 14:07:43 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:06:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:07:44 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource NetworkPolicy:my-cluster-131d7a1d-allow
2022-03-28 14:07:44 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (179832ms till timeout)
2022-03-28 14:07:44 [ForkJoinPool-1-worker-15] INFO  [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}) successfully created
2022-03-28 14:07:44 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:155] Create/Update KafkaConnect my-cluster-131d7a1d in namespace namespace-9
2022-03-28 14:07:44 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:164] Using Namespace: namespace-9
2022-03-28 14:07:44 [ForkJoinPool-1-worker-15] WARN  [VersionUsageUtils:60] The client is using resource type 'kafkaconnects' with unstable version 'v1beta2'
2022-03-28 14:07:44 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:433] Wait for KafkaConnect: my-cluster-131d7a1d will have desired state: ReconciliationPaused
2022-03-28 14:07:44 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for KafkaConnect: my-cluster-131d7a1d will have desired state: ReconciliationPaused
2022-03-28 14:07:44 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:444] KafkaConnect: my-cluster-131d7a1d is in desired state: ReconciliationPaused
2022-03-28 14:07:44 [ForkJoinPool-1-worker-15] INFO  [PodUtils:209] Wait until Pod my-cluster-131d7a1d-connect will have stable 0 replicas
2022-03-28 14:07:44 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for  Podmy-cluster-131d7a1d-connect will have 0 replicas
2022-03-28 14:07:44 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 19 polls
2022-03-28 14:07:44 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-131d7a1d-connect will have 0 replicas not ready, will try again in 1000 ms (179712ms till timeout)
2022-03-28 14:07:45 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:06:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:07:45 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (178636ms till timeout)
2022-03-28 14:07:46 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 18 polls
2022-03-28 14:07:46 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-131d7a1d-connect will have 0 replicas not ready, will try again in 1000 ms (178335ms till timeout)
2022-03-28 14:07:46 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:06:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:07:46 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (177440ms till timeout)
2022-03-28 14:07:47 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 17 polls
2022-03-28 14:07:47 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-131d7a1d-connect will have 0 replicas not ready, will try again in 1000 ms (177047ms till timeout)
2022-03-28 14:07:47 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:06:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:07:47 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (176245ms till timeout)
2022-03-28 14:07:48 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 16 polls
2022-03-28 14:07:48 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-131d7a1d-connect will have 0 replicas not ready, will try again in 1000 ms (175762ms till timeout)
2022-03-28 14:07:48 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:06:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:07:48 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (175043ms till timeout)
2022-03-28 14:07:50 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:06:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:07:50 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 15 polls
2022-03-28 14:07:50 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-131d7a1d-connect will have 0 replicas not ready, will try again in 1000 ms (174384ms till timeout)
2022-03-28 14:07:50 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (173712ms till timeout)
2022-03-28 14:07:51 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:06:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:07:51 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 14 polls
2022-03-28 14:07:51 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-131d7a1d-connect will have 0 replicas not ready, will try again in 1000 ms (173005ms till timeout)
2022-03-28 14:07:51 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (172334ms till timeout)
2022-03-28 14:07:52 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:06:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:07:52 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 13 polls
2022-03-28 14:07:52 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-131d7a1d-connect will have 0 replicas not ready, will try again in 1000 ms (171628ms till timeout)
2022-03-28 14:07:53 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (170955ms till timeout)
2022-03-28 14:07:54 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:06:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:07:54 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 12 polls
2022-03-28 14:07:54 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-131d7a1d-connect will have 0 replicas not ready, will try again in 1000 ms (170249ms till timeout)
2022-03-28 14:07:54 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (169577ms till timeout)
2022-03-28 14:07:55 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:06:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:07:55 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 11 polls
2022-03-28 14:07:55 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-131d7a1d-connect will have 0 replicas not ready, will try again in 1000 ms (168872ms till timeout)
2022-03-28 14:07:55 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (168200ms till timeout)
2022-03-28 14:07:56 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:06:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:07:56 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 10 polls
2022-03-28 14:07:56 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-131d7a1d-connect will have 0 replicas not ready, will try again in 1000 ms (167587ms till timeout)
2022-03-28 14:07:57 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (166915ms till timeout)
2022-03-28 14:07:58 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:06:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:07:58 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 9 polls
2022-03-28 14:07:58 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-131d7a1d-connect will have 0 replicas not ready, will try again in 1000 ms (166210ms till timeout)
2022-03-28 14:07:58 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (165538ms till timeout)
2022-03-28 14:07:59 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:06:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:07:59 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 8 polls
2022-03-28 14:07:59 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-131d7a1d-connect will have 0 replicas not ready, will try again in 1000 ms (164833ms till timeout)
2022-03-28 14:07:59 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (164161ms till timeout)
2022-03-28 14:08:01 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:06:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:08:01 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 7 polls
2022-03-28 14:08:01 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-131d7a1d-connect will have 0 replicas not ready, will try again in 1000 ms (163456ms till timeout)
2022-03-28 14:08:01 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (162784ms till timeout)
2022-03-28 14:08:02 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:06:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:08:02 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 6 polls
2022-03-28 14:08:02 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-131d7a1d-connect will have 0 replicas not ready, will try again in 1000 ms (162080ms till timeout)
2022-03-28 14:08:02 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (161407ms till timeout)
2022-03-28 14:08:03 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:06:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:08:03 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 5 polls
2022-03-28 14:08:03 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-131d7a1d-connect will have 0 replicas not ready, will try again in 1000 ms (160703ms till timeout)
2022-03-28 14:08:03 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (160029ms till timeout)
2022-03-28 14:08:05 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:06:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:08:05 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 4 polls
2022-03-28 14:08:05 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-131d7a1d-connect will have 0 replicas not ready, will try again in 1000 ms (159326ms till timeout)
2022-03-28 14:08:05 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (158653ms till timeout)
2022-03-28 14:08:06 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:06:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:08:06 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 3 polls
2022-03-28 14:08:06 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-131d7a1d-connect will have 0 replicas not ready, will try again in 1000 ms (157949ms till timeout)
2022-03-28 14:08:06 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (157276ms till timeout)
2022-03-28 14:08:08 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:06:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:08:08 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 2 polls
2022-03-28 14:08:08 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-131d7a1d-connect will have 0 replicas not ready, will try again in 1000 ms (156567ms till timeout)
2022-03-28 14:08:08 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (155894ms till timeout)
2022-03-28 14:08:09 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:06:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:08:09 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 1 polls
2022-03-28 14:08:09 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-131d7a1d-connect will have 0 replicas not ready, will try again in 1000 ms (155189ms till timeout)
2022-03-28 14:08:09 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (154516ms till timeout)
2022-03-28 14:08:10 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:06:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:08:10 [ForkJoinPool-1-worker-15] INFO  [PodUtils:217] Pod replicas are stable for 20 polls intervals
2022-03-28 14:08:10 [ForkJoinPool-1-worker-15] INFO  [PodUtils:228] Pod my-cluster-131d7a1d-connect has 0 replicas
2022-03-28 14:08:10 [ForkJoinPool-1-worker-15] INFO  [ReconciliationST:108] Setting annotation to "false" and creating KafkaConnector
2022-03-28 14:08:10 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (153140ms till timeout)
2022-03-28 14:08:11 [ForkJoinPool-1-worker-15] INFO  [DeploymentUtils:161] Wait for Deployment: my-cluster-131d7a1d-connect will be ready
2022-03-28 14:08:11 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Wait for Deployment: my-cluster-131d7a1d-connect will be ready
2022-03-28 14:08:11 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-131d7a1d-connect will be ready not ready, will try again in 1000 ms (479810ms till timeout)
2022-03-28 14:08:11 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:06:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:08:12 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (151942ms till timeout)
2022-03-28 14:08:12 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-131d7a1d-connect will be ready not ready, will try again in 1000 ms (478713ms till timeout)
2022-03-28 14:08:13 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:06:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:08:13 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (150747ms till timeout)
2022-03-28 14:08:13 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-131d7a1d-connect will be ready not ready, will try again in 1000 ms (477616ms till timeout)
2022-03-28 14:08:14 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:06:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:08:14 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (149552ms till timeout)
2022-03-28 14:08:14 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-131d7a1d-connect will be ready not ready, will try again in 1000 ms (476518ms till timeout)
2022-03-28 14:08:15 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:06:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:08:15 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-131d7a1d-connect will be ready not ready, will try again in 1000 ms (475418ms till timeout)
2022-03-28 14:08:15 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (148261ms till timeout)
2022-03-28 14:08:16 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-131d7a1d-connect will be ready not ready, will try again in 1000 ms (474319ms till timeout)
2022-03-28 14:08:16 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:06:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:08:16 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (147065ms till timeout)
2022-03-28 14:08:17 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-131d7a1d-connect will be ready not ready, will try again in 1000 ms (473218ms till timeout)
2022-03-28 14:08:18 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:06:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:08:18 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (145871ms till timeout)
2022-03-28 14:08:18 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-131d7a1d-connect will be ready not ready, will try again in 1000 ms (472120ms till timeout)
2022-03-28 14:08:19 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:06:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:08:19 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (144677ms till timeout)
2022-03-28 14:08:20 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-131d7a1d-connect will be ready not ready, will try again in 1000 ms (471019ms till timeout)
2022-03-28 14:08:20 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:06:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:08:20 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (143483ms till timeout)
2022-03-28 14:08:21 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-131d7a1d-connect will be ready not ready, will try again in 1000 ms (469918ms till timeout)
2022-03-28 14:08:21 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:06:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:08:21 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (142290ms till timeout)
2022-03-28 14:08:22 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-131d7a1d-connect will be ready not ready, will try again in 1000 ms (468819ms till timeout)
2022-03-28 14:08:22 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:06:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:08:22 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (141095ms till timeout)
2022-03-28 14:08:23 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-131d7a1d-connect will be ready not ready, will try again in 1000 ms (467722ms till timeout)
2022-03-28 14:08:24 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:06:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:08:24 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (139900ms till timeout)
2022-03-28 14:08:24 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-131d7a1d-connect will be ready not ready, will try again in 1000 ms (466625ms till timeout)
2022-03-28 14:08:25 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:06:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:08:25 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (138705ms till timeout)
2022-03-28 14:08:25 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-131d7a1d-connect will be ready not ready, will try again in 1000 ms (465527ms till timeout)
2022-03-28 14:08:26 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:06:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:08:26 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (137511ms till timeout)
2022-03-28 14:08:26 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-131d7a1d-connect will be ready not ready, will try again in 1000 ms (464430ms till timeout)
2022-03-28 14:08:27 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:06:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:08:27 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (136314ms till timeout)
2022-03-28 14:08:27 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-131d7a1d-connect will be ready not ready, will try again in 1000 ms (463281ms till timeout)
2022-03-28 14:08:28 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:06:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:08:28 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-131d7a1d-connect will be ready not ready, will try again in 1000 ms (462184ms till timeout)
2022-03-28 14:08:28 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (135027ms till timeout)
2022-03-28 14:08:29 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-131d7a1d-connect will be ready not ready, will try again in 1000 ms (461084ms till timeout)
2022-03-28 14:08:30 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:06:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:08:30 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (133831ms till timeout)
2022-03-28 14:08:31 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-131d7a1d-connect will be ready not ready, will try again in 1000 ms (459986ms till timeout)
2022-03-28 14:08:31 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:06:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:08:31 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (132637ms till timeout)
2022-03-28 14:08:32 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-131d7a1d-connect will be ready not ready, will try again in 1000 ms (458888ms till timeout)
2022-03-28 14:08:32 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:06:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:08:32 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (131430ms till timeout)
2022-03-28 14:08:33 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-131d7a1d-connect will be ready not ready, will try again in 1000 ms (457789ms till timeout)
2022-03-28 14:08:33 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:06:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:08:33 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (130236ms till timeout)
2022-03-28 14:08:34 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-131d7a1d-connect will be ready not ready, will try again in 1000 ms (456691ms till timeout)
2022-03-28 14:08:34 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:06:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:08:34 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (129041ms till timeout)
2022-03-28 14:08:35 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-131d7a1d-connect will be ready not ready, will try again in 1000 ms (455594ms till timeout)
2022-03-28 14:08:36 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:06:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:08:36 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (127847ms till timeout)
2022-03-28 14:08:36 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-131d7a1d-connect will be ready not ready, will try again in 1000 ms (454496ms till timeout)
2022-03-28 14:08:37 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:06:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:08:37 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (126651ms till timeout)
2022-03-28 14:08:37 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-131d7a1d-connect will be ready not ready, will try again in 1000 ms (453399ms till timeout)
2022-03-28 14:08:38 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:06:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:08:38 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (125456ms till timeout)
2022-03-28 14:08:38 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-131d7a1d-connect will be ready not ready, will try again in 1000 ms (452302ms till timeout)
2022-03-28 14:08:39 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:06:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:08:39 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (124261ms till timeout)
2022-03-28 14:08:39 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-131d7a1d-connect will be ready not ready, will try again in 1000 ms (451204ms till timeout)
2022-03-28 14:08:40 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:06:59Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:08:40 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (123067ms till timeout)
2022-03-28 14:08:41 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-131d7a1d-connect will be ready not ready, will try again in 1000 ms (450033ms till timeout)
2022-03-28 14:08:42 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=null, completedIndexes=null, completionTime=2022-03-28T14:08:37Z, conditions=[JobCondition(lastProbeTime=2022-03-28T14:08:37Z, lastTransitionTime=2022-03-28T14:08:37Z, message=null, reason=null, status=True, type=Complete, additionalProperties={})], failed=null, ready=null, startTime=2022-03-28T14:06:59Z, succeeded=1, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:08:42 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-131d7a1d-connect will be ready not ready, will try again in 1000 ms (448935ms till timeout)
2022-03-28 14:08:42 [ForkJoinPool-1-worker-9] INFO  [PodUtils:186] Waiting for message will be in the log
2022-03-28 14:08:42 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Waiting for message will be in the log
2022-03-28 14:08:42 [ForkJoinPool-1-worker-9] INFO  [PodUtils:189] Message All topics created found in create-admin-my-cluster-419d5c31-kafka-clients-ld5xw log
2022-03-28 14:08:42 [ForkJoinPool-1-worker-9] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment create-admin-my-cluster-419d5c31-kafka-clients deletion
2022-03-28 14:08:42 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for ReplicaSet create-admin-my-cluster-419d5c31-kafka-clients to be deleted
2022-03-28 14:08:42 [ForkJoinPool-1-worker-9] DEBUG [JobUtils:40] Job create-admin-my-cluster-419d5c31-kafka-clients was deleted
2022-03-28 14:08:42 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:155] Create/Update Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st
2022-03-28 14:08:42 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:delete-admin-my-cluster-419d5c31-kafka-clients
2022-03-28 14:08:42 [ForkJoinPool-1-worker-9] INFO  [JobUtils:81] Waiting for job: delete-admin-my-cluster-419d5c31-kafka-clients will be in active state
2022-03-28 14:08:42 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 14:08:43 [ForkJoinPool-1-worker-9] INFO  [PodUtils:186] Waiting for message will be in the log
2022-03-28 14:08:43 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Waiting for message will be in the log
2022-03-28 14:08:43 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (299898ms till timeout)
2022-03-28 14:08:43 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-131d7a1d-connect will be ready not ready, will try again in 1000 ms (447750ms till timeout)
2022-03-28 14:08:44 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (298796ms till timeout)
2022-03-28 14:08:44 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-131d7a1d-connect will be ready not ready, will try again in 1000 ms (446644ms till timeout)
2022-03-28 14:08:45 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (297692ms till timeout)
2022-03-28 14:08:45 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-131d7a1d-connect will be ready not ready, will try again in 1000 ms (445543ms till timeout)
2022-03-28 14:08:46 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (296407ms till timeout)
2022-03-28 14:08:46 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-131d7a1d-connect will be ready not ready, will try again in 1000 ms (444354ms till timeout)
2022-03-28 14:08:47 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (295122ms till timeout)
2022-03-28 14:08:47 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-131d7a1d-connect will be ready not ready, will try again in 1000 ms (443070ms till timeout)
2022-03-28 14:08:49 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (293837ms till timeout)
2022-03-28 14:08:49 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-131d7a1d-connect will be ready not ready, will try again in 1000 ms (441785ms till timeout)
2022-03-28 14:08:50 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (292552ms till timeout)
2022-03-28 14:08:50 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-131d7a1d-connect will be ready not ready, will try again in 1000 ms (440499ms till timeout)
2022-03-28 14:08:51 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (291268ms till timeout)
2022-03-28 14:08:51 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-131d7a1d-connect will be ready not ready, will try again in 1000 ms (439216ms till timeout)
2022-03-28 14:08:53 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (289984ms till timeout)
2022-03-28 14:08:53 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-131d7a1d-connect will be ready not ready, will try again in 1000 ms (437931ms till timeout)
2022-03-28 14:08:54 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (288699ms till timeout)
2022-03-28 14:08:54 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-131d7a1d-connect will be ready not ready, will try again in 1000 ms (436646ms till timeout)
2022-03-28 14:08:55 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (287415ms till timeout)
2022-03-28 14:08:55 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-131d7a1d-connect will be ready not ready, will try again in 1000 ms (435362ms till timeout)
2022-03-28 14:08:56 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (286131ms till timeout)
2022-03-28 14:08:56 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-131d7a1d-connect will be ready not ready, will try again in 1000 ms (434078ms till timeout)
2022-03-28 14:08:58 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (284845ms till timeout)
2022-03-28 14:08:58 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-131d7a1d-connect will be ready not ready, will try again in 1000 ms (432793ms till timeout)
2022-03-28 14:08:59 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (283561ms till timeout)
2022-03-28 14:08:59 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-131d7a1d-connect will be ready not ready, will try again in 1000 ms (431508ms till timeout)
2022-03-28 14:09:00 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (282277ms till timeout)
2022-03-28 14:09:00 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-131d7a1d-connect will be ready not ready, will try again in 1000 ms (430224ms till timeout)
2022-03-28 14:09:02 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (280991ms till timeout)
2022-03-28 14:09:02 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-131d7a1d-connect will be ready not ready, will try again in 1000 ms (428938ms till timeout)
2022-03-28 14:09:03 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (279707ms till timeout)
2022-03-28 14:09:03 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-131d7a1d-connect will be ready not ready, will try again in 1000 ms (427655ms till timeout)
2022-03-28 14:09:04 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (278419ms till timeout)
2022-03-28 14:09:04 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-131d7a1d-connect will be ready not ready, will try again in 1000 ms (426366ms till timeout)
2022-03-28 14:09:05 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (277136ms till timeout)
2022-03-28 14:09:05 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-131d7a1d-connect will be ready not ready, will try again in 1000 ms (425083ms till timeout)
2022-03-28 14:09:07 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (275851ms till timeout)
2022-03-28 14:09:07 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-131d7a1d-connect will be ready not ready, will try again in 1000 ms (423798ms till timeout)
2022-03-28 14:09:08 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (274567ms till timeout)
2022-03-28 14:09:08 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-131d7a1d-connect will be ready not ready, will try again in 1000 ms (422514ms till timeout)
2022-03-28 14:09:09 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (273283ms till timeout)
2022-03-28 14:09:09 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-131d7a1d-connect will be ready not ready, will try again in 1000 ms (421230ms till timeout)
2022-03-28 14:09:11 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (271999ms till timeout)
2022-03-28 14:09:11 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-131d7a1d-connect will be ready not ready, will try again in 1000 ms (419947ms till timeout)
2022-03-28 14:09:12 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (270716ms till timeout)
2022-03-28 14:09:12 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-131d7a1d-connect will be ready not ready, will try again in 1000 ms (418663ms till timeout)
2022-03-28 14:09:13 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (269430ms till timeout)
2022-03-28 14:09:13 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-131d7a1d-connect will be ready not ready, will try again in 1000 ms (417378ms till timeout)
2022-03-28 14:09:14 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (268145ms till timeout)
2022-03-28 14:09:14 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-131d7a1d-connect will be ready not ready, will try again in 1000 ms (416093ms till timeout)
2022-03-28 14:09:16 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (266860ms till timeout)
2022-03-28 14:09:16 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-131d7a1d-connect will be ready not ready, will try again in 1000 ms (414807ms till timeout)
2022-03-28 14:09:17 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (265576ms till timeout)
2022-03-28 14:09:17 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-131d7a1d-connect will be ready not ready, will try again in 1000 ms (413522ms till timeout)
2022-03-28 14:09:18 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (264293ms till timeout)
2022-03-28 14:09:18 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-131d7a1d-connect will be ready not ready, will try again in 1000 ms (412240ms till timeout)
2022-03-28 14:09:20 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (263007ms till timeout)
2022-03-28 14:09:20 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-131d7a1d-connect will be ready not ready, will try again in 1000 ms (410954ms till timeout)
2022-03-28 14:09:21 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (261723ms till timeout)
2022-03-28 14:09:21 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-131d7a1d-connect will be ready not ready, will try again in 1000 ms (409670ms till timeout)
2022-03-28 14:09:22 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (260439ms till timeout)
2022-03-28 14:09:22 [ForkJoinPool-1-worker-15] INFO  [DeploymentUtils:168] Deployment: my-cluster-131d7a1d-connect is ready
2022-03-28 14:09:22 [ForkJoinPool-1-worker-15] INFO  [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-131d7a1d-connect to be ready
2022-03-28 14:09:22 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=KafkaConnect, strimzi.io/name=my-cluster-131d7a1d-connect}, additionalProperties={})to be ready
2022-03-28 14:09:22 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-connect-6597748bd8-wl6qh not ready: my-cluster-131d7a1d-connect)
2022-03-28 14:09:22 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-131d7a1d-connect-6597748bd8-wl6qh are ready
2022-03-28 14:09:22 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=KafkaConnect, strimzi.io/name=my-cluster-131d7a1d-connect}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599901ms till timeout)
2022-03-28 14:09:23 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (259153ms till timeout)
2022-03-28 14:09:23 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-connect-6597748bd8-wl6qh not ready: my-cluster-131d7a1d-connect)
2022-03-28 14:09:23 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-131d7a1d-connect-6597748bd8-wl6qh are ready
2022-03-28 14:09:23 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=KafkaConnect, strimzi.io/name=my-cluster-131d7a1d-connect}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598801ms till timeout)
2022-03-28 14:09:25 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (257831ms till timeout)
2022-03-28 14:09:25 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-connect-6597748bd8-wl6qh not ready: my-cluster-131d7a1d-connect)
2022-03-28 14:09:25 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-131d7a1d-connect-6597748bd8-wl6qh are ready
2022-03-28 14:09:25 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=KafkaConnect, strimzi.io/name=my-cluster-131d7a1d-connect}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597489ms till timeout)
2022-03-28 14:09:26 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (256548ms till timeout)
2022-03-28 14:09:26 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-connect-6597748bd8-wl6qh not ready: my-cluster-131d7a1d-connect)
2022-03-28 14:09:26 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-131d7a1d-connect-6597748bd8-wl6qh are ready
2022-03-28 14:09:26 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=KafkaConnect, strimzi.io/name=my-cluster-131d7a1d-connect}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596206ms till timeout)
2022-03-28 14:09:27 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (255264ms till timeout)
2022-03-28 14:09:27 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-connect-6597748bd8-wl6qh not ready: my-cluster-131d7a1d-connect)
2022-03-28 14:09:27 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-131d7a1d-connect-6597748bd8-wl6qh are ready
2022-03-28 14:09:27 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=KafkaConnect, strimzi.io/name=my-cluster-131d7a1d-connect}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594920ms till timeout)
2022-03-28 14:09:29 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (253979ms till timeout)
2022-03-28 14:09:29 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-connect-6597748bd8-wl6qh not ready: my-cluster-131d7a1d-connect)
2022-03-28 14:09:29 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-131d7a1d-connect-6597748bd8-wl6qh are ready
2022-03-28 14:09:29 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=KafkaConnect, strimzi.io/name=my-cluster-131d7a1d-connect}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593637ms till timeout)
2022-03-28 14:09:30 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (252695ms till timeout)
2022-03-28 14:09:30 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-connect-6597748bd8-wl6qh not ready: my-cluster-131d7a1d-connect)
2022-03-28 14:09:30 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-131d7a1d-connect-6597748bd8-wl6qh are ready
2022-03-28 14:09:30 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=KafkaConnect, strimzi.io/name=my-cluster-131d7a1d-connect}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592353ms till timeout)
2022-03-28 14:09:31 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (251411ms till timeout)
2022-03-28 14:09:31 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-connect-6597748bd8-wl6qh not ready: my-cluster-131d7a1d-connect)
2022-03-28 14:09:31 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-131d7a1d-connect-6597748bd8-wl6qh are ready
2022-03-28 14:09:31 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=KafkaConnect, strimzi.io/name=my-cluster-131d7a1d-connect}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591069ms till timeout)
2022-03-28 14:09:32 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (250127ms till timeout)
2022-03-28 14:09:32 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-connect-6597748bd8-wl6qh not ready: my-cluster-131d7a1d-connect)
2022-03-28 14:09:32 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-131d7a1d-connect-6597748bd8-wl6qh are ready
2022-03-28 14:09:32 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=KafkaConnect, strimzi.io/name=my-cluster-131d7a1d-connect}, additionalProperties={})to be ready not ready, will try again in 1000 ms (589785ms till timeout)
2022-03-28 14:09:34 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (248844ms till timeout)
2022-03-28 14:09:34 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-connect-6597748bd8-wl6qh not ready: my-cluster-131d7a1d-connect)
2022-03-28 14:09:34 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-131d7a1d-connect-6597748bd8-wl6qh are ready
2022-03-28 14:09:34 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-131d7a1d, strimzi.io/kind=KafkaConnect, strimzi.io/name=my-cluster-131d7a1d-connect}, additionalProperties={})to be ready not ready, will try again in 1000 ms (588502ms till timeout)
2022-03-28 14:09:35 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (247561ms till timeout)
2022-03-28 14:09:35 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-131d7a1d-connect-6597748bd8-wl6qh not ready: my-cluster-131d7a1d-connect)
2022-03-28 14:09:35 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-131d7a1d-connect-6597748bd8-wl6qh are ready
2022-03-28 14:09:35 [ForkJoinPool-1-worker-15] INFO  [DeploymentUtils:197] Deployment my-cluster-131d7a1d-connect is ready
2022-03-28 14:09:35 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:155] Create/Update KafkaConnector my-cluster-131d7a1d in namespace namespace-9
2022-03-28 14:09:35 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:164] Using Namespace: namespace-9
2022-03-28 14:09:35 [ForkJoinPool-1-worker-15] WARN  [VersionUsageUtils:60] The client is using resource type 'kafkaconnectors' with unstable version 'v1beta2'
2022-03-28 14:09:35 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaConnector:my-cluster-131d7a1d
2022-03-28 14:09:35 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:433] Wait for KafkaConnector: my-cluster-131d7a1d will have desired state: Ready
2022-03-28 14:09:35 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for KafkaConnector: my-cluster-131d7a1d will have desired state: Ready
2022-03-28 14:09:35 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaConnector: my-cluster-131d7a1d will have desired state: Ready not ready, will try again in 1000 ms (239904ms till timeout)
2022-03-28 14:09:36 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (246276ms till timeout)
2022-03-28 14:09:37 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:444] KafkaConnector: my-cluster-131d7a1d is in desired state: Ready
2022-03-28 14:09:37 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 exec my-cluster-131d7a1d-connect-6597748bd8-wl6qh -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-131d7a1d
2022-03-28 14:09:38 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (244990ms till timeout)
2022-03-28 14:09:38 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-9 exec my-cluster-131d7a1d-connect-6597748bd8-wl6qh -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-131d7a1d
2022-03-28 14:09:38 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 14:09:38 [ForkJoinPool-1-worker-15] INFO  [ReconciliationST:118] Adding pause annotation into the KafkaConnector and scaling taskMax to 4
2022-03-28 14:09:39 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:433] Wait for KafkaConnector: my-cluster-131d7a1d will have desired state: ReconciliationPaused
2022-03-28 14:09:39 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for KafkaConnector: my-cluster-131d7a1d will have desired state: ReconciliationPaused
2022-03-28 14:09:39 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (243692ms till timeout)
2022-03-28 14:09:39 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:444] KafkaConnector: my-cluster-131d7a1d is in desired state: ReconciliationPaused
2022-03-28 14:09:39 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Connector's spec will be stable
2022-03-28 14:09:39 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 exec my-cluster-131d7a1d-connect-6597748bd8-wl6qh -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-131d7a1d
2022-03-28 14:09:40 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-9 exec my-cluster-131d7a1d-connect-6597748bd8-wl6qh -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-131d7a1d
2022-03-28 14:09:40 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 14:09:40 [ForkJoinPool-1-worker-15] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 19 polls
2022-03-28 14:09:40 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (178810ms till timeout)
2022-03-28 14:09:40 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (242404ms till timeout)
2022-03-28 14:09:41 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 exec my-cluster-131d7a1d-connect-6597748bd8-wl6qh -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-131d7a1d
2022-03-28 14:09:42 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (241116ms till timeout)
2022-03-28 14:09:42 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-9 exec my-cluster-131d7a1d-connect-6597748bd8-wl6qh -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-131d7a1d
2022-03-28 14:09:42 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 14:09:42 [ForkJoinPool-1-worker-15] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 18 polls
2022-03-28 14:09:42 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (176598ms till timeout)
2022-03-28 14:09:43 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (239832ms till timeout)
2022-03-28 14:09:43 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 exec my-cluster-131d7a1d-connect-6597748bd8-wl6qh -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-131d7a1d
2022-03-28 14:09:44 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (238548ms till timeout)
2022-03-28 14:09:45 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-9 exec my-cluster-131d7a1d-connect-6597748bd8-wl6qh -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-131d7a1d
2022-03-28 14:09:45 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 14:09:45 [ForkJoinPool-1-worker-15] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 17 polls
2022-03-28 14:09:45 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (174273ms till timeout)
2022-03-28 14:09:45 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (237257ms till timeout)
2022-03-28 14:09:46 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 exec my-cluster-131d7a1d-connect-6597748bd8-wl6qh -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-131d7a1d
2022-03-28 14:09:47 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (235969ms till timeout)
2022-03-28 14:09:47 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-9 exec my-cluster-131d7a1d-connect-6597748bd8-wl6qh -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-131d7a1d
2022-03-28 14:09:47 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 14:09:47 [ForkJoinPool-1-worker-15] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 16 polls
2022-03-28 14:09:47 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (172069ms till timeout)
2022-03-28 14:09:48 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 exec my-cluster-131d7a1d-connect-6597748bd8-wl6qh -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-131d7a1d
2022-03-28 14:09:48 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (234683ms till timeout)
2022-03-28 14:09:49 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-9 exec my-cluster-131d7a1d-connect-6597748bd8-wl6qh -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-131d7a1d
2022-03-28 14:09:49 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 14:09:49 [ForkJoinPool-1-worker-15] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 15 polls
2022-03-28 14:09:49 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (169837ms till timeout)
2022-03-28 14:09:49 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (233400ms till timeout)
2022-03-28 14:09:50 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 exec my-cluster-131d7a1d-connect-6597748bd8-wl6qh -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-131d7a1d
2022-03-28 14:09:51 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (232115ms till timeout)
2022-03-28 14:09:51 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-9 exec my-cluster-131d7a1d-connect-6597748bd8-wl6qh -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-131d7a1d
2022-03-28 14:09:51 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 14:09:51 [ForkJoinPool-1-worker-15] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 14 polls
2022-03-28 14:09:51 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (167626ms till timeout)
2022-03-28 14:09:52 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (230831ms till timeout)
2022-03-28 14:09:52 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 exec my-cluster-131d7a1d-connect-6597748bd8-wl6qh -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-131d7a1d
2022-03-28 14:09:53 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (229548ms till timeout)
2022-03-28 14:09:54 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-9 exec my-cluster-131d7a1d-connect-6597748bd8-wl6qh -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-131d7a1d
2022-03-28 14:09:54 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 14:09:54 [ForkJoinPool-1-worker-15] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 13 polls
2022-03-28 14:09:54 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (165408ms till timeout)
2022-03-28 14:09:54 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (228262ms till timeout)
2022-03-28 14:09:55 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 exec my-cluster-131d7a1d-connect-6597748bd8-wl6qh -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-131d7a1d
2022-03-28 14:09:56 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (226974ms till timeout)
2022-03-28 14:09:56 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-9 exec my-cluster-131d7a1d-connect-6597748bd8-wl6qh -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-131d7a1d
2022-03-28 14:09:56 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 14:09:56 [ForkJoinPool-1-worker-15] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 12 polls
2022-03-28 14:09:56 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (163215ms till timeout)
2022-03-28 14:09:57 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 exec my-cluster-131d7a1d-connect-6597748bd8-wl6qh -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-131d7a1d
2022-03-28 14:09:57 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (225689ms till timeout)
2022-03-28 14:09:58 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-9 exec my-cluster-131d7a1d-connect-6597748bd8-wl6qh -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-131d7a1d
2022-03-28 14:09:58 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 14:09:58 [ForkJoinPool-1-worker-15] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 11 polls
2022-03-28 14:09:58 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (160983ms till timeout)
2022-03-28 14:09:58 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (224400ms till timeout)
2022-03-28 14:09:59 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 exec my-cluster-131d7a1d-connect-6597748bd8-wl6qh -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-131d7a1d
2022-03-28 14:10:00 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (223116ms till timeout)
2022-03-28 14:10:00 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-9 exec my-cluster-131d7a1d-connect-6597748bd8-wl6qh -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-131d7a1d
2022-03-28 14:10:00 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 14:10:00 [ForkJoinPool-1-worker-15] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 10 polls
2022-03-28 14:10:00 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (158779ms till timeout)
2022-03-28 14:10:01 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (221831ms till timeout)
2022-03-28 14:10:01 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 exec my-cluster-131d7a1d-connect-6597748bd8-wl6qh -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-131d7a1d
2022-03-28 14:10:02 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (220547ms till timeout)
2022-03-28 14:10:02 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-9 exec my-cluster-131d7a1d-connect-6597748bd8-wl6qh -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-131d7a1d
2022-03-28 14:10:02 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 14:10:02 [ForkJoinPool-1-worker-15] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 9 polls
2022-03-28 14:10:02 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (156568ms till timeout)
2022-03-28 14:10:03 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (219262ms till timeout)
2022-03-28 14:10:03 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 exec my-cluster-131d7a1d-connect-6597748bd8-wl6qh -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-131d7a1d
2022-03-28 14:10:05 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-9 exec my-cluster-131d7a1d-connect-6597748bd8-wl6qh -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-131d7a1d
2022-03-28 14:10:05 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 14:10:05 [ForkJoinPool-1-worker-15] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 8 polls
2022-03-28 14:10:05 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (154356ms till timeout)
2022-03-28 14:10:05 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (217978ms till timeout)
2022-03-28 14:10:06 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 exec my-cluster-131d7a1d-connect-6597748bd8-wl6qh -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-131d7a1d
2022-03-28 14:10:06 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (216695ms till timeout)
2022-03-28 14:10:07 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-9 exec my-cluster-131d7a1d-connect-6597748bd8-wl6qh -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-131d7a1d
2022-03-28 14:10:07 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 14:10:07 [ForkJoinPool-1-worker-15] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 7 polls
2022-03-28 14:10:07 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (152116ms till timeout)
2022-03-28 14:10:07 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (215410ms till timeout)
2022-03-28 14:10:08 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 exec my-cluster-131d7a1d-connect-6597748bd8-wl6qh -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-131d7a1d
2022-03-28 14:10:08 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (214126ms till timeout)
2022-03-28 14:10:09 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-9 exec my-cluster-131d7a1d-connect-6597748bd8-wl6qh -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-131d7a1d
2022-03-28 14:10:09 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 14:10:09 [ForkJoinPool-1-worker-15] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 6 polls
2022-03-28 14:10:09 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (149883ms till timeout)
2022-03-28 14:10:10 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (212840ms till timeout)
2022-03-28 14:10:10 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 exec my-cluster-131d7a1d-connect-6597748bd8-wl6qh -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-131d7a1d
2022-03-28 14:10:11 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (211557ms till timeout)
2022-03-28 14:10:11 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-9 exec my-cluster-131d7a1d-connect-6597748bd8-wl6qh -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-131d7a1d
2022-03-28 14:10:11 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 14:10:11 [ForkJoinPool-1-worker-15] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 5 polls
2022-03-28 14:10:11 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (147683ms till timeout)
2022-03-28 14:10:12 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 exec my-cluster-131d7a1d-connect-6597748bd8-wl6qh -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-131d7a1d
2022-03-28 14:10:12 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (210132ms till timeout)
2022-03-28 14:10:13 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-9 exec my-cluster-131d7a1d-connect-6597748bd8-wl6qh -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-131d7a1d
2022-03-28 14:10:13 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 14:10:13 [ForkJoinPool-1-worker-15] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 4 polls
2022-03-28 14:10:13 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (145468ms till timeout)
2022-03-28 14:10:14 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (208844ms till timeout)
2022-03-28 14:10:14 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 exec my-cluster-131d7a1d-connect-6597748bd8-wl6qh -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-131d7a1d
2022-03-28 14:10:15 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (207559ms till timeout)
2022-03-28 14:10:16 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-9 exec my-cluster-131d7a1d-connect-6597748bd8-wl6qh -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-131d7a1d
2022-03-28 14:10:16 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 14:10:16 [ForkJoinPool-1-worker-15] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 3 polls
2022-03-28 14:10:16 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (143293ms till timeout)
2022-03-28 14:10:16 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (206275ms till timeout)
2022-03-28 14:10:17 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 exec my-cluster-131d7a1d-connect-6597748bd8-wl6qh -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-131d7a1d
2022-03-28 14:10:18 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (204987ms till timeout)
2022-03-28 14:10:18 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-9 exec my-cluster-131d7a1d-connect-6597748bd8-wl6qh -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-131d7a1d
2022-03-28 14:10:18 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 14:10:18 [ForkJoinPool-1-worker-15] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 2 polls
2022-03-28 14:10:18 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (141000ms till timeout)
2022-03-28 14:10:19 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (203703ms till timeout)
2022-03-28 14:10:19 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 exec my-cluster-131d7a1d-connect-6597748bd8-wl6qh -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-131d7a1d
2022-03-28 14:10:20 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-9 exec my-cluster-131d7a1d-connect-6597748bd8-wl6qh -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-131d7a1d
2022-03-28 14:10:20 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 14:10:20 [ForkJoinPool-1-worker-15] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 1 polls
2022-03-28 14:10:20 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (138781ms till timeout)
2022-03-28 14:10:20 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (202418ms till timeout)
2022-03-28 14:10:21 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 exec my-cluster-131d7a1d-connect-6597748bd8-wl6qh -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-131d7a1d
2022-03-28 14:10:21 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (201131ms till timeout)
2022-03-28 14:10:22 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-9 exec my-cluster-131d7a1d-connect-6597748bd8-wl6qh -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-131d7a1d
2022-03-28 14:10:22 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 14:10:22 [ForkJoinPool-1-worker-15] INFO  [KafkaConnectorUtils:154] Connector's spec is stable for 20 polls intervals
2022-03-28 14:10:22 [ForkJoinPool-1-worker-15] INFO  [ReconciliationST:127] Setting annotation to "false", taskMax should be increased to 4
2022-03-28 14:10:23 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (199826ms till timeout)
2022-03-28 14:10:23 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Wait for KafkaConnector config will contain desired config
2022-03-28 14:10:23 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 exec my-cluster-131d7a1d-connect-6597748bd8-wl6qh -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-131d7a1d/config
2022-03-28 14:10:24 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-9 exec my-cluster-131d7a1d-connect-6597748bd8-wl6qh -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-131d7a1d/config
2022-03-28 14:10:24 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 14:10:24 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 exec my-cluster-131d7a1d-connect-6597748bd8-wl6qh -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-131d7a1d/config
2022-03-28 14:10:24 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (198531ms till timeout)
2022-03-28 14:10:25 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-9 exec my-cluster-131d7a1d-connect-6597748bd8-wl6qh -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-131d7a1d/config
2022-03-28 14:10:25 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 14:10:25 [ForkJoinPool-1-worker-15] DEBUG [AbstractST:674] ============================================================================
2022-03-28 14:10:25 [ForkJoinPool-1-worker-15] DEBUG [AbstractST:675] [operators.ReconciliationST - After Each] - Clean up after test
2022-03-28 14:10:25 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:344] ############################################################################
2022-03-28 14:10:25 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:348] Delete all resources for testPauseReconciliationInKafkaAndKafkaConnectWithConnector
2022-03-28 14:10:25 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:241] Delete of NetworkPolicy my-cluster-131d7a1d-allow in namespace namespace-9
2022-03-28 14:10:25 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of Deployment my-cluster-131d7a1d-kafka-clients in namespace namespace-9
2022-03-28 14:10:25 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:241] Delete of KafkaConnector my-cluster-131d7a1d in namespace namespace-9
2022-03-28 14:10:25 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:241] Delete of Deployment my-cluster-131d7a1d-scraper in namespace namespace-9
2022-03-28 14:10:25 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:241] Delete of Kafka my-cluster-131d7a1d in namespace namespace-9
2022-03-28 14:10:25 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (197246ms till timeout)
2022-03-28 14:10:25 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource NetworkPolicy:my-cluster-131d7a1d-allow
2022-03-28 14:10:25 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:my-cluster-131d7a1d-kafka-clients
2022-03-28 14:10:25 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:my-cluster-131d7a1d-scraper
2022-03-28 14:10:25 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaConnector:my-cluster-131d7a1d
2022-03-28 14:10:25 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-131d7a1d
2022-03-28 14:10:26 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:241] Delete of KafkaConnect my-cluster-131d7a1d in namespace namespace-9
2022-03-28 14:10:26 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaConnect:my-cluster-131d7a1d
2022-03-28 14:10:26 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-131d7a1d-scraper not ready, will try again in 10000 ms (479510ms till timeout)
2022-03-28 14:10:26 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-131d7a1d-kafka-clients not ready, will try again in 10000 ms (479399ms till timeout)
2022-03-28 14:10:27 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (196049ms till timeout)
2022-03-28 14:10:28 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (194760ms till timeout)
2022-03-28 14:10:29 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (193476ms till timeout)
2022-03-28 14:10:30 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (192192ms till timeout)
2022-03-28 14:10:32 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (190903ms till timeout)
2022-03-28 14:10:33 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (189618ms till timeout)
2022-03-28 14:10:34 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (188335ms till timeout)
2022-03-28 14:10:36 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (187050ms till timeout)
2022-03-28 14:10:36 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-131d7a1d-scraper not ready, will try again in 10000 ms (469300ms till timeout)
2022-03-28 14:10:36 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-131d7a1d-kafka-clients not ready, will try again in 10000 ms (469012ms till timeout)
2022-03-28 14:10:37 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (185858ms till timeout)
2022-03-28 14:10:38 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (184575ms till timeout)
2022-03-28 14:10:39 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (183292ms till timeout)
2022-03-28 14:10:41 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (182007ms till timeout)
2022-03-28 14:10:42 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (180724ms till timeout)
2022-03-28 14:10:43 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (179441ms till timeout)
2022-03-28 14:10:44 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (178157ms till timeout)
2022-03-28 14:10:46 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (176870ms till timeout)
2022-03-28 14:10:46 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-131d7a1d-scraper not ready, will try again in 10000 ms (459089ms till timeout)
2022-03-28 14:10:47 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-131d7a1d-kafka-clients not ready, will try again in 10000 ms (458716ms till timeout)
2022-03-28 14:10:47 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (175655ms till timeout)
2022-03-28 14:10:48 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (174370ms till timeout)
2022-03-28 14:10:50 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (173085ms till timeout)
2022-03-28 14:10:51 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (171800ms till timeout)
2022-03-28 14:10:52 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (170511ms till timeout)
2022-03-28 14:10:53 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (169226ms till timeout)
2022-03-28 14:10:55 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (167934ms till timeout)
2022-03-28 14:10:56 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (166650ms till timeout)
2022-03-28 14:10:57 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-131d7a1d-scraper not ready, will try again in 10000 ms (448864ms till timeout)
2022-03-28 14:10:57 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (165288ms till timeout)
2022-03-28 14:10:57 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-131d7a1d-kafka-clients not ready, will try again in 10000 ms (448028ms till timeout)
2022-03-28 14:10:59 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (164002ms till timeout)
2022-03-28 14:11:00 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (162719ms till timeout)
2022-03-28 14:11:01 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (161435ms till timeout)
2022-03-28 14:11:02 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (160152ms till timeout)
2022-03-28 14:11:04 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (158867ms till timeout)
2022-03-28 14:11:05 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (157583ms till timeout)
2022-03-28 14:11:06 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (156299ms till timeout)
2022-03-28 14:11:08 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (155013ms till timeout)
2022-03-28 14:11:08 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:369] ############################################################################
2022-03-28 14:11:08 [ForkJoinPool-1-worker-15] INFO  [TestSuiteNamespaceManager:200] Deleting namespace:namespace-9 for test case:testPauseReconciliationInKafkaAndKafkaConnectWithConnector
2022-03-28 14:11:08 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Namespace namespace-9 removal
2022-03-28 14:11:08 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 14:11:08 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 14:11:08 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 14:11:08 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-9 removal not ready, will try again in 1000 ms (479535ms till timeout)
2022-03-28 14:11:09 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (153727ms till timeout)
2022-03-28 14:11:09 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 14:11:10 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 14:11:10 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 14:11:10 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-9 removal not ready, will try again in 1000 ms (478044ms till timeout)
2022-03-28 14:11:10 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (152443ms till timeout)
2022-03-28 14:11:11 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 14:11:11 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 14:11:11 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 14:11:11 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-9 removal not ready, will try again in 1000 ms (476512ms till timeout)
2022-03-28 14:11:11 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (151159ms till timeout)
2022-03-28 14:11:12 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 14:11:13 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 14:11:13 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 14:11:13 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-9 removal not ready, will try again in 1000 ms (475063ms till timeout)
2022-03-28 14:11:13 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (149876ms till timeout)
2022-03-28 14:11:14 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 14:11:14 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (148593ms till timeout)
2022-03-28 14:11:14 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 14:11:14 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 14:11:14 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-9 removal not ready, will try again in 1000 ms (473592ms till timeout)
2022-03-28 14:11:15 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 14:11:15 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (147305ms till timeout)
2022-03-28 14:11:16 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 14:11:16 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 1
2022-03-28 14:11:16 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 14:11:16 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Error from server (NotFound): namespaces "namespace-9" not found
2022-03-28 14:11:16 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] ======STDERR END======
2022-03-28 14:11:16 [ForkJoinPool-1-worker-15] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[], io.strimzi.test.logs.CollectorElement@c4c0ea0=[], io.strimzi.test.logs.CollectorElement@aec142ef=[], io.strimzi.test.logs.CollectorElement@24b97ba9=[], io.strimzi.test.logs.CollectorElement@f851b6c3=[], io.strimzi.test.logs.CollectorElement@3881d5f2=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@e3067dd1=[], io.strimzi.test.logs.CollectorElement@5c7379cb=[], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[], io.strimzi.test.logs.CollectorElement@f4405b3b=[], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 14:11:16 [ForkJoinPool-1-worker-15] DEBUG [SuiteThreadController:267] testPauseReconciliationInKafkaAndKafkaConnectWithConnector - Notifies waiting test cases:[testCapacityFile, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testThrottlingQuotasCreateAlterPartitions, testSendSimpleMessageTls, testReceiveSimpleMessageTlsScramSha, testSendSimpleMessageTlsScramSha, testReceiveSimpleMessageTls, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testConfigurationFileIsCreated, testPauseReconciliationInKafkaRebalanceAndTopic, testTopicModificationOfReplicationFactor, testMoreReplicasThanAvailableBrokers, testCreateTopicViaKafka, testCreateTopicAfterUnsupportedOperation, testPauseReconciliationInKafkaAndKafkaConnectWithConnector] to and randomly select one to start execution
2022-03-28 14:11:16 [ForkJoinPool-1-worker-15] DEBUG [SuiteThreadController:93] [operators.ReconciliationST] - Removing parallel test: testPauseReconciliationInKafkaAndKafkaConnectWithConnector
2022-03-28 14:11:16 [ForkJoinPool-1-worker-15] DEBUG [SuiteThreadController:97] [operators.ReconciliationST] - Parallel test count: 1
2022-03-28 14:11:16 [ForkJoinPool-1-worker-15] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.ReconciliationST.testPauseReconciliationInKafkaAndKafkaConnectWithConnector-FINISHED
2022-03-28 14:11:16 [ForkJoinPool-1-worker-15] INFO  [TestSeparator:30] ############################################################################
2022-03-28 14:11:16 [ForkJoinPool-1-worker-15] DEBUG [AbstractST:689] ============================================================================
2022-03-28 14:11:16 [ForkJoinPool-1-worker-15] DEBUG [AbstractST:690] [operators.ReconciliationST - After All] - Clean up after test suite
2022-03-28 14:11:16 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:344] ############################################################################
2022-03-28 14:11:16 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:346] In context ReconciliationST is everything deleted.
2022-03-28 14:11:16 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:369] ############################################################################
2022-03-28 14:11:16 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Namespace reconciliation-st removal
2022-03-28 14:11:16 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace reconciliation-st -o yaml
2022-03-28 14:11:16 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace reconciliation-st -o yaml
2022-03-28 14:11:16 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 14:11:16 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace reconciliation-st removal not ready, will try again in 1000 ms (479510ms till timeout)
2022-03-28 14:11:17 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (146022ms till timeout)
2022-03-28 14:11:17 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace reconciliation-st -o yaml
2022-03-28 14:11:18 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace reconciliation-st -o yaml
2022-03-28 14:11:18 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 14:11:18 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace reconciliation-st removal not ready, will try again in 1000 ms (478010ms till timeout)
2022-03-28 14:11:18 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (144735ms till timeout)
2022-03-28 14:11:19 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace reconciliation-st -o yaml
2022-03-28 14:11:19 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (143451ms till timeout)
2022-03-28 14:11:19 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace reconciliation-st -o yaml
2022-03-28 14:11:19 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 14:11:19 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace reconciliation-st removal not ready, will try again in 1000 ms (476535ms till timeout)
2022-03-28 14:11:20 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace reconciliation-st -o yaml
2022-03-28 14:11:20 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (142164ms till timeout)
2022-03-28 14:11:21 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace reconciliation-st -o yaml
2022-03-28 14:11:21 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 14:11:21 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace reconciliation-st removal not ready, will try again in 1000 ms (474979ms till timeout)
2022-03-28 14:11:22 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace reconciliation-st -o yaml
2022-03-28 14:11:22 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (140861ms till timeout)
2022-03-28 14:11:22 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-9 get Namespace reconciliation-st -o yaml
2022-03-28 14:11:22 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 1
2022-03-28 14:11:22 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 14:11:22 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Error from server (NotFound): namespaces "reconciliation-st" not found
2022-03-28 14:11:22 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] ======STDERR END======
2022-03-28 14:11:22 [ForkJoinPool-1-worker-15] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[], io.strimzi.test.logs.CollectorElement@c4c0ea0=[], io.strimzi.test.logs.CollectorElement@aec142ef=[], io.strimzi.test.logs.CollectorElement@24b97ba9=[], io.strimzi.test.logs.CollectorElement@f851b6c3=[], io.strimzi.test.logs.CollectorElement@3881d5f2=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@e3067dd1=[], io.strimzi.test.logs.CollectorElement@5c7379cb=[], io.strimzi.test.logs.CollectorElement@62b9e483=[], io.strimzi.test.logs.CollectorElement@b850ef2a=[], io.strimzi.test.logs.CollectorElement@f4405b3b=[], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 14:11:22 [ForkJoinPool-1-worker-15] DEBUG [SuiteThreadController:254] ReconciliationST - Notifies waiting test suites:[UserST, ThrottlingQuotaST, TopicST, CruiseControlConfigurationST, HttpBridgeScramShaST, HttpBridgeTlsST] to and randomly select one to start execution
2022-03-28 14:11:22 [ForkJoinPool-1-worker-15] DEBUG [SuiteThreadController:85] [operators.ReconciliationST] - Removing parallel suite: ReconciliationST
2022-03-28 14:11:22 [ForkJoinPool-1-worker-15] DEBUG [SuiteThreadController:89] [operators.ReconciliationST] - Parallel suites count: 2
[INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1,349.855 s - in io.strimzi.systemtest.operators.ReconciliationST
2022-03-28 14:11:23 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (139554ms till timeout)
2022-03-28 14:11:24 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (138270ms till timeout)
2022-03-28 14:11:26 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (136986ms till timeout)
2022-03-28 14:11:27 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (135701ms till timeout)
2022-03-28 14:11:28 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (134386ms till timeout)
2022-03-28 14:11:30 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (133102ms till timeout)
2022-03-28 14:11:31 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (131818ms till timeout)
2022-03-28 14:11:32 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (130533ms till timeout)
2022-03-28 14:11:33 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (129249ms till timeout)
2022-03-28 14:11:35 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (127964ms till timeout)
2022-03-28 14:11:36 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (126680ms till timeout)
2022-03-28 14:11:37 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (125396ms till timeout)
2022-03-28 14:11:39 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (124101ms till timeout)
2022-03-28 14:11:40 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (122817ms till timeout)
2022-03-28 14:11:41 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (121533ms till timeout)
2022-03-28 14:11:42 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (120249ms till timeout)
2022-03-28 14:11:44 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (118964ms till timeout)
2022-03-28 14:11:45 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (117680ms till timeout)
2022-03-28 14:11:46 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (116396ms till timeout)
2022-03-28 14:11:48 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (115112ms till timeout)
2022-03-28 14:11:49 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (113820ms till timeout)
2022-03-28 14:11:50 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (112535ms till timeout)
2022-03-28 14:11:51 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (111250ms till timeout)
2022-03-28 14:11:53 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (109966ms till timeout)
2022-03-28 14:11:54 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (108679ms till timeout)
2022-03-28 14:11:55 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (107395ms till timeout)
2022-03-28 14:11:57 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (106110ms till timeout)
2022-03-28 14:11:58 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (104825ms till timeout)
2022-03-28 14:11:59 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (103507ms till timeout)
2022-03-28 14:12:00 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (102223ms till timeout)
2022-03-28 14:12:02 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (100937ms till timeout)
2022-03-28 14:12:03 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (99653ms till timeout)
2022-03-28 14:12:04 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (98369ms till timeout)
2022-03-28 14:12:06 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (97084ms till timeout)
2022-03-28 14:12:07 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (95799ms till timeout)
2022-03-28 14:12:08 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (94515ms till timeout)
2022-03-28 14:12:09 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (93230ms till timeout)
2022-03-28 14:12:11 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (91946ms till timeout)
2022-03-28 14:12:12 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (90662ms till timeout)
2022-03-28 14:12:13 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (89378ms till timeout)
2022-03-28 14:12:15 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (88091ms till timeout)
2022-03-28 14:12:16 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (86807ms till timeout)
2022-03-28 14:12:17 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (85521ms till timeout)
2022-03-28 14:12:18 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (84237ms till timeout)
2022-03-28 14:12:20 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (82950ms till timeout)
2022-03-28 14:12:21 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (81660ms till timeout)
2022-03-28 14:12:22 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (80376ms till timeout)
2022-03-28 14:12:24 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (79091ms till timeout)
2022-03-28 14:12:25 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (77806ms till timeout)
2022-03-28 14:12:26 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (76519ms till timeout)
2022-03-28 14:12:27 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (75234ms till timeout)
2022-03-28 14:12:29 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (73949ms till timeout)
2022-03-28 14:12:30 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (72664ms till timeout)
2022-03-28 14:12:31 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (71379ms till timeout)
2022-03-28 14:12:33 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (70096ms till timeout)
2022-03-28 14:12:34 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (68812ms till timeout)
2022-03-28 14:12:35 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (67528ms till timeout)
2022-03-28 14:12:36 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (66243ms till timeout)
2022-03-28 14:12:38 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (64960ms till timeout)
2022-03-28 14:12:39 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (63676ms till timeout)
2022-03-28 14:12:40 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (62390ms till timeout)
2022-03-28 14:12:42 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (61101ms till timeout)
2022-03-28 14:12:43 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (59816ms till timeout)
2022-03-28 14:12:44 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (58533ms till timeout)
2022-03-28 14:12:46 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (57119ms till timeout)
2022-03-28 14:12:47 [ForkJoinPool-1-worker-9] INFO  [PodUtils:189] Message org.apache.kafka.common.errors.ThrottlingQuotaExceededException: The throttling quota has been exceeded. found in delete-admin-my-cluster-419d5c31-kafka-clients-w78kq log
2022-03-28 14:12:47 [ForkJoinPool-1-worker-9] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment delete-admin-my-cluster-419d5c31-kafka-clients deletion
2022-03-28 14:12:47 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for ReplicaSet delete-admin-my-cluster-419d5c31-kafka-clients to be deleted
2022-03-28 14:12:47 [ForkJoinPool-1-worker-9] DEBUG [JobUtils:40] Job delete-admin-my-cluster-419d5c31-kafka-clients was deleted
2022-03-28 14:12:47 [ForkJoinPool-1-worker-9] INFO  [ThrottlingQuotaST:144] Executing 1/5 iteration for delete-admin-my-cluster-419d5c31-kafka-clients.
2022-03-28 14:12:47 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:155] Create/Update Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st
2022-03-28 14:12:47 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:delete-admin-my-cluster-419d5c31-kafka-clients
2022-03-28 14:12:47 [ForkJoinPool-1-worker-9] INFO  [JobUtils:81] Waiting for job: delete-admin-my-cluster-419d5c31-kafka-clients will be in active state
2022-03-28 14:12:47 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 14:12:47 [ForkJoinPool-1-worker-9] INFO  [ClientUtils:76] Waiting for producer/consumer:delete-admin-my-cluster-419d5c31-kafka-clients to finished
2022-03-28 14:12:47 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for job finished
2022-03-28 14:12:47 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:12:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:12:47 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (129807ms till timeout)
2022-03-28 14:12:49 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:12:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:12:49 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (128611ms till timeout)
2022-03-28 14:12:50 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:12:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:12:50 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (127416ms till timeout)
2022-03-28 14:12:51 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:12:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:12:51 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (126220ms till timeout)
2022-03-28 14:12:52 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:12:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:12:52 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (125025ms till timeout)
2022-03-28 14:12:53 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:12:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:12:53 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (123830ms till timeout)
2022-03-28 14:12:55 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:12:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:12:55 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (122635ms till timeout)
2022-03-28 14:12:56 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:12:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:12:56 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (121441ms till timeout)
2022-03-28 14:12:57 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:12:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:12:57 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (120246ms till timeout)
2022-03-28 14:12:58 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:12:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:12:58 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (119051ms till timeout)
2022-03-28 14:12:59 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:12:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:12:59 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (117855ms till timeout)
2022-03-28 14:13:01 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:12:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:13:01 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (116659ms till timeout)
2022-03-28 14:13:02 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:12:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:13:02 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (115462ms till timeout)
2022-03-28 14:13:03 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:12:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:13:03 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (114265ms till timeout)
2022-03-28 14:13:04 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:12:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:13:04 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (113068ms till timeout)
2022-03-28 14:13:05 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:12:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:13:05 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (111871ms till timeout)
2022-03-28 14:13:07 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:12:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:13:07 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (110673ms till timeout)
2022-03-28 14:13:08 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:12:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:13:08 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (109475ms till timeout)
2022-03-28 14:13:09 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:12:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:13:09 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (108280ms till timeout)
2022-03-28 14:13:10 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:12:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:13:10 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (107084ms till timeout)
2022-03-28 14:13:11 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:12:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:13:11 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (105887ms till timeout)
2022-03-28 14:13:13 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:12:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:13:13 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (104687ms till timeout)
2022-03-28 14:13:14 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:12:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:13:14 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (103491ms till timeout)
2022-03-28 14:13:15 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:12:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:13:15 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (102295ms till timeout)
2022-03-28 14:13:16 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:12:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:13:16 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (101100ms till timeout)
2022-03-28 14:13:17 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:12:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:13:17 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (99905ms till timeout)
2022-03-28 14:13:19 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:12:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:13:19 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (98706ms till timeout)
2022-03-28 14:13:20 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:12:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:13:20 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (97511ms till timeout)
2022-03-28 14:13:21 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:12:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:13:21 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (96314ms till timeout)
2022-03-28 14:13:22 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:12:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:13:22 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (95115ms till timeout)
2022-03-28 14:13:23 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:12:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:13:23 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (93917ms till timeout)
2022-03-28 14:13:24 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:12:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:13:25 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (92722ms till timeout)
2022-03-28 14:13:26 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:12:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:13:26 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (91525ms till timeout)
2022-03-28 14:13:27 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:12:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:13:27 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (90323ms till timeout)
2022-03-28 14:13:28 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:12:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:13:28 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (89127ms till timeout)
2022-03-28 14:13:29 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:12:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:13:29 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (87933ms till timeout)
2022-03-28 14:13:30 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:12:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:13:31 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (86738ms till timeout)
2022-03-28 14:13:32 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:12:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:13:32 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (85541ms till timeout)
2022-03-28 14:13:33 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:12:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:13:33 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (84346ms till timeout)
2022-03-28 14:13:34 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:12:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:13:34 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (83116ms till timeout)
2022-03-28 14:13:35 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:12:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:13:35 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (81920ms till timeout)
2022-03-28 14:13:36 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:12:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:13:37 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (80724ms till timeout)
2022-03-28 14:13:38 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:12:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:13:38 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (79528ms till timeout)
2022-03-28 14:13:39 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:12:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:13:39 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (78332ms till timeout)
2022-03-28 14:13:40 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:12:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:13:40 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (77135ms till timeout)
2022-03-28 14:13:41 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:12:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:13:41 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (75937ms till timeout)
2022-03-28 14:13:42 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:12:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:13:43 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (74739ms till timeout)
2022-03-28 14:13:44 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:12:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:13:44 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (73541ms till timeout)
2022-03-28 14:13:45 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:12:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:13:45 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (72345ms till timeout)
2022-03-28 14:13:46 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:12:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:13:46 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (71148ms till timeout)
2022-03-28 14:13:47 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:12:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:13:47 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (69950ms till timeout)
2022-03-28 14:13:48 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:12:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:13:49 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (68752ms till timeout)
2022-03-28 14:13:50 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:12:43Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:13:50 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (67557ms till timeout)
2022-03-28 14:13:51 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=null, completedIndexes=null, completionTime=2022-03-28T14:13:46Z, conditions=[JobCondition(lastProbeTime=2022-03-28T14:13:46Z, lastTransitionTime=2022-03-28T14:13:46Z, message=null, reason=null, status=True, type=Complete, additionalProperties={})], failed=null, ready=null, startTime=2022-03-28T14:12:43Z, succeeded=1, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:13:51 [ForkJoinPool-1-worker-9] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment delete-admin-my-cluster-419d5c31-kafka-clients deletion
2022-03-28 14:13:51 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for ReplicaSet delete-admin-my-cluster-419d5c31-kafka-clients to be deleted
2022-03-28 14:13:51 [ForkJoinPool-1-worker-9] DEBUG [JobUtils:40] Job delete-admin-my-cluster-419d5c31-kafka-clients was deleted
2022-03-28 14:13:51 [ForkJoinPool-1-worker-9] INFO  [ThrottlingQuotaST:144] Executing 2/5 iteration for delete-admin-my-cluster-419d5c31-kafka-clients.
2022-03-28 14:13:51 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:155] Create/Update Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st
2022-03-28 14:13:51 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:delete-admin-my-cluster-419d5c31-kafka-clients
2022-03-28 14:13:51 [ForkJoinPool-1-worker-9] INFO  [JobUtils:81] Waiting for job: delete-admin-my-cluster-419d5c31-kafka-clients will be in active state
2022-03-28 14:13:51 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 14:13:51 [ForkJoinPool-1-worker-9] INFO  [ClientUtils:76] Waiting for producer/consumer:delete-admin-my-cluster-419d5c31-kafka-clients to finished
2022-03-28 14:13:51 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for job finished
2022-03-28 14:13:52 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:13:47Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:13:52 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (129789ms till timeout)
2022-03-28 14:13:53 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:13:47Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:13:53 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (128593ms till timeout)
2022-03-28 14:13:54 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:13:47Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:13:54 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (127396ms till timeout)
2022-03-28 14:13:55 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:13:47Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:13:55 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (126200ms till timeout)
2022-03-28 14:13:56 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:13:47Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:13:56 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (125004ms till timeout)
2022-03-28 14:13:58 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:13:47Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:13:58 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (123809ms till timeout)
2022-03-28 14:13:59 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:13:47Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:13:59 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (122614ms till timeout)
2022-03-28 14:14:00 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:13:47Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:14:00 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (121418ms till timeout)
2022-03-28 14:14:01 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:13:47Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:14:01 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (120222ms till timeout)
2022-03-28 14:14:02 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:13:47Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:14:02 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (119024ms till timeout)
2022-03-28 14:14:04 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:13:47Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:14:04 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (117829ms till timeout)
2022-03-28 14:14:05 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:13:47Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:14:05 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (116633ms till timeout)
2022-03-28 14:14:06 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:13:47Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:14:06 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (115437ms till timeout)
2022-03-28 14:14:07 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:13:47Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:14:07 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (114240ms till timeout)
2022-03-28 14:14:08 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:13:47Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:14:08 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (113044ms till timeout)
2022-03-28 14:14:09 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:13:47Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:14:10 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (111850ms till timeout)
2022-03-28 14:14:11 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:13:47Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:14:11 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (110650ms till timeout)
2022-03-28 14:14:12 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:13:47Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:14:12 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (109455ms till timeout)
2022-03-28 14:14:13 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:13:47Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:14:13 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (108259ms till timeout)
2022-03-28 14:14:14 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:13:47Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:14:14 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (107064ms till timeout)
2022-03-28 14:14:15 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:13:47Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:14:16 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (105865ms till timeout)
2022-03-28 14:14:17 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:13:47Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:14:17 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (104669ms till timeout)
2022-03-28 14:14:18 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:13:47Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:14:18 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (103472ms till timeout)
2022-03-28 14:14:19 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:13:47Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:14:19 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (102275ms till timeout)
2022-03-28 14:14:20 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:13:47Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:14:20 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (101080ms till timeout)
2022-03-28 14:14:21 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:13:47Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:14:22 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (99883ms till timeout)
2022-03-28 14:14:23 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:13:47Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:14:23 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (98687ms till timeout)
2022-03-28 14:14:24 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:13:47Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:14:24 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (97492ms till timeout)
2022-03-28 14:14:25 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:13:47Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:14:25 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (96297ms till timeout)
2022-03-28 14:14:26 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:13:47Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:14:26 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (95102ms till timeout)
2022-03-28 14:14:27 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:13:47Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:14:28 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (93906ms till timeout)
2022-03-28 14:14:29 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:13:47Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:14:29 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (92710ms till timeout)
2022-03-28 14:14:30 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:13:47Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:14:30 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (91515ms till timeout)
2022-03-28 14:14:31 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:13:47Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:14:31 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (90318ms till timeout)
2022-03-28 14:14:32 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:13:47Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:14:32 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (89122ms till timeout)
2022-03-28 14:14:33 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:13:47Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:14:34 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (87928ms till timeout)
2022-03-28 14:14:35 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:13:47Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:14:35 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (86732ms till timeout)
2022-03-28 14:14:36 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:13:47Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:14:36 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (85538ms till timeout)
2022-03-28 14:14:37 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:13:47Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:14:37 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (84341ms till timeout)
2022-03-28 14:14:38 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:13:47Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:14:38 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (83145ms till timeout)
2022-03-28 14:14:39 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:13:47Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:14:39 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (81950ms till timeout)
2022-03-28 14:14:41 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:13:47Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:14:41 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (80755ms till timeout)
2022-03-28 14:14:42 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:13:47Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:14:42 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (79559ms till timeout)
2022-03-28 14:14:43 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:13:47Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:14:43 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (78359ms till timeout)
2022-03-28 14:14:44 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:13:47Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:14:44 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (77162ms till timeout)
2022-03-28 14:14:45 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:13:47Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:14:45 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (75968ms till timeout)
2022-03-28 14:14:47 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:13:47Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:14:47 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (74772ms till timeout)
2022-03-28 14:14:48 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:13:47Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:14:48 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (73577ms till timeout)
2022-03-28 14:14:49 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:13:47Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:14:49 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (72377ms till timeout)
2022-03-28 14:14:50 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:13:47Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:14:50 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (71183ms till timeout)
2022-03-28 14:14:51 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:13:47Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:14:51 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (69988ms till timeout)
2022-03-28 14:14:53 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:13:47Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:14:53 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (68791ms till timeout)
2022-03-28 14:14:54 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:13:47Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:14:54 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (67595ms till timeout)
2022-03-28 14:14:55 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=null, completedIndexes=null, completionTime=2022-03-28T14:14:50Z, conditions=[JobCondition(lastProbeTime=2022-03-28T14:14:50Z, lastTransitionTime=2022-03-28T14:14:50Z, message=null, reason=null, status=True, type=Complete, additionalProperties={})], failed=null, ready=null, startTime=2022-03-28T14:13:47Z, succeeded=1, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:14:55 [ForkJoinPool-1-worker-9] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment delete-admin-my-cluster-419d5c31-kafka-clients deletion
2022-03-28 14:14:55 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for ReplicaSet delete-admin-my-cluster-419d5c31-kafka-clients to be deleted
2022-03-28 14:14:55 [ForkJoinPool-1-worker-9] DEBUG [JobUtils:40] Job delete-admin-my-cluster-419d5c31-kafka-clients was deleted
2022-03-28 14:14:55 [ForkJoinPool-1-worker-9] INFO  [ThrottlingQuotaST:144] Executing 3/5 iteration for delete-admin-my-cluster-419d5c31-kafka-clients.
2022-03-28 14:14:55 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:155] Create/Update Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st
2022-03-28 14:14:55 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:delete-admin-my-cluster-419d5c31-kafka-clients
2022-03-28 14:14:55 [ForkJoinPool-1-worker-9] INFO  [JobUtils:81] Waiting for job: delete-admin-my-cluster-419d5c31-kafka-clients will be in active state
2022-03-28 14:14:55 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 14:14:56 [ForkJoinPool-1-worker-9] INFO  [ClientUtils:76] Waiting for producer/consumer:delete-admin-my-cluster-419d5c31-kafka-clients to finished
2022-03-28 14:14:56 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for job finished
2022-03-28 14:14:56 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:14:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:14:56 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (129715ms till timeout)
2022-03-28 14:14:57 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:14:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:14:57 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (128519ms till timeout)
2022-03-28 14:14:58 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:14:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:14:58 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (127322ms till timeout)
2022-03-28 14:14:59 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:14:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:14:59 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (126126ms till timeout)
2022-03-28 14:15:01 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:14:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:15:01 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (124929ms till timeout)
2022-03-28 14:15:02 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:14:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:15:02 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (123733ms till timeout)
2022-03-28 14:15:03 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:14:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:15:03 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (122527ms till timeout)
2022-03-28 14:15:04 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:14:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:15:04 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (121323ms till timeout)
2022-03-28 14:15:05 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:14:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:15:05 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (120126ms till timeout)
2022-03-28 14:15:07 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:14:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:15:07 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (118927ms till timeout)
2022-03-28 14:15:08 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:14:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:15:08 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (117728ms till timeout)
2022-03-28 14:15:09 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:14:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:15:09 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (116532ms till timeout)
2022-03-28 14:15:10 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:14:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:15:10 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (115337ms till timeout)
2022-03-28 14:15:11 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:14:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:15:11 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (114141ms till timeout)
2022-03-28 14:15:12 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:14:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:15:13 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (112946ms till timeout)
2022-03-28 14:15:14 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:14:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:15:14 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (111751ms till timeout)
2022-03-28 14:15:15 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:14:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:15:15 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (110555ms till timeout)
2022-03-28 14:15:16 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:14:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:15:16 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (109358ms till timeout)
2022-03-28 14:15:17 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:14:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:15:17 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (108163ms till timeout)
2022-03-28 14:15:18 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:14:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:15:19 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (106968ms till timeout)
2022-03-28 14:15:20 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:14:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:15:20 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (105774ms till timeout)
2022-03-28 14:15:21 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:14:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:15:21 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (104577ms till timeout)
2022-03-28 14:15:22 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:14:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:15:22 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (103382ms till timeout)
2022-03-28 14:15:23 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:14:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:15:23 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (102186ms till timeout)
2022-03-28 14:15:24 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:14:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:15:25 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (100991ms till timeout)
2022-03-28 14:15:26 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:14:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:15:26 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (99757ms till timeout)
2022-03-28 14:15:27 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:14:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:15:27 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (98558ms till timeout)
2022-03-28 14:15:28 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:14:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:15:28 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (97362ms till timeout)
2022-03-28 14:15:29 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:14:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:15:29 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (96167ms till timeout)
2022-03-28 14:15:30 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:14:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:15:31 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (94969ms till timeout)
2022-03-28 14:15:32 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:14:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:15:32 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (93772ms till timeout)
2022-03-28 14:15:33 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:14:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:15:33 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (92574ms till timeout)
2022-03-28 14:15:34 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:14:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:15:34 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (91376ms till timeout)
2022-03-28 14:15:35 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:14:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:15:35 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (90180ms till timeout)
2022-03-28 14:15:36 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:14:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:15:37 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (88983ms till timeout)
2022-03-28 14:15:38 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:14:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:15:38 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (87786ms till timeout)
2022-03-28 14:15:39 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:14:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:15:39 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (86588ms till timeout)
2022-03-28 14:15:40 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:14:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:15:40 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (85392ms till timeout)
2022-03-28 14:15:41 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:14:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:15:41 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (84197ms till timeout)
2022-03-28 14:15:42 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:14:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:15:43 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (82959ms till timeout)
2022-03-28 14:15:44 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:14:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:15:44 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (81762ms till timeout)
2022-03-28 14:15:45 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:14:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:15:45 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (80565ms till timeout)
2022-03-28 14:15:46 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:14:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:15:46 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (79369ms till timeout)
2022-03-28 14:15:47 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:14:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:15:47 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (78174ms till timeout)
2022-03-28 14:15:48 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:14:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:15:49 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (76978ms till timeout)
2022-03-28 14:15:50 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:14:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:15:50 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (75782ms till timeout)
2022-03-28 14:15:51 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:14:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:15:51 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (74584ms till timeout)
2022-03-28 14:15:52 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:14:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:15:52 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (73387ms till timeout)
2022-03-28 14:15:53 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:14:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:15:53 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (72186ms till timeout)
2022-03-28 14:15:54 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:14:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:15:55 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (70989ms till timeout)
2022-03-28 14:15:56 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:14:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:15:56 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (69760ms till timeout)
2022-03-28 14:15:57 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:14:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:15:57 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (68563ms till timeout)
2022-03-28 14:15:58 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:14:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:15:58 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (67366ms till timeout)
2022-03-28 14:15:59 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:14:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:15:59 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (66170ms till timeout)
2022-03-28 14:16:00 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=null, completedIndexes=null, completionTime=2022-03-28T14:15:55Z, conditions=[JobCondition(lastProbeTime=2022-03-28T14:15:55Z, lastTransitionTime=2022-03-28T14:15:55Z, message=null, reason=null, status=True, type=Complete, additionalProperties={})], failed=null, ready=null, startTime=2022-03-28T14:14:51Z, succeeded=1, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:16:01 [ForkJoinPool-1-worker-9] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment delete-admin-my-cluster-419d5c31-kafka-clients deletion
2022-03-28 14:16:01 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for ReplicaSet delete-admin-my-cluster-419d5c31-kafka-clients to be deleted
2022-03-28 14:16:01 [ForkJoinPool-1-worker-9] DEBUG [JobUtils:40] Job delete-admin-my-cluster-419d5c31-kafka-clients was deleted
2022-03-28 14:16:01 [ForkJoinPool-1-worker-9] INFO  [ThrottlingQuotaST:144] Executing 4/5 iteration for delete-admin-my-cluster-419d5c31-kafka-clients.
2022-03-28 14:16:01 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:155] Create/Update Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st
2022-03-28 14:16:01 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:delete-admin-my-cluster-419d5c31-kafka-clients
2022-03-28 14:16:01 [ForkJoinPool-1-worker-9] INFO  [JobUtils:81] Waiting for job: delete-admin-my-cluster-419d5c31-kafka-clients will be in active state
2022-03-28 14:16:01 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 14:16:01 [ForkJoinPool-1-worker-9] INFO  [ClientUtils:76] Waiting for producer/consumer:delete-admin-my-cluster-419d5c31-kafka-clients to finished
2022-03-28 14:16:01 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for job finished
2022-03-28 14:16:01 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:15:56Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:16:01 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (129779ms till timeout)
2022-03-28 14:16:02 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:15:56Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:16:02 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (128583ms till timeout)
2022-03-28 14:16:04 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:15:56Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:16:04 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (127388ms till timeout)
2022-03-28 14:16:05 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:15:56Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:16:05 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (126192ms till timeout)
2022-03-28 14:16:06 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:15:56Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:16:06 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (124995ms till timeout)
2022-03-28 14:16:07 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:15:56Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:16:07 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (123800ms till timeout)
2022-03-28 14:16:08 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:15:56Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:16:08 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (122605ms till timeout)
2022-03-28 14:16:10 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:15:56Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:16:10 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (121411ms till timeout)
2022-03-28 14:16:11 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:15:56Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:16:11 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (120216ms till timeout)
2022-03-28 14:16:12 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:15:56Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:16:12 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (119020ms till timeout)
2022-03-28 14:16:13 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:15:56Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:16:13 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (117821ms till timeout)
2022-03-28 14:16:14 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:15:56Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:16:14 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (116624ms till timeout)
2022-03-28 14:16:16 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:15:56Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:16:16 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (115429ms till timeout)
2022-03-28 14:16:17 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:15:56Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:16:17 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (114233ms till timeout)
2022-03-28 14:16:18 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:15:56Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:16:18 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (113037ms till timeout)
2022-03-28 14:16:19 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:15:56Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:16:19 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (111841ms till timeout)
2022-03-28 14:16:20 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:15:56Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:16:20 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (110647ms till timeout)
2022-03-28 14:16:22 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:15:56Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:16:22 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (109452ms till timeout)
2022-03-28 14:16:23 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:15:56Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:16:23 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (108255ms till timeout)
2022-03-28 14:16:24 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:15:56Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:16:24 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (107060ms till timeout)
2022-03-28 14:16:25 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:15:56Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:16:25 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (105867ms till timeout)
2022-03-28 14:16:26 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:15:56Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:16:26 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (104672ms till timeout)
2022-03-28 14:16:27 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:15:56Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:16:28 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (103477ms till timeout)
2022-03-28 14:16:29 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:15:56Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:16:29 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (102282ms till timeout)
2022-03-28 14:16:30 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:15:56Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:16:30 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (101088ms till timeout)
2022-03-28 14:16:31 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:15:56Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:16:31 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (99892ms till timeout)
2022-03-28 14:16:32 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:15:56Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:16:32 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (98696ms till timeout)
2022-03-28 14:16:33 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:15:56Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:16:34 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (97501ms till timeout)
2022-03-28 14:16:35 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:15:56Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:16:35 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (96305ms till timeout)
2022-03-28 14:16:36 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:15:56Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:16:36 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (95108ms till timeout)
2022-03-28 14:16:37 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:15:56Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:16:37 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (93912ms till timeout)
2022-03-28 14:16:38 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:15:56Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:16:38 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (92718ms till timeout)
2022-03-28 14:16:39 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:15:56Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:16:40 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (91519ms till timeout)
2022-03-28 14:16:41 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:15:56Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:16:41 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (90324ms till timeout)
2022-03-28 14:16:42 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:15:56Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:16:42 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (89127ms till timeout)
2022-03-28 14:16:43 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:15:56Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:16:43 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (87931ms till timeout)
2022-03-28 14:16:44 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:15:56Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:16:44 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (86735ms till timeout)
2022-03-28 14:16:45 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:15:56Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:16:46 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (85539ms till timeout)
2022-03-28 14:16:47 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:15:56Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:16:47 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (84341ms till timeout)
2022-03-28 14:16:48 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:15:56Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:16:48 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (83146ms till timeout)
2022-03-28 14:16:49 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:15:56Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:16:49 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (81950ms till timeout)
2022-03-28 14:16:50 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:15:56Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:16:50 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (80752ms till timeout)
2022-03-28 14:16:51 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:15:56Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:16:52 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (79558ms till timeout)
2022-03-28 14:16:53 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:15:56Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:16:53 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (78363ms till timeout)
2022-03-28 14:16:54 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:15:56Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:16:54 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (77168ms till timeout)
2022-03-28 14:16:55 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:15:56Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:16:55 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (75972ms till timeout)
2022-03-28 14:16:56 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:15:56Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:16:56 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (74776ms till timeout)
2022-03-28 14:16:57 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:15:56Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:16:57 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (73573ms till timeout)
2022-03-28 14:16:59 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:15:56Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:16:59 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (72377ms till timeout)
2022-03-28 14:17:00 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:15:56Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:17:00 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (71185ms till timeout)
2022-03-28 14:17:01 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:15:56Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:17:01 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (69988ms till timeout)
2022-03-28 14:17:02 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:15:56Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:17:02 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (68792ms till timeout)
2022-03-28 14:17:03 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:15:56Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:17:03 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (67596ms till timeout)
2022-03-28 14:17:05 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:15:56Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:17:05 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (66402ms till timeout)
2022-03-28 14:17:06 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=null, completedIndexes=null, completionTime=2022-03-28T14:17:00Z, conditions=[JobCondition(lastProbeTime=2022-03-28T14:17:00Z, lastTransitionTime=2022-03-28T14:17:00Z, message=null, reason=null, status=True, type=Complete, additionalProperties={})], failed=null, ready=null, startTime=2022-03-28T14:15:56Z, succeeded=1, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:17:06 [ForkJoinPool-1-worker-9] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment delete-admin-my-cluster-419d5c31-kafka-clients deletion
2022-03-28 14:17:06 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for ReplicaSet delete-admin-my-cluster-419d5c31-kafka-clients to be deleted
2022-03-28 14:17:06 [ForkJoinPool-1-worker-9] DEBUG [JobUtils:40] Job delete-admin-my-cluster-419d5c31-kafka-clients was deleted
2022-03-28 14:17:06 [ForkJoinPool-1-worker-9] INFO  [ThrottlingQuotaST:144] Executing 5/5 iteration for delete-admin-my-cluster-419d5c31-kafka-clients.
2022-03-28 14:17:06 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:155] Create/Update Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st
2022-03-28 14:17:06 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:delete-admin-my-cluster-419d5c31-kafka-clients
2022-03-28 14:17:06 [ForkJoinPool-1-worker-9] INFO  [JobUtils:81] Waiting for job: delete-admin-my-cluster-419d5c31-kafka-clients will be in active state
2022-03-28 14:17:06 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 14:17:06 [ForkJoinPool-1-worker-9] INFO  [ClientUtils:76] Waiting for producer/consumer:delete-admin-my-cluster-419d5c31-kafka-clients to finished
2022-03-28 14:17:06 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for job finished
2022-03-28 14:17:06 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:17:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:17:07 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (129807ms till timeout)
2022-03-28 14:17:08 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:17:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:17:08 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (128611ms till timeout)
2022-03-28 14:17:09 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:17:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:17:09 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (127416ms till timeout)
2022-03-28 14:17:10 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:17:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:17:10 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (126221ms till timeout)
2022-03-28 14:17:11 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:17:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:17:11 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (125024ms till timeout)
2022-03-28 14:17:12 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:17:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:17:13 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (123829ms till timeout)
2022-03-28 14:17:14 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:17:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:17:14 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (122634ms till timeout)
2022-03-28 14:17:15 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:17:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:17:15 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (121438ms till timeout)
2022-03-28 14:17:16 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:17:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:17:16 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (120242ms till timeout)
2022-03-28 14:17:17 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:17:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:17:17 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (119044ms till timeout)
2022-03-28 14:17:18 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:17:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:17:19 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (117849ms till timeout)
2022-03-28 14:17:20 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:17:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:17:20 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (116654ms till timeout)
2022-03-28 14:17:21 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:17:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:17:21 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (115458ms till timeout)
2022-03-28 14:17:22 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:17:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:17:22 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (114262ms till timeout)
2022-03-28 14:17:23 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:17:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:17:23 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (113067ms till timeout)
2022-03-28 14:17:24 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:17:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:17:24 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (111870ms till timeout)
2022-03-28 14:17:26 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:17:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:17:26 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (110586ms till timeout)
2022-03-28 14:17:27 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:17:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:17:27 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (109390ms till timeout)
2022-03-28 14:17:28 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:17:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:17:28 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (108195ms till timeout)
2022-03-28 14:17:29 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:17:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:17:29 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (106999ms till timeout)
2022-03-28 14:17:30 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:17:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:17:31 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (105805ms till timeout)
2022-03-28 14:17:32 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:17:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:17:32 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (104597ms till timeout)
2022-03-28 14:17:33 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:17:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:17:33 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (103402ms till timeout)
2022-03-28 14:17:34 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:17:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:17:34 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (102206ms till timeout)
2022-03-28 14:17:35 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:17:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:17:35 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (101011ms till timeout)
2022-03-28 14:17:36 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:17:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:17:37 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (99814ms till timeout)
2022-03-28 14:17:38 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:17:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:17:38 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (98616ms till timeout)
2022-03-28 14:17:39 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:17:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:17:39 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (97419ms till timeout)
2022-03-28 14:17:40 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:17:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:17:40 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (96221ms till timeout)
2022-03-28 14:17:41 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:17:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:17:41 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (95026ms till timeout)
2022-03-28 14:17:42 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:17:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:17:43 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (93829ms till timeout)
2022-03-28 14:17:44 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:17:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:17:44 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (92633ms till timeout)
2022-03-28 14:17:45 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:17:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:17:45 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (91437ms till timeout)
2022-03-28 14:17:46 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:17:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:17:46 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (90240ms till timeout)
2022-03-28 14:17:47 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:17:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:17:47 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (89044ms till timeout)
2022-03-28 14:17:48 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:17:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:17:49 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (87847ms till timeout)
2022-03-28 14:17:50 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:17:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:17:50 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (86652ms till timeout)
2022-03-28 14:17:51 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:17:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:17:51 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (85421ms till timeout)
2022-03-28 14:17:52 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:17:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:17:52 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (84226ms till timeout)
2022-03-28 14:17:53 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:17:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:17:53 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (83031ms till timeout)
2022-03-28 14:17:54 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:17:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:17:55 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (81836ms till timeout)
2022-03-28 14:17:56 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:17:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:17:56 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (80571ms till timeout)
2022-03-28 14:17:57 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:17:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:17:57 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (79376ms till timeout)
2022-03-28 14:17:58 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:17:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:17:58 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (78181ms till timeout)
2022-03-28 14:17:59 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:17:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:17:59 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (76986ms till timeout)
2022-03-28 14:18:00 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:17:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:18:01 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (75791ms till timeout)
2022-03-28 14:18:02 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:17:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:18:02 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (74595ms till timeout)
2022-03-28 14:18:03 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:17:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:18:03 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (73399ms till timeout)
2022-03-28 14:18:04 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:17:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:18:04 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (72205ms till timeout)
2022-03-28 14:18:05 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:17:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:18:05 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (71010ms till timeout)
2022-03-28 14:18:06 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:17:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:18:07 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (69811ms till timeout)
2022-03-28 14:18:08 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:17:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:18:08 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (68615ms till timeout)
2022-03-28 14:18:09 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:17:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:18:09 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (67418ms till timeout)
2022-03-28 14:18:10 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:17:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:18:10 [ForkJoinPool-1-worker-9] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment delete-admin-my-cluster-419d5c31-kafka-clients deletion
2022-03-28 14:18:10 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for ReplicaSet delete-admin-my-cluster-419d5c31-kafka-clients to be deleted
2022-03-28 14:18:10 [ForkJoinPool-1-worker-9] DEBUG [JobUtils:40] Job delete-admin-my-cluster-419d5c31-kafka-clients was deleted
2022-03-28 14:18:10 [ForkJoinPool-1-worker-9] DEBUG [AbstractST:674] ============================================================================
2022-03-28 14:18:10 [ForkJoinPool-1-worker-9] DEBUG [AbstractST:675] [operators.topic.ThrottlingQuotaST - After Each] - Clean up after test
2022-03-28 14:18:10 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:344] ############################################################################
2022-03-28 14:18:10 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:348] Delete all resources for testThrottlingQuotasDeleteTopic
2022-03-28 14:18:10 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:241] Delete of Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st
2022-03-28 14:18:10 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:241] Delete of Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st
2022-03-28 14:18:10 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:241] Delete of Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st
2022-03-28 14:18:10 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:241] Delete of Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st
2022-03-28 14:18:10 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:241] Delete of Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st
2022-03-28 14:18:10 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st
2022-03-28 14:18:10 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:delete-admin-my-cluster-419d5c31-kafka-clients
2022-03-28 14:18:10 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:delete-admin-my-cluster-419d5c31-kafka-clients
2022-03-28 14:18:11 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:create-admin-my-cluster-419d5c31-kafka-clients
2022-03-28 14:18:11 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:create-admin-my-cluster-419d5c31-kafka-clients
2022-03-28 14:18:11 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:delete-admin-my-cluster-419d5c31-kafka-clients
2022-03-28 14:18:11 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:delete-admin-my-cluster-419d5c31-kafka-clients
2022-03-28 14:18:11 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:241] Delete of Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st
2022-03-28 14:18:11 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:241] Delete of Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st
2022-03-28 14:18:11 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:241] Delete of Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st
2022-03-28 14:18:11 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:241] Delete of Job create-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st
2022-03-28 14:18:11 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of Job delete-admin-my-cluster-419d5c31-kafka-clients in namespace throttling-quota-st
2022-03-28 14:18:11 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:241] Delete of KafkaUser my-user-1782685497-1980640589 in namespace throttling-quota-st
2022-03-28 14:18:11 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:delete-admin-my-cluster-419d5c31-kafka-clients
2022-03-28 14:18:11 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:create-admin-my-cluster-419d5c31-kafka-clients
2022-03-28 14:18:11 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:create-admin-my-cluster-419d5c31-kafka-clients
2022-03-28 14:18:11 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:create-admin-my-cluster-419d5c31-kafka-clients
2022-03-28 14:18:11 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:delete-admin-my-cluster-419d5c31-kafka-clients
2022-03-28 14:18:11 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-1782685497-1980640589
2022-03-28 14:18:11 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:369] ############################################################################
2022-03-28 14:18:11 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:267] testThrottlingQuotasDeleteTopic - Notifies waiting test cases:[testCapacityFile, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testThrottlingQuotasCreateAlterPartitions, testSendSimpleMessageTls, testReceiveSimpleMessageTlsScramSha, testSendSimpleMessageTlsScramSha, testReceiveSimpleMessageTls, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testConfigurationFileIsCreated, testPauseReconciliationInKafkaRebalanceAndTopic, testTopicModificationOfReplicationFactor, testMoreReplicasThanAvailableBrokers, testCreateTopicViaKafka, testCreateTopicAfterUnsupportedOperation, testPauseReconciliationInKafkaAndKafkaConnectWithConnector] to and randomly select one to start execution
2022-03-28 14:18:11 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:93] [operators.topic.ThrottlingQuotaST] - Removing parallel test: testThrottlingQuotasDeleteTopic
2022-03-28 14:18:11 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:97] [operators.topic.ThrottlingQuotaST] - Parallel test count: 0
2022-03-28 14:18:11 [ForkJoinPool-1-worker-9] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.topic.ThrottlingQuotaST.testThrottlingQuotasDeleteTopic-FINISHED
2022-03-28 14:18:11 [ForkJoinPool-1-worker-9] INFO  [TestSeparator:30] ############################################################################
2022-03-28 14:18:11 [ForkJoinPool-1-worker-9] INFO  [ThrottlingQuotaST:353] Tearing down resources after all test
2022-03-28 14:18:11 [ForkJoinPool-1-worker-11] INFO  [TestSeparator:23] ############################################################################
2022-03-28 14:18:11 [ForkJoinPool-1-worker-11] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.topic.TopicST.testDeleteTopicEnableFalse-STARTED
2022-03-28 14:18:11 [ForkJoinPool-1-worker-11] DEBUG [AbstractST:658] ============================================================================
2022-03-28 14:18:11 [ForkJoinPool-1-worker-11] DEBUG [AbstractST:659] [operators.topic.TopicST - Before Each] - Setup test case environment
2022-03-28 14:18:11 [ForkJoinPool-1-worker-11] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 14:18:11 [ForkJoinPool-1-worker-11] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-af298d01, testThrottlingQuotasCreateAlterPartitions=my-cluster-8f6c28cb, testSendSimpleMessageTls=my-cluster-fbc845dd, testConfigurationReflection=my-cluster-5137fff1, testUserWithNameMoreThan64Chars=my-cluster-02e25de6, testDeployAndUnDeployCruiseControl=my-cluster-844045c3, testCapacityFile=my-cluster-0531d5ce, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-131d7a1d, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-8af61d52, testTlsExternalUser=my-cluster-43f83aef, testSendSimpleMessageTlsScramSha=my-cluster-e353214b, testCreateTopicAfterUnsupportedOperation=my-cluster-8e26464c, testScramUserWithQuotas=my-cluster-6f653060, testTopicModificationOfReplicationFactor=my-cluster-a1a53143, testKafkaAdminTopicOperations=my-cluster-600e8332, testConfigurationFileIsCreated=my-cluster-3244df0d, testCreateTopicViaKafka=my-cluster-51bcec77, testCreatingUsersWithSecretPrefix=my-cluster-0c65d2dd, testUpdateUser=my-cluster-c2c4d05a, testThrottlingQuotasDeleteTopic=my-cluster-419d5c31, testDeleteTopicEnableFalse=my-cluster-7eab7e8d, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-0444229e, testTlsExternalUserWithQuotas=my-cluster-08d24b8e, testTlsUserWithQuotas=my-cluster-9832c9ae, testThrottlingQuotasCreateTopic=my-cluster-ac2cadd6, testSendingMessagesToNonExistingTopic=my-cluster-abe3633a, testReceiveSimpleMessageTls=my-cluster-ace1fa24, testConfigurationPerformanceOptions=my-cluster-4994018d, testUserTemplate=my-cluster-e0217138, testMoreReplicasThanAvailableBrokers=my-cluster-203feca8}
2022-03-28 14:18:11 [ForkJoinPool-1-worker-11] TRACE [AbstractST:607] USERS_NAME_MAP: {testReceiveSimpleMessageTlsScramSha=my-user-1597210213-707359243, testThrottlingQuotasCreateAlterPartitions=my-user-1557959119-1722636050, testSendSimpleMessageTls=my-user-1754673553-2077518027, testConfigurationReflection=my-user-2029261898-1375854235, testUserWithNameMoreThan64Chars=my-user-2124392695-340403978, testDeployAndUnDeployCruiseControl=my-user-2117120687-604608920, testCapacityFile=my-user-155969448-1270555822, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-user-1962143551-1221981023, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-1807142870-188660517, testTlsExternalUser=my-user-370513135-1989152728, testSendSimpleMessageTlsScramSha=my-user-182357028-1132873010, testCreateTopicAfterUnsupportedOperation=my-user-917806669-1508124143, testScramUserWithQuotas=my-user-27563760-1632863338, testTopicModificationOfReplicationFactor=my-user-1764997166-1500934155, testKafkaAdminTopicOperations=my-user-1735322302-803507072, testConfigurationFileIsCreated=my-user-1799841978-1468805631, testCreateTopicViaKafka=my-user-1501031580-1495561780, testCreatingUsersWithSecretPrefix=my-user-144001639-226807213, testUpdateUser=my-user-1870578302-1216914412, testThrottlingQuotasDeleteTopic=my-user-1782685497-1980640589, testDeleteTopicEnableFalse=my-user-1306940157-296227105, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-1010649803-214635564, testTlsExternalUserWithQuotas=my-user-247789923-205827680, testTlsUserWithQuotas=my-user-1602931187-443256751, testThrottlingQuotasCreateTopic=my-user-46611685-1079680568, testSendingMessagesToNonExistingTopic=my-user-954109284-2118832263, testReceiveSimpleMessageTls=my-user-1418772225-1024845009, testConfigurationPerformanceOptions=my-user-882948629-1821489961, testUserTemplate=my-user-896194994-133213923, testMoreReplicasThanAvailableBrokers=my-user-413609357-1989111041}
2022-03-28 14:18:11 [ForkJoinPool-1-worker-11] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-topic-649423016-1623551209, testThrottlingQuotasCreateAlterPartitions=my-topic-1449977352-344480794, testSendSimpleMessageTls=my-topic-115477502-2025274778, testConfigurationReflection=my-topic-1621095669-1276202830, testUserWithNameMoreThan64Chars=my-topic-1053060011-1199293498, testDeployAndUnDeployCruiseControl=my-topic-1101175281-2107378523, testCapacityFile=my-topic-296605537-1008743104, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-topic-553801381-847208295, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-1367124108-1748498395, testTlsExternalUser=my-topic-2081139838-1814296134, testSendSimpleMessageTlsScramSha=my-topic-652506937-1802310709, testCreateTopicAfterUnsupportedOperation=my-topic-1639085682-1344322559, testScramUserWithQuotas=my-topic-1345794255-226872460, testTopicModificationOfReplicationFactor=my-topic-2129693330-635016449, testKafkaAdminTopicOperations=my-topic-1935791607-1758305855, testConfigurationFileIsCreated=my-topic-1435014906-681104826, testCreateTopicViaKafka=my-topic-300455033-999140097, testCreatingUsersWithSecretPrefix=my-topic-130462979-2061578809, testUpdateUser=my-topic-829325799-540543436, testThrottlingQuotasDeleteTopic=my-topic-1797350406-813887294, testDeleteTopicEnableFalse=my-topic-365925864-1884907136, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-889283546-691691584, testTlsExternalUserWithQuotas=my-topic-2037225970-59246891, testTlsUserWithQuotas=my-topic-985048738-437665866, testThrottlingQuotasCreateTopic=my-topic-114922478-1506136333, testSendingMessagesToNonExistingTopic=my-topic-441360312-1858428966, testReceiveSimpleMessageTls=my-topic-1131768783-120051821, testConfigurationPerformanceOptions=my-topic-627301835-843876360, testUserTemplate=my-topic-1175486259-481154370, testMoreReplicasThanAvailableBrokers=my-topic-71698317-859778131}
2022-03-28 14:18:11 [ForkJoinPool-1-worker-11] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-af298d01-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-8f6c28cb-kafka-clients, testSendSimpleMessageTls=my-cluster-fbc845dd-kafka-clients, testConfigurationReflection=my-cluster-5137fff1-kafka-clients, testUserWithNameMoreThan64Chars=my-cluster-02e25de6-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-844045c3-kafka-clients, testCapacityFile=my-cluster-0531d5ce-kafka-clients, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-131d7a1d-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-8af61d52-kafka-clients, testTlsExternalUser=my-cluster-43f83aef-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-e353214b-kafka-clients, testCreateTopicAfterUnsupportedOperation=my-cluster-8e26464c-kafka-clients, testScramUserWithQuotas=my-cluster-6f653060-kafka-clients, testTopicModificationOfReplicationFactor=my-cluster-a1a53143-kafka-clients, testKafkaAdminTopicOperations=my-cluster-600e8332-kafka-clients, testConfigurationFileIsCreated=my-cluster-3244df0d-kafka-clients, testCreateTopicViaKafka=my-cluster-51bcec77-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-0c65d2dd-kafka-clients, testUpdateUser=my-cluster-c2c4d05a-kafka-clients, testThrottlingQuotasDeleteTopic=my-cluster-419d5c31-kafka-clients, testDeleteTopicEnableFalse=my-cluster-7eab7e8d-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-0444229e-kafka-clients, testTlsExternalUserWithQuotas=my-cluster-08d24b8e-kafka-clients, testTlsUserWithQuotas=my-cluster-9832c9ae-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-ac2cadd6-kafka-clients, testSendingMessagesToNonExistingTopic=my-cluster-abe3633a-kafka-clients, testReceiveSimpleMessageTls=my-cluster-ace1fa24-kafka-clients, testConfigurationPerformanceOptions=my-cluster-4994018d-kafka-clients, testUserTemplate=my-cluster-e0217138-kafka-clients, testMoreReplicasThanAvailableBrokers=my-cluster-203feca8-kafka-clients}
2022-03-28 14:18:11 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:155] Create/Update Kafka my-cluster-7eab7e8d-isolated in namespace topic-st
2022-03-28 14:18:11 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-7eab7e8d-isolated
2022-03-28 14:18:11 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-471
2022-03-28 14:18:12 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:433] Wait for Kafka: my-cluster-7eab7e8d-isolated will have desired state: Ready
2022-03-28 14:18:12 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Kafka: my-cluster-7eab7e8d-isolated will have desired state: Ready
2022-03-28 14:18:12 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-7eab7e8d-isolated will have desired state: Ready not ready, will try again in 1000 ms (839903ms till timeout)
2022-03-28 14:18:12 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-471
2022-03-28 14:18:12 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:18:12 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-102
2022-03-28 14:18:13 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-7eab7e8d-isolated will have desired state: Ready not ready, will try again in 1000 ms (838803ms till timeout)
2022-03-28 14:18:14 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-7eab7e8d-isolated will have desired state: Ready not ready, will try again in 1000 ms (837703ms till timeout)
2022-03-28 14:18:15 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-7eab7e8d-isolated will have desired state: Ready not ready, will try again in 1000 ms (836601ms till timeout)
2022-03-28 14:18:16 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-7eab7e8d-isolated will have desired state: Ready not ready, will try again in 1000 ms (835502ms till timeout)
2022-03-28 14:18:17 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-7eab7e8d-isolated will have desired state: Ready not ready, will try again in 1000 ms (834402ms till timeout)
2022-03-28 14:18:18 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-102
2022-03-28 14:18:18 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:18:18 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-103
2022-03-28 14:18:18 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-7eab7e8d-isolated will have desired state: Ready not ready, will try again in 1000 ms (833304ms till timeout)
2022-03-28 14:18:19 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-7eab7e8d-isolated will have desired state: Ready not ready, will try again in 1000 ms (832204ms till timeout)
2022-03-28 14:18:20 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-7eab7e8d-isolated will have desired state: Ready not ready, will try again in 1000 ms (831106ms till timeout)
2022-03-28 14:18:22 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-7eab7e8d-isolated will have desired state: Ready not ready, will try again in 1000 ms (830008ms till timeout)
2022-03-28 14:18:23 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-7eab7e8d-isolated will have desired state: Ready not ready, will try again in 1000 ms (828909ms till timeout)
2022-03-28 14:18:24 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-103
2022-03-28 14:18:24 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:18:24 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-104
2022-03-28 14:18:24 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-7eab7e8d-isolated will have desired state: Ready not ready, will try again in 1000 ms (827809ms till timeout)
2022-03-28 14:18:24 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-104
2022-03-28 14:18:24 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:18:24 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-130
2022-03-28 14:18:25 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-7eab7e8d-isolated will have desired state: Ready not ready, will try again in 1000 ms (826710ms till timeout)
2022-03-28 14:18:26 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-7eab7e8d-isolated will have desired state: Ready not ready, will try again in 1000 ms (825612ms till timeout)
2022-03-28 14:18:27 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-7eab7e8d-isolated will have desired state: Ready not ready, will try again in 1000 ms (824512ms till timeout)
2022-03-28 14:18:28 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-7eab7e8d-isolated will have desired state: Ready not ready, will try again in 1000 ms (823412ms till timeout)
2022-03-28 14:18:29 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-7eab7e8d-isolated will have desired state: Ready not ready, will try again in 1000 ms (822314ms till timeout)
2022-03-28 14:18:30 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-130
2022-03-28 14:18:30 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:18:30 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-131
2022-03-28 14:18:30 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-7eab7e8d-isolated will have desired state: Ready not ready, will try again in 1000 ms (821215ms till timeout)
2022-03-28 14:18:30 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-131
2022-03-28 14:18:30 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:18:30 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-132
2022-03-28 14:18:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-132
2022-03-28 14:18:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:18:31 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-133
2022-03-28 14:18:31 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-7eab7e8d-isolated will have desired state: Ready not ready, will try again in 1000 ms (820117ms till timeout)
2022-03-28 14:18:33 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-7eab7e8d-isolated will have desired state: Ready not ready, will try again in 1000 ms (819015ms till timeout)
2022-03-28 14:18:34 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-7eab7e8d-isolated will have desired state: Ready not ready, will try again in 1000 ms (817916ms till timeout)
2022-03-28 14:18:35 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-7eab7e8d-isolated will have desired state: Ready not ready, will try again in 1000 ms (816815ms till timeout)
2022-03-28 14:18:36 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-7eab7e8d-isolated will have desired state: Ready not ready, will try again in 1000 ms (815714ms till timeout)
2022-03-28 14:18:37 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-133
2022-03-28 14:18:37 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:18:37 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-134
2022-03-28 14:18:37 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-7eab7e8d-isolated will have desired state: Ready not ready, will try again in 1000 ms (814616ms till timeout)
2022-03-28 14:18:37 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-134
2022-03-28 14:18:37 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:18:37 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-135
2022-03-28 14:18:38 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-135
2022-03-28 14:18:38 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:18:38 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-136
2022-03-28 14:18:38 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-7eab7e8d-isolated will have desired state: Ready not ready, will try again in 1000 ms (813519ms till timeout)
2022-03-28 14:18:38 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-136
2022-03-28 14:18:38 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:18:38 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-137
2022-03-28 14:18:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-137
2022-03-28 14:18:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:18:39 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-138
2022-03-28 14:18:39 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-7eab7e8d-isolated will have desired state: Ready not ready, will try again in 1000 ms (812421ms till timeout)
2022-03-28 14:18:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-138
2022-03-28 14:18:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:18:39 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-139
2022-03-28 14:18:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-139
2022-03-28 14:18:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:18:40 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-140
2022-03-28 14:18:40 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-7eab7e8d-isolated will have desired state: Ready not ready, will try again in 1000 ms (811318ms till timeout)
2022-03-28 14:18:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-140
2022-03-28 14:18:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:18:41 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-141
2022-03-28 14:18:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-141
2022-03-28 14:18:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:18:41 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-142
2022-03-28 14:18:41 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-7eab7e8d-isolated will have desired state: Ready not ready, will try again in 1000 ms (810218ms till timeout)
2022-03-28 14:18:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-142
2022-03-28 14:18:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:18:42 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-143
2022-03-28 14:18:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-143
2022-03-28 14:18:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:18:42 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-144
2022-03-28 14:18:42 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-7eab7e8d-isolated will have desired state: Ready not ready, will try again in 1000 ms (809120ms till timeout)
2022-03-28 14:18:43 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-144
2022-03-28 14:18:43 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:18:43 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-145
2022-03-28 14:18:44 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-145
2022-03-28 14:18:44 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:18:44 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-146
2022-03-28 14:18:44 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-7eab7e8d-isolated will have desired state: Ready not ready, will try again in 1000 ms (808020ms till timeout)
2022-03-28 14:18:44 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-146
2022-03-28 14:18:44 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:18:44 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-147
2022-03-28 14:18:45 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-7eab7e8d-isolated will have desired state: Ready not ready, will try again in 1000 ms (806923ms till timeout)
2022-03-28 14:18:45 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-147
2022-03-28 14:18:45 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:18:45 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-148
2022-03-28 14:18:45 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-148
2022-03-28 14:18:45 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:18:45 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-149
2022-03-28 14:18:46 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-7eab7e8d-isolated will have desired state: Ready not ready, will try again in 1000 ms (805823ms till timeout)
2022-03-28 14:18:46 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-149
2022-03-28 14:18:46 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:18:46 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-150
2022-03-28 14:18:46 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-150
2022-03-28 14:18:46 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:18:46 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-151
2022-03-28 14:18:47 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-7eab7e8d-isolated will have desired state: Ready not ready, will try again in 1000 ms (804725ms till timeout)
2022-03-28 14:18:47 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-151
2022-03-28 14:18:47 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:18:47 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-152
2022-03-28 14:18:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-152
2022-03-28 14:18:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:18:48 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-153
2022-03-28 14:18:48 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-7eab7e8d-isolated will have desired state: Ready not ready, will try again in 1000 ms (803627ms till timeout)
2022-03-28 14:18:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-153
2022-03-28 14:18:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:18:48 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-154
2022-03-28 14:18:49 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-154
2022-03-28 14:18:49 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:18:49 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-155
2022-03-28 14:18:49 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-7eab7e8d-isolated will have desired state: Ready not ready, will try again in 1000 ms (802528ms till timeout)
2022-03-28 14:18:49 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-155
2022-03-28 14:18:49 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:18:49 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-156
2022-03-28 14:18:50 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-156
2022-03-28 14:18:50 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:18:50 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-157
2022-03-28 14:18:50 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-7eab7e8d-isolated will have desired state: Ready not ready, will try again in 1000 ms (801429ms till timeout)
2022-03-28 14:18:50 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-157
2022-03-28 14:18:50 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:18:50 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-158
2022-03-28 14:18:51 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-158
2022-03-28 14:18:51 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:18:51 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-159
2022-03-28 14:18:51 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-7eab7e8d-isolated will have desired state: Ready not ready, will try again in 1000 ms (800329ms till timeout)
2022-03-28 14:18:51 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-159
2022-03-28 14:18:51 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:18:51 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-160
2022-03-28 14:18:52 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-160
2022-03-28 14:18:52 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:18:52 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-161
2022-03-28 14:18:52 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-7eab7e8d-isolated will have desired state: Ready not ready, will try again in 1000 ms (799230ms till timeout)
2022-03-28 14:18:53 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-161
2022-03-28 14:18:53 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:18:53 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-162
2022-03-28 14:18:53 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-162
2022-03-28 14:18:53 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:18:53 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-163
2022-03-28 14:18:53 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-7eab7e8d-isolated will have desired state: Ready not ready, will try again in 1000 ms (798132ms till timeout)
2022-03-28 14:18:54 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-163
2022-03-28 14:18:54 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:18:54 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-164
2022-03-28 14:18:54 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-164
2022-03-28 14:18:54 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:18:54 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-165
2022-03-28 14:18:55 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-7eab7e8d-isolated will have desired state: Ready not ready, will try again in 1000 ms (797033ms till timeout)
2022-03-28 14:18:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-165
2022-03-28 14:18:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:18:55 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-166
2022-03-28 14:18:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-166
2022-03-28 14:18:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:18:55 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-167
2022-03-28 14:18:56 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-7eab7e8d-isolated will have desired state: Ready not ready, will try again in 1000 ms (795853ms till timeout)
2022-03-28 14:18:56 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-167
2022-03-28 14:18:56 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:18:56 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-168
2022-03-28 14:18:57 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-168
2022-03-28 14:18:57 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:18:57 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-169
2022-03-28 14:18:57 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-7eab7e8d-isolated will have desired state: Ready not ready, will try again in 1000 ms (794755ms till timeout)
2022-03-28 14:18:57 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-169
2022-03-28 14:18:57 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:18:57 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-170
2022-03-28 14:18:58 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-170
2022-03-28 14:18:58 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:18:58 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-171
2022-03-28 14:18:58 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-7eab7e8d-isolated will have desired state: Ready not ready, will try again in 1000 ms (793653ms till timeout)
2022-03-28 14:18:58 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-171
2022-03-28 14:18:58 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:18:58 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-172
2022-03-28 14:18:59 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-172
2022-03-28 14:18:59 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:18:59 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-173
2022-03-28 14:18:59 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-7eab7e8d-isolated will have desired state: Ready not ready, will try again in 1000 ms (792553ms till timeout)
2022-03-28 14:18:59 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-173
2022-03-28 14:18:59 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:18:59 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-174
2022-03-28 14:19:00 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-174
2022-03-28 14:19:00 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:00 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-175
2022-03-28 14:19:00 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-7eab7e8d-isolated will have desired state: Ready not ready, will try again in 1000 ms (791454ms till timeout)
2022-03-28 14:19:01 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-175
2022-03-28 14:19:01 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:01 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-176
2022-03-28 14:19:01 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-176
2022-03-28 14:19:01 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:01 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-177
2022-03-28 14:19:01 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-7eab7e8d-isolated will have desired state: Ready not ready, will try again in 1000 ms (790357ms till timeout)
2022-03-28 14:19:02 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-177
2022-03-28 14:19:02 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:02 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-178
2022-03-28 14:19:02 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-7eab7e8d-isolated will have desired state: Ready not ready, will try again in 1000 ms (789256ms till timeout)
2022-03-28 14:19:02 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-178
2022-03-28 14:19:02 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:02 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-179
2022-03-28 14:19:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-179
2022-03-28 14:19:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:03 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-180
2022-03-28 14:19:03 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-7eab7e8d-isolated will have desired state: Ready not ready, will try again in 1000 ms (788158ms till timeout)
2022-03-28 14:19:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-180
2022-03-28 14:19:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:03 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-181
2022-03-28 14:19:04 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-181
2022-03-28 14:19:04 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:04 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-182
2022-03-28 14:19:04 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-7eab7e8d-isolated will have desired state: Ready not ready, will try again in 1000 ms (787060ms till timeout)
2022-03-28 14:19:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-182
2022-03-28 14:19:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:05 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-183
2022-03-28 14:19:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-183
2022-03-28 14:19:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:05 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-184
2022-03-28 14:19:06 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-7eab7e8d-isolated will have desired state: Ready not ready, will try again in 1000 ms (785960ms till timeout)
2022-03-28 14:19:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-184
2022-03-28 14:19:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:06 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-185
2022-03-28 14:19:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-185
2022-03-28 14:19:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:06 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-186
2022-03-28 14:19:07 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-7eab7e8d-isolated will have desired state: Ready not ready, will try again in 1000 ms (784862ms till timeout)
2022-03-28 14:19:07 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-186
2022-03-28 14:19:07 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:07 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-187
2022-03-28 14:19:07 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-187
2022-03-28 14:19:07 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:07 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-188
2022-03-28 14:19:08 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-7eab7e8d-isolated will have desired state: Ready not ready, will try again in 1000 ms (783762ms till timeout)
2022-03-28 14:19:08 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-188
2022-03-28 14:19:08 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:08 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-189
2022-03-28 14:19:08 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-189
2022-03-28 14:19:08 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:08 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-190
2022-03-28 14:19:09 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-7eab7e8d-isolated will have desired state: Ready not ready, will try again in 1000 ms (782663ms till timeout)
2022-03-28 14:19:09 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-190
2022-03-28 14:19:09 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:09 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-191
2022-03-28 14:19:10 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-191
2022-03-28 14:19:10 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:10 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-192
2022-03-28 14:19:10 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-7eab7e8d-isolated will have desired state: Ready not ready, will try again in 1000 ms (781565ms till timeout)
2022-03-28 14:19:10 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-192
2022-03-28 14:19:10 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:10 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-193
2022-03-28 14:19:11 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-193
2022-03-28 14:19:11 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:11 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-194
2022-03-28 14:19:11 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-7eab7e8d-isolated will have desired state: Ready not ready, will try again in 1000 ms (780466ms till timeout)
2022-03-28 14:19:11 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-194
2022-03-28 14:19:11 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:11 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-195
2022-03-28 14:19:12 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-195
2022-03-28 14:19:12 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:12 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-196
2022-03-28 14:19:12 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-7eab7e8d-isolated will have desired state: Ready not ready, will try again in 1000 ms (779368ms till timeout)
2022-03-28 14:19:12 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-196
2022-03-28 14:19:12 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:12 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-197
2022-03-28 14:19:13 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-197
2022-03-28 14:19:13 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:13 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-198
2022-03-28 14:19:13 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-7eab7e8d-isolated will have desired state: Ready not ready, will try again in 1000 ms (778268ms till timeout)
2022-03-28 14:19:13 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-198
2022-03-28 14:19:13 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:13 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-199
2022-03-28 14:19:14 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-199
2022-03-28 14:19:14 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:14 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-200
2022-03-28 14:19:14 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-7eab7e8d-isolated will have desired state: Ready not ready, will try again in 1000 ms (777168ms till timeout)
2022-03-28 14:19:15 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-200
2022-03-28 14:19:15 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:15 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-201
2022-03-28 14:19:15 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-201
2022-03-28 14:19:15 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:15 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-202
2022-03-28 14:19:15 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-7eab7e8d-isolated will have desired state: Ready not ready, will try again in 1000 ms (776071ms till timeout)
2022-03-28 14:19:16 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-202
2022-03-28 14:19:16 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:16 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-203
2022-03-28 14:19:16 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-203
2022-03-28 14:19:16 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:16 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-204
2022-03-28 14:19:17 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-7eab7e8d-isolated will have desired state: Ready not ready, will try again in 1000 ms (774971ms till timeout)
2022-03-28 14:19:17 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-204
2022-03-28 14:19:17 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:17 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-205
2022-03-28 14:19:17 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-205
2022-03-28 14:19:17 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:17 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-206
2022-03-28 14:19:18 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:444] Kafka: my-cluster-7eab7e8d-isolated is in desired state: Ready
2022-03-28 14:19:18 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:155] Create/Update Deployment my-cluster-7eab7e8d-isolated-kafka-clients in namespace topic-st
2022-03-28 14:19:18 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:my-cluster-7eab7e8d-isolated-kafka-clients
2022-03-28 14:19:18 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-206
2022-03-28 14:19:18 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:18 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-207
2022-03-28 14:19:18 [ForkJoinPool-1-worker-11] INFO  [DeploymentUtils:161] Wait for Deployment: my-cluster-7eab7e8d-isolated-kafka-clients will be ready
2022-03-28 14:19:18 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Wait for Deployment: my-cluster-7eab7e8d-isolated-kafka-clients will be ready
2022-03-28 14:19:18 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Wait for Deployment: my-cluster-7eab7e8d-isolated-kafka-clients will be ready not ready, will try again in 1000 ms (479903ms till timeout)
2022-03-28 14:19:18 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-207
2022-03-28 14:19:18 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:18 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-219
2022-03-28 14:19:19 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-219
2022-03-28 14:19:19 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:19 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-223
2022-03-28 14:19:19 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Wait for Deployment: my-cluster-7eab7e8d-isolated-kafka-clients will be ready not ready, will try again in 1000 ms (478804ms till timeout)
2022-03-28 14:19:20 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-223
2022-03-28 14:19:20 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:20 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-224
2022-03-28 14:19:20 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-224
2022-03-28 14:19:20 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:20 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-225
2022-03-28 14:19:20 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Wait for Deployment: my-cluster-7eab7e8d-isolated-kafka-clients will be ready not ready, will try again in 1000 ms (477702ms till timeout)
2022-03-28 14:19:21 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-225
2022-03-28 14:19:21 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:21 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-250
2022-03-28 14:19:21 [ForkJoinPool-1-worker-11] INFO  [DeploymentUtils:168] Deployment: my-cluster-7eab7e8d-isolated-kafka-clients is ready
2022-03-28 14:19:21 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:155] Create/Update KafkaTopic my-topic-365925864-1884907136 in namespace topic-st
2022-03-28 14:19:21 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-250
2022-03-28 14:19:21 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:21 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-251
2022-03-28 14:19:22 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-365925864-1884907136
2022-03-28 14:19:22 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:433] Wait for KafkaTopic: my-topic-365925864-1884907136 will have desired state: Ready
2022-03-28 14:19:22 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for KafkaTopic: my-topic-365925864-1884907136 will have desired state: Ready
2022-03-28 14:19:22 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] KafkaTopic: my-topic-365925864-1884907136 will have desired state: Ready not ready, will try again in 1000 ms (179900ms till timeout)
2022-03-28 14:19:22 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-251
2022-03-28 14:19:22 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:22 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-252
2022-03-28 14:19:23 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-252
2022-03-28 14:19:23 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:23 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-253
2022-03-28 14:19:23 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:444] KafkaTopic: my-topic-365925864-1884907136 is in desired state: Ready
2022-03-28 14:19:23 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-253
2022-03-28 14:19:23 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:23 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-254
2022-03-28 14:19:23 [ForkJoinPool-1-worker-11] INFO  [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-03-28 14:19:23 [ForkJoinPool-1-worker-11] DEBUG [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@61b57b0c, which are set.
2022-03-28 14:19:23 [ForkJoinPool-1-worker-11] INFO  [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@6cbb60e1, messages=[], arguments=[--topic, my-topic-365925864-1884907136, --bootstrap-server, my-cluster-7eab7e8d-isolated-kafka-bootstrap.topic-st.svc:9092, --max-messages, 100], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-7eab7e8d-isolated-kafka-clients-bf694bf65-9rs5b', podNamespace='topic-st', bootstrapServer='my-cluster-7eab7e8d-isolated-kafka-bootstrap.topic-st.svc:9092', topicName='my-topic-365925864-1884907136', maxMessages=100, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@61b57b0c}
2022-03-28 14:19:23 [ForkJoinPool-1-worker-11] INFO  [InternalKafkaClient:94] Producing 100 messages to my-cluster-7eab7e8d-isolated-kafka-bootstrap.topic-st.svc:9092:my-topic-365925864-1884907136 from pod my-cluster-7eab7e8d-isolated-kafka-clients-bf694bf65-9rs5b
2022-03-28 14:19:23 [ForkJoinPool-1-worker-11] INFO  [VerifiableClient:192] Client command: oc exec my-cluster-7eab7e8d-isolated-kafka-clients-bf694bf65-9rs5b -n topic-st -- /opt/kafka/producer.sh --topic my-topic-365925864-1884907136 --bootstrap-server my-cluster-7eab7e8d-isolated-kafka-bootstrap.topic-st.svc:9092 --max-messages 100
2022-03-28 14:19:23 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc exec my-cluster-7eab7e8d-isolated-kafka-clients-bf694bf65-9rs5b -n topic-st -- /opt/kafka/producer.sh --topic my-topic-365925864-1884907136 --bootstrap-server my-cluster-7eab7e8d-isolated-kafka-bootstrap.topic-st.svc:9092 --max-messages 100
2022-03-28 14:19:24 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-254
2022-03-28 14:19:24 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:24 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-255
2022-03-28 14:19:24 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-255
2022-03-28 14:19:24 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:24 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-256
2022-03-28 14:19:25 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-256
2022-03-28 14:19:25 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:25 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-257
2022-03-28 14:19:25 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-257
2022-03-28 14:19:25 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:25 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-258
2022-03-28 14:19:26 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-258
2022-03-28 14:19:26 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:26 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-259
2022-03-28 14:19:26 [ForkJoinPool-1-worker-11] INFO  [InternalKafkaClient:97] Producer finished correctly: true
2022-03-28 14:19:26 [ForkJoinPool-1-worker-11] INFO  [InternalKafkaClient:101] Producer produced 100 messages
2022-03-28 14:19:26 [ForkJoinPool-1-worker-11] INFO  [TopicST:395] Deleting KafkaTopic: my-topic-365925864-1884907136
2022-03-28 14:19:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-259
2022-03-28 14:19:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:27 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-260
2022-03-28 14:19:27 [ForkJoinPool-1-worker-11] INFO  [TopicST:397] KafkaTopic my-topic-365925864-1884907136 deleted
2022-03-28 14:19:27 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Topic my-topic-365925864-1884907136 has rolled
2022-03-28 14:19:27 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (299902ms till timeout)
2022-03-28 14:19:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-260
2022-03-28 14:19:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:27 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-261
2022-03-28 14:19:28 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-261
2022-03-28 14:19:28 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:28 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-262
2022-03-28 14:19:28 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (298801ms till timeout)
2022-03-28 14:19:28 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-262
2022-03-28 14:19:28 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:28 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-263
2022-03-28 14:19:29 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-263
2022-03-28 14:19:29 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:29 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-264
2022-03-28 14:19:29 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (297702ms till timeout)
2022-03-28 14:19:29 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-264
2022-03-28 14:19:29 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:29 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-265
2022-03-28 14:19:30 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-265
2022-03-28 14:19:30 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:30 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-266
2022-03-28 14:19:30 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (296605ms till timeout)
2022-03-28 14:19:30 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-266
2022-03-28 14:19:30 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:30 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-267
2022-03-28 14:19:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-267
2022-03-28 14:19:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:31 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-268
2022-03-28 14:19:31 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (295508ms till timeout)
2022-03-28 14:19:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-268
2022-03-28 14:19:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:31 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-269
2022-03-28 14:19:32 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-269
2022-03-28 14:19:32 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:32 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-270
2022-03-28 14:19:32 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (294411ms till timeout)
2022-03-28 14:19:32 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-270
2022-03-28 14:19:32 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:32 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-271
2022-03-28 14:19:33 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-271
2022-03-28 14:19:33 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:33 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-272
2022-03-28 14:19:33 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (293312ms till timeout)
2022-03-28 14:19:34 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-272
2022-03-28 14:19:34 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:34 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-273
2022-03-28 14:19:34 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-273
2022-03-28 14:19:34 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:34 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-274
2022-03-28 14:19:34 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (292212ms till timeout)
2022-03-28 14:19:35 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-274
2022-03-28 14:19:35 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:35 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-275
2022-03-28 14:19:35 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-275
2022-03-28 14:19:35 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:35 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-276
2022-03-28 14:19:35 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (291114ms till timeout)
2022-03-28 14:19:36 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-276
2022-03-28 14:19:36 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:36 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-277
2022-03-28 14:19:36 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-277
2022-03-28 14:19:36 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:36 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-278
2022-03-28 14:19:37 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (290017ms till timeout)
2022-03-28 14:19:37 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-278
2022-03-28 14:19:37 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:37 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-279
2022-03-28 14:19:37 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-279
2022-03-28 14:19:37 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:37 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-280
2022-03-28 14:19:38 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (288920ms till timeout)
2022-03-28 14:19:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-280
2022-03-28 14:19:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:39 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-281
2022-03-28 14:19:39 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (287821ms till timeout)
2022-03-28 14:19:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-281
2022-03-28 14:19:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:39 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-282
2022-03-28 14:19:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-282
2022-03-28 14:19:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:40 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-283
2022-03-28 14:19:40 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (286723ms till timeout)
2022-03-28 14:19:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-283
2022-03-28 14:19:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:40 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-284
2022-03-28 14:19:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-284
2022-03-28 14:19:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:41 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-285
2022-03-28 14:19:41 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (285623ms till timeout)
2022-03-28 14:19:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-285
2022-03-28 14:19:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:41 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-286
2022-03-28 14:19:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-286
2022-03-28 14:19:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:42 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-287
2022-03-28 14:19:42 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (284525ms till timeout)
2022-03-28 14:19:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-287
2022-03-28 14:19:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:42 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-288
2022-03-28 14:19:43 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-288
2022-03-28 14:19:43 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:43 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-289
2022-03-28 14:19:43 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (283425ms till timeout)
2022-03-28 14:19:44 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-289
2022-03-28 14:19:44 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:44 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-290
2022-03-28 14:19:44 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-290
2022-03-28 14:19:44 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:44 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-291
2022-03-28 14:19:44 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (282325ms till timeout)
2022-03-28 14:19:45 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-291
2022-03-28 14:19:45 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:45 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-292
2022-03-28 14:19:45 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-292
2022-03-28 14:19:45 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:45 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-293
2022-03-28 14:19:45 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (281227ms till timeout)
2022-03-28 14:19:46 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-293
2022-03-28 14:19:46 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:46 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-294
2022-03-28 14:19:46 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-294
2022-03-28 14:19:46 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:46 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-295
2022-03-28 14:19:46 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (280127ms till timeout)
2022-03-28 14:19:47 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-295
2022-03-28 14:19:47 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:47 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-296
2022-03-28 14:19:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-296
2022-03-28 14:19:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:48 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-297
2022-03-28 14:19:48 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (279028ms till timeout)
2022-03-28 14:19:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-297
2022-03-28 14:19:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:48 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-298
2022-03-28 14:19:49 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (277926ms till timeout)
2022-03-28 14:19:49 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-298
2022-03-28 14:19:49 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:49 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-299
2022-03-28 14:19:49 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-299
2022-03-28 14:19:49 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:49 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-300
2022-03-28 14:19:50 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (276827ms till timeout)
2022-03-28 14:19:50 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-300
2022-03-28 14:19:50 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:50 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-301
2022-03-28 14:19:50 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-301
2022-03-28 14:19:50 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:50 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-302
2022-03-28 14:19:51 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (275729ms till timeout)
2022-03-28 14:19:51 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-302
2022-03-28 14:19:51 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:51 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-303
2022-03-28 14:19:52 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-303
2022-03-28 14:19:52 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:52 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-304
2022-03-28 14:19:52 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (274631ms till timeout)
2022-03-28 14:19:52 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-304
2022-03-28 14:19:52 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:52 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-305
2022-03-28 14:19:53 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-305
2022-03-28 14:19:53 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:53 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-306
2022-03-28 14:19:53 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (273532ms till timeout)
2022-03-28 14:19:53 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-306
2022-03-28 14:19:53 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:53 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-307
2022-03-28 14:19:54 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-307
2022-03-28 14:19:54 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:54 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-308
2022-03-28 14:19:54 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (272433ms till timeout)
2022-03-28 14:19:54 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-308
2022-03-28 14:19:54 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:54 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-309
2022-03-28 14:19:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-309
2022-03-28 14:19:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:55 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-310
2022-03-28 14:19:55 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (271335ms till timeout)
2022-03-28 14:19:56 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-310
2022-03-28 14:19:56 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:56 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-311
2022-03-28 14:19:56 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-311
2022-03-28 14:19:56 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:56 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-312
2022-03-28 14:19:56 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (270237ms till timeout)
2022-03-28 14:19:57 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-312
2022-03-28 14:19:57 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:57 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-313
2022-03-28 14:19:57 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-313
2022-03-28 14:19:57 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:57 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-314
2022-03-28 14:19:57 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (269137ms till timeout)
2022-03-28 14:19:58 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-314
2022-03-28 14:19:58 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:58 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-315
2022-03-28 14:19:58 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-315
2022-03-28 14:19:58 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:58 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-316
2022-03-28 14:19:58 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (268038ms till timeout)
2022-03-28 14:19:59 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-316
2022-03-28 14:19:59 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:19:59 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-317
2022-03-28 14:20:00 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-317
2022-03-28 14:20:00 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:00 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-318
2022-03-28 14:20:00 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (266940ms till timeout)
2022-03-28 14:20:00 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-318
2022-03-28 14:20:00 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:00 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-319
2022-03-28 14:20:01 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-319
2022-03-28 14:20:01 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:01 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-320
2022-03-28 14:20:01 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (265843ms till timeout)
2022-03-28 14:20:01 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-320
2022-03-28 14:20:01 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:01 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-321
2022-03-28 14:20:02 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-321
2022-03-28 14:20:02 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:02 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-322
2022-03-28 14:20:02 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (264745ms till timeout)
2022-03-28 14:20:02 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-322
2022-03-28 14:20:02 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:02 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-323
2022-03-28 14:20:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-323
2022-03-28 14:20:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:03 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-324
2022-03-28 14:20:03 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (263635ms till timeout)
2022-03-28 14:20:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-324
2022-03-28 14:20:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:03 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-325
2022-03-28 14:20:04 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-325
2022-03-28 14:20:04 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:04 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-326
2022-03-28 14:20:04 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (262537ms till timeout)
2022-03-28 14:20:04 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-326
2022-03-28 14:20:04 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:04 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-327
2022-03-28 14:20:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-327
2022-03-28 14:20:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:05 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-328
2022-03-28 14:20:05 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (261439ms till timeout)
2022-03-28 14:20:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-328
2022-03-28 14:20:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:06 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-344
2022-03-28 14:20:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-344
2022-03-28 14:20:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:06 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-345
2022-03-28 14:20:06 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (260340ms till timeout)
2022-03-28 14:20:07 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-345
2022-03-28 14:20:07 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:07 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-346
2022-03-28 14:20:07 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-346
2022-03-28 14:20:07 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:07 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-370
2022-03-28 14:20:07 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (259243ms till timeout)
2022-03-28 14:20:08 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-370
2022-03-28 14:20:08 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:08 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-371
2022-03-28 14:20:08 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-371
2022-03-28 14:20:08 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:08 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-372
2022-03-28 14:20:08 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (258145ms till timeout)
2022-03-28 14:20:09 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-372
2022-03-28 14:20:09 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:09 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-373
2022-03-28 14:20:09 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-373
2022-03-28 14:20:09 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:09 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-374
2022-03-28 14:20:09 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (257047ms till timeout)
2022-03-28 14:20:10 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-374
2022-03-28 14:20:10 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:10 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-375
2022-03-28 14:20:10 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-375
2022-03-28 14:20:10 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:10 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-376
2022-03-28 14:20:11 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (255950ms till timeout)
2022-03-28 14:20:11 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-376
2022-03-28 14:20:11 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:11 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-377
2022-03-28 14:20:12 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-377
2022-03-28 14:20:12 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:12 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-378
2022-03-28 14:20:12 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (254853ms till timeout)
2022-03-28 14:20:12 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-378
2022-03-28 14:20:12 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:12 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-379
2022-03-28 14:20:13 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-379
2022-03-28 14:20:13 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:13 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-380
2022-03-28 14:20:13 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (253753ms till timeout)
2022-03-28 14:20:13 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-380
2022-03-28 14:20:13 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:13 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-381
2022-03-28 14:20:14 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-381
2022-03-28 14:20:14 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:14 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-382
2022-03-28 14:20:14 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (252655ms till timeout)
2022-03-28 14:20:14 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-382
2022-03-28 14:20:14 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:14 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-383
2022-03-28 14:20:15 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-383
2022-03-28 14:20:15 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:15 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-384
2022-03-28 14:20:15 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (251557ms till timeout)
2022-03-28 14:20:15 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-384
2022-03-28 14:20:15 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:15 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-385
2022-03-28 14:20:16 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-385
2022-03-28 14:20:16 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:16 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-386
2022-03-28 14:20:16 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (250460ms till timeout)
2022-03-28 14:20:17 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-386
2022-03-28 14:20:17 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:17 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-387
2022-03-28 14:20:17 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-387
2022-03-28 14:20:17 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:17 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-388
2022-03-28 14:20:17 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (249361ms till timeout)
2022-03-28 14:20:18 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-388
2022-03-28 14:20:18 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:18 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-389
2022-03-28 14:20:18 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-389
2022-03-28 14:20:18 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:18 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-390
2022-03-28 14:20:18 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (248260ms till timeout)
2022-03-28 14:20:19 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-390
2022-03-28 14:20:19 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:19 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-391
2022-03-28 14:20:19 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (247163ms till timeout)
2022-03-28 14:20:19 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-391
2022-03-28 14:20:19 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:19 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-392
2022-03-28 14:20:20 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-392
2022-03-28 14:20:20 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:20 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-393
2022-03-28 14:20:20 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-393
2022-03-28 14:20:20 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:20 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-394
2022-03-28 14:20:20 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (246066ms till timeout)
2022-03-28 14:20:21 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-394
2022-03-28 14:20:21 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:21 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-395
2022-03-28 14:20:22 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (244968ms till timeout)
2022-03-28 14:20:22 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-395
2022-03-28 14:20:22 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:22 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-396
2022-03-28 14:20:22 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-396
2022-03-28 14:20:22 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:22 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-397
2022-03-28 14:20:23 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (243872ms till timeout)
2022-03-28 14:20:23 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-397
2022-03-28 14:20:23 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:23 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-398
2022-03-28 14:20:23 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-398
2022-03-28 14:20:23 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:23 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-399
2022-03-28 14:20:24 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (242775ms till timeout)
2022-03-28 14:20:24 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-399
2022-03-28 14:20:24 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:24 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-406
2022-03-28 14:20:24 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-406
2022-03-28 14:20:24 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:24 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-407
2022-03-28 14:20:25 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (241674ms till timeout)
2022-03-28 14:20:25 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-407
2022-03-28 14:20:25 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:25 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-408
2022-03-28 14:20:26 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-408
2022-03-28 14:20:26 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:26 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-409
2022-03-28 14:20:26 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (240576ms till timeout)
2022-03-28 14:20:26 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-409
2022-03-28 14:20:26 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:26 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-410
2022-03-28 14:20:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-410
2022-03-28 14:20:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:27 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-411
2022-03-28 14:20:27 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (239476ms till timeout)
2022-03-28 14:20:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-411
2022-03-28 14:20:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:27 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-412
2022-03-28 14:20:28 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-412
2022-03-28 14:20:28 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:28 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-413
2022-03-28 14:20:28 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (238377ms till timeout)
2022-03-28 14:20:28 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-413
2022-03-28 14:20:28 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:28 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-414
2022-03-28 14:20:29 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-414
2022-03-28 14:20:29 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:29 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-415
2022-03-28 14:20:29 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (237280ms till timeout)
2022-03-28 14:20:30 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-415
2022-03-28 14:20:30 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:30 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-416
2022-03-28 14:20:30 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-416
2022-03-28 14:20:30 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:30 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-417
2022-03-28 14:20:30 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (236170ms till timeout)
2022-03-28 14:20:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-417
2022-03-28 14:20:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:31 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-418
2022-03-28 14:20:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-418
2022-03-28 14:20:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:31 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-419
2022-03-28 14:20:31 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (235071ms till timeout)
2022-03-28 14:20:32 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-419
2022-03-28 14:20:32 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:32 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-420
2022-03-28 14:20:32 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-420
2022-03-28 14:20:32 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:32 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-421
2022-03-28 14:20:33 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (233972ms till timeout)
2022-03-28 14:20:33 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-421
2022-03-28 14:20:33 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:33 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-422
2022-03-28 14:20:33 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-422
2022-03-28 14:20:33 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:33 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-423
2022-03-28 14:20:34 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (232874ms till timeout)
2022-03-28 14:20:34 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-423
2022-03-28 14:20:34 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:34 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-424
2022-03-28 14:20:35 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-424
2022-03-28 14:20:35 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:35 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-425
2022-03-28 14:20:35 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (231776ms till timeout)
2022-03-28 14:20:35 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-425
2022-03-28 14:20:35 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:35 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-426
2022-03-28 14:20:36 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-426
2022-03-28 14:20:36 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:36 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-427
2022-03-28 14:20:36 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (230677ms till timeout)
2022-03-28 14:20:36 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-427
2022-03-28 14:20:36 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:36 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-428
2022-03-28 14:20:37 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-428
2022-03-28 14:20:37 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:37 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-429
2022-03-28 14:20:37 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (229575ms till timeout)
2022-03-28 14:20:37 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-429
2022-03-28 14:20:37 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:37 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-430
2022-03-28 14:20:38 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-430
2022-03-28 14:20:38 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:38 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-431
2022-03-28 14:20:38 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (228477ms till timeout)
2022-03-28 14:20:38 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-431
2022-03-28 14:20:38 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:38 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-432
2022-03-28 14:20:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-432
2022-03-28 14:20:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:39 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-433
2022-03-28 14:20:39 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (227378ms till timeout)
2022-03-28 14:20:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-433
2022-03-28 14:20:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:39 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-434
2022-03-28 14:20:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-434
2022-03-28 14:20:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:40 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-435
2022-03-28 14:20:40 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (226282ms till timeout)
2022-03-28 14:20:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-435
2022-03-28 14:20:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:41 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-436
2022-03-28 14:20:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-436
2022-03-28 14:20:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:41 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-437
2022-03-28 14:20:41 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (225182ms till timeout)
2022-03-28 14:20:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-437
2022-03-28 14:20:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:42 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-438
2022-03-28 14:20:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-438
2022-03-28 14:20:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:42 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-439
2022-03-28 14:20:42 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (224086ms till timeout)
2022-03-28 14:20:43 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-439
2022-03-28 14:20:43 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:43 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-440
2022-03-28 14:20:43 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-440
2022-03-28 14:20:43 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:43 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-441
2022-03-28 14:20:44 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (222988ms till timeout)
2022-03-28 14:20:44 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-441
2022-03-28 14:20:44 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:44 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-442
2022-03-28 14:20:45 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-442
2022-03-28 14:20:45 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:45 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-443
2022-03-28 14:20:45 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (221889ms till timeout)
2022-03-28 14:20:45 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-443
2022-03-28 14:20:45 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:45 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-444
2022-03-28 14:20:46 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-444
2022-03-28 14:20:46 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:46 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-445
2022-03-28 14:20:46 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (220790ms till timeout)
2022-03-28 14:20:46 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-445
2022-03-28 14:20:46 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:46 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-446
2022-03-28 14:20:47 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-446
2022-03-28 14:20:47 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:47 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-447
2022-03-28 14:20:47 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (219690ms till timeout)
2022-03-28 14:20:47 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-447
2022-03-28 14:20:47 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:47 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-448
2022-03-28 14:20:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-448
2022-03-28 14:20:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:48 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-449
2022-03-28 14:20:48 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (218592ms till timeout)
2022-03-28 14:20:49 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-449
2022-03-28 14:20:49 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:49 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-464
2022-03-28 14:20:49 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (217493ms till timeout)
2022-03-28 14:20:49 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-464
2022-03-28 14:20:49 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:49 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-465
2022-03-28 14:20:50 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-465
2022-03-28 14:20:50 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:50 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-466
2022-03-28 14:20:50 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (216396ms till timeout)
2022-03-28 14:20:50 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-466
2022-03-28 14:20:50 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:50 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-467
2022-03-28 14:20:51 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-467
2022-03-28 14:20:51 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:51 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-490
2022-03-28 14:20:51 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (215299ms till timeout)
2022-03-28 14:20:51 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-490
2022-03-28 14:20:51 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:51 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-491
2022-03-28 14:20:52 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-491
2022-03-28 14:20:52 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:52 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-492
2022-03-28 14:20:52 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (214199ms till timeout)
2022-03-28 14:20:53 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-492
2022-03-28 14:20:53 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:53 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-493
2022-03-28 14:20:53 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-493
2022-03-28 14:20:53 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:53 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-494
2022-03-28 14:20:53 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (213100ms till timeout)
2022-03-28 14:20:54 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-494
2022-03-28 14:20:54 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:54 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-495
2022-03-28 14:20:54 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-495
2022-03-28 14:20:54 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:54 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-496
2022-03-28 14:20:55 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (212002ms till timeout)
2022-03-28 14:20:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-496
2022-03-28 14:20:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:55 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-497
2022-03-28 14:20:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-497
2022-03-28 14:20:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:55 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-498
2022-03-28 14:20:56 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (210851ms till timeout)
2022-03-28 14:20:56 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-498
2022-03-28 14:20:56 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:56 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-499
2022-03-28 14:20:56 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-499
2022-03-28 14:20:56 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:56 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-0
2022-03-28 14:20:57 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (209753ms till timeout)
2022-03-28 14:20:57 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-0
2022-03-28 14:20:57 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:57 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-1
2022-03-28 14:20:58 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-1
2022-03-28 14:20:58 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:58 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-10
2022-03-28 14:20:58 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (208655ms till timeout)
2022-03-28 14:20:58 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-10
2022-03-28 14:20:58 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:58 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-11
2022-03-28 14:20:59 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-11
2022-03-28 14:20:59 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:59 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-12
2022-03-28 14:20:59 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (207556ms till timeout)
2022-03-28 14:20:59 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-12
2022-03-28 14:20:59 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:20:59 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-13
2022-03-28 14:21:00 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-13
2022-03-28 14:21:00 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:21:00 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-14
2022-03-28 14:21:00 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (206458ms till timeout)
2022-03-28 14:21:00 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-14
2022-03-28 14:21:00 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:21:00 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-15
2022-03-28 14:21:01 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-15
2022-03-28 14:21:01 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:21:01 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-16
2022-03-28 14:21:01 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (205359ms till timeout)
2022-03-28 14:21:01 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-16
2022-03-28 14:21:01 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:21:01 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-17
2022-03-28 14:21:02 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-17
2022-03-28 14:21:02 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:21:02 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-18
2022-03-28 14:21:02 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (204259ms till timeout)
2022-03-28 14:21:02 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-18
2022-03-28 14:21:02 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:21:02 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-19
2022-03-28 14:21:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-19
2022-03-28 14:21:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:21:03 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-2
2022-03-28 14:21:03 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (203160ms till timeout)
2022-03-28 14:21:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-2
2022-03-28 14:21:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:21:03 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-3
2022-03-28 14:21:04 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-3
2022-03-28 14:21:04 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:21:04 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-4
2022-03-28 14:21:04 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (202063ms till timeout)
2022-03-28 14:21:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-4
2022-03-28 14:21:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:21:05 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-5
2022-03-28 14:21:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-5
2022-03-28 14:21:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:21:05 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-6
2022-03-28 14:21:06 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (200965ms till timeout)
2022-03-28 14:21:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-6
2022-03-28 14:21:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:21:06 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-7
2022-03-28 14:21:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-7
2022-03-28 14:21:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:21:06 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-8
2022-03-28 14:21:07 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (199866ms till timeout)
2022-03-28 14:21:07 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-8
2022-03-28 14:21:07 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:21:07 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-9
2022-03-28 14:21:07 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-9
2022-03-28 14:21:07 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:21:07 [ForkJoinPool-1-worker-9] DEBUG [AbstractST:689] ============================================================================
2022-03-28 14:21:07 [ForkJoinPool-1-worker-9] DEBUG [AbstractST:690] [operators.topic.ThrottlingQuotaST - After All] - Clean up after test suite
2022-03-28 14:21:07 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:344] ############################################################################
2022-03-28 14:21:07 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:348] Delete all resources for ThrottlingQuotaST
2022-03-28 14:21:07 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:241] Delete of Kafka quota-cluster in namespace throttling-quota-st
2022-03-28 14:21:08 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:quota-cluster
2022-03-28 14:21:08 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:quota-cluster not ready, will try again in 10000 ms (839896ms till timeout)
2022-03-28 14:21:08 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (198755ms till timeout)
2022-03-28 14:21:09 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-365925864-1884907136 has rolled not ready, will try again in 1000 ms (197654ms till timeout)
2022-03-28 14:21:10 [ForkJoinPool-1-worker-11] INFO  [TopicST:401] Wait KafkaTopic my-topic-365925864-1884907136 recreation
2022-03-28 14:21:10 [ForkJoinPool-1-worker-11] INFO  [KafkaTopicUtils:78] Waiting for KafkaTopic my-topic-365925864-1884907136 creation 
2022-03-28 14:21:10 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for KafkaTopic creation my-topic-365925864-1884907136
2022-03-28 14:21:10 [ForkJoinPool-1-worker-11] INFO  [TopicST:403] KafkaTopic my-topic-365925864-1884907136 recreated
2022-03-28 14:21:10 [ForkJoinPool-1-worker-11] DEBUG [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@6b9064d4, which are set.
2022-03-28 14:21:10 [ForkJoinPool-1-worker-11] INFO  [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@5c1bad2f, messages=[], arguments=[--group-id, my-consumer-group-278200087, --topic, my-topic-365925864-1884907136, --group-instance-id, instance2073314001, --bootstrap-server, my-cluster-7eab7e8d-isolated-kafka-bootstrap.topic-st.svc:9092, --max-messages, 100], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-7eab7e8d-isolated-kafka-clients-bf694bf65-9rs5b', podNamespace='topic-st', bootstrapServer='my-cluster-7eab7e8d-isolated-kafka-bootstrap.topic-st.svc:9092', topicName='my-topic-365925864-1884907136', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-278200087', consumerInstanceId='instance2073314001', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@6b9064d4}
2022-03-28 14:21:10 [ForkJoinPool-1-worker-11] INFO  [InternalKafkaClient:157] Consuming 100 messages from my-cluster-7eab7e8d-isolated-kafka-bootstrap.topic-st.svc:9092#my-topic-365925864-1884907136 from pod my-cluster-7eab7e8d-isolated-kafka-clients-bf694bf65-9rs5b
2022-03-28 14:21:10 [ForkJoinPool-1-worker-11] INFO  [VerifiableClient:192] Client command: oc exec my-cluster-7eab7e8d-isolated-kafka-clients-bf694bf65-9rs5b -n topic-st -- /opt/kafka/consumer.sh --group-id my-consumer-group-278200087 --topic my-topic-365925864-1884907136 --group-instance-id instance2073314001 --bootstrap-server my-cluster-7eab7e8d-isolated-kafka-bootstrap.topic-st.svc:9092 --max-messages 100
2022-03-28 14:21:10 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc exec my-cluster-7eab7e8d-isolated-kafka-clients-bf694bf65-9rs5b -n topic-st -- /opt/kafka/consumer.sh --group-id my-consumer-group-278200087 --topic my-topic-365925864-1884907136 --group-instance-id instance2073314001 --bootstrap-server my-cluster-7eab7e8d-isolated-kafka-bootstrap.topic-st.svc:9092 --max-messages 100
2022-03-28 14:21:16 [ForkJoinPool-1-worker-11] INFO  [InternalKafkaClient:160] Consumer finished correctly: true
2022-03-28 14:21:16 [ForkJoinPool-1-worker-11] INFO  [InternalKafkaClient:163] Consumer consumed 100 messages
2022-03-28 14:21:16 [ForkJoinPool-1-worker-11] DEBUG [AbstractST:674] ============================================================================
2022-03-28 14:21:16 [ForkJoinPool-1-worker-11] DEBUG [AbstractST:675] [operators.topic.TopicST - After Each] - Clean up after test
2022-03-28 14:21:16 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:344] ############################################################################
2022-03-28 14:21:16 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:348] Delete all resources for testDeleteTopicEnableFalse
2022-03-28 14:21:16 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:241] Delete of Deployment my-cluster-7eab7e8d-isolated-kafka-clients in namespace topic-st
2022-03-28 14:21:16 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:241] Delete of Kafka my-cluster-7eab7e8d-isolated in namespace topic-st
2022-03-28 14:21:16 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of KafkaTopic my-topic-365925864-1884907136 in namespace topic-st
2022-03-28 14:21:17 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:my-cluster-7eab7e8d-isolated-kafka-clients
2022-03-28 14:21:17 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-7eab7e8d-isolated
2022-03-28 14:21:17 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-365925864-1884907136
2022-03-28 14:21:17 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-7eab7e8d-isolated-kafka-clients not ready, will try again in 10000 ms (479502ms till timeout)
2022-03-28 14:21:18 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:369] ############################################################################
2022-03-28 14:21:18 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Namespace throttling-quota-st removal
2022-03-28 14:21:18 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 14:21:18 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 14:21:18 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:21:18 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Namespace throttling-quota-st removal not ready, will try again in 1000 ms (479528ms till timeout)
2022-03-28 14:21:19 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 14:21:20 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 14:21:20 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:21:20 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Namespace throttling-quota-st removal not ready, will try again in 1000 ms (478066ms till timeout)
2022-03-28 14:21:21 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 14:21:21 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 14:21:21 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:21:21 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Namespace throttling-quota-st removal not ready, will try again in 1000 ms (476605ms till timeout)
2022-03-28 14:21:22 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 14:21:23 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 14:21:23 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:21:23 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Namespace throttling-quota-st removal not ready, will try again in 1000 ms (475163ms till timeout)
2022-03-28 14:21:24 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 14:21:24 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 14:21:24 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:21:24 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Namespace throttling-quota-st removal not ready, will try again in 1000 ms (473740ms till timeout)
2022-03-28 14:21:25 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 14:21:25 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 14:21:25 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:21:25 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Namespace throttling-quota-st removal not ready, will try again in 1000 ms (472329ms till timeout)
2022-03-28 14:21:26 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 14:21:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 14:21:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:21:27 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Namespace throttling-quota-st removal not ready, will try again in 1000 ms (470863ms till timeout)
2022-03-28 14:21:27 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-7eab7e8d-isolated-kafka-clients not ready, will try again in 10000 ms (469115ms till timeout)
2022-03-28 14:21:28 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 14:21:28 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 14:21:28 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:21:28 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Namespace throttling-quota-st removal not ready, will try again in 1000 ms (469423ms till timeout)
2022-03-28 14:21:29 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 14:21:30 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 14:21:30 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:21:30 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Namespace throttling-quota-st removal not ready, will try again in 1000 ms (467983ms till timeout)
2022-03-28 14:21:31 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 14:21:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 14:21:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 1
2022-03-28 14:21:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 14:21:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Error from server (NotFound): namespaces "throttling-quota-st" not found
2022-03-28 14:21:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] ======STDERR END======
2022-03-28 14:21:31 [ForkJoinPool-1-worker-9] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@34093228=[], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[], io.strimzi.test.logs.CollectorElement@c4c0ea0=[], io.strimzi.test.logs.CollectorElement@aec142ef=[], io.strimzi.test.logs.CollectorElement@24b97ba9=[], io.strimzi.test.logs.CollectorElement@f851b6c3=[], io.strimzi.test.logs.CollectorElement@3881d5f2=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@e3067dd1=[], io.strimzi.test.logs.CollectorElement@5c7379cb=[], io.strimzi.test.logs.CollectorElement@62b9e483=[], io.strimzi.test.logs.CollectorElement@b850ef2a=[], io.strimzi.test.logs.CollectorElement@f4405b3b=[], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 14:21:31 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:254] ThrottlingQuotaST - Notifies waiting test suites:[UserST, ThrottlingQuotaST, TopicST, CruiseControlConfigurationST, HttpBridgeScramShaST, HttpBridgeTlsST] to and randomly select one to start execution
2022-03-28 14:21:31 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:85] [operators.topic.ThrottlingQuotaST] - Removing parallel suite: ThrottlingQuotaST
2022-03-28 14:21:31 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:89] [operators.topic.ThrottlingQuotaST] - Parallel suites count: 1
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1,961.783 s - in io.strimzi.systemtest.operators.topic.ThrottlingQuotaST
2022-03-28 14:21:38 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-7eab7e8d-isolated-kafka-clients not ready, will try again in 10000 ms (458721ms till timeout)
2022-03-28 14:21:48 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:369] ############################################################################
2022-03-28 14:21:48 [ForkJoinPool-1-worker-11] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.topic.TopicST.testDeleteTopicEnableFalse-FINISHED
2022-03-28 14:21:48 [ForkJoinPool-1-worker-11] INFO  [TestSeparator:30] ############################################################################
2022-03-28 14:21:48 [ForkJoinPool-1-worker-11] DEBUG [AbstractST:689] ============================================================================
2022-03-28 14:21:48 [ForkJoinPool-1-worker-11] DEBUG [AbstractST:690] [operators.topic.TopicST - After All] - Clean up after test suite
2022-03-28 14:21:48 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:344] ############################################################################
2022-03-28 14:21:48 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:348] Delete all resources for TopicST
2022-03-28 14:21:48 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:241] Delete of Kafka topic-cluster-name in namespace topic-st
2022-03-28 14:21:48 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:topic-cluster-name
2022-03-28 14:21:48 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:topic-cluster-name not ready, will try again in 10000 ms (839885ms till timeout)
2022-03-28 14:21:58 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:369] ############################################################################
2022-03-28 14:21:58 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Namespace topic-st removal
2022-03-28 14:21:58 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 14:21:59 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 14:21:59 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 14:21:59 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Namespace topic-st removal not ready, will try again in 1000 ms (479539ms till timeout)
2022-03-28 14:22:00 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 14:22:00 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 14:22:00 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 14:22:00 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Namespace topic-st removal not ready, will try again in 1000 ms (478096ms till timeout)
2022-03-28 14:22:01 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 14:22:02 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 14:22:02 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 14:22:02 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Namespace topic-st removal not ready, will try again in 1000 ms (476673ms till timeout)
2022-03-28 14:22:03 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 14:22:03 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 14:22:03 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 14:22:03 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Namespace topic-st removal not ready, will try again in 1000 ms (475226ms till timeout)
2022-03-28 14:22:04 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 14:22:05 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 14:22:05 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 14:22:05 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Namespace topic-st removal not ready, will try again in 1000 ms (473797ms till timeout)
2022-03-28 14:22:06 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 14:22:06 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 14:22:06 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 1
2022-03-28 14:22:06 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 14:22:06 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Error from server (NotFound): namespaces "topic-st" not found
2022-03-28 14:22:06 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] ======STDERR END======
2022-03-28 14:22:06 [ForkJoinPool-1-worker-11] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@34093228=[], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[], io.strimzi.test.logs.CollectorElement@c45aa4b1=[], io.strimzi.test.logs.CollectorElement@c4c0ea0=[], io.strimzi.test.logs.CollectorElement@aec142ef=[], io.strimzi.test.logs.CollectorElement@24b97ba9=[], io.strimzi.test.logs.CollectorElement@f851b6c3=[], io.strimzi.test.logs.CollectorElement@3881d5f2=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@e3067dd1=[], io.strimzi.test.logs.CollectorElement@5c7379cb=[], io.strimzi.test.logs.CollectorElement@62b9e483=[], io.strimzi.test.logs.CollectorElement@b850ef2a=[], io.strimzi.test.logs.CollectorElement@f4405b3b=[], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 14:22:06 [ForkJoinPool-1-worker-11] DEBUG [SuiteThreadController:254] TopicST - Notifies waiting test suites:[UserST, ThrottlingQuotaST, TopicST, CruiseControlConfigurationST, HttpBridgeScramShaST, HttpBridgeTlsST] to and randomly select one to start execution
2022-03-28 14:22:06 [ForkJoinPool-1-worker-11] DEBUG [SuiteThreadController:85] [operators.topic.TopicST] - Removing parallel suite: TopicST
2022-03-28 14:22:06 [ForkJoinPool-1-worker-11] DEBUG [SuiteThreadController:89] [operators.topic.TopicST] - Parallel suites count: 0
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1,996.544 s - in io.strimzi.systemtest.operators.topic.TopicST
2022-03-28 14:22:06 [ForkJoinPool-1-worker-3] INFO  [SetupClusterOperator:618] ============================================================================
2022-03-28 14:22:06 [ForkJoinPool-1-worker-3] INFO  [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-03-28 14:22:06 [ForkJoinPool-1-worker-3] INFO  [SetupClusterOperator:620] ============================================================================
2022-03-28 14:22:06 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:344] ############################################################################
2022-03-28 14:22:06 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-03-28 14:22:06 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-28 14:22:06 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-28 14:22:06 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-28 14:22:06 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-28 14:22:06 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-28 14:22:06 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-28 14:22:06 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-28 14:22:06 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-namespaced
2022-03-28 14:22:06 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-28 14:22:06 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-28 14:22:06 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-28 14:22:06 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-28 14:22:06 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-28 14:22:06 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-28 14:22:06 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-28 14:22:07 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-28 14:22:07 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-28 14:22:07 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-28 14:22:07 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-28 14:22:07 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-28 14:22:07 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-28 14:22:07 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-28 14:22:07 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-28 14:22:07 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-28 14:22:07 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-28 14:22:07 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io not ready, will try again in 10000 ms (179706ms till timeout)
2022-03-28 14:22:07 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-28 14:22:07 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-28 14:22:07 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-28 14:22:07 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-28 14:22:07 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-28 14:22:07 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-28 14:22:08 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io not ready, will try again in 10000 ms (179624ms till timeout)
2022-03-28 14:22:08 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-03-28 14:22:08 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-28 14:22:08 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-28 14:22:08 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-28 14:22:08 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-entity-operator
2022-03-28 14:22:08 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-28 14:22:08 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-28 14:22:08 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-28 14:22:08 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-28 14:22:08 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-28 14:22:08 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-28 14:22:08 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-28 14:22:08 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-28 14:22:08 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-28 14:22:08 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-28 14:22:09 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-28 14:22:09 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-28 14:22:09 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-28 14:22:09 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-28 14:22:18 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:369] ############################################################################
2022-03-28 14:22:18 [ForkJoinPool-1-worker-3] DEBUG [KubeClusterResource:216] Deleting Namespace: infra-namespace
2022-03-28 14:22:18 [ForkJoinPool-1-worker-3] DEBUG [DefaultSharedIndexInformer:142] informer: ready to run resync and reflector for class io.fabric8.kubernetes.api.model.Namespace with resync 0
2022-03-28 14:22:18 [ForkJoinPool-1-worker-3] DEBUG [DefaultSharedIndexInformer:212] informer#Controller: resync skipped due to 0 full resync period class io.fabric8.kubernetes.api.model.Namespace
2022-03-28 14:22:18 [ForkJoinPool-1-worker-3] DEBUG [Reflector:95] Listing items (1) for resource class io.fabric8.kubernetes.api.model.Namespace v109856
2022-03-28 14:22:18 [ForkJoinPool-1-worker-3] DEBUG [Reflector:103] Starting watcher for resource class io.fabric8.kubernetes.api.model.Namespace v109856
2022-03-28 14:22:18 [ForkJoinPool-1-worker-3] DEBUG [AbstractWatchManager:222] Watching https://api.morsak-410.strimzi.app-services-dev.net:6443/api/v1/namespaces?fieldSelector=metadata.name%3Dinfra-namespace&resourceVersion=109856&allowWatchBookmarks=true&watch=true...
2022-03-28 14:22:19 [OkHttp https://api.morsak-410.strimzi.app-services-dev.net:6443/...] DEBUG [WatcherWebSocketListener:43] WebSocket successfully opened
2022-03-28 14:22:24 [OkHttp https://api.morsak-410.strimzi.app-services-dev.net:6443/...] DEBUG [Reflector:139] Event received MODIFIED Namespace resourceVersion 109914
2022-03-28 14:22:29 [OkHttp https://api.morsak-410.strimzi.app-services-dev.net:6443/...] DEBUG [Reflector:139] Event received MODIFIED Namespace resourceVersion 109945
2022-03-28 14:22:29 [OkHttp https://api.morsak-410.strimzi.app-services-dev.net:6443/...] DEBUG [Reflector:64] Stopping watcher for resource class io.fabric8.kubernetes.api.model.Namespace v109914 in namespace default
2022-03-28 14:22:29 [OkHttp https://api.morsak-410.strimzi.app-services-dev.net:6443/...] DEBUG [AbstractWatchManager:230] Force closing the watch io.fabric8.kubernetes.client.dsl.internal.WatchConnectionManager@74265dfe
2022-03-28 14:22:29 [OkHttp https://api.morsak-410.strimzi.app-services-dev.net:6443/...] DEBUG [Reflector:181] Watch gracefully closed
2022-03-28 14:22:29 [OkHttp https://api.morsak-410.strimzi.app-services-dev.net:6443/...] DEBUG [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@600d7243
2022-03-28 14:22:29 [OkHttp https://api.morsak-410.strimzi.app-services-dev.net:6443/...] DEBUG [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@600d7243
2022-03-28 14:22:29 [main] INFO  [TestExecutionListener:40] =======================================================================
2022-03-28 14:22:29 [main] INFO  [TestExecutionListener:41] =======================================================================
2022-03-28 14:22:29 [main] INFO  [TestExecutionListener:42]                         Test run finished
2022-03-28 14:22:29 [main] INFO  [TestExecutionListener:43] =======================================================================
2022-03-28 14:22:29 [OkHttp https://api.morsak-410.strimzi.app-services-dev.net:6443/...] DEBUG [WatchConnectionManager:63] Websocket already closed io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@600d7243
2022-03-28 14:22:29 [main] INFO  [TestExecutionListener:44] =======================================================================
[INFO] 
[INFO] Results:
[INFO] 
[INFO] Tests run: 30, Failures: 0, Errors: 0, Skipped: 0
[INFO] 
2022-03-28 14:22:29 [OkHttp https://api.morsak-410.strimzi.app-services-dev.net:6443/...] DEBUG [Reflector:139] Event received DELETED Namespace resourceVersion 109946
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:verify (default) @ systemtest ---
[INFO] 
[INFO] --- maven-dependency-plugin:3.1.1:analyze-only (analyze) @ systemtest ---
2022-03-28 14:22:30 [OkHttp https://api.morsak-410.strimzi.app-services-dev.net:6443/...] DEBUG [WatcherWebSocketListener:79] WebSocket close received. code: 1000, reason: 
2022-03-28 14:22:30 [OkHttp https://api.morsak-410.strimzi.app-services-dev.net:6443/...] DEBUG [AbstractWatchManager:140] Ignoring error for already closed/closing connection
[INFO] No dependency problems found
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary for Strimzi - Apache Kafka on Kubernetes and OpenShift 0.29.0-SNAPSHOT:
[INFO] 
[INFO] Strimzi - Apache Kafka on Kubernetes and OpenShift . SUCCESS [  3.779 s]
[INFO] test ............................................... SUCCESS [  5.492 s]
[INFO] crd-annotations .................................... SUCCESS [  3.809 s]
[INFO] crd-generator ...................................... SUCCESS [  6.618 s]
[INFO] api ................................................ SUCCESS [ 27.824 s]
[INFO] mockkube ........................................... SUCCESS [  4.496 s]
[INFO] config-model ....................................... SUCCESS [  3.398 s]
[INFO] certificate-manager ................................ SUCCESS [  3.943 s]
[INFO] operator-common .................................... SUCCESS [  7.946 s]
[INFO] systemtest ......................................... SUCCESS [34:06 min]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  35:13 min
[INFO] Finished at: 2022-03-28T10:22:30-04:00
[INFO] ------------------------------------------------------------------------
